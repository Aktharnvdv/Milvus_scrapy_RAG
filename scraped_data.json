[{"url": "https://docs.nvidia.com/cuda/", "parent_url": "https://docs.nvidia.com/cuda/", "content_type": "text/html", "text": "CUDA Toolkit Documentation 12.5 Release Notes CUDA Features Archive EULA Installation Guides Quick Start Guide Installation Guide Windows Installation Guide Linux Programming Guides Programming Guide Best Practices Guide Maxwell Compatibility Guide Pascal Compatibility Guide Volta Compatibility Guide Turing Compatibility Guide NVIDIA Ampere GPU Architecture Compatibility Guide Hopper Compatibility Guide Ada Compatibility Guide Maxwell Tuning Guide Pascal Tuning Guide Volta Tuning Guide Turing Tuning Guide NVIDIA Ampere GPU Architecture Tuning Guide Hopper Tuning Guide Ada Tuning Guide PTX ISA Video Decoder PTX Interoperability Inline PTX Assembly CUDA API References CUDA Runtime API CUDA Driver API CUDA Math API cuBLAS cuDLA API NVBLAS nvJPEG cuFFT CUB CUDA C++ Standard Library cuFile API Reference Guide cuRAND cuSPARSE NPP nvJitLink nvFatbin NVRTC (Runtime Compilation) Thrust cuSOLVER PTX Compiler API References PTX Compiler APIs Miscellaneous CUDA Demo Suite CUDA on WSL CUDA on EFLOW Multi-Instance GPU (MIG) CUDA Compatibility CUPTI Debugger API GPUDirect RDMA GPUDirect Storage vGPU Tools NVCC CUDA-GDB Compute Sanitizer Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Systems Nsight Compute Nsight Visual Studio Edition Profiler CUDA Binary Utilities White Papers Floating Point and IEEE 754 Incomplete-LU and Cholesky Preconditioned Iterative Methods Application Notes CUDA for Tegra Compiler SDK libNVVM API libdevice User’s Guide NVVM IR landing » CUDA Toolkit Documentation 12.5 Update 1 CUDA Toolkit Archive - Send Feedback CUDA Toolkit Documentation 12.5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated\r\napplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\r\nembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\r\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime\r\nlibrary to deploy your application. Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers\r\ncan develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs. Release Notes The Release Notes for the CUDA Toolkit. CUDA Features Archive The list of CUDA features by release. EULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. Installation Guides  Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system. Installation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems. Installation Guide Linux This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems. Programming Guides  Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface. It then describes the hardware implementation, and provides guidance on how to achieve maximum performance. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API. Best Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit. Maxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture. This document provides guidance to ensure that your software applications are compatible with Maxwell. Pascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture. This document provides guidance to ensure that your software applications are compatible with Pascal. Volta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture. This document provides guidance to ensure that your software applications are compatible with Volta. Turing Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture. This document provides guidance to ensure that your software applications are compatible with Turing. NVIDIA Ampere GPU Architecture Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture. This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture. Hopper Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs. This document provides guidance to ensure that your software applications are compatible with Hopper architecture. Ada Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs. This document provides guidance to ensure that your software applications are compatible with Ada architecture. Maxwell Tuning Guide Maxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features. Pascal Tuning Guide Pascal is NVIDIA’s 5th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features. Volta Tuning Guide Volta is NVIDIA’s 6th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features. Turing Tuning Guide Turing is NVIDIA’s 7th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features. NVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features. Hopper Tuning Guide Hopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features. Ada Tuning Guide The NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features. PTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device. Video Decoder NVIDIA Video Decoder (NVCUVID) is deprecated. Instead, use the NVIDIA Video Codec SDK ( https://developer.nvidia.com/nvidia-video-codec-sdk ). PTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code. Inline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter. CUDA API References  CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration. CUDA Driver API Fields in structures might appear in order that is different from the order of declaration. CUDA Math API The CUDA math API. cuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs. cuDLA API The cuDLA API. NVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library. nvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. cuFFT The cuFFT library user guide. CUB The user guide for CUB. CUDA C++ Standard Library The API reference for libcu++, the CUDA C++ standard library. cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. cuRAND The cuRAND library user guide. cuSPARSE The cuSPARSE library user guide. NPP NVIDIA NPP is a library of functions for performing CUDA accelerated processing. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. The NPP library is written to maximize flexibility, while maintaining high performance. nvJitLink The user guide for the nvJitLink library. nvFatbin The user guide for the nvFatbin library. NVRTC (Runtime Compilation) NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. This facility can often provide optimizations and performance not possible in a purely offline static compilation. Thrust The C++ parallel algorithms library. cuSOLVER The cuSOLVER library user guide. PTX Compiler API References  PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library. Miscellaneous  CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite. CUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment. Multi-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU. CUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade. CUPTI The CUPTI-API. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications. Debugger API The CUDA debugger API. GPUDirect RDMA A technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express. This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model. GPUDirect Storage The documentation for GPUDirect Storage. vGPU vGPUs that support CUDA. Tools  NVCC This is a reference document for nvcc, the CUDA compiler driver. nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. CUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger. Compute Sanitizer The user guide for Compute Sanitizer. Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems. Nsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. Nsight Visual Studio Edition The documentation for Nsight Visual Studio Edition. Profiler This is the guide to the Profiler. CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, and nvprune. White Papers  Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide. Incomplete-LU and Cholesky Preconditioned Iterative Methods In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms. Application Notes  CUDA for Tegra This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability. Compiler SDK  libNVVM API The libNVVM API. libdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels. NVVM IR NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cufft/index.html", "parent_url": "https://docs.nvidia.com/cuda/cufft/index.html", "content_type": "text/html", "text": "cuFFT 1. Introduction 2. Using the cuFFT API 2.1. Accessing cuFFT 2.2. Fourier Transform Setup 2.2.1. Free Memory Requirement 2.2.2. Plan Initialization Time 2.3. Fourier Transform Types 2.3.1. Half-precision cuFFT Transforms 2.3.2. Bfloat16-precision cuFFT Transforms 2.4. Data Layout 2.5. Multidimensional Transforms 2.6. Advanced Data Layout 2.7. Streamed cuFFT Transforms 2.8. Multiple GPU cuFFT Transforms 2.8.1. Plan Specification and Work Areas 2.8.2. Helper Functions 2.8.3. Multiple GPU 2D and 3D Transforms on Permuted Input 2.8.4. Supported Functionality 2.9. cuFFT Callback Routines 2.9.1. Overview of the cuFFT Callback Routine Feature 2.9.2. Specifying Load and Store Callback Routines 2.9.3. Callback Routine Function Details 2.9.4. Coding Considerations for the cuFFT Callback Routine Feature 2.9.4.1. No Ordering Guarantees Within a Kernel 2.10. Thread Safety 2.11. CUDA Graphs Support 2.12. Static Library and Callback Support 2.12.1. Static library without callback support 2.13. Accuracy and Performance 2.14. Caller Allocated Work Area Support 2.15. cuFFT Link-Time Optimized Kernels 2.15.1. Overview of the cuFFT Callback Routine Feature 3. cuFFT API Reference 3.1. Return value cufftResult 3.2. cuFFT Basic Plans 3.2.1. cufftPlan1d() 3.2.2. cufftPlan2d() 3.2.3. cufftPlan3d() 3.2.4. cufftPlanMany() 3.3. cuFFT Extensible Plans 3.3.1. cufftCreate() 3.3.2. cufftDestroy() 3.3.3. cufftMakePlan1d() 3.3.4. cufftMakePlan2d() 3.3.5. cufftMakePlan3d() 3.3.6. cufftMakePlanMany() 3.3.7. cufftMakePlanMany64() 3.3.8. cufftXtMakePlanMany() 3.4. cuFFT Plan Properties 3.4.1. cufftSetPlanPropertyInt64() 3.4.2. cufftGetPlanPropertyInt64() 3.4.3. cufftResetPlanProperty() 3.5. cuFFT Estimated Size of Work Area 3.5.1. cufftEstimate1d() 3.5.2. cufftEstimate2d() 3.5.3. cufftEstimate3d() 3.5.4. cufftEstimateMany() 3.6. cuFFT Refined Estimated Size of Work Area 3.6.1. cufftGetSize1d() 3.6.2. cufftGetSize2d() 3.6.3. cufftGetSize3d() 3.6.4. cufftGetSizeMany() 3.6.5. cufftGetSizeMany64() 3.6.6. cufftXtGetSizeMany() 3.7. cufftGetSize() 3.8. cuFFT Caller Allocated Work Area Support 3.8.1. cufftSetAutoAllocation() 3.8.2. cufftSetWorkArea() 3.8.3. cufftXtSetWorkAreaPolicy() 3.9. cuFFT Execution 3.9.1. cufftExecC2C() and cufftExecZ2Z() 3.9.2. cufftExecR2C() and cufftExecD2Z() 3.9.3. cufftExecC2R() and cufftExecZ2D() 3.9.4. cufftXtExec() 3.9.5. cufftXtExecDescriptor() 3.10. cuFFT and Multiple GPUs 3.10.1. cufftXtSetGPUs() 3.10.2. cufftXtSetWorkArea() 3.10.3. cuFFT Multiple GPU Execution 3.10.3.1. cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z() 3.10.3.2. cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z() 3.10.3.3. cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D() 3.10.4. Memory Allocation and Data Movement Functions 3.10.4.1. cufftXtMalloc() 3.10.4.1.1. Parameter cufftXtSubFormat 3.10.4.2. cufftXtFree() 3.10.4.3. cufftXtMemcpy() 3.10.4.3.1. Parameter cufftXtCopyType 3.10.5. General Multiple GPU Descriptor Types 3.10.5.1. cudaXtDesc 3.10.5.2. cudaLibXtDesc 3.11. cuFFT Callbacks 3.11.1. cufftXtSetCallback() 3.11.2. cufftXtClearCallback() 3.11.3. cufftXtSetCallbackSharedSize() 3.12. cufftSetStream() 3.13. cufftGetVersion() 3.14. cufftGetProperty() 3.15. cuFFT Types 3.15.1. Parameter cufftType 3.15.2. Parameters for Transform Direction 3.15.3. Type definitions for callbacks 3.15.4. Other cuFFT Types 3.15.4.1. cufftHandle 3.15.4.2. cufftReal 3.15.4.3. cufftDoubleReal 3.15.4.4. cufftComplex 3.15.4.5. cufftDoubleComplex 3.16. Common types 3.16.1. cudaDataType 3.16.2. libraryPropertyType 4. cuFFT Code Examples 5. Multiple GPU Data Organization 5.1. Multiple GPU Data Organization for Batched Transforms 5.2. Multiple GPU Data Organization for Single 2D and 3D Transforms 5.3. Multiple-GPU Data Organization for Single 1D Transforms 6. FFTW Conversion Guide 7. FFTW Interface to cuFFT 8. Deprecated Functionality 9. Notices 9.1. Notice 9.2. OpenCL 9.3. Trademarks cuFFT » 1. Introduction v12.5 | PDF | Archive cuFFT API Reference The API reference guide for cuFFT, the CUDA Fast Fourier Transform library. 1. Introduction  This document describes cuFFT, the NVIDIA® CUDA® Fast Fourier Transform (FFT) product. It consists of two separate libraries: cuFFT and cuFFTW. The cuFFT library is designed to provide high performance on NVIDIA GPUs. The cuFFTW library is provided as a porting tool to enable users of FFTW to start using NVIDIA GPUs with a minimum amount of effort. The FFT is a divide-and-conquer algorithm for efficiently computing discrete Fourier transforms of complex or real-valued data sets. It is one of the most important and widely used numerical algorithms in computational physics and general signal processing. The cuFFT library provides a simple interface for computing FFTs on an NVIDIA GPU, which allows users to quickly leverage the floating-point power and parallelism of the GPU in a highly optimized and tested FFT library. The cuFFT product supports a wide range of FFT inputs and options efficiently on NVIDIA GPUs. This version of the cuFFT library supports the following features: Algorithms highly optimized for input sizes that can be written in the form \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) . In general the smaller the prime factor, the better the performance, i.e., powers of two are fastest. An \\(O\\left( n\\log n \\right)\\) algorithm for every input data size Half-precision (16-bit floating point), single-precision (32-bit floating point) and double-precision (64-bit floating point). Transforms of lower precision have higher performance. Complex and real-valued input and output. Real valued input or output require less computations and data than complex values and often have faster time to solution. Types supported are: C2C - Complex input to complex output R2C - Real input to complex output C2R - Symmetric complex input to real output 1D, 2D and 3D transforms Execution of multiple 1D, 2D and 3D transforms simultaneously. These batched transforms have higher performance than single transforms. In-place and out-of-place transforms Arbitrary intra- and inter-dimension element strides (strided layout) FFTW compatible data layout Execution of transforms across multiple GPUs Streamed execution, enabling asynchronous computation and data movement The cuFFTW library provides the FFTW3 API to facilitate porting of existing FFTW applications. Please note that starting from CUDA 11.0, the minimum supported GPU architecture is SM35. See Deprecated Functionality . 2. Using the cuFFT API  This chapter provides a general overview of the cuFFT library API. For more complete information on specific functions, see cuFFT API Reference . Users are encouraged to read this chapter before continuing with more detailed descriptions. The Discrete Fourier transform (DFT) maps a complex-valued vector \\(x_{k}\\) ( time domain ) into its frequency domain representation given by: \\(X_{k} = \\sum\\limits_{n = 0}^{N - 1}x_{n}e^{-2\\pi i\\frac{kn}{N}}\\) where \\(X_{k}\\) is a complex-valued vector of the same size. This is known as a forward DFT. If the sign on the exponent of e is changed to be positive, the transform is an inverse transform. Depending on \\(N\\) , different algorithms are deployed for the best performance. The cuFFT API is modeled after FFTW , which is one of the most popular and efficient CPU-based FFT libraries. cuFFT provides a simple configuration mechanism called a plan that uses internal building blocks to optimize the transform for the given configuration and the particular GPU hardware selected. Then, when the execution function is called, the actual transform takes place following the plan of execution. The advantage of this approach is that once the user creates a plan, the library retains whatever state is needed to execute the plan multiple times without recalculation of the configuration. This model works well for cuFFT because different kinds of FFTs require different thread configurations and GPU resources, and the plan interface provides a simple way of reusing configurations. Computing a number BATCH of one-dimensional DFTs of size NX using cuFFT will typically look like this: #define NX 256 #define BATCH 10 #define RANK 1 ... { cufftHandle plan ; cufftComplex * data ; ... cudaMalloc (( void ** ) & data , sizeof ( cufftComplex ) * NX * BATCH ); cufftPlanMany ( & plan , RANK , NX , & iembed , istride , idist , & oembed , ostride , odist , CUFFT_C2C , BATCH ); ... cufftExecC2C ( plan , data , data , CUFFT_FORWARD ); cudaDeviceSynchronize (); ... cufftDestroy ( plan ); cudaFree ( data ); } 2.1. Accessing cuFFT  The cuFFT and cuFFTW libraries are available as shared libraries. They consist of compiled programs ready for users to incorporate into applications with the compiler and linker. cuFFT can be downloaded from https://developer.nvidia.com/cufft . By selecting Download CUDA Production Release users are all able to install the package containing the CUDA Toolkit, SDK code samples and development drivers. The CUDA Toolkit contains cuFFT and the samples include simplecuFFT . The Linux release for simplecuFFT assumes that the root install directory is /usr/local/cuda and that the locations of the products are contained there as follows. Modify the Makefile as appropriate for your system. Product Location and name Include file nvcc compiler /bin/nvcc cuFFT library {lib, lib64}/libcufft.so inc/cufft.h cuFFT library with Xt functionality {lib, lib64}/libcufft.so inc/cufftXt.h cuFFTW library {lib, lib64}/libcufftw.so inc/cufftw.h The most common case is for developers to modify an existing CUDA routine (for example, filename.cu ) to call cuFFT routines. In this case the include file cufft.h or cufftXt.h should be inserted into filename.cu file and the library included in the link line. A single compile and link line might appear as /usr/local/cuda/bin/nvcc [options] filename.cu … -I/usr/local/cuda/inc -L/usr/local/cuda/lib -lcufft Of course there will typically be many compile lines and the compiler g++ may be used for linking so long as the library path is set correctly. Users of the FFTW interface (see FFTW Interface to cuFFT ) should include cufftw.h and link with both cuFFT and cuFFTW libraries. Functions in the cuFFT and cuFFTW library assume that the data is in GPU visible memory. This means any memory allocated by cudaMalloc , cudaMallocHost and cudaMallocManaged or registered with cudaHostRegister can be used as input, output or plan work area with cuFFT and cuFFTW functions. For the best performance input data, output data and plan work area should reside in device memory. cuFFTW library also supports input data and output data that is not GPU visible. 2.2. Fourier Transform Setup  The first step in using the cuFFT Library is to create a plan using one of the following: cufftPlan1D() / cufftPlan2D() / cufftPlan3D() - Create a simple plan for a 1D/2D/3D transform respectively. cufftPlanMany() - Creates a plan supporting batched input and strided data layouts. cufftXtMakePlanMany() - Creates a plan supporting batched input and strided data layouts for any supported precision. Among the plan creation functions, cufftPlanMany() allows use of more complicated data layouts and batched executions. Execution of a transform of a particular size and type may take several stages of processing. When a plan for the transform is generated, cuFFT derives the internal steps that need to be taken. These steps may include multiple kernel launches, memory copies, and so on. In addition, all the intermediate buffer allocations (on CPU/GPU memory) take place during planning. These buffers are released when the plan is destroyed. In the worst case, the cuFFT Library allocates space for 8*batch*n[0]*..*n[rank-1] cufftComplex or cufftDoubleComplex elements (where batch denotes the number of transforms that will be executed in parallel, rank is the number of dimensions of the input data (see Multidimensional Transforms ) and n[] is the array of transform dimensions) for single and double-precision transforms respectively. Depending on the configuration of the plan, less memory may be used. In some specific cases, the temporary space allocations can be as low as 1*batch*n[0]*..*n[rank-1] cufftComplex or cufftDoubleComplex elements. This temporary space is allocated separately for each individual plan when it is created (i.e., temporary space is not shared between the plans). The next step in using the library is to call an execution function such as cufftExecC2C() (see Parameter cufftType ) which will perform the transform with the specifications defined at planning. One can create a cuFFT plan and perform multiple transforms on different data sets by providing different input and output pointers. Once the plan is no longer needed, the cufftDestroy() function should be called to release the resources allocated for the plan. 2.2.1. Free Memory Requirement  The first program call to any cuFFT function causes the initialization of the cuFFT kernels. This can fail if there is not enough free memory on the GPU. It is advisable to initialize cufft first (e.g. by creating a plan) and then allocating memory. 2.2.2. Plan Initialization Time  During plan initialization, cuFFT conducts a series of steps, including heuristics to determine which kernels to be used as well as kernel module loads. Starting from CUDA 12.0, cuFFT delivers a larger portion of kernels using the CUDA Parallel Thread eXecution assembly form (PTX code), instead of the binary form (cubin object). The PTX code of cuFFT kernels are loaded and compiled further to the binary code by the CUDA device driver at runtime when a cuFFT plan is initialized. This is called just-in-time (JIT) compilation . JIT compilation slightly increases cuFFT plan initialization time, depending on the transform size and the speed of the host CPU (see Module load driver API ) . But the JIT overhead occurs only when a binary code is generated for the first time during plan initialization using one of the plan creation functions . The device driver automatically caches a copy of the generated binary code to avoid repeating the compilation in subsequent invocations. If necessary, CUDA_CACHE_PATH or CUDA_CACHE_MAXSIZE can be customized to set the cache folder and max size (see detail in CUDA Environmental Variables ), but the default settings are fine in general. 2.3. Fourier Transform Types  Apart from the general complex-to-complex (C2C) transform, cuFFT implements efficiently two other types: real-to-complex (R2C) and complex-to-real (C2R). In many practical applications the input vector is real-valued. It can be easily shown that in this case the output satisfies Hermitian symmetry ( \\(X_{k} = X_{N - k}^{\\ast}\\) , where the star denotes complex conjugation). The converse is also true: for complex-Hermitian input the inverse transform will be purely real-valued. cuFFT takes advantage of this redundancy and works only on the first half of the Hermitian vector. Transform execution functions for single and double-precision are defined separately as: cufftExecC2C() / cufftExecZ2Z() - complex-to-complex transforms for single/double precision. cufftExecR2C() / cufftExecD2Z() - real-to-complex forward transform for single/double precision. cufftExecC2R() / cufftExecZ2D() - complex-to-real inverse transform for single/double precision. Each of those functions demands different input data layout (see Data Layout for details). Note Complex-to-real (C2R) transforms accept complex-Hermitian input. For one-dimensional signals, this requires the 0th element (and the \\(\\frac{N}{2}\\) th input if N is even) to be real-valued, i.e. its imaginary part should be zero.\nFor d-dimension signals, this means \\(x_{(n_{1},n_{2},\\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\\ldots,N_{d} - n_{d})}^{\\ast}\\) .\nOtherwise, the behavior of the transform is undefined. Also see Multidimensional Transforms . Functions cufftXtExec() and cufftXtExecDescriptor() can perform transforms on any of the supported types. 2.3.1. Half-precision cuFFT Transforms  Half-precision transforms have the following limitations: Minimum GPU architecture is SM_53 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details. The CUDA Toolkit provides the cuda_fp16.h header with types and intrinsic functions for handling half-precision arithmetic. 2.3.2. Bfloat16-precision cuFFT Transforms  cuFFT supports bfloat16 precision using the nv_bfloat16 data type. Please note that cuFFT utilizes a combination of single- and bfloat16-precision arithmetic operations when computing the FFT in bfloat16 precision. Bfloat16-precision transforms have similar limitations to half-precision transforms: Minimum GPU architecture is SM_80 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details. The CUDA Toolkit provides the cuda_bf16.h header with types and intrinsic functions for handling bfloat16-precision arithmetic. 2.4. Data Layout  In the cuFFT Library, data layout depends strictly on the configuration and the transform type. In the case of general complex-to-complex transform both the input and output data shall be a cufftComplex / cufftDoubleComplex array in single- and double-precision modes respectively. In C2R mode an input array \\((x_{1},x_{2},\\ldots,x_{\\lfloor\\frac{N}{2}\\rfloor + 1})\\) of only non-redundant complex elements is required. The output array \\((X_{1},X_{2},\\ldots,X_{N})\\) consists of cufftReal / cufftDouble elements in this mode. Finally, R2C demands an input array \\((X_{1},X_{2},\\ldots,X_{N})\\) of real values and returns an array \\((x_{1},x_{2},\\ldots,x_{\\lfloor\\frac{N}{2}\\rfloor + 1})\\) of non-redundant complex elements. In real-to-complex and complex-to-real transforms the size of input data and the size of output data differ. For out-of-place transforms a separate array of appropriate size is created. For in-place transforms the user should use padded data layout. This layout is FFTW compatibile. In the padded layout output signals begin at the same memory addresses as the input data. Therefore input data for real-to-complex and output data for complex-to-real must be padded. Expected sizes of input/output data for 1-d transforms are summarized in the table below: FFT type input data size output data size C2C \\(x\\) cufftComplex \\(x\\) cufftComplex C2R \\(\\left\\lfloor \\frac{x}{2} \\right\\rfloor + 1\\) cufftComplex \\(x\\) cufftReal R2C* \\(x\\) cufftReal \\(\\left\\lfloor \\frac{x}{2} \\right\\rfloor + 1\\) cufftComplex The real-to-complex transform is implicitly a forward transform. For an in-place real-to-complex transform where FFTW compatible output is desired, the input size must be padded to \\(\\left( {\\lfloor\\frac{N}{2}\\rfloor + 1} \\right)\\) complex elements. For out-of-place transforms, input and output sizes match the logical transform size \\(N\\) and the non-redundant size \\(\\lfloor\\frac{N}{2}\\rfloor + 1\\) , respectively. The complex-to-real transform is implicitly inverse. For in-place complex-to-real FFTs where FFTW compatible output is selected (default padding mode), the input size is assumed to be \\(\\lfloor\\frac{N}{2}\\rfloor + 1\\) cufftComplex elements. Note that in-place complex-to-real FFTs may overwrite arbitrary imaginary input point values when non-unit input and output strides are chosen. Out-of-place complex-to-real FFT will always overwrite input buffer. For out-of-place transforms, input and output sizes match the logical transform non-redundant size \\(\\lfloor\\frac{N}{2}\\rfloor + 1\\) and size \\(N\\) , respectively. 2.5. Multidimensional Transforms  Multidimensional DFT map a \\(d\\) -dimensional array \\(x_{\\mathbf{n}}\\) , where \\(\\mathbf{n} = (n_{1},n_{2},\\ldots,n_{d})\\) into its frequency domain array given by: \\(X_{\\mathbf{k}} = \\sum\\limits_{n = 0}^{N - 1}x_{\\mathbf{n}}e^{-2\\pi i\\frac{\\mathbf{k}\\mathbf{n}}{\\mathbf{N}}}\\) where \\(\\frac{\\mathbf{n}}{\\mathbf{N}} = (\\frac{n_{1}}{N_{1}},\\frac{n_{2}}{N_{2}},\\ldots,\\frac{n_{d}}{N_{d}})\\) , and the summation denotes the set of nested summations \\(\\sum\\limits_{n_{1} = 0}^{N_{1} - 1}\\sum\\limits_{n_{2} = 0}^{N_{2} - 1}\\ldots\\sum\\limits_{n_{d} = 0}^{N_{d} - 1}\\) cuFFT supports one-dimensional, two-dimensional and three-dimensional transforms, which can all be called by the same cufftExec* functions (see Fourier Transform Types ). Similar to the one-dimensional case, the frequency domain representation of real-valued input data satisfies Hermitian symmetry, defined as: \\(x_{(n_{1},n_{2},\\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\\ldots,N_{d} - n_{d})}^{\\ast}\\) . C2R and R2C algorithms take advantage of this fact by operating only on half of the elements of signal array, namely on: \\(x_{\\mathbf{n}}\\) for \\(\\mathbf{n} \\in \\{ 1,\\ldots,N_{1}\\} \\times \\ldots \\times \\{ 1,\\ldots,N_{d - 1}\\} \\times \\{ 1,\\ldots,\\lfloor\\frac{N_{d}}{2}\\rfloor + 1\\}\\) . The general rules of data alignment described in Data Layout apply to higher-dimensional transforms. The following table summarizes input and output data sizes for multidimensional DFTs: Dims FFT type Input data size Output data size 1D C2C \\(\\mathbf{N}_{1}\\) cufftComplex \\(\\mathbf{N}_{1}\\) cufftComplex 1D C2R \\(\\lfloor\\frac{\\mathbf{N}_{1}}{2}\\rfloor + 1\\) cufftComplex \\(\\mathbf{N}_{1}\\) cufftReal 1D R2C \\(\\mathbf{N}_{1}\\) cufftReal \\(\\lfloor\\frac{\\mathbf{N}_{1}}{2}\\rfloor + 1\\) cufftComplex 2D C2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftComplex 2D C2R \\(\\mathbf{N}_{1}(\\lfloor\\frac{\\mathbf{N}_{2}}{2}\\rfloor + 1)\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftReal 2D R2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftReal \\(\\mathbf{N}_{1}(\\lfloor\\frac{\\mathbf{N}_{2}}{2}\\rfloor + 1)\\) cufftComplex 3D C2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftComplex 3D C2R \\(\\mathbf{N}_{1}\\mathbf{N}_{2}(\\lfloor\\frac{\\mathbf{N}_{3}}{2}\\rfloor + 1)\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftReal 3D R2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftReal \\(\\mathbf{N}_{1}\\mathbf{N}_{2}(\\lfloor\\frac{\\mathbf{N}_{3}}{2}\\rfloor + 1)\\) cufftComplex For example, static declaration of a three-dimensional array for the output of an out-of-place real-to-complex transform will look like this: cufftComplex odata [ N1 ][ N2 ][ N3 / 2 + 1 ]; 2.6. Advanced Data Layout  The advanced data layout feature allows transforming only a subset of an input array, or outputting to only a portion of a larger data structure. It can be set by calling function: cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ); Passing inembed or onembed set to NULL is a special case and is equivalent to passing n for each. This is same as the basic data layout and other advanced parameters such as istride are ignored. If the advanced parameters are to be used, then all of the advanced interface parameters must be specified correctly. Advanced parameters are defined in units of the relevant data type ( cufftReal , cufftDoubleReal , cufftComplex , or cufftDoubleComplex ). Advanced layout can be perceived as an additional layer of abstraction above the access to input/output data arrays. An element of coordinates [z][y][x] in signal number b in the batch will be associated with the following addresses in the memory: 1D input[ b * idist + x * istride ] output[ b * odist + x * ostride ] 2D input[ b * idist` + (x * inembed[1] + y) * istride ] output[ b * odist + (x * onembed[1] + y) * ostride ] 3D input[ b * idist + ((x * inembed[1] + y) * inembed[2] + z) * istride ] output[ b * odist + ((x * onembed[1] + y) * onembed[2] + z) * ostride ] The istride and ostride parameters denote the distance between two successive input and output elements in the least significant (that is, the innermost) dimension respectively. In a single 1D transform, if every input element is to be used in the transform, istride should be set to \\(1\\) ; if every other input element is to be used in the transform, then istride should be set to \\(2\\) . Similarly, in a single 1D transform, if it is desired to output final elements one after another compactly, ostride should be set to \\(1\\) ; if spacing is desired between the least significant dimension output data, ostride should be set to the distance between the elements. The inembed and onembed parameters define the number of elements in each dimension in the input array and the output array respectively. The inembed[rank-1] contains the number of elements in the least significant (innermost) dimension of the input data excluding the istride elements; the number of total elements in the least significant dimension of the input array is then istride*inembed[rank-1] . The inembed[0] or onembed[0] corresponds to the most significant (that is, the outermost) dimension and is effectively ignored since the idist or odist parameter provides this information instead. Note that the size of each dimension of the transform should be less than or equal to the inembed and onembed values for the corresponding dimension, that is n[i] ≤ inembed[i] , n[i] ≤ onembed[i] , where \\(i \\in \\{ 0,\\ldots,rank - 1\\}\\) . The idist and odist parameters indicate the distance between the first element of two consecutive batches in the input and output data. 2.7. Streamed cuFFT Transforms  Every cuFFT plan may be associated with a CUDA stream. Once so associated, all launches of the internal stages of that plan take place through the specified stream. Streaming of cuFFT execution allows for potential overlap between transforms and memory copies. (See the NVIDIA CUDA Programming Guide for more information on streams.) If no stream is associated with a plan, launches take place in stream(0) , the default CUDA stream. Note that many plan executions require multiple kernel launches. cuFFT uses private streams internally to sort operations, including event syncrhonization. cuFFT does not guarantee ordering of internal operations, and the order is only preserved with respect to the streams set by the user. As of CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported in multiple GPU cases. However, calls to cufftXtMemcpy() are still synchronous across multiple GPUs when using streams. In previous versions of cuFFT, cufftSetStream() returns an error in the multiple GPU case. Likewise, calling certain multi-GPU functions such as cufftXtSetCallback() after setting a stream with cufftSetStream() will result in an error (see API functions for more details). Please note that in order to overlap plans using single plan handle user needs to manage work area buffers. Each concurrent plan execution needs it’s exclusive work area. Work area can be set by cufftSetWorkArea function. 2.8. Multiple GPU cuFFT Transforms  cuFFT supports using up to sixteen GPUs connected to a CPU to perform Fourier Transforms whose calculations are distributed across the GPUs. An API has been defined to allow users to write new code or modify existing code to use this functionality. Some existing functions such as the creation of a plan using cufftCreate() also apply in the multiple GPU case. Multiple GPU routines contain Xt in their name. The memory on the GPUs is managed by helper functions cufftXtMalloc()/cufftXtFree() and cufftXtMemcpy() using the cudaLibXtDesc descriptor. Performance is a function of the bandwidth between the GPUs, the computational ability of the individual GPUs, and the type and number of FFT to be performed. The highest performance is obtained using NVLink interconnect ( https://www.nvidia.com/object/nvlink.html ). The second best option is using PCI Express 3.0 between the GPUs and ensuring that both GPUs are on the same switch. Note that multiple GPU execution is not guaranteed to solve a given size problem in a shorter time than single GPU execution. The multiple GPU extensions to cuFFT are built on the extensible cuFFT API. The general steps in defining and executing a transform with this API are: cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used Optional: cufftEstimate{1d,2d,3d,Many}() - estimate the sizes of the work areas required. These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. cufftMakePlan{1d,2d,3d,Many}() - create the plan. These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. Optional: cufftGetSize{1d,2d,3d,Many}() - refined estimate of the sizes of the work areas required. These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. Optional: cufftGetSize() - check workspace size. This is the same function used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. Optional: cufftXtSetWorkArea() - do your own workspace allocation. cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - execute the plan cufftXtMemcpy() - copy data from the GPUs cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2.8.1. Plan Specification and Work Areas  In the single GPU case a plan is created by a call to cufftCreate() followed by a call to cufftMakePlan*() . For multiple GPUs, the GPUs to use for execution are identified by a call to cufftXtSetGPUs() and this must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() . Note that when cufftMakePlan*() is called for a single GPU, the work area is on that GPU. In a multiple GPU plan, the returned work area has multiple entries; one value per GPU. That is workSize points to a size_t array, one entry per GPU. Also the strides and batches apply to the entire plan across all GPUs associated with the plan. Once a plan is locked by a call to cufftMakePlan*() , different descriptors may be specified in calls to cufftXtExecDescriptor*() to execute the plan on different data sets, but the new descriptors must use the same GPUs in the same order. As in the single GPU case, cufftEstimateSize{Many,1d,2d,3d}() and cufftGetSize{Many,1d,2d,3d}() give estimates of the work area sizes required for a multiple GPU plan and in this case workSize points to a size_t array, one entry per GPU. Similarly the actual work size returned by cufftGetSize() is a size_t array, one entry per GPU in the multiple GPU case. 2.8.2. Helper Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution. cuFFT provides functions to assist users in manipulating data on multiple GPUs. These must be called after the call to cufftMakePlan*() . On a single GPU users may call cudaMalloc() and cudaFree() to allocate and free GPU memory. To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMalloc() and cufftXtFree() functions. The function cufftXtMalloc() returns a descriptor which specifies the location of these memories. On a single GPU users may call cudaMemcpy() to transfer data between host and GPU memory. To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMemcpy() which allows users to copy between host and multiple GPU memories or even between the GPU memories. All single GPU cuFFT FFTs return output the data in natural order, that is the ordering of the result is the same as if a DFT had been performed on the data. Some Fast Fourier Transforms produce intermediate results where the data is left in a permutation of the natural output. When batch is one, data is left in the GPU memory in a permutation of the natural output. When cufftXtMemcpy() is used to copy data from GPU memory back to host memory, the results are in natural order regardless of whether the data on the GPUs is in natural order or permuted. Using CUFFT_COPY_DEVICE_TO_DEVICE allows users to copy data from the permuted data format produced after a single transform to the natural order on GPUs. 2.8.3. Multiple GPU 2D and 3D Transforms on Permuted Input  For single 2D or 3D transforms on multiple GPUs, when cufftXtMemcpy() distributes the data to the GPUs, the array is divided on the X axis. E.G. for two GPUs half of the X dimenson points, for all Y (and Z) values, are copied to each of the GPUs. When the transform is computed, the data are permuted such that they are divided on the Y axis. I.E. half of the Y dimension points, for all X (and Z) values are on each of the GPUs. When cuFFT creates a 2D or 3D plan for a single transform on multiple GPUs, it actually creates two plans. One plan expects input to be divided on the X axis. The other plan expects data to be divided on the Y axis. This is done because many algorithms compute a forward FFT, then perform some point-wise operation on the result, and then compute the inverse FFT. A memory copy to restore the data to the original order would be expensive. To avoid this, cufftXtMemcpy and cufftXtExecDescriptor() keep track of the data ordering so that the correct operation is used. The ability of cuFFT to process data in either order makes the following sequence possible. cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used cufftMakePlan{1d,2d,3d,Many}() - create the plan. cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the forward FFT userFunction() - modify the data in the frequency domain cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the inverse FFT Note that it was not necessary to copy/permute the data between execute calls cufftXtMemcpy() - copy data to the host cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2.8.4. Supported Functionality  Starting with cuFFT version 7.0, a subset of single GPU functionality is supported for multiple GPU execution. Requirements and limitations: All GPUs must have the same CUDA architecture level and support Unified Virtual Address Space. On Windows, the GPU boards must be operating in Tesla Compute Cluster (TCC) mode. For an application that uses the CUDA Driver API, running cuFFT on multiple GPUs is only compatible with applications using the primary context on each GPU. Strided input and output are not supported. Running cuFFT on more than 8 GPUs (16 GPUs is max) is supported on machines with NVLink only. While transforms with batch count greater than one do not impose additional constraints, those with a single batch have some restrictions. Single-batch FFTs support only in-place mode, and have additional constraints depending on the FFT type. This behavior is summarized in the following table: batch=1 1D 2D 3D C2C / Z2Z 2,4,8,16 GPUs power of 2 sizes only Minimum size for 2-4 GPUs is 64 Minimum size for 8 GPUs is 128 Minimum size for 16 GPUs is 1024 2-16 GPUs One of the following conditions is met for each dimension: Dimension must factor into primes less than or equal to 127 Maximum dimension size is 4096 for single precision Maximum dimension size is 2048 for double precision Minimum size is 32 R2C / D2Z not supported 2-16 GPUs One of the following conditions is met for each dimension: Dimension must factor into primes less than or equal to 127 Maximum dimension size is 4096 for single precision Maximum dimension size is 2048 for double precision Minimum size is 32 Fastest changing dimension size needs to be even Supports only CUFFT_XT_FORMAT_INPLACE input descriptor format No callback support C2R / Z2D not supported 2-16 GPUs One of the following conditions is met for each dimension: Dimension must factor into primes less than or equal to 127 Maximum dimension size is 4096 for single precision Maximum dimension size is 2048 for double precision Minimum size is 32 Fastest changing dimension size needs to be even Supports only CUFFT_XT_FORMAT_INPLACE_SHUFFLED input descriptor format No callback support General guidelines are: Parameter whichGPUs of cufftXtSetGPUs() function determines ordering of the GPUs with respect to data decomposition (first data chunk is placed on GPU denoted by first element of whichGPUs ) The data for the entire transform must fit within the memory of the GPUs assigned to it. For batch size m on n GPUs : The first m % n GPUs execute \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor+\\ 1\\) transforms. The remaining GPUs execute \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor\\) transforms. Batch size output differences: Single GPU cuFFT results are always returned in natural order. When multiple GPUs are used to perform more than one transform, the results are also returned in natural order. When multiple GPUs are used to perform a single transform the results are returned in a permutation of the normal results to reduce communication time. This behavior is summarized in the following table: Number of GPUs Number of transforms Output Order on GPUs One One or multiple transforms Natural order Multiple One Permuted results Multiple Multiple Natural order To produce natural order results in GPU memory for multi-GPU runs in the 1D single transform case, requires calling cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE . 2D and 3D multi-GPU transforms support execution of a transform given permuted order results as input. After execution in this case, the output will be in natural order. It is also possible to use cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE to return 2D or 3D data to natural order. See the cuFFT Code Examples section for single GPU and multiple GPU examples. 2.9. cuFFT Callback Routines  Callback routines are user-supplied kernel routines that cuFFT will call when loading or storing data. They allow the user to do data pre- or post- processing without additional kernel calls. Note Starting from CUDA 11.4, support for callback functionality using separately compiled device code is deprecated on all GPU architectures. Callback functionality will continue to be supported for all GPU architectures. 2.9.1. Overview of the cuFFT Callback Routine Feature  cuFFT provides a set of APIs that allow the cuFFT user to provide CUDA functions that re-direct or manipulate the data as it is loaded prior to processing the FFT, or stored once the FFT has been done. For the load callback, cuFFT passes the callback routine the address of the input data and the offset to the value to be loaded from device memory, and the callback routine returns the value it wishes cuFFT to use instead. For the store callback, cuFFT passes the callback routine the value it has computed, along with the address of the output data and the offset to the value to be written to device memory, and the callback routine modifies the value and stores the modified result. In order to provide a callback to cuFFT, a plan is created and configured normally using the extensible plan APIs. After the call to cufftCreate and cufftMakePlan , the user may associate a load callback routine, or a store callback routine, or both, with the plan, by calling cufftXtSetCallback . The caller also has the option to specify a device pointer to an opaque structure they wish to associate with the plan. This pointer will be passed to the callback routine by the cuFFT library. The caller may use this structure to remember plan dimensions and strides, or have a pointer to auxiliary data, etc. With some restrictions, the callback routine is allowed to request shared memory for its own use. If the requested amount of shared memory is available, cufft will pass a pointer to it when it calls the callback routine. CUFFT allows for 8 types of callback routine, one for each possible combination of: load or store, real or complex, single precision or double. It is the caller’s responsibility to provide a routine that matches the function prototype for the type of routine specified. If there is already a callback of the specified type associated with the plan, the set callback function will replace it with the new one. The callback routine extensions to cuFFT are built on the extensible cuFFT API. The general steps in defining and executing a transform with callbacks are: cufftCreate() - create an empty plan, as in the single GPU case cufftMakePlan{1d,2d,3d,Many}() - create the plan. These are the same functions used in the single GPU case. cufftXtSetCallback() - called for load and/or store callback for this plan cufftExecC2C() etc. - execute the plan cufftDestroy() - free cuFFT plan resources Callback functions are not supported on transforms with a dimension size that does not factor into primes smaller than 127. Callback functions on plans whose dimensions’ prime factors are limited to 2, 3, 5, and 7 can safely call __syncthreads() . On other plans, results are not defined. Note The callback API is available in the statically linked cuFFT library only, and only on 64 bit LINUX operating systems. 2.9.2. Specifying Load and Store Callback Routines  In order to associate a callback routine with a plan, it is necessary to obtain a device pointer to the callback routine. As an example, if the user wants to specify a load callback for an R2C transform, they would write the device code for the callback function, and define a global device variable that contains a pointer to the function: __device__ cufftReal myOwnCallback ( void * dataIn , size_t offset , void * callerInfo , void * sharedPtr ) { cufftReal ret ; // use offset, dataIn, and optionally callerInfo to // compute the return value return ret ; } __device__ cufftCallbackLoadR myOwnCallbackPtr = myOwnCallback ; From the host side, the user then has to get the address of the callback routine, which is stored in myOwnCallbackPtr . This is done with cudaMemcpyFromSymbol , as follows: cufftCallbackLoadR hostCopyOfCallbackPtr ; cudaMemcpyFromSymbol ( & hostCopyOfCallbackPtr , myOwnCallbackPtr , sizeof ( hostCopyOfCallbackPtr )); hostCopyOfCallbackPtr then contains the device address of the callback routine, that should be passed to cufftXtSetCallback . Note that, for multi-GPU transforms, hostCopyOfCallbackPtr will need to be an array of pointers, and the cudaMemcpyFromSymbol will have to be invoked for each GPU. Please note that __managed__ variables are not suitable to pass to cufftSetCallback due to restrictions on variable usage (See the NVIDIA CUDA Programming Guide for more information about __managed__ variables). 2.9.3. Callback Routine Function Details  Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to load data prior to the transform. typedef cufftComplex ( * cufftCallbackLoadC )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleComplex ( * cufftCallbackLoadZ )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftReal ( * cufftCallbackLoadR )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleReal ( * cufftCallbackLoadD )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); Parameters for all of the load callbacks are defined as below: offset : offset of the input element from the start of output data. This is not a byte offset, rather it is the number of elements from start of data. dataIn : device pointer to the start of the input array that was passed in the cufftExecute call. callerInfo : device pointer to the optional caller specified data passed in the cufftXtSetCallback call. sharedPointer : pointer to shared memory, valid only if the user has called cufftXtSetCallbackSharedSize() . Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to store data after completion of the transform. Note that the store callback functions do not return a value. This is because a store callback function is responsible not only for transforming the data as desired, but also for writing the data to the desired location. This allows the store callback to rearrange the data, for example to shift the zero frequency result to the center of the ouput. typedef void ( * cufftCallbackStoreC )( void * dataOut , size_t offset , cufftComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreZ )( void * dataOut , size_t offset , cufftDoubleComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreR )( void * dataOut , size_t offset , cufftReal element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreD )( void * dataOut , size_t offset , cufftDoubleReal element , void * callerInfo , void * sharedPointer ); Parameters for all of the store callbacks are defined as below: offset : offset of the output element from the start of output data. This is not a byte offset, rather it is the number of elements from start of data. dataOut : device pointer to the start of the output array that was passed in the cufftExecute call. element : the real or complex result computed by CUFFT for the element specified by the offset argument. callerInfo : device pointer to the optional caller specified data passed in the cufftXtSetCallback call. sharedPointer : pointer to shared memory, valid only if the user has called cufftXtSetCallbackSharedSize() . 2.9.4. Coding Considerations for the cuFFT Callback Routine Feature  cuFFT supports callbacks on all types of transforms, dimension, batch, stride between elements or number of GPUs. Callbacks are supported for transforms of single and double precision. cuFFT supports a wide range of parameters, and based on those for a given plan, it attempts to optimize performance. The number of kernels launched, and for each of those, the number of blocks launched and the number of threads per block, will vary depending on how cuFFT decomposes the transform. For some configurations, cuFFT will load or store (and process) multiple inputs or outputs per thread. For some configurations, threads may load or store inputs or outputs in any order, and cuFFT does not guarantee that the inputs or outputs handled by a given thread will be contiguous. These characteristics may vary with transform size, transform type (e.g. C2C vs C2R), number of dimensions, and GPU architecture. These variations may also change from one library version to the next. cuFFT will call the load callback routine, for each point in the input, once and only once. Similarly it will call the store callback routine, for each point in the output, once and only once. If the transform is being done in-place (i.e. the input and output data are in the same memory location) the store callback for a given element cannot overwrite other elements. It can either overwrite the given element, or write in a completely distinct output buffer. When more than one kernel are used to implement a transform, the thread and block structure of the first kernel (the one that does the load) is often different from the thread and block structure of the last kernel (the one that does the store). One common use of callbacks is to reduce the amount of data read or written to memory, either by selective filtering or via type conversions. When more than one kernel are used to implement a transform, cuFFT alternates using the workspace and the output buffer to write intermediate results. This means that the output buffer must always be large enough to accommodate the entire transform. For multi-GPU transforms, the index passed to the callback routine is the element index from the start of data on that GPU , not from the start of the entire input or output data array. For transforms whose dimensions can be factored into powers of 2, 3, 5, or 7, cuFFT guarantees that it will call the load and store callback routines from points in the kernel that is safe to call __syncthreads function from within callback routine. Caller is responsible for guaranteeing that the callback routine is at a point where the callback code has converged, to avoid deadlock. For plans whose dimensions are factored into higher primes, results of a callback routine calling __syncthreads are not defined. 2.9.4.1. No Ordering Guarantees Within a Kernel  Note that there are no guarantees on the relative order of execution of blocks within a grid. As such, callbacks should not rely on any particular ordering within a kernel. For instance, reordering data (such as an FFT-shift) could rely on the order of execution of the blocks. Results in this case would be undefined. 2.10. Thread Safety  cuFFT APIs are thread safe as long as different host threads execute FFTs using different plans and the output data are disjoint. 2.11. CUDA Graphs Support  Using CUDA Graphs with cuFFT is supported on single GPU plans. It is also supported on multiple GPU plans starting with cuFFT version 10.4.0. The stream associated with a cuFFT plan must meet the requirements stated in Creating a Graph Using Stream Capture . Note Starting from CUDA 11.8 (including CUDA 12.0 onward), CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. 2.12. Static Library and Callback Support  Starting with release 6.5, the cuFFT libraries are also delivered in a static form as libcufft_static.a and libcufftw_static.a on Linux and Mac. Static libraries are not supported on Windows. The static cufft and cufftw libraries depend on thread abstraction layer library libculibos.a . For example, on linux, to compile a small application using cuFFT against the dynamic library, the following command can be used: nvcc mCufftApp.c  -lcufft  -o myCufftApp For cufftw on Linux, to compile a small application against the dynamic library, the following command can be used: nvcc mCufftwApp.c  -lcufftw  -lcufft  -o myCufftwApp Whereas to compile against the static cuFFT library, extra steps need to be taken. The library needs to be device linked. It may happen during building and linking of a simple program, or as a separate step. The entire process is described in Using Separarate Compilation in CUDA . For cuFFT and cufftw in version 9.0 or later any supported architecture can be used to do the device linking: Static cuFFT compilation command: nvcc mCufftApp.c  -lcufft_static   -lculibos -o myCufftApp Static cufftw compilation command: nvcc mCufftwApp.c   -lcufftw_static  -lcufft_static   -lculibos  -o myCufftwApp Prior to version 9.0 proper linking required specifying a subset of supported architectures, as shown in the following commands: Static cuFFT compilation command: nvcc mCufftApp.c  -lcufft_static   -lculibos -o myCufftApp\\\n    -gencode arch=compute_20,\\\"code=sm_20\\\"\\\n    -gencode arch=compute_30,\\\"code=sm_30\\\"\\\n    -gencode arch=compute_35,\\\"code=sm_35\\\"\\\n    -gencode arch=compute_50,\\\"code=sm_50\\\"\\\n    -gencode arch=compute_60,\\\"code=sm_60\\\"\\\n    -gencode arch=compute_60,\\\"code=compute_60\\\" Static cufftw compilation command: nvcc mCufftwApp.c    -lcufftw_static  -lcufft_static   -lculibos  -o myCufftwApp\\\n    -gencode arch=compute_20,\\\"code=sm_20\\\"\\\n    -gencode arch=compute_30,\\\"code=sm_30\\\"\\\n    -gencode arch=compute_35,\\\"code=sm_35\\\"\\\n    -gencode arch=compute_50,\\\"code=sm_50\\\"\\\n    -gencode arch=compute_60,\\\"code=sm_60\\\"\\\n    -gencode arch=compute_60,\\\"code=compute_60\\\" Please note that the cuFFT library might not contain code for certain architectures as long as there is code for a lower architecture that is binary compatibile (e.g. SM37, SM52, SM61). This is reflected in link commands above and significant when using versions prior r9.0. To determine if a specific SM is included in the cuFFT library, one may use cuobjdump utility. For example, if you wish to know if SM_50 is included, the command to run is cuobjdump -arch sm_50 libcufft_static.a . Some kernels are built only on select architectures (e.g. kernels with half precision arithmetics are present only for SM53 and above). This can cause warnings at link time that architectures are missing from these kernels. These warnings can be safely ignored. It is also possible to use the native Host C++ compiler and perform device link as a separate step. Please consult NVCC documentation for more details. Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line. Note that in this case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. The cuFFT static library supports user supplied callback routines. The callback routines are CUDA device code, and must be separately compiled with NVCC and linked with the cuFFT library. Please refer to the NVCC documentation regarding separate compilation for details. If you specify an SM when compiling your callback functions, you must specify one of the SM’s cuFFT includes. 2.12.1. Static library without callback support  Starting with cuFFT version 9.2, a new variant of the cuFTT static library, libcufft_static_nocallback.a , was added. This new version does not contain callback functionality and can be linked using the host compiler only. 2.13. Accuracy and Performance  A DFT can be implemented as a matrix vector multiplication that requires \\(O(N^{2})\\) operations. However, the cuFFT Library employs the Cooley-Tukey algorithm to reduce the number of required operations to optimize the performance of particular transform sizes. This algorithm expresses the DFT matrix as a product of sparse building block matrices. The cuFFT Library implements the following building blocks: radix-2, radix-3, radix-5, and radix-7. Hence the performance of any transform size that can be factored as \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) (where a , b , c , and d are non-negative integers) is optimized in the cuFFT library. There are also radix-m building blocks for other primes, m, whose value is < 128. When the length cannot be decomposed as multiples of powers of primes from 2 to 127, Bluestein’s algorithm is used. Since the Bluestein implementation requires more computations per output point than the Cooley-Tukey implementation, the accuracy of the Cooley-Tukey algorithm is better. The pure Cooley-Tukey implementation has excellent accuracy, with the relative error growing proportionally to \\(\\log_{2}(N)\\) , where \\(N\\) is the transform size in points. For sizes handled by the Cooley-Tukey code path, the most efficient implementation is obtained by applying the following constraints (listed in order from the most generic to the most specialized constraint, with each subsequent constraint providing the potential of an additional performance improvement). Half precision transforms might not be suitable for all kinds of problems due to limited range represented by half precision floating point arithmetics. Please note that the first element of FFT result is the sum of all input elements and it is likely to overflow for certain inputs. Results produced by the cuFFT library are deterministic (ie, bitwise reproducible) as long as the following are kept constant between runs: plan input parameters, cuFFT version, and GPU model. cuFFT batched plans require that input data includes valid signal for all batches. Performance optimizations in batched mode can combine signal from different batches for processing. Optimizations used in cuFFT can vary from version to version. Applies to Recommendation Comment All Use single precision transforms. Single precision transforms require less bandwidth per computation than double precision transforms. All Restrict the size along all dimensions to be representable as \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) . The cuFFT library has highly optimized kernels for transforms whose dimensions have these prime factors. In general the best performance occurs when using powers of 2, followed by powers of 3, then 5, 7. All Restrict the size along each dimension to use fewer distinct prime factors. A transform of size \\(2^{n}\\) or \\(3^{n}\\) will usually be faster than one of size \\(2^{i} \\times 3^{j}\\) even if the latter is slightly smaller, due to the composition of specialized paths. All Restrict the data to be contiguous in memory when performing a single transform. When performing multiple transforms make the individual datasets contiguous The cuFFT library has been optimized for this data layout. All Perform multiple (i.e., batched) transforms. Additional optimizations are performed in batched mode. real-to-complex transforms or complex-to-real transforms Ensure problem size of x dimension is a multiple of 4. This scheme uses more efficient kernels to implement conjugate symmetry property. real-to-complex transforms or complex-to-real transforms Use out-of-place mode. This scheme uses more efficient kernels than in-place mode. Multiple GPU transforms Use PCI Express 3.0 between GPUs and ensure the GPUs are on the same switch. The faster the interconnect between the GPUs, the faster the performance. 2.14. Caller Allocated Work Area Support  cuFFT plans may use additional memory to store intermediate results. The cuFFT library offers several functions to manage this temporary memory utilization behavior: cufftSetAutoAllocation cufftEstimate1d , cufftEstimate2d , cufftEstimate3d and cufftEstimateMany cufftGetSize cufftXtSetWorkAreaPolicy The first two functions manage allocation and ownership of temporary memory. By default cuFFT always allocates its own work area in GPU memory. Each cuFFT handle allocates data separately. If multiple cuFFT plans are to be launched sequentially it is possible to assign the same memory chunk as work area to all those plans and reduce memory overhead. The memory assigned as work area needs to be GPU visible. In addition to the regular memory acquired with cudaMalloc , usage of CUDA Unified Virtual Addressing enables cuFFT to use the following types of memory as work area memory: pinned host memory, managed memory, memory on GPU other than the one performing the calculations. While this provides flexibility, it comes with a performance penalty whose magnitude depends on the available memory bandwidth. The cufftEstimateNd , cufftEstimateMany , and cufftGetSize functions provide information about the required memory size for cases where the user is allocating the work space buffer. In version 9.2 cuFFT also introduced the cufftXtSetWorkAreaPolicy function. This function allows fine tuning of work area memory usage. cuFFT 9.2 version supports only the CUFFT_WORKAREA_MINIMAL policy, which instructs cuFFT to re-plan the existing plan without the need to use work area memory. Also as of cuFFT 9.2, supported FFT transforms that allow for CUFFT_WORKAREA_MINIMAL policy are as follows: Transforms of type C2C are supported with sizes up to 4096 in any dimension. Transforms of type Z2Z are supported with sizes up to 2048 in any dimension. Only single GPU transforms are supported. Depending on the FFT transform size, a different FFT algorithm may be used when the CUFFT_WORKAREA_MINIMAL policy is set. 2.15. cuFFT Link-Time Optimized Kernels  Starting from CUDA 12.4, cuFFT ships Link-Time Optimized (LTO) kernels. These kernels are linked and finalized at runtime as part of the cuFFT planning routines. This enables the cuFFT library to generate kernels optimized for the underlying architecture and the specific problem to solve. The current LTO kernel coverage includes: Kernels for 64-bit addressing (with FFTs spanning addresses greater than 2^(32)-1 elements). Some single- and double-precision R2C and C2R sizes. The number and coverage of LTO kernels will grow with future releases of cuFFT. We encourage our users to test whether LTO kernels improve the performance for their use case. Users can opt-in into LTO kernels by setting the NVFFT_PLAN_PROPERTY_INT64_PATIENT_JIT plan property using the cufftSetPlanProperty routine. In order to finalize LTO kernels, cuFFT relies on the nvJitLink library that ships as part of the CUDA Toolkit. Finalizing the kernels at runtime can cause an increase in planning time (which could be in the order of hundreds of milliseconds, depending on the cuFFT plan and hardware characteristics of the host system), in exchange for faster execution time of the optimized kernels. Note that nvJitLink caches kernels linked at runtime to speed-up subsequent kernel finalizations in repeated planning routines. If for any reason the runtime linking of the kernel fails, cuFFT will fall back to offline-compiled kernels to compute the FFT. Note cuFFT LTO kernels for a given toolkit version require using the nvJitLink library from the same toolkit or greater, but within the same toolkit major. For example, cuFFT in 12.4 requires nvJitLink to be from a CUDA Toolkit 12.X, with X >= 4 . The nvJitLink library is loaded dynamically, and should be present in the system’s dynamic linking path (e.g. LD_LIBRARY_PATH on Unix systems, or PATH on Windows systems). 2.15.1. Overview of the cuFFT Callback Routine Feature  cuFFT provides a set of APIs that allow the cuFFT user to provide CUDA functions that re-direct or manipulate the data as it is loaded prior to processing the FFT, or stored once the FFT has been done. For the load callback, cuFFT passes the callback routine the address of the input data and the offset to the value to be loaded from device memory, and the callback routine returns the value it wishes cuFFT to use instead. For the store callback, cuFFT passes the callback routine the value it has computed, along with the address of the output data and the offset to the value to be written to device memory, and the callback routine modifies the value and stores the modified result. 3. cuFFT API Reference  This chapter specifies the behavior of the cuFFT library functions by describing their input/output parameters, data types, and error codes. The cuFFT library is initialized upon the first invocation of an API function, and cuFFT shuts down automatically when all user-created FFT plans are destroyed. 3.1. Return value cufftResult  All cuFFT Library return values except for CUFFT_SUCCESS indicate that the current API call failed and the user should reconfigure to correct the problem. The possible return values are defined as follows: typedef enum cufftResult_t { CUFFT_SUCCESS = 0 , //  The cuFFT operation was successful CUFFT_INVALID_PLAN = 1 , //  cuFFT was passed an invalid plan handle CUFFT_ALLOC_FAILED = 2 , //  cuFFT failed to allocate GPU or CPU memory CUFFT_INVALID_TYPE = 3 , //  No longer used CUFFT_INVALID_VALUE = 4 , //  User specified an invalid pointer or parameter CUFFT_INTERNAL_ERROR = 5 , //  Driver or internal cuFFT library error CUFFT_EXEC_FAILED = 6 , //  Failed to execute an FFT on the GPU CUFFT_SETUP_FAILED = 7 , //  The cuFFT library failed to initialize CUFFT_INVALID_SIZE = 8 , //  User specified an invalid transform size CUFFT_UNALIGNED_DATA = 9 , //  No longer used CUFFT_INCOMPLETE_PARAMETER_LIST = 10 , //  Missing parameters in call CUFFT_INVALID_DEVICE = 11 , //  Execution of a plan was on different GPU than plan creation CUFFT_PARSE_ERROR = 12 , //  Internal plan database error CUFFT_NO_WORKSPACE = 13 //  No workspace has been provided prior to plan execution CUFFT_NOT_IMPLEMENTED = 14 , // Function does not implement functionality for parameters given. CUFFT_LICENSE_ERROR = 15 , // Used in previous versions. CUFFT_NOT_SUPPORTED = 16 // Operation is not supported for parameters given. } cufftResult ; Users are encouraged to check return values from cuFFT functions for errors as shown in cuFFT Code Examples . 3.2. cuFFT Basic Plans  3.2.1. cufftPlan1d()  cufftResult cufftPlan1d ( cufftHandle * plan , int nx , cufftType type , int batch ) ;  Creates a 1D FFT plan configuration for a specified signal size and data type. The batch input parameter tells cuFFT how many 1D transforms to configure. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. nx[In] – The transform size (e.g. 256 for a 256-point FFT). type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). batch[In] – Number of transforms of size nx . Please consider using cufftPlanMany for multiple transforms. plan[Out] – Contains a cuFFT 1D plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx or batch parameter is not a supported size. 3.2.2. cufftPlan2d()  cufftResult cufftPlan2d ( cufftHandle * plan , int nx , int ny , cufftType type ) ;  Creates a 2D FFT plan configuration according to specified signal sizes and data type. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. nx[In] – The transform size in the x dimension This is slowest changing dimension of a transform (strided in memory). ny[In] – The transform size in the y dimension. This is fastest changing dimension of a transform (contiguous in memory). type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). plan[Out] – Contains a cuFFT 2D plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.2.3. cufftPlan3d()  cufftResult cufftPlan3d ( cufftHandle * plan , int nx , int ny , int nz , cufftType type ) ;  Creates a 3D FFT plan configuration according to specified signal sizes and data type. This function is the same as cufftPlan2d() except that it takes a third size parameter nz . This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. nx[In] – The transform size in the x dimension. This is slowest changing dimension of a transform (strided in memory). ny[In] – The transform size in the y dimension. nz[In] – The transform size in the z dimension. This is fastest changing dimension of a transform (contiguous in memory). type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). plan[Out] – Contains a cuFFT 3D plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.2.4. cufftPlanMany()  cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ) ;  Creates a FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. The cufftPlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. All arrays are assumed to be in CPU memory. Please note that behavior of cufftPlanMany function when inembed and onembed is NULL is different than corresponding function in FFTW library fftw_plan_many_dft . This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. plan[Out] – Contains a cuFFT plan handle. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3. cuFFT Extensible Plans  This API separates handle creation from plan generation. This makes it possible to change plan settings, which may alter the outcome of the plan generation phase, before the plan is actually generated. 3.3.1. cufftCreate()  cufftResult cufftCreate ( cufftHandle * plan )  Creates only an opaque handle, and allocates small data structures on the host. The cufftMakePlan*() calls actually do the plan generation. Parameters plan[In] – Pointer to a cufftHandle object. plan[Out] – Contains a cuFFT plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_ALLOC_FAILED – The allocation of resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.3.2. cufftDestroy()  cufftResult cufftDestroy ( cufftHandle plan )  Frees all GPU resources associated with a cuFFT plan and destroys the internal plan data structure. This function should be called once a plan is no longer needed, to avoid wasting GPU memory.\nIn the case of multi-GPU plans, the plan created first should be destroyed last. Parameters plan[In] – The cufftHandle object of the plan to be destroyed. Return values CUFFT_SUCCESS – cuFFT successfully destroyed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. 3.3.3. cufftMakePlan1d()  cufftResult cufftMakePlan1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a 1D FFT plan configuration for a specified signal size and data type. The batch input parameter tells cuFFT how many 1D transforms to configure. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size (e.g. 256 for a 256-point FFT). For multiple GPUs, this must be a power of 2. type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). For multiple GPUs this must be a complex to complex transform. batch[In] – Number of transforms of size nx . Please consider using cufftMakePlanMany for multiple transforms. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked or multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED` – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx or batch parameter is not a supported size. 3.3.4. cufftMakePlan2d()  cufftResult cufftMakePlan2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 2D FFT plan configuration according to specified signal sizes and data type. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension. This is slowest changing dimension of a transform (strided in memory). For multiple GPUs, this must be factorable into primes less than or equal to 127. ny[In] – The transform size in the y dimension. This is fastest changing dimension of a transform (contiguous in memory). For 2 GPUs, this must be factorable into primes less than or equal to 127. type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.3.5. cufftMakePlan3d()  cufftResult cufftMakePlan3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 3D FFT plan configuration according to specified signal sizes and data type. This function is the same as cufftPlan2d() except that it takes a third size parameter nz . This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension. This is slowest changing dimension of a transform (strided in memory). For multiple GPUs, this must be factorable into primes less than or equal to 127. ny[In] – The transform size in the y dimension. For multiple GPUs, this must be factorable into primes less than or equal to 127. nz[In] – The transform size in the z dimension. This is fastest changing dimension of a transform (contiguous in memory). For multiple GPUs, this must be factorable into primes less than or equal to 127. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work area(s). Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.3.6. cufftMakePlanMany()  cufftResult cufftMakePlanMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. The cufftPlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. All arrays are assumed to be in CPU memory. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3) n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform. For multiple GPUs and rank equal to 1, the sizes must be a power of 2. For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory, inembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). For 2 GPUs this must be a complex to complex transform. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked or multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3.7. cufftMakePlanMany64()  cufftResult cufftMakePlanMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. This API is identical to cufftMakePlanMany except that the arguments specifying sizes and strides are 64 bit integers. This API makes very large transforms possible. cuFFT includes kernels that use 32 bit indexes, and kernels that use 64 bit indexes. cuFFT planning selects 32 bit kernels whenever possible to avoid any overhead due to 64 bit arithmetic. All sizes and types of transform are supported by this interface, with two exceptions. For transforms whose size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127. For real to complex and complex to real transforms whose size exceeds 4G elements, the fastest changing dimension must be even. The cufftPlanMany64() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. All arrays are assumed to be in CPU memory. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. For multiple GPUs and rank equal to 1, the sizes must be a power of 2. For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). For 2 GPUs this must be a complex to complex transform. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked or multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3.8. cufftXtMakePlanMany()  cufftResult cufftXtMakePlanMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  Following a call to cufftCreate() makes an FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. Type specifiers inputtype , outputtype and executiontype dictate type and precision of transform to be performed. Not all combinations of parameters are supported. Currently all three parameters need to match precision. Parameters inputtype and outputtype need to match transform type complex-to-complex, real-to-complex or complex-to-real. Parameter executiontype needs to match precision and be of a complex type. Example: for a half-precision real-to-complex transform, parameters inputtype , outputtype and executiontype would have values of CUDA_R_16F , CUDA_C_16F and CUDA_C_16F respectively. Similarly, a bfloat16 complex-to-real transform would use CUDA_C_16BF for inputtype and executiontype , and CUDA_R_16BF for outputtype . The cufftXtMakePlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. All arrays are assumed to be in CPU memory. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform. For multiple GPUs and rank equal to 1, the sizes must be a power of 2. For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory, inembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. inputtype[In] – Type of input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. outputtype[In] – Type of output data. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. executiontype[In] – Type of data to be used for computations. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.4. cuFFT Plan Properties  Users can further customize cuFFT plans using plan properties. These properties can be set, queried and reset on a per-plan basis as needed, using the routines listed in this section. The current supported properties are listed below: Property Underlying Type Description Behavior NVFFT_PLAN_PROPERTY_INT64_PATIENT_JIT long long int Runtime LTO kernels are enabled when set to not-zero value. See Link-Time Optimized Kernels Runtime LTO kernles are disabled when set to zero (default) Can be set / reset before planning Cannot be set / reset after planning 3.4.1. cufftSetPlanPropertyInt64()  cufftResult cufftSetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , const long long int propertyValueInt64 ) ;  Associates a cuFFT plan with a property identified by the key property . The value for the property is given by value propertyValueInt64 , which is a signed long long integer. Parameters plan[In] – cufftHandle returned by cufftCreate . property[In] – The property identifier, of type cufftPlanProperty . propertyValueInt64[In] – Value to set for the property, a long long signed integer. Return values CUFFT_SUCCESS – cuFFT successfully set the property. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_NOT_SUPPORTED – The property is not supported, or it cannot be set at the time (e.g. some properties cannot be set after calling a planning routine for the plan, see cuFFT Plan Properties ). CUFFT_INVALID_VALUE – Invalid property or value with which to set the property 3.4.2. cufftGetPlanPropertyInt64()  cufftResult cufftGetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , long long int * propertyValueInt64 ) ;  Retrieves the property value identified by the key property associated with the cuFFT plan plan . The value for the property, which is a signed long long integer, is set in the address space pointed by propertyValueInt64 . Parameters plan[In] – cufftHandle returned by cufftCreate . property[In] – The property identifier, of type cufftPlanProperty . propertyValueInt64[In] – Pointer to the value to be set with the value of the property. Return values CUFFT_SUCCESS – cuFFT successfully retrieved the property value. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_NOT_SUPPORTED – The property is not supported. CUFFT_INVALID_VALUE – Invalid property, or pointer propertyValueInt64 is null 3.4.3. cufftResetPlanProperty()  cufftResult cufftResetPlanProperty ( cufftHandle plan , cufftProperty property ) ;  Resets the value of the property identified by the key property , associated with the cuFFT plan plan , to its default value. Parameters plan[In] – cufftHandle returned by cufftCreate . property[In] – The property identifier, of type cufftPlanProperty . Return values CUFFT_SUCCESS – cuFFT successfully reset the property value. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_NOT_SUPPORTED – The property is not supported for plan , or cannot be reset at present time (see Behavior column on cuFFT Plan Properties ). CUFFT_INVALID_VALUE – Invalid property 3.5. cuFFT Estimated Size of Work Area  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. The cufftEstimate*() calls return an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Some problem sizes require much more storage than others. In particular powers of 2 are very efficient in terms of temporary storage. Large prime numbers, however, use different algorithms and may need up to the eight times that of a similarly sized power of 2. These routines return estimated workSize values which may still be smaller than the actual values needed especially for values of n that are not multiples of powers of 2, 3, 5 and 7. More refined values are given by the cufftGetSize*() routines, but these values may still be conservative. 3.5.1. cufftEstimate1d()  cufftResult cufftEstimate1d ( int nx , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Parameters nx[In] – The transform size (e.g. 256 for a 256-point FFT). type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). batch[In] – Number of transforms of size nx . Please consider using cufftEstimateMany for multiple transforms. *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx parameter is not a supported size. 3.5.2. cufftEstimate2d()  cufftResult cufftEstimate2d ( int nx , int ny , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Parameters nx[In] – The transform size in the x dimension (number of rows). ny[In] – The transform size in the y dimension (number of columns). type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.5.3. cufftEstimate3d()  cufftResult cufftEstimate3d ( int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Parameters nx[In] – The transform size in the x dimension. ny[In] – The transform size in the y dimension. nz[In] – The transform size in the z dimension. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.5.4. cufftEstimateMany()  cufftResult cufftEstimateMany ( int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. The cufftEstimateMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . All arrays are assumed to be in CPU memory. Parameters rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.6. cuFFT Refined Estimated Size of Work Area  The cufftGetSize*() routines give a more accurate estimate of the work area size required for a plan than the cufftEstimate*() routines as they take into account any plan settings that may have been made. As discussed in the section cuFFT Estimated Size of Work Area , the workSize value(s) returned may be conservative especially for values of n that are not multiples of powers of 2, 3, 5 and 7. 3.6.1. cufftGetSize1d()  cufftResult cufftGetSize1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate1d() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size (e.g. 256 for a 256-point FFT). type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). batch[In] – Number of transforms of size nx . Please consider using cufftGetSizeMany for multiple transforms. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx parameter is not a supported size. 3.6.2. cufftGetSize2d()  cufftResult cufftGetSize2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate2d() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension (number of rows). ny[In] – The transform size in the y dimension (number of columns). type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.6.3. cufftGetSize3d()  cufftResult cufftGetSize3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate3d() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension. ny[In] – The transform size in the y dimension. nz[In] – The transform size in the z dimension. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.6.4. cufftGetSizeMany()  cufftResult cufftGetSizeMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.6.5. cufftGetSizeMany64()  cufftResult cufftGetSizeMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made. This API is identical to cufftMakePlanMany except that the arguments specifying sizes and strides are 64 bit integers. This API makes very large transforms possible. cuFFT includes kernels that use 32 bit indexes, and kernels that use 64 bit indexes. cuFFT planning selects 32 bit kernels whenever possible to avoid any overhead due to 64 bit arithmetic. All sizes and types of transform are supported by this interface, with two exceptions. For transforms whose total size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127. For real to complex and complex to real transforms whose total size exceeds 4G elements, the fastest changing dimension must be even. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.6.6. cufftXtGetSizeMany()  cufftResult cufftXtGetSizeMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters that match signature of cufftXtMakePlanMany function, and taking into account any plan settings that may have been made. For more information about valid combinations of inputtype , outputtype and executiontype parameters please refer to documentation of cufftXtMakePlanMany function. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. inputtype[In] ( cudaDataType ) – Type of input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. outputtype[In] ( cudaDataType ) – Type of output data. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. executiontype[In] ( cudaDataType ) – Type of data to be used for computations. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.7. cufftGetSize()  cufftResult cufftGetSize ( cufftHandle plan , size_t * workSize ) ;  Once plan generation has been done, either with the original API or the extensible API, this call returns the actual size of the work area required to support the plan. Callers who choose to manage work area allocation within their application must use this call after plan generation, and after any cufftSet*() calls subsequent to plan generation, if those calls might alter the required work space size. Parameters plan[In] – cufftHandle returned by cufftCreate . *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.8. cuFFT Caller Allocated Work Area Support  3.8.1. cufftSetAutoAllocation()  cufftResult cufftSetAutoAllocation ( cufftHandle plan , int autoAllocate ) ;  cufftSetAutoAllocation() indicates that the caller intends to allocate and manage work areas for plans that have been generated. cuFFT default behavior is to allocate the work area at plan generation time. If cufftSetAutoAllocation() has been called with autoAllocate set to 0 (“false”) prior to one of the cufftMakePlan*() calls, cuFFT does not allocate the work area. This is the preferred sequence for callers wishing to manage work area allocation. Parameters plan[In] – cufftHandle returned by cufftCreate . autoAllocate[In] – Indicates whether to allocate work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.8.2. cufftSetWorkArea()  cufftResult cufftSetWorkArea ( cufftHandle plan , void * workArea ) ;  cufftSetWorkArea() overrides the work area pointer associated with a plan. If the work area was auto-allocated, cuFFT frees the auto-allocated space. The cufftExecute*() calls assume that the work area pointer is valid and that it points to a contiguous region in device memory that does not overlap with any other work area. If this is not the case, results are indeterminate. Parameters plan[In] – cufftHandle returned by cufftCreate . *workArea[In] – Pointer to workArea . For multiple GPUs, multiple work area pointers must be given. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.8.3. cufftXtSetWorkAreaPolicy()  cufftResult cufftXtSetWorkAreaPolicy ( cufftHandle plan , cufftXtWorkAreaPolicy policy , size_t * workSize ) ;  cufftXtSetWorkAreaPolicy() indicates that the caller intends to change work area size for a given plan handle. cuFFT’s default behavior is to allocate the work area at plan generation time with a default size that depends on the plan type and other parameters. If cufftXtSetWorkAreaPolicy() has been called with the policy parameter set to CUFFT_WORKAREA_MINIMAL , cuFFT will attempt to re-plan the handle to use zero bytes of work area memory. If the cufftXtSetWorkAreaPolicy() call is successful the auto-allocated work area memory is released. Currently the policies CUFFT_WORKAREA_PERFORMANCE , CUFFT_WORKAREA_USER and the workSize parameter are not supported and reserved for use in future cuFFT releases. This function can be called once per lifetime of a plan handle. Parameters plan[In] – cufftHandle returned by cufftCreate . policy[In] – Type of work area policy to apply. *workSize[In] – Reserved for future use. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_SIZE – FFT size does not allow use of the selected policy. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9. cuFFT Execution  3.9.1. cufftExecC2C() and cufftExecZ2Z()  cufftResult cufftExecC2C ( cufftHandle plan , cufftComplex * idata , cufftComplex * odata , int direction ) ;  cufftResult cufftExecZ2Z ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleComplex * odata , int direction ) ;  cufftExecC2C() ( cufftExecZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter. cuFFT uses the GPU memory pointed to by the idata parameter as input data. This function stores the Fourier coefficients in the odata array. If idata and odata are the same, this method does an in-place transform. Parameters plan[In] – cufftHandle returned by cufftCreate . idata[In] – Pointer to the complex input data (in GPU memory) to transform. odata[In] – Pointer to the complex output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . odata[Out] – ontains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata , odata , and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.2. cufftExecR2C() and cufftExecD2Z()  cufftResult cufftExecR2C ( cufftHandle plan , cufftReal * idata , cufftComplex * odata ) ;  cufftResult cufftExecD2Z ( cufftHandle plan , cufftDoubleReal * idata , cufftDoubleComplex * odata ) ;  cufftExecR2C() ( cufftExecD2Z() ) executes a single-precision (double-precision) real-to-complex, implicitly forward, cuFFT transform plan. cuFFT uses as input data the GPU memory pointed to by the idata parameter. This function stores the nonredundant Fourier coefficients in the odata array. Pointers to idata and odata are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex data type in double-precision transforms. If idata and odata are the same, this method does an in-place transform. Note the data layout differences between in-place and out-of-place transforms as described in Parameter cufftType . Parameters plan[In] – cufftHandle returned by cufftCreate . idata[In] – Pointer to the real input data (in GPU memory) to transform. odata[In] – Pointer to the complex output data (in GPU memory). odata[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata and odata is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.3. cufftExecC2R() and cufftExecZ2D()  cufftResult cufftExecC2R ( cufftHandle plan , cufftComplex * idata , cufftReal * odata ) ;  cufftResult cufftExecZ2D ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleReal * odata ) ;  cufftExecC2R() ( cufftExecZ2D() ) executes a single-precision (double-precision) complex-to-real, implicitly inverse, cuFFT transform plan. cuFFT uses as input data the GPU memory pointed to by the idata parameter. The input array holds only the nonredundant complex Fourier coefficients. This function stores the real output values in the odata array. and pointers are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex type in double-precision transforms. If idata and odata are the same, this method does an in-place transform. Parameters plan[In] – cufftHandle returned by cufftCreate . idata[In] – Pointer to the complex input data (in GPU memory) to transform. odata[In] – Pointer to the real output data (in GPU memory). odata[Out] – Contains the real output data. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata and odata is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.4. cufftXtExec()  cufftResult cufftXtExec ( cufftHandle plan , void * input , void * output , int direction ) ;  Function cufftXtExec executes any cuFFT transform regardless of precision and type. In case of complex-to-real and real-to-complex transforms direction parameter is ignored. cuFFT uses the GPU memory pointed to by the input parameter as input data. This function stores the Fourier coefficients in the output array. If input and output are the same, this method does an in-place transform. Parameters plan[In] – cufftHandle returned by cufftCreate . input[In] – Pointer to the input data (in GPU memory) to transform. output[In] – Pointer to the output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . Ignored for complex-to-real and real-to-complex transforms. output[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata , odata , and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.5. cufftXtExecDescriptor()  cufftResult cufftXtExecDescriptor ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  Function cufftXtExecDescriptor() executes any cuFFT transform regardless of precision and type. In case of complex-to-real and real-to-complex transforms direction parameter is ignored. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input descriptor as input data and cudaLibXtDesc *output as output data. Parameters plan[In] – cufftHandle returned by cufftCreate . input[In] – Pointer to the complex input data (in GPU memory) to transform. output[In] – Pointer to the complex output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . Ignored for complex-to-real and real-to-complex transforms. idata[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10. cuFFT and Multiple GPUs  3.10.1. cufftXtSetGPUs()  cufftResult cufftXtSetGPUs ( cufftHandle plan , int nGPUs , int * whichGPUs ) ;  cufftXtSetGPUs() identifies which GPUs are to be used with the plan. As in the single GPU case cufftCreate() creates a plan and cufftMakePlan*() does the plan generation. In cuFFT prior to 10.4.0, this call will return an error if a non-default stream has been associated with the plan. Note that the call to cufftXtSetGPUs() must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() . Parameter whichGPUs of cufftXtSetGPUs() function determines ordering of the GPUs with respect to data decomposition (first data chunk is placed on GPU denoted by first element of whichGPUs ). Parameters plan[In] – cufftHandle returned by cufftCreate . nGPUs[In] – Number of GPUs to use. whichGPUs[In] – The GPUs to use. Return values CUFFT_SUCCESS – cuFFT successfully set the GPUs to use. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_VALUE – The requested number of GPUs was less than 2 or more than 8. CUFFT_INVALID_DEVICE – An invalid GPU index was specified. CUFFT_INVALID_SIZE – Transform size that plan was created for does not meet minimum size criteria. 3.10.2. cufftXtSetWorkArea()  cufftResult cufftXtSetWorkArea ( cufftHandle plan , void * * workArea ) ;  cufftXtSetWorkArea() overrides the work areas associated with a plan. If the work area was auto-allocated, cuFFT frees the auto-allocated space. The cufftXtExec*() calls assume that the work area is valid and that it points to a contiguous region in each device memory that does not overlap with any other work area. If this is not the case, results are indeterminate. Parameters plan[In] – cufftHandle returned by cufftCreate . workArea[In] – Pointer to the pointers to workArea. Return values CUFFT_SUCCESS – cuFFT successfully set the GPUs to use. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – A GPU associated with the plan could not be selected. 3.10.3. cuFFT Multiple GPU Execution  3.10.3.1. cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z()  cufftResult cufftXtExecDescriptorC2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftResult cufftXtExecDescriptorZ2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftXtExecDescriptorC2C() ( cufftXtExecDescriptorZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input as input data. Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays. Parameters plan[In] – cufftHandle returned by cufftCreate . *input[In] – Pointer to the complex input data (in GPU memory) to transform. *output[In] – Pointer to the complex output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . input[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.3.2. cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z()  cufftResult cufftXtExecDescriptorR2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorD2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorR2C() ( cufftXtExecDescriptorD2Z() ) executes a single-precision (double-precision) real-to-complex transform plan. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input as input data. Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays. Parameters plan[In] – cufftHandle returned by cufftCreate . *input[In] – Pointer to the complex input data (in GPU memory) to transform. *output[In] – Pointer to the complex output data (in GPU memory). input[Out] – Contains the complex Fourier coefficients Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.3.3. cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D()  cufftResult cufftXtExecDescriptorC2R ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorZ2D ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorC2R() ( cufftXtExecDescriptorZ2D() ) executes a single-precision (double-precision) complex-to-real transform plan in the transform direction as specified by direction parameter. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input as input data. Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays. Parameters plan[In] – cufftHandle returned by cufftCreate . *input[In] – Pointer to the complex input data (in GPU memory) to transform. *output[In] – Pointer to the complex output data (in GPU memory). input[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.4. Memory Allocation and Data Movement Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution. The following functions assist in allocation, setup and retrieval of the data. They must be called after the call to cufftMakePlan*() . 3.10.4.1. cufftXtMalloc()  cufftResult cufftXtMalloc ( cufftHandle plan , cudaLibXtDesc * * descriptor , cufftXtSubFormat format ) ;  cufftXtMalloc() allocates a descriptor, and all memory for data in GPUs associated with the plan, and returns a pointer to the descriptor. Note the descriptor contains an array of device pointers so that the application may preprocess or postprocess the data on the GPUs. The enumerated parameter cufftXtSubFormat_t indicates if the buffer will be used for input or output. Parameters plan[In] – cufftHandle returned by cufftCreate . **descriptor[In] – Pointer to a pointer to a cudaLibXtDesc object. format[In] – cufftXtSubFormat`` value. **descriptor[Out] – Pointer to a pointer to a cudaLibXtDesc object. Return values CUFFT_SUCCESS – cuFFT successfully allows user to allocate descriptor and GPU memory. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle or it is not a multiple GPU plan . CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in the descriptor. 3.10.4.1.1. Parameter cufftXtSubFormat  cufftXtSubFormat_t is an enumerated type that indicates if the buffer will be used for input or output and the ordering of the data. typedef enum cufftXtSubFormat_t { CUFFT_XT_FORMAT_INPUT , //by default input is in linear order across GPUs CUFFT_XT_FORMAT_OUTPUT , //by default output is in scrambled order depending on transform CUFFT_XT_FORMAT_INPLACE , //by default inplace is input order, which is linear across GPUs CUFFT_XT_FORMAT_INPLACE_SHUFFLED , //shuffled output order after execution of the transform CUFFT_FORMAT_UNDEFINED } cufftXtSubFormat ; 3.10.4.2. cufftXtFree()  cufftResult cufftXtFree ( cudaLibXtDesc * descriptor ) ;  cufftXtFree() frees the descriptor and all memory associated with it. The descriptor and memory must have been returned by a previous call to cufftXtMalloc() . Parameters *descriptor[In] – Pointer to a cudaLibXtDesc object. Return values CUFFT_SUCCESS – cuFFT successfully allows user to free descriptor and associated GPU memory. CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.10.4.3. cufftXtMemcpy()  cufftResult cufftXtMemcpy ( cufftHandle plan , void * dstPointer , void * srcPointer , cufftXtCopyType type ) ;  cufftXtMemcpy() copies data between buffers on the host and GPUs or between GPUs. The enumerated parameter cufftXtCopyType_t indicates the type and direction of transfer. Calling cufftXtMemcpy function for multi-GPU batched FFT plans with CUFFT_COPY_DEVICE_TO_DEVICE transfer type is not supported. Note that starting from CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported on multi-GPU plans. When associating a stream with a plan, cufftXtMemcpy() remains synchronous across the multiple GPUs. Parameters plan[In] – cufftHandle returned by cufftCreate . dstPointer[In] – Pointer to the destination address(es). srcPointer[In] – Pointer to the source address(es). type[In] – cufftXtCopyType value. Return values CUFFT_SUCCESS – cuFFT successfully allows user to copy memory between host and GPUs or between GPUs. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.4.3.1. Parameter cufftXtCopyType  cufftXtCopyType_t is an enumerated type for multiple GPU functions that specifies the type of copy for cufftXtMemcpy() . CUFFT_COPY_HOST_TO_DEVICE copies data from a contiguous host buffer to multiple device buffers, in the layout cuFFT requires for input data. dstPointer must point to a cudaLibXtDesc structure, and srcPointer must point to a host memory buffer. CUFFT_COPY_DEVICE_TO_HOST copies data from multiple device buffers, in the layout cuFFT produces for output data, to a contiguous host buffer. dstPointer must point to a host memory buffer, and srcPointer must point to a cudaLibXtDesc structure. CUFFT_COPY_DEVICE_TO_DEVICE copies data from multiple device buffers, in the layout cuFFT produces for output data, to multiple device buffers, in the layout cuFFT requires for input data. dstPointer and srcPointer must point to different cudaLibXtDesc structures (and therefore memory locations). That is, the copy cannot be in-place. Note that device_to_device cufftXtMemcpy() for 2D and 3D data is not currently supported. typedef enum cufftXtCopyType_t { CUFFT_COPY_HOST_TO_DEVICE , CUFFT_COPY_DEVICE_TO_HOST , CUFFT_COPY_DEVICE_TO_DEVICE } cufftXtCopyType ; 3.10.5. General Multiple GPU Descriptor Types  3.10.5.1. cudaXtDesc  A descriptor type used in multiple GPU routines that contains information about the GPUs and their memory locations. struct cudaXtDesc_t { int version ; //descriptor version int nGPUs ; //number of GPUs int GPUs [ MAX_CUDA_DESCRIPTOR_GPUS ]; //array of device IDs void * data [ MAX_CUDA_DESCRIPTOR_GPUS ]; //array of pointers to data, one per GPU size_t size [ MAX_CUDA_DESCRIPTOR_GPUS ]; //array of data sizes, one per GPU void * cudaXtState ; //opaque CUDA utility structure }; typedef struct cudaXtDesc_t cudaXtDesc ; 3.10.5.2. cudaLibXtDesc  A descriptor type used in multiple GPU routines that contains information about the library used. struct cudaLibXtDesc_t { int version ; //descriptor version cudaXtDesc * descriptor ; //multi-GPU memory descriptor libFormat library ; //which library recognizes the format int subFormat ; //library specific enumerator of sub formats void * libDescriptor ; //library specific descriptor e.g. FFT transform plan object }; typedef struct cudaLibXtDesc_t cudaLibXtDesc ; 3.11. cuFFT Callbacks  3.11.1. cufftXtSetCallback()  cufftResult cufftXtSetCallback ( cufftHandle plan , void * * callbackRoutine , cufftXtCallbackType type , void * * callerInfo )  cufftXtSetCallback() specifies a load or store callback to be used with the plan. This call is valid only after a call to cufftMakePlan*() , which does the plan generation. If there was already a callback of this type associated with the plan, this new callback routine replaces it. If the new callback requires shared memory, you must call cufftXtSetCallbackSharedSize with the amount of shared memory it needs. cuFFT will not retain the amount of shared memory associated with the previous callback. Parameters plan[In] – cufftHandle returned by cufftCreate . callbackRoutine[In] – Array of callback routine pointers, one per GPU. type[In] – Type of callback routine. callerInfo[In] – Optional array of device pointers to caller specific information, one per GPU. Return values CUFFT_SUCCESS – cuFFT successfully associated the callback function with the plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.11.2. cufftXtClearCallback()  cufftResult cufftXtClearCallback ( cufftHandle plan , cufftXtCallbackType type )  cufftXtClearCallback() instructs cuFFT to stop invoking the specified callback type when executing the plan. Only the specified callback is cleared. If no callback of this type had been specified, the return code is CUFFT_SUCCESS . Parameters plan[In] – cufftHandle returned by cufftCreate . type[In] – Type of callback routine. Return values CUFFT_SUCCESS – cuFFT successfully disassociated the callback function with the plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.11.3. cufftXtSetCallbackSharedSize()  cufftResult cufftXtSetCallbackSharedSize ( cufftHandle plan , cufftXtCallbackType type , size_t sharedSize )  cufftXtSetCallbackSharedSize() instructs cuFFT to dynamically allocate shared memory at launch time, for use by the callback. The maximum allowable amount of shared memory is 16K bytes. cuFFT passes a pointer to this shared memory to the callback routine at execution time. This shared memory is only valid for the life of the load or store callback operation. During execution, cuFFT may overwrite shared memory for its own purposes. Parameters plan[In] – cufftHandle returned by cufftCreate . type[In] – Type of callback routine. sharedSize[In] – Amount of shared memory requested. Return values CUFFT_SUCCESS – cuFFT will invoke the callback routine with a pointer to the requested amount of shared memory. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_ALLOC_FAILED – cuFFT will not be able to allocate the requested amount of shared memory. 3.12. cufftSetStream()  cufftResult cufftSetStream ( cufftHandle plan , cudaStream_t stream ) ;  Associates a CUDA stream with a cuFFT plan. All kernel launches made during plan execution are now done through the associated stream, enabling overlap with activity in other streams (e.g. data copying). The association remains until the plan is destroyed or the stream is changed with another call to cufftSetStream() . Note that starting from CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported on multi-GPU plans. When associating a stream with a plan, cufftXtMemcpy() remains synchronous across the multiple GPUs. For previous versions of cuFFT, cufftSetStream() will return an error in multiple GPU plans. Note that starting from CUDA 12.2 (cuFFT 11.0.8), on multi-GPU plans, stream can be associated with any context on any GPU. However, repeated calls to cufftSetStream() with streams from different contexts incur a small time penalty. Optimal performance is obtained when repeated calls to cufftSetStream use streams from the same CUDA context. Parameters plan[In] – The cufftHandle object to associate with the stream. stream[In] – A valid CUDA stream created with cudaStreamCreate() ; 0 for the default stream. Return values CUFFT_SUCCESS – The stream was associated with the plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or plan is multi-gpu in cuFFT version prior to 10.4.0. 3.13. cufftGetVersion()  cufftResult cufftGetVersion ( int * version ) ;  Returns the version number of cuFFT. Parameters *version[In] – Pointer to the version number. *version[Out] – Contains the version number. Return values CUFFT_SUCCESS – cuFFT successfully returned the version number. 3.14. cufftGetProperty()  cufftResult cufftGetProperty ( libraryPropertyType type , int * value ) ;  Return in *value the number for the property described by type of the dynamically linked CUFFT library. Parameters type[In] – CUDA library property. value[Out] – Contains the integer value for the requested property. Return values CUFFT_SUCCESS – The property value was successfully returned. CUFFT_INVALID_TYPE – The property type is not recognized. CUFFT_INVALID_VALUE – value is NULL . 3.15. cuFFT Types  3.15.1. Parameter cufftType  The cuFFT library supports complex- and real-data transforms. The cufftType data type is an enumeration of the types of transform data supported by cuFFT. typedef enum cufftType_t { CUFFT_R2C = 0x2a , // Real to complex (interleaved) CUFFT_C2R = 0x2c , // Complex (interleaved) to real CUFFT_C2C = 0x29 , // Complex to complex (interleaved) CUFFT_D2Z = 0x6a , // Double to double-complex (interleaved) CUFFT_Z2D = 0x6c , // Double-complex (interleaved) to double CUFFT_Z2Z = 0x69 // Double-complex to double-complex (interleaved) } cufftType ; 3.15.2. Parameters for Transform Direction  The cuFFT library defines forward and inverse Fast Fourier Transforms according to the sign of the complex exponential term. #define cuFFTFORWARD -1 #define cuFFTINVERSE 1 cuFFT performs un-normalized FFTs; that is, performing a forward FFT on an input data set followed by an inverse FFT on the resulting set yields data that is equal to the input, scaled by the number of elements. Scaling either transform by the reciprocal of the size of the data set is left for the user to perform as seen fit. 3.15.3. Type definitions for callbacks  The cuFFT library supports callback funtions for all combinations of single or double precision, real or complex data, load or store. These are enumerated in the parameter cufftXtCallbackType . typedef enum cufftXtCallbackType_t { CUFFT_CB_LD_COMPLEX = 0x0 , CUFFT_CB_LD_COMPLEX_DOUBLE = 0x1 , CUFFT_CB_LD_REAL = 0x2 , CUFFT_CB_LD_REAL_DOUBLE = 0x3 , CUFFT_CB_ST_COMPLEX = 0x4 , CUFFT_CB_ST_COMPLEX_DOUBLE = 0x5 , CUFFT_CB_ST_REAL = 0x6 , CUFFT_CB_ST_REAL_DOUBLE = 0x7 , CUFFT_CB_UNDEFINED = 0x8 } cufftXtCallbackType ; The corresponding function prototypes and pointer type definitions are as follows: typedef cufftComplex ( * cufftCallbackLoadC )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleComplex ( * cufftCallbackLoadZ )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftReal ( * cufftCallbackLoadR )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleReal ( * cufftCallbackLoadD )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreC )( void * dataOut , size_t offset , cufftComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreZ )( void * dataOut , size_t offset , cufftDoubleComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreR )( void * dataOut , size_t offset , cufftReal element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreD )( void * dataOut , size_t offset , cufftDoubleReal element , void * callerInfo , void * sharedPointer ); 3.15.4. Other cuFFT Types  3.15.4.1. cufftHandle  type cufftHandle  A handle type used to store and access cuFFT plans. The user receives a handle after creating a cuFFT plan and uses this handle to execute the plan. typedef unsigned int cufftHandle ; 3.15.4.2. cufftReal  A single-precision, floating-point real data type. typedef float cufftReal ; 3.15.4.3. cufftDoubleReal  A double-precision, floating-point real data type. typedef double cufftDoubleReal ; 3.15.4.4. cufftComplex  A single-precision, floating-point complex data type that consists of interleaved real and imaginary components. typedef cuComplex cufftComplex ; 3.15.4.5. cufftDoubleComplex  A double-precision, floating-point complex data type that consists of interleaved real and imaginary components. typedef cuDoubleComplex cufftDoubleComplex ; 3.16. Common types  3.16.1. cudaDataType  The cudaDataType data type is an enumeration of the types supported by CUDA libraries. typedef enum cudaDataType_t { CUDA_R_16F = 2 , // 16 bit real CUDA_C_16F = 6 , // 16 bit complex CUDA_R_32F = 0 , // 32 bit real CUDA_C_32F = 4 , // 32 bit complex CUDA_R_64F = 1 , // 64 bit real CUDA_C_64F = 5 , // 64 bit complex CUDA_R_8I = 3 , // 8 bit real as a signed integer CUDA_C_8I = 7 , // 8 bit complex as a pair of signed integers CUDA_R_8U = 8 , // 8 bit real as an unsigned integer CUDA_C_8U = 9 // 8 bit complex as a pair of unsigned integers } cudaDataType ; 3.16.2. libraryPropertyType  The libraryPropertyType data type is an enumeration of library property types. (ie. CUDA version X.Y.Z would yield MAJOR_VERSION=X , MINOR_VERSION=Y , PATCH_LEVEL=Z ) typedef enum libraryPropertyType_t { MAJOR_VERSION , MINOR_VERSION , PATCH_LEVEL } libraryPropertyType ; 4. cuFFT Code Examples  For simple examples of complex and real 1D, 2D, and 3D transforms that use cuFFT to perform forward and inverse FFTs, refer to the cuFFT Library samples on GitHub . 5. Multiple GPU Data Organization  This chapter explains how data are distributed between the GPUs, before and after a multiple GPU transform. For simplicity, it is assumed in this chapter that the caller has specified GPU 0 and GPU 1 to perform the transform. 5.1. Multiple GPU Data Organization for Batched Transforms  For batches of transforms, each individual transform is executed on a single GPU. If possible the batches are evenly distributed among the GPUs. For a batch of size m performed on n GPUs, where m is not divisible by n , the first m % n GPUs will perform \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor+\\ 1\\) transforms. The remaining GPUs will perform \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor\\) transforms. For example, in a batch of 15 transforms performed on 4 GPUs, the first three GPUs would perform 4 transforms, and the last GPU would perform 3 transforms. This approach removes the need for data exchange between the GPUs, and results in nearly perfect scaling for cases where the batch size is divisible by the number of GPUs. 5.2. Multiple GPU Data Organization for Single 2D and 3D Transforms  Single transforms performed on multiple GPUs require the data to be divided between the GPUs. Then execution takes place in phases. For example with 2 GPUs, for 2D and 3D transforms with even sized dimensions, each GPU does half of the transform in (rank - 1) dimensions. Then data are exchanged between the GPUs so that the final dimension can be processed. Since 2D and 3D transforms support sizes other than powers of 2, it is possible that the data can not be evenly distributed among the GPUs. In general for the case of n GPUs, a dimension of size m that is not a multiple of n would be distributed such that the first m % n GPUs would get one extra row for 2D transforms, one extra plane for 3D transforms. Take for example, a 2D transform on 4 GPUs, using an array declared in C as data[x][y] , where x is 65 and y is 99. The surface is distributed prior to the transform such that GPU 0 receives a surface with dimensions [17][99] , and GPUs 1…3 receive surfaces with dimensions [16][99] . After the transform, each GPU again has a portion of the surface, but divided in the y dimension. GPUs 0…2 have surfaces with dimensions [65][25] . GPU 3 has a surface with dimensions [65][24] For a 3D transform on 4 GPUs consider an array declared in C as data[x][y][z] , where x is 103, y is 122, and z is 64. The volume is distributed prior to the transform such that each GPUs 0…2 receive volumes with dimensions [26][122][64] , and GPU 3 receives a volume with dimensions [25][122][64] . After the transform, each GPU again has a portion of the surface, but divided in the y dimension. GPUs 0 and 1 have a volumes with dimensions [103][31][64] , and GPUs 2 and 3 have volumes with dimensions [103][30][64] . 5.3. Multiple-GPU Data Organization for Single 1D Transforms  By default for 1D transforms, the initial distribution of data to the GPUs is similar to the 2D and 3D cases. For a transform of dimension x on two GPUs, GPU 0 receives data ranging from 0…(x/2-1). GPU 1 receives data ranging from (x/2)…(x-1). Similarly, with 4 GPUs, the data are evenly distributed among all 4 GPUs. Before computation can begin, data are redistributed among the GPUs. It is possible to perform this redistribution in the copy from host memory, in cases where the application does not need to pre-process the data prior to the transform. To do this, the application can create the data descriptor with cufftXtMalloc using the sub-format CUFFT_XT_FORMAT_1D_INPUT_SHUFFLED . This can significantly reduce the time it takes to execute the transform. cuFFT performs multiple GPU 1D transforms by decomposing the transform size into factors Factor1 and Factor2 , and treating the data as a grid of size Factor1 x Factor2 . The four steps done to calculate the 1D FFT are: Factor1 transforms of size Factor2 , data exchange between the GPUs, a pointwise twiddle multiplication, and Factor2 transforms of size Factor1 . To gain efficiency by overlapping computation with data exchange, cuFFT breaks the whole transform into independent segments or strings, which can be processed while others are in flight. A side effect of this algorithm is that the output of the transform is not in linear order. The output in GPU memory is in strings, each of which is composed of Factor2 substrings of equal size. Each substring contains contiguous results starting Factor1 elements subsequent to start of the previous substring. Each string starts substring size elements after the start of the previous string. The strings appear in order, the first half on GPU 0, and the second half on GPU 1. See the example below: transform size = 1024 number of strings = 8 Factor1 = 64 Factor2 = 16 substrings per string for output layout is Factor2 ( 16 ) string size = 1024 / 8 = 128 substring size = 128 / 16 = 8 stride between substrings = 1024 / 16 = Factor1 ( 64 ) On GPU 0 : string 0 has substrings with indices 0. . .7 64. . .71 128. . .135 ... 960. . .967 string 1 has substrings with indices 8. . .15 72. . .79 136. . .143 ... 968. . .975 ... On GPU 1 : string 4 has substrings with indices 32. . .39 96. . .103 160. . .167 ... 992. . .999 ... string 7 has substrings with indices 56. . .63 120. . .127 184. . .191 ... 1016. . .1023 The cufftXtQueryPlan API allows the caller to retrieve a structure containing the number of strings, the decomposition factors, and (in the case of power of 2 size) some useful mask and shift elements. The example below shows how cufftXtQueryPlan is invoked. It also shows how to translate from an index in the host input array to the corresponding index on the device, and vice versa. /* * These routines demonstrate the use of cufftXtQueryPlan to get the 1D * factorization and convert between permuted and linear indexes. */ /* * Set up a 1D plan that will execute on GPU 0 and GPU1, and query * the decomposition factors */ int main ( int argc , char ** argv ){ cufftHandle plan ; cufftResult stat ; int whichGPUs [ 2 ] = { 0 , 1 }; cufftXt1dFactors factors ; stat = cufftCreate ( & plan ); if ( stat != CUFFT_SUCCESS ) { printf ( \"Create error %d \\n \" , stat ); return 1 ; } stat = cufftXtSetGPUs ( plan , 2 , whichGPUs ); if ( stat != CUFFT_SUCCESS ) { printf ( \"SetGPU error %d \\n \" , stat ); return 1 ; } stat = cufftMakePlan1d ( plan , size , CUFFT_C2C , 1 , workSizes ); if ( stat != CUFFT_SUCCESS ) { printf ( \"MakePlan error %d \\n \" , stat ); return 1 ; } stat = cufftXtQueryPlan ( plan , ( void * ) & factors , CUFFT_QUERY_1D_FACTORS ); if ( stat != CUFFT_SUCCESS ) { printf ( \"QueryPlan error %d \\n \" , stat ); return 1 ; } printf ( \"Factor 1 %zd, Factor2 %zd \\n \" , factors . factor1 , factors . factor2 ); cufftDestroy ( plan ); return 0 ; } /* * Given an index into a permuted array, and the GPU index return the * corresponding linear index from the beginning of the input buffer. * * Parameters: *      factors     input:  pointer to cufftXt1dFactors as returned by *                          cufftXtQueryPlan *      permutedIx  input:  index of the desired element in the device output *                          array *      linearIx    output: index of the corresponding input element in the *                          host array *      GPUix       input:  index of the GPU containing the desired element */ cufftResult permuted2Linear ( cufftXt1dFactors * factors , size_t permutedIx , size_t * linearIx , int GPUIx ) { size_t indexInSubstring ; size_t whichString ; size_t whichSubstring ; // the low order bits of the permuted index match those of the linear index indexInSubstring = permutedIx & factors -> substringMask ; // the next higher bits are the substring index whichSubstring = ( permutedIx >> factors -> substringShift ) & factors -> factor2Mask ; // the next higher bits are the string index on this GPU whichString = ( permutedIx >> factors -> stringShift ) & factors -> stringMask ; // now adjust the index for the second GPU if ( GPUIx ) { whichString += factors -> stringCount / 2 ; } // linear index low order bits are the same // next higher linear index bits are the string index * linearIx = indexInSubstring + ( whichString << factors -> substringShift ); // next higher bits of linear address are the substring index * linearIx += whichSubstring << factors -> factor1Shift ; return CUFFT_SUCCESS ; } /* * Given a linear index into a 1D array, return the GPU containing the permuted * result, and index from the start of the data buffer for that element. * * Parameters: *      factors     input:  pointer to cufftXt1dFactors as returned by *                          cufftXtQueryPlan *      linearIx    input:  index of the desired element in the host input *                          array *      permutedIx  output: index of the corresponding result in the device *                          output array *      GPUix       output: index of the GPU containing the result */ cufftResult linear2Permuted ( cufftXt1dFactors * factors , size_t linearIx , size_t * permutedIx , int * GPUIx ) { size_t indexInSubstring ; size_t whichString ; size_t whichSubstring ; size_t whichStringMask ; int whichStringShift ; if ( linearIx >= factors -> size ) { return CUFFT_INVALID_VALUE ; } // get a useful additional mask and shift count whichStringMask = factors -> stringCount -1 ; whichStringShift = ( factors -> factor1Shift + factors -> factor2Shift ) - factors -> stringShift ; // the low order bits identify the index within the substring indexInSubstring = linearIx & factors -> substringMask ; // first determine which string has our linear index. // the low order bits indentify the index within the substring. // the next higher order bits identify which string. whichString = ( linearIx >> factors -> substringShift ) & whichStringMask ; // the first stringCount/2 strings are in the first GPU, // the rest are in the second. * GPUIx = whichString / ( factors -> stringCount / 2 ); // next determine which substring within the string has our index // the substring index is in the next higher order bits of the index whichSubstring = ( linearIx >> ( factors -> substringShift + whichStringShift )) & factors -> factor2Mask ; // now we can re-assemble the index * permutedIx = indexInSubstring ; * permutedIx += whichSubstring << factors -> substringShift ; if ( !* GPUIx ) { * permutedIx += whichString << factors -> stringShift ; } else { * permutedIx += ( whichString - ( factors -> stringCount / 2 ) ) << factors -> stringShift ; } return CUFFT_SUCCESS ; } 6. FFTW Conversion Guide  cuFFT differs from FFTW in that FFTW has many plans and a single execute function while cuFFT has fewer plans, but multiple execute functions. The cuFFT execute functions determine the precision (single or double) and whether the input is complex or real valued. The following table shows the relationship between the two interfaces. FFTW function cuFFT function fftw_plan_dft_1d(), fftw_plan_dft_r2c_1d(), fftw_plan_dft_c2r_1d() cufftPlan1d() fftw_plan_dft_2d(), fftw_plan_dft_r2c_2d(), fftw_plan_dft_c2r_2d() cufftPlan2d() fftw_plan_dft_3d(), fftw_plan_dft_r2c_3d(), fftw_plan_dft_c2r_3d() cufftPlan3d() fftw_plan_dft(), fftw_plan_dft_r2c(), fftw_plan_dft_c2r() cufftPlanMany() fftw_plan_many_dft(), fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() cufftPlanMany() fftw_execute() cufftExecC2C(), cufftExecZ2Z(), cufftExecR2C(), cufftExecD2Z(), cufftExecC2R(), cufftExecZ2D() fftw_destroy_plan() cufftDestroy() 7. FFTW Interface to cuFFT  NVIDIA provides FFTW3 interfaces to the cuFFT library. This allows applications using FFTW to use NVIDIA GPUs with minimal modifications to program source code. To use the interface first do the following two steps It is recommended that you replace the include file fftw3.h with cufftw.h Instead of linking with the double/single precision libraries such as fftw3/fftw3f libraries, link with both the cuFFT and cuFFTW libraries Ensure the search path includes the directory containing cuda_runtime_api.h After an application is working using the FFTW3 interface, users may want to modify their code to move data to and from the GPU and use the routines documented in the FFTW Conversion Guide for the best performance. The following tables show which components and functions of FFTW3 are supported in cuFFT. Section in FFTW manual Supported Unsupported Complex numbers fftw_complex, fftwf_complex types Precision double fftw3 , single fftwf3 long double fftw3l , quad precision fftw3q are not supported since CUDA functions operate on double and single precision floating-point quantities Memory Allocation fftw_malloc(), fftw_free(), fftw_alloc_real(), fftw_alloc_complex(), fftwf_alloc_real(), fftwf_alloc_complex() Multi-threaded FFTW fftw3_threads, fftw3_omp are not supported Distributed-memory FFTW with MPI fftw3_mpi,fftw3f_mpi are not supported Note that for each of the double precision functions below there is a corresponding single precision version with the letters fftw replaced by fftwf . Section in FFTW manual Supported Unsupported Using Plans fftw_execute(), fftw_destroy_plan(), fftw_cleanup() fftw_print_plan(), fftw_cost(), fftw_flops() exist but are not functional Basic Interface Complex DFTs fftw_plan_dft_1d(), fftw_plan_dft_2d(), fftw_plan_dft_3d(), fftw_plan_dft() Planner Flags Planner flags are ignored and the same plan is returned regardless Real-data DFTs fftw_plan_dft_r2c_1d(), fftw_plan_dft_r2c_2d(), fftw_plan_dft_r2c_3d(), fftw_plan_dft_r2c(), fftw_plan_dft_c2r_1d(), fftw_plan_dft_c2r_2d(), fftw_plan_dft_c2r_3d(), fftw_plan_dft_c2r() Read-data DFT Array Format Not supported Read-to-Real Transform Not supported Read-to-Real Transform Kinds Not supported Advanced Interface Advanced Complex DFTs fftw_plan_many_dft() with multiple 1D, 2D, 3D transforms fftw_plan_many_dft() with 4D or higher transforms or a 2D or higher batch of embedded transforms Advanced Real-data DFTs fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() with multiple 1D, 2D, 3D transforms fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() with 4D or higher transforms or a 2D or higher batch of embedded transforms Advanced Real-to-Real Transforms Not supported Guru Interface Interleaved and split arrays Interleaved format Split format Guru vector and transform sizes fftw_iodim struct Guru Complex DFTs fftw_plan_guru_dft(), fftw_plan_guru_dft_r2c(), fftw_plan_guru_dft_c2r() with multiple 1D, 2D, 3D transforms fftw_plan_guru_dft(), fftw_plan_guru_dft_r2c(), fftw_plan_guru_dft_c2r() with 4D or higher transforms or a 2D or higher batch of transforms Guru Real-data DFTs Not supported Guru Real-to-real Transforms Not supported 64-bit Guru Interface fftw_plan_guru64_dft(), fftw_plan_guru64_dft_r2c(), fftw_plan_guru64_dft_c2r() with multiple 1D, 2D, 3D transforms fftw_plan_guru64_dft(), fftw_plan_guru64_dft_r2c(), fftw_plan_guru64_dft_c2r() with 4D or higher transforms or a 2D or higher batch of transforms New-array Execute Functions fftw_execute_dft(), fftw_execute_dft_r2c(), fftw_execute_dft_c2r() with interleaved format Split format and real-to-real functions Wisdom fftw_export_wisdom_to_file(), fftw_import_wisdom_from_file() exist but are not functional. Other wisdom functions do not have entry points in the library. 8. Deprecated Functionality  Starting from CUDA 12.0: GPU architectures SM35 and SM37 are no longer supported. The minimum required architecture is SM50. Starting from CUDA 11.8: CUDA Graphs capture is no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. Starting from CUDA 11.4: Support for callback functionality using separately compiled device code is deprecated on all GPU architectures. Callback functionality will continue to be supported for all GPU architectures. Starting from CUDA 11.0: GPU architecture SM30 is no longer supported. The minimum required architecture is SM35. Support for GPU architectures SM35, SM37 (Kepler), and SM50, SM52 (Maxwell) is deprecated. Function cufftSetCompatibilityMode was removed in version 9.1. 9. Notices  9.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 9.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 9.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cudla-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cudla-api/index.html", "content_type": "text/html", "text": "cuDLA API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 cuDLA API 1. Modules 1.1. Data types used by cuDLA driver 1.2. ModulesData StructuresData FieldsNotices 2. Data Structures 2.1. cudlaDevAttribute 2.2. cudlaExternalMemoryHandleDesc_t 2.3. cudlaExternalSemaphoreHandleDesc_t 2.4. CudlaFence 2.5. cudlaModuleAttribute 2.6. cudlaModuleTensorDescriptor 2.7. cudlaSignalEvents 2.8. cudlaTask 2.9. cudlaWaitEvents 3. Data Fields Search Results cuDLA API\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 1. Modules Here is a list of all modules: Data types used by cuDLA driver cuDLA API 1.1. Data types used by cuDLA driver Classes struct CudlaFence union cudlaDevAttribute struct cudlaExternalMemoryHandleDesc_t struct cudlaExternalSemaphoreHandleDesc_t union cudlaModuleAttribute struct cudlaModuleTensorDescriptor struct cudlaSignalEvents struct cudlaTask struct cudlaWaitEvents Typedefs typedef cudlaDevHandle_t * cudlaDevHandle typedef cudlaModule_t * cudlaModule Enumerations enum cudlaAccessPermissionFlags enum cudlaDevAttributeType enum cudlaFenceType enum cudlaMode enum cudlaModuleAttributeType enum cudlaModuleLoadFlags enum cudlaNvSciSyncAttributes enum cudlaStatus enum cudlaSubmissionFlags Typedefs typedef cudlaDevHandle_t *  cudlaDevHandle cuDLA Device Handle typedef cudlaModule_t *  cudlaModule cuDLA Module Handle Enumerations enum cudlaAccessPermissionFlags Access permission flags for importing NvSciBuffers Values CUDLA_READ_WRITE_PERM = 0 Flag to import memory with read-write permission CUDLA_READ_ONLY_PERM = 1 Flag to import memory with read-only permission CUDLA_TASK_STATISTICS = 1<<1 Flag to indicate buffer as layerwise statistics buffer. enum cudlaDevAttributeType Device attribute type. Values CUDLA_UNIFIED_ADDRESSING = 0 Flag to check for support for UVA. CUDLA_DEVICE_VERSION = 1 Flag to check for DLA HW version. enum cudlaFenceType Supported fence types. Values CUDLA_NVSCISYNC_FENCE = 1 NvSciSync fence type for EOF. CUDLA_NVSCISYNC_FENCE_SOF = 2 enum cudlaMode Device creation modes. Values CUDLA_CUDA_DLA = 0 Hyrbid mode. CUDLA_STANDALONE = 1 Standalone mode. enum cudlaModuleAttributeType Module attribute types. Values CUDLA_NUM_INPUT_TENSORS = 0 Flag to retrieve number of input tensors. CUDLA_NUM_OUTPUT_TENSORS = 1 Flag to retrieve number of output tensors. CUDLA_INPUT_TENSOR_DESCRIPTORS = 2 Flag to retrieve all the input tensor descriptors. CUDLA_OUTPUT_TENSOR_DESCRIPTORS = 3 Flag to retrieve all the output tensor descriptors. CUDLA_NUM_OUTPUT_TASK_STATISTICS = 4 Flag to retrieve total number of output task statistics buffer. CUDLA_OUTPUT_TASK_STATISTICS_DESCRIPTORS = 5 Flag to retrieve all the output task statistics descriptors. enum cudlaModuleLoadFlags Module load flags for cudlaModuleLoadFromMemory . Values CUDLA_MODULE_DEFAULT = 0 Default flag. CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS = 1 Flag to load a module that is used to perform permanent fault diagnostics for DLA HW. enum cudlaNvSciSyncAttributes cuDLA NvSciSync attributes. Values CUDLA_NVSCISYNC_ATTR_WAIT = 1 Wait attribute. CUDLA_NVSCISYNC_ATTR_SIGNAL = 2 Signal attribute. enum cudlaStatus Error codes. Values cudlaSuccess = 0 The API call returned with no errors. cudlaErrorInvalidParam = 1 This indicates that one or more parameters passed to the API is/are incorrect. cudlaErrorOutOfResources = 2 This indicates that the API call failed due to lack of underlying resources. cudlaErrorCreationFailed = 3 This indicates that an internal error occurred during creation of device handle. cudlaErrorInvalidAddress = 4 This indicates that the memory object being passed in the API call has not been registered before. cudlaErrorOs = 5 This indicates that an OS error occurred. cudlaErrorCuda = 6 This indicates that there was an error in a CUDA operation as part of the API call. cudlaErrorUmd = 7 This indicates that there was an error in the DLA runtime for the API call. cudlaErrorInvalidDevice = 8 This indicates that the device handle passed to the API call is invalid. cudlaErrorInvalidAttribute = 9 This indicates that an invalid attribute is being requested. cudlaErrorIncompatibleDlaSWVersion = 10 This indicates that the underlying DLA runtime is incompatible with the current cuDLA version. cudlaErrorMemoryRegistered = 11 This indicates that the memory object is already registered. cudlaErrorInvalidModule = 12 This indicates that the module being passed is invalid. cudlaErrorUnsupportedOperation = 13 This indicates that the operation being requested by the API call is unsupported. cudlaErrorNvSci = 14 This indicates that the NvSci operation requested by the API call failed. cudlaErrorDlaErrInvalidInput = 0x40000001 DLA HW Error. cudlaErrorDlaErrInvalidPreAction = 0x40000002 DLA HW Error. cudlaErrorDlaErrNoMem = 0x40000003 DLA HW Error. cudlaErrorDlaErrProcessorBusy = 0x40000004 DLA HW Error. cudlaErrorDlaErrTaskStatusMismatch = 0x40000005 DLA HW Error. cudlaErrorDlaErrEngineTimeout = 0x40000006 DLA HW Error. cudlaErrorDlaErrDataMismatch = 0x40000007 DLA HW Error. cudlaErrorUnknown = 0x7fffffff This indicates that an unknown error has occurred. enum cudlaSubmissionFlags Task submission flags for cudlaSubmitTask . Values CUDLA_SUBMIT_NOOP = 1 Flag to specify that the submitted task must be bypassed for execution. CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE = 1<<1 Flag to specify that the global lock acquire must be skipped. CUDLA_SUBMIT_DIAGNOSTICS_TASK = 1<<2 Flag to specify that the submitted task is to run permanent fault diagnostics for DLA HW. 2. Data Structures Here are the data structures with brief descriptions: cudlaDevAttribute cudlaExternalMemoryHandleDesc cudlaExternalSemaphoreHandleDesc CudlaFence cudlaModuleAttribute cudlaModuleTensorDescriptor cudlaSignalEvents cudlaTask cudlaWaitEvents 2.1. cudlaDevAttribute Union Reference [ Data types used by cuDLA driver ] Device attribute. Public Variables uint32_t deviceVersion uint8_t unifiedAddressingSupported Variables uint32_t cudlaDevAttribute :: deviceVersion [inherited] DLA device version. Xavier has 1.0 and Orin has 2.0. uint8_t cudlaDevAttribute :: unifiedAddressingSupported [inherited] Returns 0 if unified addressing is not supported. 2.2. cudlaExternalMemoryHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External memory handle descriptor. Public Variables const \n                              void \n                              * extBufObject unsigned long long size Variables const \n                                 \n                                 void \n                                 * cudlaExternalMemoryHandleDesc_t :: extBufObject [inherited] A handle representing an external memory object. unsigned long long cudlaExternalMemoryHandleDesc_t :: size [inherited] Size of the memory allocation 2.3. cudlaExternalSemaphoreHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External semaphore handle descriptor. Public Variables const \n                              void \n                              * extSyncObject Variables const \n                                 \n                                 void \n                                 * cudlaExternalSemaphoreHandleDesc_t :: extSyncObject [inherited] A handle representing an external synchronization object. 2.4. CudlaFence Struct Reference [ Data types used by cuDLA driver ] Fence description. Public Variables void \n                              * fence cudlaFenceType type Variables void \n                                 * CudlaFence :: fence [inherited] Fence. cudlaFenceType CudlaFence :: type [inherited] Fence type. 2.5. cudlaModuleAttribute Union Reference [ Data types used by cuDLA driver ] Module attribute. Public Variables cudlaModuleTensorDescriptor * inputTensorDesc uint32_t numInputTensors uint32_t numOutputTensors cudlaModuleTensorDescriptor * outputTensorDesc Variables cudlaModuleTensorDescriptor * cudlaModuleAttribute :: inputTensorDesc [inherited] Returns an array of input tensor descriptors. uint32_t cudlaModuleAttribute :: numInputTensors [inherited] Returns the number of input tensors. uint32_t cudlaModuleAttribute :: numOutputTensors [inherited] Returns the number of output tensors. cudlaModuleTensorDescriptor * cudlaModuleAttribute :: outputTensorDesc [inherited] Returns an array of output tensor descriptors. 2.6. cudlaModuleTensorDescriptor Struct Reference [ Data types used by cuDLA driver ] Tensor descriptor. 2.7. cudlaSignalEvents Struct Reference [ Data types used by cuDLA driver ] Signal events for cudlaSubmitTask Public Variables const \n                              \n                              \n                              * devPtrs CudlaFence * eofFences uint32_t numEvents Variables const \n                                 \n                                 \n                                 \n                                 * cudlaSignalEvents :: devPtrs [inherited] Array of registered synchronization objects (via cudlaImportExternalSemaphore ). CudlaFence * cudlaSignalEvents :: eofFences [inherited] Array of fences pointers for all the signal events corresponding to the synchronization objects. uint32_t cudlaSignalEvents :: numEvents [inherited] Total number of signal events. 2.8. cudlaTask Struct Reference [ Data types used by cuDLA driver ] Structure of Task. Public Variables const \n                              \n                              \n                              * inputTensor cudlaModule moduleHandle uint32_t numInputTensors uint32_t numOutputTensors const \n                              \n                              \n                              * outputTensor cudlaSignalEvents * signalEvents const cudlaWaitEvents * waitEvents Variables const \n                                 \n                                 \n                                 \n                                 * cudlaTask :: inputTensor [inherited] Array of input tensors. cudlaModule cudlaTask :: moduleHandle [inherited] cuDLA module handle. uint32_t cudlaTask :: numInputTensors [inherited] Number of input tensors. uint32_t cudlaTask :: numOutputTensors [inherited] Number of output tensors. const \n                                 \n                                 \n                                 \n                                 * cudlaTask :: outputTensor [inherited] Array of output tensors. cudlaSignalEvents * cudlaTask :: signalEvents [inherited] Signal events. const cudlaWaitEvents * cudlaTask :: waitEvents [inherited] Wait events. 2.9. cudlaWaitEvents Struct Reference [ Data types used by cuDLA driver ] Wait events for cudlaSubmitTask . Public Variables uint32_t numEvents const CudlaFence * preFences Variables uint32_t cudlaWaitEvents :: numEvents [inherited] Total number of wait events. const CudlaFence * cudlaWaitEvents :: preFences [inherited] Array of fence pointers for all the wait events. 3. Data Fields Here is a list of all documented struct and union fields with links to the struct/union documentation for each field: deviceVersion cudlaDevAttribute devPtrs cudlaSignalEvents eofFences cudlaSignalEvents extBufObject cudlaExternalMemoryHandleDesc extSyncObject cudlaExternalSemaphoreHandleDesc fence CudlaFence inputTensor cudlaTask inputTensorDesc cudlaModuleAttribute moduleHandle cudlaTask numEvents cudlaWaitEvents cudlaSignalEvents numInputTensors cudlaTask cudlaModuleAttribute numOutputTensors cudlaTask cudlaModuleAttribute outputTensor cudlaTask outputTensorDesc cudlaModuleAttribute preFences cudlaWaitEvents signalEvents cudlaTask size cudlaExternalMemoryHandleDesc type CudlaFence unifiedAddressingSupported cudlaDevAttribute waitEvents cudlaTask Notices Notice This document is provided for information\n                              purposes only and shall not be regarded as a warranty of a\n                              certain functionality, condition, or quality of a product.\n                              NVIDIA Corporation (“NVIDIA”) makes no representations or\n                              warranties, expressed or implied, as to the accuracy or\n                              completeness of the information contained in this document\n                              and assumes no responsibility for any errors contained\n                              herein. NVIDIA shall have no liability for the consequences\n                              or use of such information or for any infringement of\n                              patents or other rights of third parties that may result\n                              from its use. This document is not a commitment to develop,\n                              release, or deliver any Material (defined below), code, or\n                              functionality. NVIDIA reserves the right to make corrections, modifications,\n                              enhancements, improvements, and any other changes to this\n                              document, at any time without notice. Customer should obtain the latest relevant information before\n                              placing orders and should verify that such information is\n                              current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and\n                              conditions of sale supplied at the time of order\n                              acknowledgement, unless otherwise agreed in an individual\n                              sales agreement signed by authorized representatives of\n                              NVIDIA and customer (“Terms of Sale”). NVIDIA hereby\n                              expressly objects to applying any customer general terms and\n                              conditions with regards to the purchase of the NVIDIA\n                              product referenced in this document. No contractual\n                              obligations are formed either directly or indirectly by this\n                              document. NVIDIA products are not designed, authorized, or warranted to be\n                              suitable for use in medical, military, aircraft, space, or\n                              life support equipment, nor in applications where failure or\n                              malfunction of the NVIDIA product can reasonably be expected\n                              to result in personal injury, death, or property or\n                              environmental damage. NVIDIA accepts no liability for\n                              inclusion and/or use of NVIDIA products in such equipment or\n                              applications and therefore such inclusion and/or use is at\n                              customer’s own risk. NVIDIA makes no representation or warranty that products based on\n                              this document will be suitable for any specified use.\n                              Testing of all parameters of each product is not necessarily\n                              performed by NVIDIA. It is customer’s sole responsibility to\n                              evaluate and determine the applicability of any information\n                              contained in this document, ensure the product is suitable\n                              and fit for the application planned by customer, and perform\n                              the necessary testing for the application in order to avoid\n                              a default of the application or the product. Weaknesses in\n                              customer’s product designs may affect the quality and\n                              reliability of the NVIDIA product and may result in\n                              additional or different conditions and/or requirements\n                              beyond those contained in this document. NVIDIA accepts no\n                              liability related to any default, damage, costs, or problem\n                              which may be based on or attributable to: (i) the use of the\n                              NVIDIA product in any manner that is contrary to this\n                              document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA\n                              patent right, copyright, or other NVIDIA intellectual\n                              property right under this document. Information published by\n                              NVIDIA regarding third-party products or services does not\n                              constitute a license from NVIDIA to use such products or\n                              services or a warranty or endorsement thereof. Use of such\n                              information may require a license from a third party under\n                              the patents or other intellectual property rights of the\n                              third party, or a license from NVIDIA under the patents or\n                              other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if\n                              approved in advance by NVIDIA in writing, reproduced without\n                              alteration and in full compliance with all applicable export\n                              laws and regulations, and accompanied by all associated\n                              conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE\n                              BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER\n                              DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING\n                              PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED,\n                              IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE\n                              MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF\n                              NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A\n                              PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN\n                              NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING\n                              WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL,\n                              INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER\n                              CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING\n                              OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN\n                              ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding\n                              any damages that customer might incur for any reason\n                              whatsoever, NVIDIA’s aggregate and cumulative liability\n                              towards customer for the products described herein shall be\n                              limited in accordance with the Terms of Sale for the\n                              product. OpenCL OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. Trademarks NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation\n                              in the U.S. and other countries.  Other company and product names may be trademarks of\n                              the respective companies with which they are associated. Copyright © 2021 - 2024 NVIDIA Corporation &\n                              affiliates. All rights reserved. This product includes software developed by the Syncro Soft SRL (http://www.sync.ro/). Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2021-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/cuda-runtime-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-runtime-api/index.html", "content_type": "text/html", "text": "CUDA Runtime API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 CUDA Runtime API 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Device Management 6.2. Device Management [DEPRECATED] 6.3. Thread Management [DEPRECATED] 6.4. Error Handling 6.5. Stream Management 6.6. Event Management 6.7. External Resource Interoperability 6.8. Execution Control 6.9. Execution Control [DEPRECATED] 6.10. Occupancy 6.11. Memory Management 6.12. Memory Management [DEPRECATED] 6.13. Stream Ordered Memory Allocator 6.14. Unified Addressing 6.15. Peer Device Memory Access 6.16. OpenGL Interoperability 6.17. OpenGL Interoperability [DEPRECATED] 6.18. Direct3D 9 Interoperability 6.19. Direct3D 9 Interoperability [DEPRECATED] 6.20. Direct3D 10 Interoperability 6.21. Direct3D 10 Interoperability [DEPRECATED] 6.22. Direct3D 11 Interoperability 6.23. Direct3D 11 Interoperability [DEPRECATED] 6.24. VDPAU Interoperability 6.25. EGL Interoperability 6.26. Graphics Interoperability 6.27. Texture Object Management 6.28. Surface Object Management 6.29. Version Management 6.30. Graph Management 6.31. Driver Entry Point Access 6.32. C++ API Routines 6.33. Interactions with the CUDA Driver API 6.34. Profiler Control 6.35. Data types used by CUDA Runtime 7. Data Structures 7.1. __cudaOccupancyB2DHelper 7.2. cudaAccessPolicyWindow 7.3. cudaArrayMemoryRequirements 7.4. cudaArraySparseProperties 7.5. cudaAsyncNotificationInfo_t 7.6. cudaChannelFormatDesc 7.7. cudaChildGraphNodeParams 7.8. cudaConditionalNodeParams 7.9. cudaDeviceProp 7.10. cudaEglFrame 7.11. cudaEglPlaneDesc 7.12. cudaEventRecordNodeParams 7.13. cudaEventWaitNodeParams 7.14. cudaExtent 7.15. cudaExternalMemoryBufferDesc 7.16. cudaExternalMemoryHandleDesc 7.17. cudaExternalMemoryMipmappedArrayDesc 7.18. cudaExternalSemaphoreHandleDesc 7.19. cudaExternalSemaphoreSignalNodeParams 7.20. cudaExternalSemaphoreSignalNodeParamsV2 7.21. cudaExternalSemaphoreSignalParams 7.22. cudaExternalSemaphoreSignalParams_v1 7.23. cudaExternalSemaphoreWaitNodeParams 7.24. cudaExternalSemaphoreWaitNodeParamsV2 7.25. cudaExternalSemaphoreWaitParams 7.26. cudaExternalSemaphoreWaitParams_v1 7.27. cudaFuncAttributes 7.28. cudaGraphEdgeData 7.29. cudaGraphExecUpdateResultInfo 7.30. cudaGraphInstantiateParams 7.31. cudaGraphKernelNodeUpdate 7.32. cudaGraphNodeParams 7.33. cudaHostNodeParams 7.34. cudaHostNodeParamsV2 7.35. cudaIpcEventHandle_t 7.36. cudaIpcMemHandle_t 7.37. cudaKernelNodeParams 7.38. cudaKernelNodeParamsV2 7.39. cudaLaunchAttribute 7.40. cudaLaunchAttributeValue 7.41. cudaLaunchConfig_t 7.42. cudaLaunchMemSyncDomainMap 7.43. cudaLaunchParams 7.44. cudaMemAccessDesc 7.45. cudaMemAllocNodeParams 7.46. cudaMemAllocNodeParamsV2 7.47. cudaMemcpy3DParms 7.48. cudaMemcpy3DPeerParms 7.49. cudaMemcpyNodeParams 7.50. cudaMemFreeNodeParams 7.51. cudaMemLocation 7.52. cudaMemPoolProps 7.53. cudaMemPoolPtrExportData 7.54. cudaMemsetParams 7.55. cudaMemsetParamsV2 7.56. cudaPitchedPtr 7.57. cudaPointerAttributes 7.58. cudaPos 7.59. cudaResourceDesc 7.60. cudaResourceViewDesc 7.61. cudaTextureDesc 7.62. CUuuid_st 8. Data Fields 9. Deprecated List Search Results CUDA Runtime API\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback Table of Contents 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Device Management 6.2. Device Management [DEPRECATED] 6.3. Thread Management [DEPRECATED] 6.4. Error Handling 6.5. Stream Management 6.6. Event Management 6.7. External Resource Interoperability 6.8. Execution Control 6.9. Execution Control [DEPRECATED] 6.10. Occupancy 6.11. Memory Management 6.12. Memory Management [DEPRECATED] 6.13. Stream Ordered Memory Allocator 6.14. Unified Addressing 6.15. Peer Device Memory Access 6.16. OpenGL Interoperability 6.17. OpenGL Interoperability [DEPRECATED] 6.18. Direct3D 9 Interoperability 6.19. Direct3D 9 Interoperability [DEPRECATED] 6.20. Direct3D 10 Interoperability 6.21. Direct3D 10 Interoperability [DEPRECATED] 6.22. Direct3D 11 Interoperability 6.23. Direct3D 11 Interoperability [DEPRECATED] 6.24. VDPAU Interoperability 6.25. EGL Interoperability 6.26. Graphics Interoperability 6.27. Texture Object Management 6.28. Surface Object Management 6.29. Version Management 6.30. Graph Management 6.31. Driver Entry Point Access 6.32. C++ API Routines 6.33. Interactions with the CUDA Driver API 6.34. Profiler Control 6.35. Data types used by CUDA Runtime 7. Data Structures 7.1. __cudaOccupancyB2DHelper 7.2. cudaAccessPolicyWindow 7.3. cudaArrayMemoryRequirements 7.4. cudaArraySparseProperties 7.5. cudaAsyncNotificationInfo_t 7.6. cudaChannelFormatDesc 7.7. cudaChildGraphNodeParams 7.8. cudaConditionalNodeParams 7.9. cudaDeviceProp 7.10. cudaEglFrame 7.11. cudaEglPlaneDesc 7.12. cudaEventRecordNodeParams 7.13. cudaEventWaitNodeParams 7.14. cudaExtent 7.15. cudaExternalMemoryBufferDesc 7.16. cudaExternalMemoryHandleDesc 7.17. cudaExternalMemoryMipmappedArrayDesc 7.18. cudaExternalSemaphoreHandleDesc 7.19. cudaExternalSemaphoreSignalNodeParams 7.20. cudaExternalSemaphoreSignalNodeParamsV2 7.21. cudaExternalSemaphoreSignalParams 7.22. cudaExternalSemaphoreSignalParams_v1 7.23. cudaExternalSemaphoreWaitNodeParams 7.24. cudaExternalSemaphoreWaitNodeParamsV2 7.25. cudaExternalSemaphoreWaitParams 7.26. cudaExternalSemaphoreWaitParams_v1 7.27. cudaFuncAttributes 7.28. cudaGraphEdgeData 7.29. cudaGraphExecUpdateResultInfo 7.30. cudaGraphInstantiateParams 7.31. cudaGraphKernelNodeUpdate 7.32. cudaGraphNodeParams 7.33. cudaHostNodeParams 7.34. cudaHostNodeParamsV2 7.35. cudaIpcEventHandle_t 7.36. cudaIpcMemHandle_t 7.37. cudaKernelNodeParams 7.38. cudaKernelNodeParamsV2 7.39. cudaLaunchAttribute 7.40. cudaLaunchAttributeValue 7.41. cudaLaunchConfig_t 7.42. cudaLaunchMemSyncDomainMap 7.43. cudaLaunchParams 7.44. cudaMemAccessDesc 7.45. cudaMemAllocNodeParams 7.46. cudaMemAllocNodeParamsV2 7.47. cudaMemcpy3DParms 7.48. cudaMemcpy3DPeerParms 7.49. cudaMemcpyNodeParams 7.50. cudaMemFreeNodeParams 7.51. cudaMemLocation 7.52. cudaMemPoolProps 7.53. cudaMemPoolPtrExportData 7.54. cudaMemsetParams 7.55. cudaMemsetParamsV2 7.56. cudaPitchedPtr 7.57. cudaPointerAttributes 7.58. cudaPos 7.59. cudaResourceDesc 7.60. cudaResourceViewDesc 7.61. cudaTextureDesc 7.62. CUuuid_st 8. Data Fields 9. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/nvjpeg/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvjpeg/index.html", "content_type": "text/html", "text": "nvJPEG 1. Introduction 1.1. nvJPEG Decoder 1.2. nvJPEG Encoder 1.3. Thread Safety 1.4. Multi-GPU support 1.5. Hardware Acceleration 2. JPEG Decoding 2.1. Using JPEG Decoding 2.1.1. Single Image Decoding 2.1.2. Decode using Decoupled Phases 2.1.3. Batched Image Decoding 2.1.3.1. Single Phase 2.2. nvJPEG Type Declarations 2.2.1. nvJPEG Backend 2.2.2. nvJPEG Bitstream Handle 2.2.3. nvJPEG Decode Device Buffer Handle 2.2.4. nvJPEG Decode Parameter Handle 2.2.5. nvJPEG Decode Pinned Buffer Handle 2.2.6. nvJPEG Decoder Handle 2.2.7. nvJPEG Host Pinned Memory Allocator Interface 2.2.8. nvJPEG Extended Host Pinned Memory Allocator Interface 2.2.9. nvJPEG Image 2.2.10. nvJPEG Device Memory Allocator Interface 2.2.11. nvJPEG Extended Device Memory Allocator Interface 2.2.12. nvJPEG Opaque JPEG Decoding State Handle 2.2.13. nvJPEG Opaque Library Handle Struct 2.2.14. nvJPEG Output Pointer Struct 2.2.15. nvJPEG Jpeg Encoding 2.2.16. nvJPEG Scale Factor 2.2.17. nvJPEG Flags 2.2.18. nvJPEG Exif Orientation 2.3. nvJPEG API Reference 2.3.1. nvJPEG Helper API Reference 2.3.1.1. nvjpegGetProperty() 2.3.1.2. nvjpegGetCudartProperty() 2.3.1.3. nvjpegCreate() [DEPRECATED] 2.3.1.4. nvjpegCreateSimple() 2.3.1.5. nvjpegCreateEx() 2.3.1.6. nvjpegCreateExV2() 2.3.1.7. nvjpegDestroy() 2.3.1.8. nvjpegSetDeviceMemoryPadding() 2.3.1.9. nvjpegGetDeviceMemoryPadding() 2.3.1.10. nvjpegSetPinnedMemoryPadding() 2.3.1.11. nvjpegGetPinnedMemoryPadding() 2.3.1.12. nvjpegGetHardwareDecoderInfo() 2.3.1.13. nvjpegJpegStateCreate() 2.3.1.14. nvjpegJpegStateDestroy() 2.3.1.15. nvjpegDecoderCreate() 2.3.1.16. nvjpegDecoderDestroy() 2.3.1.17. nvjpegDecoderJpegSupported() 2.3.1.18. nvjpegDecoderStateCreate() 2.3.1.19. nvjpegJpegStreamCreate() 2.3.1.20. nvjpegJpegStreamDestroy() 2.3.1.21. nvjpegBufferPinnedCreate() 2.3.1.22. nvjpegBufferPinnedCreateV2() 2.3.1.23. nvjpegBufferPinnedDestroy() 2.3.1.24. nvjpegStateAttachPinnedBuffer() 2.3.1.25. nvjpegBufferPinnedRetrieve() 2.3.1.26. nvjpegBufferPinnedResize() 2.3.1.27. nvjpegBufferDeviceCreate() 2.3.1.28. nvjpegBufferDeviceCreateV2() 2.3.1.29. nvjpegBufferDeviceDestroy() 2.3.1.30. nvjpegStateAttachDeviceBuffer() 2.3.1.31. nvjpegBufferDeviceRetrieve() 2.3.1.32. nvjpegBufferDeviceResize() 2.3.1.33. nvjpegDecodeParamsCreate() 2.3.1.34. nvjpegDecodeParamsDestroy() 2.3.2. Retrieve Encoded Image Information API 2.3.2.1. nvjpegGetImageInfo() 2.3.2.2. nvJPEG Stream API 2.3.2.2.1. nvjpegJpegStreamParse() 2.3.2.2.2. nvjpegJpegStreamParseHeader() 2.3.2.2.3. nvjpegJpegStreamParseTables() 2.3.2.2.4. nvjpegJpegStreamGetFrameDimensions() 2.3.2.2.5. nvjpegJpegStreamGetComponentsNum() 2.3.2.2.6. nvjpegJpegStreamGetComponentDimensions() 2.3.2.2.7. nvjpegJpegStreamGetChromaSubsampling() 2.3.2.2.8. nvjpegJpegStreamGetJpegEncoding() 2.3.2.2.9. nvjpegJpegStreamGetExifOrientation() 2.3.2.2.10. nvjpegJpegStreamGetSamplePrecision() 2.3.3. Decode API—Single Phase 2.3.3.1. ​nvjpegDecode() 2.3.3.2. ​nvjpegDecodeBatchedInitialize() 2.3.3.3. ​nvjpegDecodeBatched() 2.3.3.4. nvjpegDecodeBatchedEx() 2.3.3.5. nvjpegDecodeBatchedSupported() 2.3.3.6. nvjpegDecodeBatchedSupportedEx() 2.3.3.7. nvjpegDecodeBatchedPreAllocate() 2.3.3.8. nvjpegDecodeBatchedParseJpegTables() 2.3.4. Decode API—Decoupled Decoding 2.3.4.1. nvjpegDecodeJpegHost() 2.3.4.2. nvjpegDecodeJpegTransferToDevice() 2.3.4.3. nvjpegDecodeJpegDevice() 2.3.4.4. nvjpegDecodeJpeg() 2.3.5. nvJPEG Decode Parameters 2.3.5.1. nvjpegDecodeParamsSetOutputFormat() 2.3.5.2. nvjpegDecodeParamsSetROI() 2.3.5.3. nvjpegDecodeParamsSetAllowCMYK() 2.3.5.4. nvjpegDecodeParamsSetScaleFactor() 2.3.5.5. nvjpegDecodeParamsSetExifOrientation() 2.3.6. nvJPEG API Return Codes 2.3.7. nvJPEG Chroma Subsampling 2.3.8. Reference Documents 2.4. Examples of nvJPEG 3. JPEG Encoding 3.1. Using the Encoder 3.1.1. Encoding the Parameters 3.1.2. Encoding the State 3.1.3. Encoding the Image 3.1.3.1. nvjpegEncodeYUV 3.1.3.2. nvjpegEncodeImage 3.1.4. Retrieving the Compressed Stream 3.1.5. JPEG Encoding Example 3.2. nvJPEG Encoder Type Declarations 3.2.1. nvjpegInputFormat_t 3.2.2. nvjpegEncoderState_t 3.2.3. nvjpegEncoderParams_t 3.3. nvJPEG Encoder Helper API Reference 3.3.1. nvjpegEncoderStateCreate() 3.3.2. nvjpegEncoderStateDestroy() 3.3.3. nvjpegEncoderParamsCreate() 3.3.4. nvjpegEncoderParamsDestroy() 3.3.5. nvjpegEncoderParamsSetEncoding() 3.3.6. nvjpegEncoderParamsSetQuality() 3.3.7. nvjpegEncoderParamsSetOptimizedHuffman() 3.3.8. nvjpegEncoderParamsSetSamplingFactors() 3.4. nvJPEG Encoder API Reference 3.4.1. nvjpegEncodeGetBufferSize() 3.4.2. nvjpegEncodeYUV() 3.4.3. nvjpegEncodeImage() 3.4.4. nvjpegEncodeRetrieveBitstream() 3.4.5. nvjpegEncodeRetrieveBitstreamDevice() 4. JPEG Transcoding 4.1. nvJPEG Transcoder Helper API Reference 4.1.1. nvjpegEncoderParamsCopyMetadata() 4.1.2. nvjpegEncoderParamsCopyQuantizationTables() 4.1.3. nvjpegEncoderParamsCopyHuffmanTables() [Deprecated] 4.2. JPEG Transcoding Example 5. List of Dropped APIs 6. Known Issues 7. Notices 7.1. Notice 7.2. OpenCL 7.3. Trademarks nvJPEG » 1. Introduction v12.5 | PDF | Archive nvJPEG A GPU accelerated JPEG codec library. 1. Introduction  1.1. nvJPEG Decoder  The nvJPEG library provides high-performance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. The library offers single and batched JPEG decoding capabilities which efficiently utilize the available GPU resources for optimum performance; and the flexibility for users to manage the memory allocation needed for decoding. The nvJPEG library enables the following functions: use the JPEG image data stream as input; retrieve the width and height of the image from the data stream, and use this retrieved information to manage the GPU memory allocation and the decoding. A dedicated API is provided for retrieving the image information from the raw JPEG image data stream. Note Throughout this document, the terms “CPU” and “Host” are used synonymously. Similarly, the terms “GPU” and “Device” are synonymous. The nvJPEG library supports the following: JPEG options: Baseline and Progressive JPEG decoding/encoding 8 bits per pixel Huffman bitstream decoding Upto 4 channel JPEG bitstreams 8- and 16-bit quantization tables The following chroma subsampling for the 3 color channels Y, Cb, Cr (Y, U, V): 4:4:4 4:2:2 4:2:0 4:4:0 4:1:1 4:1:0 Features: Hybrid decoding using both the CPU (i.e., host) and the GPU (i.e., device). Hardware acceleration for baseline JPEG decode on supported platforms . Input to the library is in the host memory, and the output is in the GPU memory. Single image and batched image decoding. Single phase and multiple phases decoding. Color space conversion. User-provided memory manager for the device and pinned host memory allocations. 1.2. nvJPEG Encoder  The encoding functions of the nvJPEG library perform GPU-accelerated compression of user’s image data to the JPEG bitstream. User can provide input data in a number of formats and colorspaces, and control the encoding process with parameters. Encoding functionality will allocate temporary buffers using user-provided memory allocator. Before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described in nvJPEG Encoder Helper API Reference . 1.3. Thread Safety  Not all nvJPEG types are thread safe. When using decoder APIs across multiple threads, the following decoder types should be instantiated separately for each thread: nvjpegJpegStream_t , nvjpegJpegState_t , nvjpegBufferDevice_t , nvjpegBufferPinned_t When using encoder APIs across multiple threads, nvjpegEncoderState_t should be instantiated separately for each thread. For user-provided allocators (inputs to nvJPEGCreateEx() ), the user needs to ensure thread safety. 1.4. Multi-GPU support  The nvJPEG states and handles are bound to the device that was set as current during their creation. Using these states and handles with another device set as current is undefined. The user is responsible of keeping track of the current device. 1.5. Hardware Acceleration  Hardware accelerated JPEG decode is available on the following GPUs - A100, A30, H100. Platforms which support hardware accelerated JPEG decode: Windows Linux (x86_64, PowerPC, ARM64) 2. JPEG Decoding  2.1. Using JPEG Decoding  ​The nvJPEG library provides functions for both the decoding of a single image, and batched decoding of multiple images. 2.1.1. Single Image Decoding  For single-image decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer. To use the nvJPEG library, start by calling the helper functions for initialization. Create nvJPEG library handle with one of the helper functions nvjpegCreateSimple() or nvjpegCreateEx() . Create JPEG state with the helper function nvjpegJpegStateCreate() . See nvJPEG Type Declarations and nvjpegJpegStateCreate() . The following helper functions are available in the nvJPEG library: nvjpegStatus_t nvjpegGetProperty(libraryPropertyType type, int *value); [DEPRECATED] nvjpegStatus_t nvjpegCreate(nvjpegBackend_t backend, nvjpegHandle_t *handle , nvjpeg_dev_allocator allocator); nvjpegStatus_t nvjpegCreateSimple(nvjpegHandle_t *handle); nvjpegStatus_t nvjpegCreateEx(nvjpegBackend_t backend, nvjpegDevAllocator_t *dev_allocator, nvjpegPinnedAllocator_t *pinned_allocator, unsigned int flags, nvjpegHandle_t *handle); nvjpegStatus_t nvjpegDestroy(nvjpegHandle_t handle); nvjpegStatus_t nvjpegJpegStateCreate(nvjpegHandle_t handle, nvjpegJpegState_t *jpeg_handle); nvjpegStatus_t nvjpegJpegStateDestroy(nvjpegJpegState handle); Other helper functions such as nvjpegSet*() and nvjpegGet*() can be used to configure the library functionality on per-handle basis. Refer to the helper API reference for more details. Retrieve the width and height information from the JPEG-encoded image by using the nvjpegGetImageInfo() function. Below is the signature of nvjpegGetImageInfo() function: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); For each image to be decoded, pass the JPEG data pointer and data length to the above function. The nvjpegGetImageInfo() function is thread safe. One of the outputs of the above nvjpegGetImageInfo() function is nvjpegChromaSubsampling_t . This parameter is an enum type, and its enumerator list is composed of the chroma subsampling property retrieved from the JPEG image. See nvJPEG Chroma Subsampling . Use the nvjpegDecode() function in the nvJPEG library to decode this single JPEG image. See the signature of this function below: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); In the above nvjpegDecode() function, the parameters nvjpegOutputFormat_t , nvjpegImage_t , and cudaStream_t can be used to set the output behavior of the nvjpegDecode() function. You provide the cudaStream_t parameter to indicate the stream to which your asynchronous tasks are submitted. The ``nvjpegOutputFormat_t`` parameter: The nvjpegOutputFormat_t parameter can be set to one of the output_format settings below: output_format Meaning NVJPEG_OUTPUT_UNCHANGED Return the decoded image planar format. NVJPEG_OUTPUT_RGB Convert to planar RGB. NVJPEG_OUTPUT_BGR Convert to planar BGR. NVJPEG_OUTPUT_RGBI Convert to interleaved RGB. NVJPEG_OUTPUT_BGRI Convert to interleaved BGR. NVJPEG_OUTPUT_Y Return the Y component only. NVJPEG_OUTPUT_YUV Return in the YUV planar format. NVJPEG_OUTPUT_UNCHANGEDI_U16 Return the decoded image interleaved format. For example, if output_format is set to NVJPEG_OUTPUT_Y or NVJPEG_OUTPUT_RGBI , or NVJPEG_OUTPUT_BGRI then the output is written only to channel[0] of nvjpegImage_t , and the other channels are not touched. Alternately, in the case of planar output, the data is written to the corresponding channels of the nvjpegImage_t destination structure. Finally, in the case of grayscale JPEG and RGB output, the luminance is used to create the grayscale RGB. The below table explains the combinations of the output formats and the number of channels supported by the library. No of Channels in bitstream 1 2 3 4 Output Format NVJPEG_OUTPUT_UNCHANGED Yes Yes Yes Yes NVJPEG_OUTPUT_YUV Only the first channel of the output is populated No Yes No NVJPEG_OUTPUT_Y Yes No Yes Yes (a) NVJPEG_OUTPUT_RGB Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGR Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_RGBI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGRI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_UNCHANGEDI_U16 Yes(c) Yes No No NOTES: Must be enabled using nvjpegDecodeParamsSetAllowCMYK() . Luminance is used to create the grayscale RGB. Supported only by NVJPEG_BACKEND_LOSSLESS_JPEG backend. As mentioned above, an important benefit of the nvjpegGetImageInfo() function is the ability to utilize the image information retrieved from the the input JPEG image to allocate proper GPU memory for your decoding operation. The nvjpegGetImageInfo() function returns the widths , heights and nComponents parameters. nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); You can use the retrieved parameters, widths , heights and nComponents , to calculate the required size for the output buffers, either for a single decoded JPEG, or for every decoded JPEG in a batch. To optimally set the destination parameter for the nvjpegDecode() function, use the following guidelines: For the output_format: NVJPEG_OUTPUT_Y destination.pitch[0] should be at least: width[0] destination.channel[0] should be at least of size: destination.pitch[0]*height[0] For the output_format destination.pitch[c] should be at least: destination.channel[c] should be at least of size: NVJPEG_OUTPUT_YUV width[c] for c = 0, 1, 2 destination.pitch[c]*height[c] for c = 0, 1, 2 NVJPEG_OUTPUT_RGB and NVJPEG_OUTPUT_BGR width[0] for c = 0, 1, 2 destination.pitch[0]*height[0] for c = 0, 1, 2 NVJPEG_OUTPUT_RGBI and NVJPEG_OUTPUT_BGRI width[0]*3 destination.pitch[0]*height[0] NVJPEG_OUTPUT_UNCHANGED width[c] for c = [ 0, nComponents - 1 ] destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] NVJPEG_OUTPUT_UNCHANGEDI_U16 width[c]* nComponents* sizeof(unsigned short) destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] Ensure that the nvjpegImage_t structure (or structures, in the case of batched decode) is filled with the pointers and pitches of allocated buffers. The nvjpegImage_t structure that holds the output pointers is defined as follows: typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; NVJPEG_MAX_COMPONENT is the maximum number of color components the nvJPEG library supports in the current release. For generic images, this is the maximum number of encoded channels that the library is able to decompress. Finally, when you call the nvjpegDecode() function with the parameters as described above, the nvjpegDecode() function fills the output buffers with the decoded data. 2.1.2. Decode using Decoupled Phases  The nvJPEG library allows further separation of the host and device phases of the decode process. The host phase of the decoding will not need to access to device resources. A few examples of decoupled APIs can be found under Decode API - Decoupled Decoding. Below is the sequence of API calls to decode a single image Initialize all the items that are used in the decoding process: Create the library handle using one of the library handle initialization routines. Choose decoder implementation nvjpegBackend_t , and create decoder using nvjpegDecoderCreate() . Create JPEG decoder state using nvjpegDecoderStateCreate() . Create JPEG stream using nvjpegJpegStreamCreate() . Create the pinned and device buffers used by the decoder using the below APIs respectively. These buffers are used to store intermediate decoding results. nvjpegBufferPinnedCreate() nvjpegBufferDeviceCreate() Link the buffers to the JPEG state using the following APIs respectively: nvjpegStateAttachPinnedBuffer() nvjpegStateAttachDeviceBuffer() Create decode parameters using the below API. This is used to set the output format, and enable ROI decode: nvjpegDecodeParamsCreate() Perform decoding: Parse the jpeg bit-stream using nvjpegJpegStreamParse() Encoded bitstream information, like channel dimensions, can be retrieved using the below API. This information is used to allocate the output pointers in nvjpegImage_t . nvjpegJpegStreamGetComponentsNum() nvjpegJpegStreamGetComponentDimensions() Call the decode API in the below sequence to decode the image: nvjpegDecodeJpegHost() nvjpegDecodeJpegTransferToDevice() nvjpegDecodeJpegDevice() 2.1.3. Batched Image Decoding  For the batched image decoding you provide pointers to multiple file data in the memory, and also provide the buffer sizes for each file data. The nvJPEG library will decode these multiple images, and will place the decoded data in the output buffers that you specified in the parameters. 2.1.3.1. Single Phase  For batched image decoding in single phase, follow these steps: Call nvjpegDecodeBatchedInitialize() function to initialize the batched decoder. Specify the batch size in the batch_size parameter. See nvjpegDecodeBatchedInitialize() . Next, call nvjpegDecodeBatched() for each new batch. Make sure to pass the parameters that are correct to the specific batch of images. If the size of the batch changes, or if the batch decoding fails, then call the nvjpegDecodeBatchedInitialize() function again. 2.2. nvJPEG Type Declarations  2.2.1. nvJPEG Backend  typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , NVJPEG_BACKEND_GPU_HYBRID = 2 , NVJPEG_BACKEND_HARDWARE = 3 , NVJPEG_BACKEND_GPU_HYBRID_DEVICE = 4 , NVJPEG_BACKEND_HARDWARE_DEVICE = 5 , NVJPEG_BACKEND_LOSSLESS_JPEG = 6 } nvjpegBackend_t ; The nvjpegBackend_t enum is used to select either default back-end by default, or use GPU decoding for baseline JPEG images, or use CPU for Huffman decoding. Member Description NVJPEG_BACKEND_DEFAULT Back-end is selected internally. NVJPEG_BACKEND_HYBRID Uses CPU for Huffman decoding. NVJPEG_BACKEND_GPU_HYBRID Uses GPU for Huffman decoding. nvjpegDecodeBatched will use GPU decoding for baseline JPEG images with interleaved scan when batch size is greater than 50. The decoupled APIs will use GPU assisted Huffman decoding. NVJPEG_BACKEND_HARDWARE Uses Hardware Acceleration for decode. Supports baseline JPEG images with single scan with 1 or 3 channels. 410 and 411 chroma subsamplings are not supported. NVJPEG_BACKEND_GPU_HYBRID_DEVICE Supports input bitstream on device memory. Can be used only with batched decode APIs for baseline JPEG images without restart intervals. NVJPEG_BACKEND_HARDWARE_DEVICE Supports input bitstream on device memory. Can be used only with batched decode APIs. Uses Hardware Acceleration for decode. Supports baseline JPEG images with single scan with 1 or 3 channels. 410 and 411 chroma subsamplings are not supported. NVJPEG_BACKEND_LOSSLESS_JPEG Supports lossless jpeg bitstreams as defined in the jpeg 92 standard. Bitstreams with up to 2 channels and prediction mode 1 are supported. 2.2.2. nvJPEG Bitstream Handle  struct nvjpegJpegStream ; typedef struct nvjpegJpegStream * nvjpegJpegStream_t ; This handle stores the bit-stream parameters on the host. This helps retrieve bitstream meta-data using APIs defined in nvJPEG Stream API . 2.2.3. nvJPEG Decode Device Buffer Handle  struct nvjpegBufferDevice ; typedef struct nvjpegBufferDevice * nvjpegBufferDevice_t ; This nvjpegBufferDevice_t is used by decoder states to store the intermediate information in device memory. 2.2.4. nvJPEG Decode Parameter Handle  struct nvjpegDecodeParams ; typedef struct nvjpegDecodeParams * nvjpegDecodeParams_t ; This decoder parameter handle stores the parameters like output format, and the ROI decode parameters that are set using APIs defined in nvJPEG Chroma Subsampling . 2.2.5. nvJPEG Decode Pinned Buffer Handle  struct nvjpegBufferPinned ; typedef struct nvjpegBufferPinned * nvjpegBufferPinned_t ; This nvjpegBufferPinned_t handle is used by decoder states to store the intermediate information on pinned memory. 2.2.6. nvJPEG Decoder Handle  struct nvjpegJpegDecoder ; typedef struct nvjpegJpegDecoder * nvjpegJpegDecoder_t ; This decoder handle stores the intermediate decoder data, which is shared across the decoding stages. This decoder handle is initialized for a given nvjpegBackend_t . It is used as input to the Decode API—Decoupled Decoding . 2.2.7. nvJPEG Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMalloc )( void ** , size_t , unsigned int flags ); typedef int ( * tPinnedFree )( void * ); typedef struct { tPinnedMalloc pinned_malloc ; tPinnedFree pinned_free ; } nvjpegPinnedAllocator_t ; When the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set as a pointer to the above nvjpegPinnedAllocator_t structure, then this structure will be used for allocating and releasing host pinned memory for copying data to/from device. The function prototypes for the memory allocation and memory freeing functions are similar to the cudaHostAlloc() and cudaFreeHost() functions. They will return 0 in case of success, and non-zero otherwise. However, if the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaHostAlloc() and cudaFreeHost() will be used. When using nvjpegCreate() or nvjpegCreateSimple() function to create library handle, the default host pinned memory allocator will be used. 2.2.8. nvJPEG Extended Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tPinnedFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tPinnedMallocV2 pinned_malloc ; tPinnedFreeV2 pinned_free ; void * pinned_ctx ; } nvjpegPinnedAllocatorV2_t ; Extended pinned allocators support stream ordered allocations along with user defined context information pinned_ctx . When invoking the allocators, nvJPEG will pass pinned_ctx as input to the extended pinned allocators. 2.2.9. nvJPEG Image  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t structure (or structures, in the case of batched decode) is used to fill with the pointers and pitches of allocated buffers. The nvjpegImage_t structure that holds the output pointers. Member Description NVJPEG_MAX_COMPONENT Maximum number of color components the nvJPEG library supports. For generic images, this is the maximum number of encoded channels that the library is able to decompress. 2.2.10. nvJPEG Device Memory Allocator Interface  typedef int ( * tDevMalloc )( void ** , size_t ); typedef int ( * tDevFree )( void * ); typedef struct { tDevMalloc dev_malloc ; tDevFree dev_free ; } nvjpegDevAllocator_t ; Users can tell the library to use their own device memory allocator. The function prototypes for the memory allocation and memory freeing functions are similar to the cudaMalloc() and cudaFree() functions. They should return 0 in case of success, and non-zero otherwise. A pointer to the nvjpegDevAllocator_t structure, with properly filled fields, should be provided to the nvjpegCreate() function. NULL is accepted, in which case the default memory allocation functions cudaMalloc() and cudaFree() is used. When the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set as a pointer to the above nvjpegDevAllocator_t structure, then this structure is used for allocating and releasing the device memory. The function prototypes for the memory allocation and memory freeing functions are similar to the cudaMalloc() and cudaFree() functions. They should return 0 in case of success, and non-zero otherwise. However, if the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaMalloc() and cudaFree() will be used. When using nvjpegCreateSimple() function to create library handle the default device memory allocator will be used. 2.2.11. nvJPEG Extended Device Memory Allocator Interface  typedef int ( * tDevMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tDevFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tDevMallocV2 dev_malloc ; tDevFreeV2 dev_free ; void * dev_ctx ; } nvjpegDevAllocatorV2_t ; Extended device allocators support stream ordered allocations along with user defined context information dev_ctx . When invoking the allocators, nvJPEG will pass dev_ctx as input to the extended device allocators. 2.2.12. nvJPEG Opaque JPEG Decoding State Handle  struct nvjpegJpegState ; typedef struct nvjpegJpegState * nvjpegJpegState_t ; The nvjpegJpegState structure stores the temporary JPEG information. It should be initialized before any usage. This JPEG state handle can be reused after being used in another decoding. The same JPEG handle should be used across the decoding phases for the same image or batch. Multiple threads are allowed to share the JPEG state handle only when processing same batch during first phase ( nvjpegDecodePhaseOne ) . 2.2.13. nvJPEG Opaque Library Handle Struct  struct nvjpegHandle ; typedef struct nvjpegHandle * nvjpegHandle_t ; The library handle is used in any consecutive nvJPEG library calls, and should be initialized first. The library handle is thread safe, and can be used by multiple threads simultaneously. 2.2.14. nvJPEG Output Pointer Struct  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t struct holds the pointers to the output buffers, and holds the corresponding strides of those buffers for the image decoding. See Single Image Decoding on how to set up the nvjpegImage_t struct. 2.2.15. nvJPEG Jpeg Encoding  typedef enum { NVJPEG_ENCODING_UNKNOWN = 0x0 , NVJPEG_ENCODING_BASELINE_DCT = 0xc0 , NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN = 0xc1 , NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN = 0xc2 , NVJPEG_ENCODING_LOSSLESS_HUFFMAN = 0xc3 } nvjpegJpegEncoding_t ; The nvjpegJpegEncoding_t enum lists the JPEG encoding types that are supported by the nvJPEG library The enum values are based on the markers defined in the JPEG specification Member Description NVJPEG_ENCODING_UNKNOWN This value is returned for all the JPEG markers not supported by the nvJPEG library. NVJPEG_ENCODING_BASELINE_DCT Corresponds to the JPEG marker 0xc0, refer to the JPEG spec for more details. NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN Corresponds to the JPEG marker 0xc1, refer to the JPEG spec for more details. NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN Corresponds to the JPEG marker 0xc2, refer to the JPEG spec for more details. NVJPEG_ENCODING_LOSSLESS_HUFFMAN Corresponds to the JPEG marker 0xc3, refer to the JPEG spec for more details. 2.2.16. nvJPEG Scale Factor  typedef enum { NVJPEG_SCALE_NONE = 0 , NVJPEG_SCALE_1_BY_2 = 1 , NVJPEG_SCALE_1_BY_4 = 2 , NVJPEG_SCALE_1_BY_8 = 3 } nvjpegScaleFactor_t ; The nvjpegScaleFactor_t enum lists all the scale factors supported by the library. This feature is supported when nvjpeg handles are intstaniated using NVJPEG_BACKEND_HARDWARE Member Description NVJPEG_SCALE_NONE Decoded output is not scaled NVJPEG_SCALE_1_BY_2 Decoded output width and height are scaled by a factor of 1/2 NVJPEG_SCALE_1_BY_4 Decoded output width and height are scaled by a factor of 1/4 NVJPEG_SCALE_1_BY_8 Decoded output width and height are scaled by a factor of 1/8 2.2.17. nvJPEG Flags  #define NVJPEG_FLAGS_DEFAULT 0 #define NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE 1 #define NVJPEG_FLAGS_ENABLE_MEMORY_POOLS   2 #define NVJPEG_FLAGS_BITSTREAM_STRICT      4 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE            8 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY 16 #define NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION   32 nvJPEG flags provide additional controls when initializing the library using nvJPEGCreateEx() or nvJPEGCreateExV2() . It is possible to combine the flags as they are bit fields. Member Description NVJPEG_FLAGS_DEFAULT Corresponds to default library behavior. NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE To be used when the library is initialized with NVJPEG_BACKEND_HARDWARE. It will be ignored for other back-ends. nvjpeg in batched decode mode buffers additional images to achieve optimal performance. Use this flag to disable buffering of additional images. NVJPEG_FLAGS_ENABLE_MEMORY_POOLS [Deprecated] Starting with CUDA 11.1 this flag will be ignored. NVJPEG_FLAGS_BITSTREAM_STRICT nvJPEG library will try to decode a bitstream even if it doesn’t strictly follow the JPEG specification. Using this flag will return an error in such cases. NVJPEG_FLAGS_REDUCED_MEMORY_DECODE When using NVJPEG_BACKEND_HYBRID or NVJPEG_BACKEND_GPU_HYBRID backends, enabling this flag will reduce the memory usage of the decoding whenever possible. NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY Using this flag enables zero-copy memory when feasible on supported platforms. NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION Using this flag enables the decoder to use interpolation when performing chroma upsampling during the YCbCr to RGB conversion stage. 2.2.18. nvJPEG Exif Orientation  typedef enum { NVJPEG_ORIENTATION_UNKNOWN = 0 , NVJPEG_ORIENTATION_NORMAL = 1 , NVJPEG_ORIENTATION_FLIP_HORIZONTAL = 2 , NVJPEG_ORIENTATION_ROTATE_180 = 3 , NVJPEG_ORIENTATION_FLIP_VERTICAL = 4 , NVJPEG_ORIENTATION_TRANSPOSE = 5 , NVJPEG_ORIENTATION_ROTATE_90 = 6 , NVJPEG_ORIENTATION_TRANSVERSE = 7 , NVJPEG_ORIENTATION_ROTATE_270 = 8 } nvjpegExifOrientation_t ; The nvjpegExifOrientation_t enum represents the exif orientation in a jfif(jpeg) file. Exif orientation information is typically used to denote the digital camera sensor orientation at the time of image capture. Member Description NVJPEG_ORIENTATION_UNKNOWN Exif orientation information is not available in the bitstream. NVJPEG_ORIENTATION_NORMAL Decode output remains unchanged. NVJPEG_ORIENTATION_FLIP_HORIZONTAL Decoded output should be mirrored/flipped horizontally. NVJPEG_ORIENTATION_ROTATE_180 Decoded output should be rotated 180 degrees. NVJPEG_ORIENTATION_FLIP_VERTICAL Decoded output should be mirrored/flipped vertically. NVJPEG_ORIENTATION_TRANSPOSE Decoded output should be flipped/mirrored horizontally followed by a 90 degrees counter-clockwise rotation. NVJPEG_ORIENTATION_ROTATE_90 Decoded output should be rotated 90 degrees counter-clockwise. NVJPEG_ORIENTATION_TRANSVERSE Decoded output should be flipped/mirrored horizontally followed by a 270 degrees counter-clockwise rotation. NVJPEG_ORIENTATION_ROTATE_270 Decoded output should be rotated 270 degrees counter-clockwise. 2.3. nvJPEG API Reference  This section describes the nvJPEG decoder API. 2.3.1. nvJPEG Helper API Reference  2.3.1.1. nvjpegGetProperty()  Gets the numeric value for the major or minor version, or the patch level, of the nvJPEG library. Signature: nvjpegStatus_t nvjpegGetProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. int *value Output Host The numeric value corresponding to the specific libraryPropertyType requested. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.2. nvjpegGetCudartProperty()  Gets the numeric value for the major version, minor version, or the patch level of the CUDA toolkit that was used to build nvJPEG library. For the same information on the nvJPEG library itself, see nvjpegGetProperty() . Signature: nvjpegStatus_t nvjpegGetCudartProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. int *value Output Host The numeric value corresponding to the specific libraryPropertyType requested. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.3. nvjpegCreate() [DEPRECATED]  Allocates and initializes the library handle. Note This function is deprecated. Use either nvjpegCreateSimple() or nvjpegCreateEx() functions to create the library handle. Signature: nvjpegStatus_t nvjpegCreate ( nvjpegBackend_t backend , nvjpegDevAllocator_t * allocator , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocator_t *allocator Input Host Device memory allocator. See nvjpegDevAllocator_t structure description. If NULL is provided, then the default CUDA runtime cudaMalloc() and cudaFree() functions will be used. nvjpegHandle_t *handle Input/Output Host The library handle. The nvjpegBackend_t parameter is an enum type, with the below enumerated list values: typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , } nvjpegBackend_t ; Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.4. nvjpegCreateSimple()  Allocates and initializes the library handle, with default codec implementations selected by library and default memory allocators. Signature: nvjpegStatus_t nvjpegCreateSimple ( nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t *handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.5. nvjpegCreateEx()  Allocates and initializes the library handle using the provided arguments. Signature: nvjpegStatus_t nvjpegCreateEx ( nvjpegBackend_t backend , nvjpegDevAllocator_t * dev_allocator , nvjpegPinnedAllocator_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocator_t *dev_allocator Input Host Device memory allocator. See nvjpegDevAllocator_t structure description. If NULL is provided, then the default CUDA runtime functions cudaMalloc() and cudaFree() will be used. nvjpegPinnedAllocator_t *pinned_allocator Input Host Pinned host memory allocator. See nvjpegPinnedAllocator_t structure description. If NULL is provided, then the default CUDA runtime functions cudaHostAlloc() and cudaFreeHost() will be used. unsigned int flags Input Host Refer to nvJPEG Flags for details. nvjpegHandle_t *handle Input/Output Host The library handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.6. nvjpegCreateExV2()  Allocates and initializes the library handle using the provided arguments. Signature: nvjpegStatus_t nvjpegCreateExV2 ( nvjpegBackend_t backend , nvjpegDevAllocatorV2_t * dev_allocator , nvjpegPinnedAllocatorV2_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocatorV2_t *dev_allocator Input Host Extended device memory allocator. See nvjpegDevAllocatorV2_t_t structure description. Cannot be NULL. nvjpegPinnedAllocatorV2_t *pinned_allocator Input Host Extended pinned memory allocator. See nvjpegPinnedAllocatorV2_t structure description. Cannot be NULL. unsigned int flags Input Host Refer to nvJPEG Flags for details. nvjpegHandle_t *handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.7. nvjpegDestroy()  Releases the library handle. Signature: nvjpegStatus_t nvjpegDestroy ( nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input/Output Host The library handle to release. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.8. nvjpegSetDeviceMemoryPadding()  Use the provided padding for all device memory allocations with specified library handle. A large number will help to amortize the need for device memory reallocations when needed. Signature: nvjpegStatus_t nvjpegSetDeviceMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Device memory padding to use for all further device memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.9. nvjpegGetDeviceMemoryPadding()  Retrieve the device memory padding that is currently used for the specified library handle. Signature: nvjpegStatus_t nvjpegGetDeviceMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Device memory padding that is currently used for device memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.10. nvjpegSetPinnedMemoryPadding()  Use the provided padding for all pinned host memory allocations with specified library handle. A large number will help to amortize the need for pinned host memory reallocations when needed. Signature: nvjpegStatus_t nvjpegSetPinnedMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Pinned host memory padding to use for all further pinned host memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.11. nvjpegGetPinnedMemoryPadding()  Retrieve the pinned host memory padding that is currently used for specified library handle. Signature: nvjpegStatus_t nvjpegGetPinnedMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Pinned host memory padding that is currently used for pinned host memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.12. nvjpegGetHardwareDecoderInfo()  Retrieve hardware decoder details such as number of engines and number of cores available in each engine. Signature: nvjpegStatus_t nvjpegGetHardwareDecoderInfo ( nvjpegHandle_t handle , unsigned int * num_engines , unsigned int * num_cores_per_engine ); Parameters: nvjpegHandle_t handle Input Host The library handle. unsigned int* num_engines Input/Output Host Retrieves number of engines available for decode. Return value of 0 indicates that hardware decoder is not available. unsigned int* num_cores_per_engine Input/Output Host Retrieves number of cores per engine. Return value of 0 indicates that hardware decoder is not available. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.13. nvjpegJpegStateCreate()  Allocates and initializes the internal structure required for the JPEG processing. Signature: nvjpegStatus_t nvjpegJpegStateCreate ( nvjpegHandle_t handle , nvjpegJpegState_t * jpeg_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t *jpeg_handle Input/Output Host The image state handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.14. nvjpegJpegStateDestroy()  Releases the image internal structure. Signature: nvjpegStatus_t nvjpegJpegStateDestroy ( nvjpegJpegState handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState handle Input/Output Host The image state handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.15. nvjpegDecoderCreate()  Creates a decoder handle. Signature: nvjpegStatus_t nvjpegDecoderCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegBackend_t implementation , nvjpegJpegDecoder_t * decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle. nvjpegBackend_t backend Input Host Backend parameter for the decoder_handle.The back end applies to all the functions under the decoupled API , when called with this handle. nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder state handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.16. nvjpegDecoderDestroy()  Destroys the decoder handle. Signature: nvjpegStatus_t nvjpegDecoderDestroy ( nvjpegJpegDecoder_t decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.17. nvjpegDecoderJpegSupported()  Determines whether the decoder_handle is able to handle the bit-stream stored in jpeg_stream . Signature: nvjpegStatus_t nvjpegDecoderJpegSupported ( nvjpegJpegDecoder_t decoder_handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input Host Decoder state handle nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data nvjpegDecodeParams_t decode_params Input Host Decoder output configuration int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.18. nvjpegDecoderStateCreate()  Creates the decoder_state internal structure. The decoder_state is associated with the nvjpegBackend_t implementation that was used to create the decoder_handle . Signature: nvjpegStatus_t nvjpegDecoderStateCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegJpegDecoder_t decoder_handle , nvjpegJpegState_t * decoder_state ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle. nvjpegJpegDecoder_t decoder_handle Input Host Decoder handle. nvjpegJpegState_t* decoder_state Input/Output Host nvJPEG Image State Handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.19. nvjpegJpegStreamCreate()  Creates jpeg_stream that is used to parse the JPEG bitstream and store bitstream parameters. Signature: nvjpegStatus_t nvjpegJpegStreamCreate ( nvjpegHandle_t handle , nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.20. nvjpegJpegStreamDestroy()  Destroys the jpeg_stream structure. Signature: nvjpegStatus_t nvjpegJpegStreamDestroy ( nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.21. nvjpegBufferPinnedCreate()  Creates a pinned buffer handle. Signature: nvjpegStatus_t nvjpegBufferPinnedCreate ( nvjpegHandle_t handle , nvjpegPinnedAllocator_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegPinnedAllocator_t* pinned_allocator Input Host Pinned host memory allocator. See nvjpegPinnedAllocator_t structure description. nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.22. nvjpegBufferPinnedCreateV2()  Creates a pinned buffer handle using extended allocators. Signature: nvjpegStatus_t nvjpegBufferPinnedCreateV2 ( nvjpegHandle_t handle , nvjpegPinnedAllocatorV2_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegPinnedAllocatorV2_t* pinned_allocator Input Host Extended pinned host memory allocator. See nvjpegPinnedAllocatorV2_t structure description. nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.23. nvjpegBufferPinnedDestroy()  Destroys a pinned buffer handle. Signature: nvjpegStatus_t nvjpegBufferPinnedDestroy ( nvjpegBufferPinned_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.24. nvjpegStateAttachPinnedBuffer()  Link the nvJPEG pinned buffer handle to decoder_state . The pinned_buffer is used by the decoder to store the intermediate information that is used across the decoding stages. Pinned buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory. Signature: nvjpegStatus_t nvjpegStateAttachPinnedBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferPinned_t pinned_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state. nvjpegBufferPinned_t pinned_buffer Input Host nvJPEG pinned buffer container. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.25. nvjpegBufferPinnedRetrieve()  Retrieves the pinned memory pointer and size from the nvJPEG pinned buffer handle. Allows the application to re-use the memory once the decode is complete. Signature: nvjpegStatus_t nvjpegBufferPinnedRetrieve ( nvjpegBufferPinned_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container. size_t* size Input/Output Host Size in bytes of the pinned buffer. void** ptr Input/Output Host Pointer to the pinned buffer. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.26. nvjpegBufferPinnedResize()  Resize the pinned buffer to the specified size in bytes. This API can be used to pre-allocate the pinned buffer\nto a large value and avoid allocator calls during decode. Signature: nvjpegStatus_t nvjpegBufferPinnedResize ( nvjpegBufferPinned_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container. size_t* size Input Host Size in bytes of the pinned buffer. cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferPinned_t buffer is initialized using stream ordered allocators. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.27. nvjpegBufferDeviceCreate()  Creates the device buffer handle. Signature: nvjpegStatus_t nvjpegBufferDeviceCreate ( nvjpegHandle_t handle , nvjpegDevAllocator_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDevAllocator_t* device_allocator Input Host Device memory allocator. See the `nvjpegDevAllocator_t <index.html#nvjpeg-memory-allocator-interface>`__ structure description. nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.28. nvjpegBufferDeviceCreateV2()  Creates the device buffer handle using extended allocators. Signature: nvjpegStatus_t nvjpegBufferDeviceCreateV2 ( nvjpegHandle_t handle , nvjpegDevAllocatorV2_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDevAllocatorV2_t* device_allocator Input Host Extended device memory allocator. See nvjpegDevAllocatorV2_t_t structure description. nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.29. nvjpegBufferDeviceDestroy()  Destroys the device buffer handle. Signature: nvjpegStatus_t nvjpegBufferDeviceDestroy ( nvjpegBufferDevice_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host/Device nvJPEG device buffer container. Device pointers are stored within the host structures. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.30. nvjpegStateAttachDeviceBuffer()  Link the nvJPEG device buffer handle to the decoder_state . The device_buffer is used by the decoder to store the intermediate information that is used across the decoding stages. Device buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory. Signature: nvjpegStatus_t nvjpegStateAttachDeviceBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferDevice_t device_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state. nvjpegBufferDevice_t device buffer Input Host/Device nvJPEG device buffer container. Device pointers are stored within the host structures. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.31. nvjpegBufferDeviceRetrieve()  Retrieve the device memory pointer and size from the nvJPEG device buffer handle. Allows the application to re-use the memory after the decode is complete. Signature: nvjpegStatus_t nvjpegBufferDeviceRetrieve ( nvjpegBufferDevice_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container. size_t* size Input/Output Host Device buffer size in bytes. void** ptr Input/Output Host Pointer to the device buffer. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.32. nvjpegBufferDeviceResize()  Resize the device buffer to the specified size in bytes. This API can be used to pre-allocate the device buffer\nto a large value and avoid allocator calls during decode. Signature: nvjpegStatus_t nvjpegBufferDeviceResize ( nvjpegBufferDevice_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container. size_t* size Input Host Size in bytes of the device buffer. cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferDevice_t buffer is initialized using stream ordered allocators. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.33. nvjpegDecodeParamsCreate()  Creates a handle for the parameters. The parameters that can be programmed include: output format, ROI decode, CMYK to RGB conversion. Signature: nvjpegStatus_t nvjpegDecodeParamsCreate ( nvjpegHandle_t handle , nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.34. nvjpegDecodeParamsDestroy()  Destroys the decode_params handle. Signature: nvjpegStatus_t nvjpegDecodeParamsDestroy ( nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2. Retrieve Encoded Image Information API  The helper functions for retrieving the encoded image information. 2.3.2.1. nvjpegGetImageInfo()  Decodes the JPEG header and retrieves the basic information about the image. Signature: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the encoded data. size_t length Input Host Size of the encoded data in bytes. int *nComponents Output Host Chroma subsampling for the 1- or 3- channel encoding. int *widths Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the width of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. If the channel is not encoded, then the corresponding value would be zero. int *heights Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the height of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. If the channel is not encoded, then the corresponding value would be zero. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2. nvJPEG Stream API  These functions store the parsed bit-stream data on the host. 2.3.2.2.1. nvjpegJpegStreamParse()  Parses the bitstream and stores the metadata in the jpeg_stream struct. Signature: nvjpegStatus_t nvjpegJpegStreamParse ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int save_metadata , int save_stream , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the bit-stream. size_t length Input Host Bit-stream size. int save_metadata Input Host (Not enabled. Marked for future use). If not 0, then the JPEG stream metadata (headers, app markers, etc.) will be saved in the internal JpegStream structure for future usage.\nIf 0, then the meta data (headers, app markerms etc.) will be discarded. int save_stream Input Host If not 0, then the whole jpeg stream will be copied to the internal JpegStream structure, and the pointer to the JPEG file data will not be needed after this call.\nIf 0, then JpegStream will just save the pointers (to JPEG file data), and these pointers will be used later during the image decoding. nvjpegJpegStream_t jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.2. nvjpegJpegStreamParseHeader()  Parses only the header of the bit-stream and stores the header information in the jpeg_stream struct. Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the bit-stream. size_t length Input Host Bit-stream size. nvjpegJpegStream_t jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.3. nvjpegJpegStreamParseTables()  To be used when decoding TIFF files with JPEG compression. Parses the JPEG tables bitstream and stores the jpeg tables in jpeg_stream Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the JPEG tables bitstream. Can be set to NULL to reset the JPEG tables. size_t length Input Host JPEG tables bitstream size. nvjpegJpegStream_t jpeg_stream Input/Output Host The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.4. nvjpegJpegStreamGetFrameDimensions()  Extracts the JPEG frame dimensions from the bitstream. Signature: nvjpegStatus_t nvjpegJpegStreamGetFrameDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int * width , unsigned int * height ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int* width Output Host Frame height. unsigned int* height Output Host Frame width. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.5. nvjpegJpegStreamGetComponentsNum()  Extracts the JPEG frame dimensions from the bitstream. Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentsNum ( nvjpegJpegStream_t jpeg_stream , unsigned int * components_num ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int* components_num Output Host Number of encoded channels in the input. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.6. nvjpegJpegStreamGetComponentDimensions()  Extracts the component dimensions from the bitstream. Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int component , unsigned int * width , unsigned int * height ) Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int component Input Host Component index. unsigned int* width Output Host Component height. unsigned int* height Output Host Component width. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.7. nvjpegJpegStreamGetChromaSubsampling()  Gets the chroma subsampling from the jpeg_stream . For grayscale (single channel) images it returns NVJPEG_CSS_GRAY. For 3-channel images it tries to assign one of the known chroma sub-sampling values based on the sampling information present in the bitstream, else it returns NVJPEG_CSS_UNKNOWN. If the number of channels is 2 or 4, then it returns NVJPEG_CSS_UNKNOWN. Signature: nvjpegStatus_t nvjpegJpegStreamGetChromaSubsampling ( nvjpegJpegStream_t jpeg_stream , nvjpegChromaSubsampling_t * chroma_subsampling ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. nvjpegChromaSubsampling_t* chroma_subsampling Output Host Chroma subsampling for the 1- or 3- channel encoding. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.8. nvjpegJpegStreamGetJpegEncoding()  This function obtains the JPEG encoding type from the jpeg_stream . For baseline images it returns NVJPEG_ENCODING_BASELINE_DCT. For progressive images it returns NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN. Signature: nvjpegStatus_t nvjpegJpegStreamGetJpegEncoding ( nvjpegJpegStream_t jpeg_stream , nvjpegJpegEncoding_t * jpeg_encoding ); Parameters: Parameter Input / Output Memory Description jpeg_stream In Host Input bitstream handle. jpeg_encoding Out Host Encoding type obtained—baseline or progressive. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.9. nvjpegJpegStreamGetExifOrientation()  Extracts the exif orientation from the bitstream. Returns NVJPEG_ORIENTATION_UNKNOWN if the exif marker/orientation information is not present. Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetExifOrientation ( nvjpegJpegStream_t jpeg_stream , nvjpegExifOrientation_t * orientation_flag ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. nvjpegExifOrientation_t *orientation_flag Output Host Exif orientation in JPEG stream. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.10. nvjpegJpegStreamGetSamplePrecision()  Extracts the sample precision(bit depth) from the bitstream. Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetSamplePrecision ( nvjpegJpegStream_t jpeg_stream , unsigned int * precision ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int *precision Output Host Sample precision value. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3. Decode API—Single Phase  Functions for decoding single image or batched images in a single phase. 2.3.3.1. ​nvjpegDecode()  Decodes a single image, and writes the decoded image in the desired format to the output buffers. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. From CUDA 11 onwards, nvjpegDecode() picks the best available back-end for a given image, user no longer has control on this. If there is a need to select the back-end, then consider using nvjpegDecodeJpeg . This is a new API added in CUDA 11 which allows user to control the back-end. Signature: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. const unsigned char *data Input Host Pointer to the encoded data. size_t length Input Host Size of the encoded data in bytes. nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved. nvjpegImage_t *destination Input/Output Host/Device Pointer to the structure that describes the output destination. This structure should be on the host (CPU), but the pointers in this structure should be pointing to the device (i.e., GPU) memory. See nvjpegImage_t. cudaStream_t stream Input Host The CUDA stream where all of the GPU work will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.2. ​nvjpegDecodeBatchedInitialize()  This function initializes the batched decoder state. The initialization parameters include the batch size, the maximum number of CPU threads, and the specific output format in which the decoded image will be saved. This function should be called once, prior to decoding the batches of images. Any currently running batched decoding should be finished before calling this function. Signature: nvjpegStatus_t nvjpegDecodeBatchedInitialize ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int max_cpu_threads , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. int batch_size Input Host Batch size. int max_cpu_threads Input Host This parameter is no longer used by the library. nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.3. ​nvjpegDecodeBatched()  Decodes the batch of images, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. Signature: nvjpegStatus_t nvjpegDecodeBatched ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. const unsigned char *const *data Input Host Pointer to the first element of array of the input data. The size of the array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() batch initialization function. const size_t *lengths Input Host Pointer to the first element of array of input sizes. Size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function. nvjpegImage_t *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors. The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize(), the batch initialization function. See also nvjpegImage_t . cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.4. nvjpegDecodeBatchedEx()  This API helps to Decodes the batch of images with ROI, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. Signature: nvjpegStatus_t nvjpegDecodeBatchedEx ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , nvjpegDecodeParams_t * decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. const unsigned char *const *data Input Host Pointer to the first element of array of the input data. The size of the array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() batch initialization function. const size_t *lengths Input Host Pointer to the first element of array of input sizes. nvjpegImage_t *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors. The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function. See also nvjpegImage_t . nvjpegDecodeParams_t *decode_params Input Host Setting ROI Decode parameters cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.5. nvjpegDecodeBatchedSupported()  This API helps determine whether an image can be decoded by nvjpegDecodeBatched . User can parse the bitstream header using nvjpegJpegStreamParseHeader and then call this API to determine whether the image can be decoded. Signature: nvjpegStatus_t nvjpegDecodeBatchedSupported ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle. nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data. int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.6. nvjpegDecodeBatchedSupportedEx()  This API helps determine whether an image can be decoded by nvjpegDecodeBatchedEx . User can parse the bitstream header using nvjpegJpegStreamParseHeader and set the ROI in the decode params then call this API to determine whether the image can be decoded. Signature: nvjpegStatus_t nvjpegDecodeBatchedSupportedEx ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle. nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data. nvjpegDecodeParams_t decode_params Input Host Setting ROI Decode parameters. int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , a non zero value indicates that the bitstream is not supported. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.7. nvjpegDecodeBatchedPreAllocate()  This is an experimental API that can be used with nvjpegDecodeBatched . When decoding images with varying sizes and chroma subsampling, performance is limited by the repeated cuda calls made by the library to free/allocate device memory. This API attempts to avoid this problem by allocating device memory prior to the actual decoding. Users have the option to call this API with values that are unlikely to be exceeded when nvjpegDecodeBatched is called. Note Note:\nThis functionality is available only when the nvjpegHandle_t is instantiated using NVJPEG_BACKEND_HARDWARE. It is currently a No Op for other backends. This API only provides a hint for initial allocation. If the image dimensions at the time of decode exceed what was provided, then the library will resize the device buffers. If the images being decoded have different chroma subsamplings, then the chroma_subsampling field should be set to NVJPEG_CSS_444 to ensure that the device memory can be reused. Signature: nvjpegStatus_t nvjpegDecodeBatchedPreAllocate ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int width , int height , nvjpegChromaSubsampling_t chroma_subsampling , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. int batch_size Input Host Batch size. int width Input Host Maximum width of image that will be decoded. int height Input Host Maximum height of image that will be decoded. nvjpegChromaSubsampling_t chroma_subsampling Input Host Chroma-subsampling of the images. nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.8. nvjpegDecodeBatchedParseJpegTables()  To be used along with batched decode APIs when decoding JPEG bitstreams from a TIFF file. This function parses the JPEG tables bitstream to extract the JPEG tables. The external Huffman and quantization tables will be applied to all the JPEG bitstreams in the batch. Signature: nvjpegStatus_t nvjpegDecodeBatchedParseJpegTables ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , const size_t length ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input/Output Host/Device The image state handle. const unsigned char *data Input Host Pointer to the JPEG tables bitstream. Can be set to NULL to reset the jpeg tables. size_t length Input Host JPEG tables bitstream size. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4. Decode API—Decoupled Decoding  This set of decoding API works with the bitstream handles, decode parameter handles, pinned and device buffers handles as input, thus decoupling JPEG bitstream parse, buffer management and setting up decoder parameters from the decode process itself. Currently only multiphase decoding is available. Multiphase decoupled single image decoding consists of three phases: Host Mixed Device Each of the above decodings is carried on according to its individual semantics. Phases on different images can be carried out with different decoding state handles simultaneously, while sharing of some helper objects is possible. See the details of semantics in the individual phases descriptions. Below are a couple of examples of using decoupled API. The following snippet explains how to use the API to prefetch the host stage of the processing: first do all of the host work on the host, and then submit the rest of decoding work to the device. #define BATCH_SIZE 2 nvjpegHandle_t nvjpeg_handle ; nvjpegJpegState_t nvjpeg_decoder_state [ BATCH_SIZE ]; nvjpegBufferPinned_t nvjpeg_pinned_buffer [ BATCH_SIZE ]; nvjpegBufferDevice_t nvjpeg_device_buffer ; nvjpegJpegStream_t nvjpeg_jpeg_stream [ BATCH_SIZE ]; nvjpegDecodeParams_t nvjpeg_decode_params ; nvjpegJpegDecoder_t nvjpeg_decoder ; nvjpegBackend_t impl = NVJPEG_BACKEND_DEFAULT ; unsigned char * bitstream [ BATCH_SIZE ] // pointers jpeg bitstreams size_t length [ BATCH_SIZE ]; // bitstream sizes nvjpegImage_t output_images [ BATCH_SIZE ]; // all the images in the batch will be decoded as RGBI nvjpegDecodeParamsSetOutputFormat ( nvjpeg_decode_params , NVJPEG_OUTPUT_RGBI ); // call host phase for two bitstreams for ( int i = 0 ; i < BATCH_SIZE ; i ++ ) { nvjpegJpegStreamParse ( nvjpeg_handle , bitstream [ i ], length [ i ], 0 , 0 , nvjpeg_jpeg_stream [ i ]); nvjpegStateAttachPinnedBuffer ( nvjpeg_decoder_state [ i ], nvjpeg_pinned_buffer [ i ]); nvjpegDecodeJpegHost ( nvjpeg_handle , nvjpeg_decoder , nvjpeg_decoder_state [ i ], nvjpeg_decode_params , nvjpeg_jpeg_stream [ i ]) } for ( int i = 0 ; i < BATCH_SIZE ; i ++ ) { // same device buffer being used for decoding bitstreams nvjpegStateAttachDeviceBuffer ( nvjpeg_decoder_state [ i ], nvjpeg_device_buffer ); // cuda stream set to NULL nvjpegDecodeJpegTransferToDevice ( nvjpeg_handle , nvjpeg_decoder , nvjpeg_decoder_state [ i ], nvjpeg_jpeg_stream [ i ], NULL ); // cuda stream set to NULL nvjpegDecodeJpegDevice ( nvjpeg_handle , nvjpeg_decoder , nvjpeg_decoder_state [ i ], & output_images [ i ], NULL ); cudaDeviceSynchronize (); } The following snippet explains how pinned and device buffers can be shared across two instances of nvJPEG Decoder Handle . #define BATCH_SIZE 4 nvjpegHandle_t nvjpeg_handle ; nvjpegJpegDecoder_t nvjpeg_decoder_impl1 ; nvjpegJpegDecoder_t nvjpeg_decoder_impl2 ; nvjpegJpegState_t nvjpeg_decoder_state_impl1 ; nvjpegJpegState_t nvjpeg_decoder_state_impl2 ; nvjpegBufferPinned_t nvjpeg_pinned_buffer ; nvjpegBufferDevice_t nvjpeg_device_buffer ; nvjpegJpegStream_t nvjpeg_jpeg_stream ; nvjpegDecodeParams_t nvjpeg_decode_params ; unsigned char * bitstream [ BATCH_SIZE ] // pointers jpeg bitstreams size_t length [ BATCH_SIZE ]; // bitstream sizes // populate bitstream and length correctly for this code to work nvjpegImage_t output_images [ BATCH_SIZE ]; // allocate device memory for output images, for this snippet to work nvjpegStateAttachPinnedBuffer ( nvjpeg_decoder_state_impl1 , nvjpeg_pinned_buffer ); nvjpegStateAttachPinnedBuffer ( nvjpeg_decoder_state_impl2 , nvjpeg_pinned_buffer ); nvjpegStateAttachDeviceBuffer ( nvjpeg_decoder_state_impl1 , nvjpeg_device_buffer ); nvjpegStateAttachDeviceBuffer ( nvjpeg_decoder_state_impl2 , nvjpeg_device_buffer ); // all the images in the batch will be decoded as RGBI nvjpegDecodeParamsSetOutputFormat ( nvjpeg_decode_params , NVJPEG_OUTPUT_RGBI ); for ( int i = 0 ; i < BATCH_SIZE ; i ++ ) { nvjpegJpegStreamParse ( nvjpeg_handle , bitstream [ i ], length [ i ], 0 , 0 , nvjpeg_jpeg_stream ); // decide which implementation to use, based on image size unsigned int frame_width ; unsigned int frame_height ; nvjpegJpegStreamGetFrameDimensions ( nvjpeg_jpeg_stream , & frame_width , & frame_height )); nvjpegJpegDecoder_t & decoder = ( frame_height * frame_width > 1024 * 768 ) ? nvjpeg_decoder_impl2 : nvjpeg_decoder_impl1 ; nvjpegJpegState_t & decoder_state = ( frame_height * frame_width > 1024 * 768 ) ? nvjpeg_decoder_state_impl2 : nvjpeg_decoder_state_impl1 ; nvjpegDecodeJpegHost ( nvjpeg_handle , decoder , decoder_state , nvjpeg_decode_params , nvjpeg_jpeg_stream ); // cuda stream set to NULL nvjpegDecodeJpegTransferToDevice ( nvjpeg_handle , decoder , decoder_state , nvjpeg_jpeg_stream , NULL ); // cuda stream set to NULL nvjpegDecodeJpegDevice ( nvjpeg_handle , nvjpeg_decoder , decoder_state , & output_images , NULL ); cudaDeviceSynchronize (); } 2.3.4.1. nvjpegDecodeJpegHost()  This is the first stage of the decoupled decoding process. It is done entirely on the host, hence it is synchronous with respect to the host. If a pinned buffer is attached to the decoder state, then the pinned buffer object will be used to allocate the pinned memory required for the host decoding phase. There wouldn’t be allocation if the pinned buffer object already handles the required amount of pinned memory. If a pinned buffer object is not attached, then the state will use heap host memory to allocate the memory required for the host processing. In this phase, device is not participating. Hence the device selection, device initialization, and device memory initialization can be done later in the decoding process. This function works on a parsed stream. The parsed stream handle that is available after calling the nvjpegJpegStreamParse() function should be provided to this function. Signature: nnvjpegStatus_t nvjpegDecodeJpegHost ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegDecodeParams_t decode_params , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegDecodeParams_t decode_params Input Host Handle to decode the output properties. nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4.2. nvjpegDecodeJpegTransferToDevice()  This phase contains both host and device operations. Hence it is a mix of synchronous and asynchronous operations with respect to the host. All the device operations will be submitted to the provided stream. This phase should be called only after the host phase with the same decoder handle, decoder state handle and parsed jpeg stream handle. Device should be initialized and device buffer should be attached to decoder_state handle using nvjpegStateAttachDeviceBuffer() prior to calling this API. This device buffer object will be resized to the required amount of memory if needed. For the host memory buffer, this phase will use whatever was used in the host phase: either the attached pinned buffer or the state’s host memory buffer. Signature: nvjpegStatus_t nvjpegDecodeJpegTransferToDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4.3. nvjpegDecodeJpegDevice()  This phase consists of decode operations that take place mainly on the device (no significant host side computation is done). Hence this phase is asynchronous with respect to the host. This phase should be called after nvjpegDecodeJpegTransferToDevice() for a given decoder_state handle and decoder handle. In this function call, the host memory buffers are not used, so if the pinned buffer was attached to the state, then it can be reused somewhere else. Note that at this point the Jpeg stream handle is not needed anymore, since parts that are needed for device decoding will be copied to the device memory in the previous phase. Signature: nvjpegStatus_t nvjpegDecodeJpegDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegImage_t *destination Input/Output Host/Device Pointer to a structure that describes the output destination. This structure should be on host, but the pointers in this structure should be pointing to the device memory. See nvJPEG Image for details. cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4.4. nvjpegDecodeJpeg()  This is a single phase API with the flexibility to select nvJPEG back-end when creating an nvjpegJpegDecoder_t object. The user has the option to call this API instead of making three separate calls to nvjpegDecodeJpegHost() , nvjpegDecodeJpegTransferToDevice() , and nvjpegDecodeJpegDevice() . It is required to atttach the device buffer to the decoder state before calling this API. The pinned buffer is optional. If the pinned buffer is not attached, then heap memory will be used for host processing. This function works on a parsed stream. The parsed stream handle that is available after calling the nvjpegJpegStreamParse() function should be provided to this function. Signature: nvjpegStatus_t nvjpegDecodeJpeg ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_bitstream , nvjpegImage_t * destination , nvjpegDecodeParams_t decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. nvjpegImage_t *destination Input/Output Host/Device Pointer to a structure that describes the output destination. This structure should be on the host, but the pointers in this structure should be pointing to the device memory. See nvJPEG Image for details. nvjpegDecodeParams_t decode_params Input Host The handle which stores the decode output properties. cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5. nvJPEG Decode Parameters  This category of APIs is used to set the decoding parameters. These APIs should be used with the decode APIs defined in Decode API—Decoupled Decoding . 2.3.5.1. nvjpegDecodeParamsSetOutputFormat()  This function is used to set the decode output format. See nvjpegOutputFormat_t described in step 6 of Single Image Decoding . The output parameter of nvjpegOutputFormat_t defaults to NVJPEG_OUTPUT_UNCHANGED if not set using this API. Signature: nvjpegStatus_t nvjpegDecodeParamsSetOutputFormat ( nvjpegDecodeParams_t decode_params , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. nvjpegOutputFormat_t output_format Input Host See step 6 of Single Image Decoding . Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.2. nvjpegDecodeParamsSetROI()  This function enables the region of interest-only (ROI-only) decode. To disable the ROI-only, i.e., to decode the whole image, set: offset_x = 0, offset_y = 0, roi_width = -1, and roi_height = -1. Note ROI decode is disabled by default. It is not supported when the nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE. The ROI window cannot go out of image bounds. That is: offset_x cannot be lower than zero, or offset_x + roi_width cannot be larger than the JPEG image width. If the output format is NVJPEG_OUTPUT_YUV or NVJPEG_OUTPUT_UNCHANGED, then the offset_x and offset_y values have to be multiples of the maximum subsampling factor, as defined in the JPEG standard. Signature: nvjpegStatus_t nvjpegDecodeParamsSetROI ( nvjpegDecodeParams_t decode_params , int offset_x , int offset_y , int roi_width , int roi_height ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host The decode output parameter handle. int offset_x Input Host Image offset along the horizontal direction relative to the top left corner. int offset_y Input Host Image offset along the vertical direction relative to the top left corner. int roi_width Input Host Image width relative to offset_x . int roi_height Input Host Image height relative to offset_y . Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.3. nvjpegDecodeParamsSetAllowCMYK()  If enabled, the nvJPEG library assumes that the JPEG with 4 encoded color components is in CMYK colorspace, and enables the conversion to RGB/YUV colorspace. The CMYK-to-RGB conversion is disabled by default. The conversion is based on the subtractive scheme—this behavior matches OpenCV’s handling of 4-component JPEGs. Signature: nvjpegStatus_t nvjpegDecodeParamsSetAllowCMYK ( nvjpegDecodeParams_t decode_params , int allow_cmyk ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. int allow_cmyk Input Host Enable CMYK to RGB conversion. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.4. nvjpegDecodeParamsSetScaleFactor()  Allows the user to scale decode output. Note This feature is currently supported only when nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE. Signature: nvjpegStatus_t nvjpegDecodeParamsSetScaleFactor ( nvjpegDecodeParams_t decode_params , nvjpegScaleFactor_t scale_factor ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. nvjpegScaleFactor_t scale_factor Input Host Set the scaling factor for the decode output. The scale factor is set to NVJPEG_SCALE_NONE by default. The supported values are listed here . When setting a scale factor value, the recommended allocation of the destination parameters is as follows: Use nvjpegGetImageInfo() , or nvjpegJpegStreamGetFrameDimensions() to extract the dimensions of each channel. Let height[NVJPEG_MAX_COMPONENT] and width[NVJPEG_MAX_COMPONENT] be 2 arrays which store the height and width. The index to these arrays correspond to the channel id. For a channel c, the scaled dimensions are calculated as follows: scaled_height[c] = (height[c] + rounding_factor - 1)/rounding_factor scaled_width[c] = (width[c] + rounding_factor - 1)/rounding_factor when scale_factor = NVJPEG_SCALE_NONE, rounding_factor = 1 when scale_factor = NVJPEG_SCALE_1_BY_2, rounding_factor = 2 when scale_factor = NVJPEG_SCALE_1_BY_4, rounding_factor = 4 when scale_factor = NVJPEG_SCALE_1_BY_8, rounding_factor = 8 For the output_format: NVJPEG_OUTPUT_Y destination.pitch[0] should be at least: width[0] destination.channel[0] should be at least of size: destination.pitch[0]*height[0] For the output_format destination.pitch[c] should be at least: destination.channel[c] should be at least of size: NVJPEG_OUTPUT_YUV width[c] for c = 0, 1, 2 destination.pitch[c]*height[c] for c = 0, 1, 2 NVJPEG_OUTPUT_RGB and NVJPEG_OUTPUT_BGR width[0] for c = 0, 1, 2 destination.pitch[0]*height[0] for c = 0, 1, 2 NVJPEG_OUTPUT_RGBI and NVJPEG_OUTPUT_BGRI width[0]*3 destination.pitch[0]*height[0] NVJPEG_OUTPUT_UNCHANGED width[c] for c = [ 0, nComponents - 1 ] destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.5. nvjpegDecodeParamsSetExifOrientation()  This function is used to generate the decoded output based on the exif orientation parameter. When ExifOrientation is enabled, the output buffers should be allocated based on the rotated dimensions. If the orientation is set as NVJPEG_ORIENTATION_UNKNOWN , the library will default to NVJPEG_ORIENTATION_HORIZONTAL . ROI Decode and EXIF rotation Exif rotation and ROI Decode can be enabled together. The ROI coordinates should be in the rotated space. Signature: nvjpegStatus_t nvjpegDecodeParamsSetExifOrientation ( nvjpegDecodeParams_t decode_params , nvjpegExifOrientation_t orientation ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. nvjpegExifOrientation_t orientation Input Host Set the exif orientation for the decode output. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.6. nvJPEG API Return Codes  The nvJPEG API adheres to the following return codes and their indicators: typedef enum { NVJPEG_STATUS_SUCCESS = 0 , NVJPEG_STATUS_NOT_INITIALIZED = 1 , NVJPEG_STATUS_INVALID_PARAMETER = 2 , NVJPEG_STATUS_BAD_JPEG = 3 , NVJPEG_STATUS_JPEG_NOT_SUPPORTED = 4 , NVJPEG_STATUS_ALLOCATOR_FAILURE = 5 , NVJPEG_STATUS_EXECUTION_FAILED = 6 , NVJPEG_STATUS_ARCH_MISMATCH = 7 , NVJPEG_STATUS_INTERNAL_ERROR = 8 , NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED = 9 } nvjpegStatus_t ; Description of the returned error codes: Returned Error (Returned Code) Description NVJPEG_STATUS_SUCCESS (0) The API call has finished successfully. Note that many of the calls are asynchronous and some of the errors may be seen only after synchronization. NVJPEG_STATUS_NOT_INITIALIZED (1) The library handle was not initialized. A call to nvjpegCreate() is required to initialize the handle. NVJPEG_STATUS_INVALID_PARAMETER (2) Wrong parameter was passed. For example, a null pointer as input data, or an image index not in the allowed range. NVJPEG_STATUS_BAD_JPEG (3) Cannot parse the JPEG stream. Check that the encoded JPEG stream and its size parameters are correct. NVJPEG_STATUS_JPEG_NOT_SUPPORTED (4) Attempting to decode a JPEG stream that is not supported by the nvJPEG library. NVJPEG_STATUS_ALLOCATOR_FAILURE (5) The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code. NVJPEG_STATUS_EXECUTION_FAILED (6) Error during the execution of the device tasks. NVJPEG_STATUS_ARCH_MISMATCH (7) The device capabilities are not enough for the set of input parameters provided (input parameters such as backend, encoded stream parameters, output format). NVJPEG_STATUS_INTERNAL_ERROR (8) Error during the execution of the device tasks. NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED (9) Not supported. NVJPEG_STATUS_INCOMPLETE_BITSTREAM (10) Bitstream input data incomplete 2.3.7. nvJPEG Chroma Subsampling  One of the outputs of the nvjpegGetImageInfo() API is nvjpegChromaSubsampling_t . This parameter is an enum type, and its enumerator list comprises of the chroma subsampling property retrieved from the encoded JPEG image. The nvjpegGetImageInfo() function currently supports the following chroma subsampling types: typedef enum { NVJPEG_CSS_444 , NVJPEG_CSS_422 , NVJPEG_CSS_420 , NVJPEG_CSS_440 , NVJPEG_CSS_411 , NVJPEG_CSS_410 , NVJPEG_CSS_GRAY , NVJPEG_CSS_410V , NVJPEG_CSS_UNKNOWN } nvjpegChromaSubsampling_t ; 2.3.8. Reference Documents  Refer to the JPEG standard: https://jpeg.org/jpeg/ 2.4. Examples of nvJPEG  nvJPEG Decode sample can be found here: https://github.com/NVIDIA/CUDALibrarySamples/tree/master/nvJPEG/nvJPEG-Decoder 3. JPEG Encoding  This section describes the encoding functions of the nvJPEG Library. 3.1. Using the Encoder  The user should perform the below prerequisite steps before calling the nvJPEG encoding functions. See also nvJPEG Encoder Helper API Reference . 3.1.1. Encoding the Parameters  The user should create an encoding parameters structure with nvjpegEncoderParamsCreate() function. The function will be initialized with default parameters. User can use an appropriate nvjpegEncoderParamsSet*() function to set a specific parameter. The quality parameter can be set, using the nvjpegEncoderParamsSetQuality() function, to an integer value between 1 and 100, and this quality parameter will be used as a base for generating the JPEG quantization tables. Note Occasionally, when encoding high entropy input data, such as random images, the encoding can fail if the quality parameter is set too high. This is due to the fact that the compressed bitstream would be larger than the input image. We recommend restarting the encoding with slightly lower quality factor or using a real-world images if possible. The parameters structure should be passed to compression functions. Note The encoding parameters structure can be reused to compress multiple images simultaneously, but no changes to the parameters should be made during the ongoing encoding, or the encoding result will be undefined. 3.1.2. Encoding the State  The user should create the encoding state structure using nvjpegEncoderStateCreate() function. This function will hold intermediate buffers for the encoding process. This state should be passed to the compression functions. Note The encoding state structure can be reused to encode a series of images, but no encoding should be performed on multiple images with the same encoding state at the same time—otherwise the result of the encodings will be undefined. 3.1.3. Encoding the Image  The nvJPEG library provides a few interfaces for compressing the image in different formats and colorspaces. See below. 3.1.3.1. nvjpegEncodeYUV  Input for this function is an image in YUV colorspace. See nvjpegEncodeYUV() . The source argument should be filled with the corresponding YUV planar data. The chroma_subsampling argument should have the chroma subsampling of the input data. If the chroma subsampling in the encoding parameters is the same as input chroma subsampling, then the user’s input data will be directly used in the JPEG compression. Otherwise chroma will be resampled to match the chroma subsampling of the encoding parameters. Input data should be provided with respect to the subsampling factors. That is, the chrominance image planes should have sizes aligned to the corresponding subsamplings. For example: Image dimensions: 123x321 Input chroma subsampling: NVJPEG_CSS_410 Chroma subsampling factor for this chroma subsampling: 4x2 Given the above, the encoder library expects the user to provide: Y plane with size: 123 x 321 Cb and Cr plane with size: 31 x 161 3.1.3.2. nvjpegEncodeImage  See nvjpegEncodeImage() . Input for this function, i.e., how data should be provided in the source argument, is determined by the input_format argument. For the interleaved formats (ending with I ) only the first channel is used. For the non-interleaved formats, all the channels in the input format are used. For example, if the user has interleaved the RGB image of size W x H , stored continuously, and the pointer to it is pImage , then source should be: source.channel[0] = pImage source.pitch[0] = W*3 When the same image is stored in planar format, with image planes pointers stored continuously in the array pImage[3] , then source should be: source.channel[0] = pImage[0] source.channel[1] = pImage[1] source.channel[2] = pImage[2] The pitch values for each channel in the source parameter should be set accordingly to the data layout. The nvJPEG library will perform the color transformation to the YCbCr, and will compress the result. 3.1.4. Retrieving the Compressed Stream  Often it is not feasible to accurately predict the final compressed data size of the final JPEG stream for any input data and parameters. The nvJPEG library, while encoding, will calculate the size of the final stream, allocate temporary buffer in the encoder state and save the compressed data in the encoding state’s buffer. In order to get final compressed JPEG stream, the user should provide the memory buffer large enough to store this compressed data. There are two options for how to do this: Use the upper bound on compressed JPEG stream size for the given parameters and image dimensions: Use the nvjpegEncodeRetrieveBitstream() function to retrieve the maximum possible JPEG stream size at any given time. Allocate the memory buffer at any given time. Encode the image using one of the encoding functions. Retrieve the compressed JPEG stream from the encoder state after successful encoding, using the nvjpegEncodeRetrieveBitstream() and the allocated buffer. Wait for the encoding to complete, and retrieve the exact size of required buffer, as below: Encode the image using one of the encoding functions. Use the nvjpegEncodeRetrieveBitstream() function to retrieve the size in bytes of the compressed JPEG stream. Allocate the memory buffer of at least this size. Use the nvjpegEncodeRetrieveBitstream() function to populate your buffer with the compressed JPEG stream. Note As the same encoding image state can be reused to compress a series of images, the nvjpegEncodeRetrieveBitstream() function will return the result for the last compressed image. 3.1.5. JPEG Encoding Example  See below the example code, and the block diagram shown in Figure 1 , for encoding with nvJPEG Encoder. JPEG Encoding Using nvJPEG Encoder  nvjpegHandle_t nv_handle ; nvjpegEncoderState_t nv_enc_state ; nvjpegEncoderParams_t nv_enc_params ; cudaStream_t stream ; // initialize nvjpeg structures nvjpegCreateSimple ( & nv_handle ); nvjpegEncoderStateCreate ( nv_handle , & nv_enc_state , stream ); nvjpegEncoderParamsCreate ( nv_handle , & nv_enc_params , stream ); nvjpegImage_t nv_image ; // Fill nv_image with image data, let's say 640x480 image in RGB format // Compress image nvjpegEncodeImage ( nv_handle , nv_enc_state , nv_enc_params , & nv_image , NVJPEG_INPUT_RGB , 640 , 480 , stream ); // get compressed stream size size_t length ; nvjpegEncodeRetrieveBitstream ( nv_handle , nv_enc_state , NULL , & length , stream ); // get stream itself cudaStreamSynchronize ( stream ); std :: vector < char > jpeg ( length ); nvjpegEncodeRetrieveBitstream ( nv_handle , nv_enc_state , jpeg . data (), & length , 0 ); // write stream to file cudaStreamSynchronize ( stream ); std :: ofstream output_file ( \"test.jpg\" , std :: ios :: out | std :: ios :: binary ); output_file . write ( jpeg . data (), length ); output_file . close (); 3.2. nvJPEG Encoder Type Declarations  This section describes the nvJPEG Encoder Type Declarations. 3.2.1. nvjpegInputFormat_t  typedef enum { NVJPEG_INPUT_RGB = 3 , NVJPEG_INPUT_BGR = 4 , NVJPEG_INPUT_RGBI = 5 , NVJPEG_INPUT_BGRI = 6 } nvjpegInputFormat_t ; The nvjpegInputFormat_t enum is used to select the color model and pixel format of the input image. It is used for conversion to YCbCr during encoding. Member Description NVJPEG_INPUT_RGB Input image is in RGB color model. Pixel format is RGB. NVJPEG_INPUT_BGR Input image is in RGB color model. Pixel format is BGR. NVJPEG_INPUT_RGBI Input image is in RGB color model. Pixel format is interleaved RGB. NVJPEG_INPUT_BGRI Input image is in RGB color model. Pixel format is interleaved BGR. 3.2.2. nvjpegEncoderState_t  The nvjpegEncoderState_t structure stores intermediate buffers and variables used for compression. 3.2.3. nvjpegEncoderParams_t  The nvjpegEncoderParams_t structure stores JPEG encode parameters. 3.3. nvJPEG Encoder Helper API Reference  The nvJPEG Encoder helper functions are used for initializing. 3.3.1. nvjpegEncoderStateCreate()  Creates encoder state that stores intermediate buffers used in compression. Signature: nvjpegStatus_t nvjpegEncoderStateCreate ( nvjpegHandle_t handle , nvjpegEncoderState_t * encoder_state , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_state Output Host Pointer to the encoder state structure, where the new state will be placed. stream Inputt Host CUDA stream where all the required device operations will be placed. 3.3.2. nvjpegEncoderStateDestroy()  Destroys the encoder state. Signature: nvjpegStatus_t nvjpegEncoderStateDestroy ( nvjpegEncoderState_t encoder_state ); Parameters: Parameter Input / Output Memory Description encoder_state Input/Output Host Encoder state structure that will be released. 3.3.3. nvjpegEncoderParamsCreate()  Creates the structure that holds the compression parameters. Signature: nvjpegStatus_t nvjpegEncoderParamsCreate ( nvjpegHandle_t handle , nvjpegEncoderParams_t * encoder_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_params Output Host Pointer to the location where the new parameters structure will be placed. stream Inputt Host CUDA stream where all the required device operations will be placed. 3.3.4. nvjpegEncoderParamsDestroy()  Destroys the encoder parameters structure. Signature: nvjpegEncoderParamsDestroy ( nvjpegEncoderParams_t encoder_params ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder params structure that will be released. 3.3.5. nvjpegEncoderParamsSetEncoding()  Sets the parameter quality in the encoder parameters structure. Signature: nvjpegStatus_t nvjpegEncoderParamsSetEncoding ( nvjpegEncoderParams_t encoder_params , nvjpegJpegEncoding_t etype , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. etype Input Host Encoding type selection (Baseline/Progressive). Default is Baseline. stream Input Host CUDA stream where all the required device operations will be placed. 3.3.6. nvjpegEncoderParamsSetQuality()  Sets the parameter quality in the encoder parameters structure. Signature: nvjpegStatus_t nvjpegEncoderParamsSetQuality ( nvjpegEncoderParams_t encoder_params , const int quality , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameterss structure handle. quality Input Host Integer value of quality between 1 and 100, where 100 is the highest quality. Default value is 70. stream Input Host CUDA stream where all the required device operations will be placed. 3.3.7. nvjpegEncoderParamsSetOptimizedHuffman()  Sets whether or not to use optimized Huffman. Using optimized Huffman produces smaller JPEG bitstream sizes with the same quality, but with slower performance. Signature: nvjpegStatus_t nvjpegEncoderParamsSetOptimizedHuffman ( nvjpegEncoderParams_t encoder_params , const int optimized , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. optimized Input Host If this value is 0 then non-optimized Huffman will be used. Otherwise optimized version will be used. Default value is 0. stream Input Host CUDA stream where all the required device operations will be placed. 3.3.8. nvjpegEncoderParamsSetSamplingFactors()  Sets which chroma subsampling will be used for JPEG compression. Signature: nvjpegStatus_t nvjpegEncoderParamsSetSamplingFactors ( nvjpegEncoderParams_t encoder_params , const nvjpegChromaSubsampling_t chroma_subsampling , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. chroma_subsampling Input Host Chroma subsampling that will be used for JPEG compression. If the input is in YUV color model and chroma_subsampling is different from the subsampling factors of source image, then the NVJPEG library will convert subsampling to the value of chroma_subsampling . Default value is 4:4:4. stream Input Host CUDA stream where all the required device operations will be placed. 3.4. nvJPEG Encoder API Reference  This section describes the nvJPEG Encoder API. 3.4.1. nvjpegEncodeGetBufferSize()  Returns the maximum possible buffer size that is needed to store the compressed JPEG stream, for the given input parameters. Signature: nvjpegStatus_t nvjpegEncodeGetBufferSize ( nvjpegHandle_t handle , const nvjpegEncoderParams_t encoder_params , int image_width , int image_height , size_t * max_stream_length ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_params Input/Output Host Encoder parameters structure handle. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.2. nvjpegEncodeYUV()  Compresses the image in YUV colorspace to JPEG stream using the provided parameters, and stores it in the state structure. Signature: nvjpegStatus_t nvjpegEncodeYUV ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegChromaSubsampling_t chroma_subsampling , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream. encoder_params Input Host Encoder parameters structure handle. source Input Host Pointer to the nvjpeg structure that holds the device pointers to the Y, U(Cb) and V(Cr) image planes and the respective strides. chroma_subsampling Input Host Chroma subsampling of the input data. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.3. nvjpegEncodeImage()  Compresses the image in the provided format to the JPEG stream using the provided parameters, and stores it in the state structure. Signature: nvjpegStatus_t nvjpegEncodeImage ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegInputFormat_t input_format , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream. encoder_params Input Host Encoder parameters structure handle. source Input Host Pointer to the nvjpeg structure that holds the device pointers to the Y, U(Cb) and V(Cr) image planes and the respective strides. input_format Input Host Value of nvjpegInputFormat_t type that describes the input data. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.4. nvjpegEncodeRetrieveBitstream()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. If data parameter is NULL then the encoder will return compressed stream size in the length parameter. If data is not NULL then the provided length parameter should contain the data buffer size. If the provided length is less than compressed stream size, then an error will be returned. Otherwise the compressed stream will be stored in the data buffer and the actual compressed buffer size will be stored in the length parameter. Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstream ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host The encoder_state that was previously used in one of the encoder functions. data Input/Output Host Pointer to the buffer in the host memory where the compressed stream will be stored. Can be NULL (see description). length Input/Output Host Pointer to the input buffer size. On return the NVJPEG library will store the actual compressed stream size in this parameter. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.5. nvjpegEncodeRetrieveBitstreamDevice()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. data parameter should be on device memory If data parameter is NULL then the encoder will return compressed stream size in the length parameter. If data is not NULL then the provided length parameter should contain the data buffer size. If the provided length is less than compressed stream size, then an error will be returned. Otherwise the compressed stream will be stored in the data buffer and the actual compressed buffer size will be stored in the length parameter. Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstreamDevice ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host The encoder_state that was previously used in one of the encoder functions. data Input/Output Device Pointer to the buffer in the device memory where the compressed stream will be stored. Can be NULL (see description). length Input/Output Host Pointer to the input buffer size. On return the NVJPEG library will store the actual compressed stream size in this parameter. stream Input Host CUDA stream where all the required device operations will be placed. 4. JPEG Transcoding  This section describes the transcoding functions of the nvJPEG Library. 4.1. nvJPEG Transcoder Helper API Reference  This section describes the nvJPEG Transcoder helper API. 4.1.1. nvjpegEncoderParamsCopyMetadata()  Copies the metadata (JFIF, APP, EXT, and COM markers) from the parsed stream. Signature: nvjpegStatus_t nvjpegEncoderParamsCopyMetadata ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. 4.1.2. nvjpegEncoderParamsCopyQuantizationTables()  Copies the quantization tables from the parsed stream. Signature: nvjpegStatus_t nvjpegEncoderParamsCopyQuantizationTables ( nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. 4.1.3. nvjpegEncoderParamsCopyHuffmanTables() [Deprecated]  nvjpegEncoderParamsCopyHuffmanTables() is now deprecated. Due to precision differences in the JPEG encode/decode process, the input huffman tables may no longer be valid for the image being encoded and may result in corrupt bitstream. Signature: nvjpegStatus_t nvjpegEncoderParamsCopyHuffmanTables ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. 4.2. JPEG Transcoding Example  See below the example code. cudaStream_t stream ; // create library handle nvjpegHandle_t handle ; nvjpegCreateSimple ( & handle ); /////////////////////////////////// nvJPEG decoding //////////////////////////////////////// // create bitstream object nvjpegJpegStream_t jpeg_stream ; nvjpegJpegStreamCreate ( handle , & jpeg_stream ); // parse jpeg stream nvjpegJpegStreamParse ( handle , data_ptr , data_size , 1 , // save metadata in the jpegStream structure 0 , jpeg_stream ); // create decoder and decoder state nvjpegJpegDecoder_t decoder ; nvjpegJpegState_t decoder_state ; nvjpegDecoderCreate ( handle , NVJPEG_BACKEND_DEFAULT , & decoder ); nvjpegDecoderStateCreate ( handle , decoder , & decoder_state ); // create and set up decoder parameters nvjpegDecodeParams_t decode_params ; nvjpegDecodeParamsCreate ( handle , & decode_params ); nvjpegDecodeParamsSetOutputFormat ( decode_params , NVJPEG_OUTPUT_RGBI ); // decode image nvjpegImage_t output_image ; nvjpegDecodeJpeg ( handle , decoder , decode_params , jpeg_stream , decoder_state , & output_image , stream ); /////////////////////////////////// nvJPEG Transcode and encode API /////////////////////////////////// nvjpegEncoderState_t encoder_state ; nvjpegEncoderParams_t encode_params ; // get encoding from the jpeg stream and copy it to the encode parameters nvjpegJpegEncoding_t jpeg_encoding ; nvjpegJpegStreamGetJpegEncoding ( jpeg_stream , & jpeg_encoding ); nvjpegEncoderParamsSetEncoding ( encode_params , jpeg_encoding ); // copies according data to the encode parameters nvjpegEncoderParamsCopyMetadata ( encode_params , jpeg_stream , stream ); nvjpegEncoderParamsCopyQuantizationTables ( encode_params , jpeg_stream , stream ); nvjpegEncoderParamsCopyHuffmanTables ( encode_params , jpeg_stream , stream ); // retrieve frame dimensions unsigned width , height ; nvjpegJpegStreamGetFrameDimensions ( jpeg_stream , & width , & height ); // encode using encode parameters nvjpegEncodeImage ( nvjpeg_handle , encoder_state , encode_params , & output_image , input_format , width , height , stream ); // get compressed stream size size_t length ; nvjpegEncodeRetrieveBitstream ( nvjpeg_handle , encoder_state , NULL , & length , stream ); // get stream itself cudaStreamSynchronize ( stream ); std :: vector < char > jpeg ( length ); nvjpegEncodeRetrieveBitstream ( nvjpeg_handle , encoder_state , jpeg . data (), & length , 0 ); 5. List of Dropped APIs  The following APIs are dropped starting CUDA 11.0 nvjpegStatus_t nvjpegDecodePhaseOne ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodePhaseTwo ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodePhaseThree ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , nvjpegImage_t * destination , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodeBatchedPhaseOne ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , int image_idx , int thread_idx , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodeBatchedPhaseTwo ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodeBatchedPhaseThree ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , nvjpegImage_t * destinations , cudaStream_t stream ); 6. Known Issues  Decoupled APIs, when initialized with NVJPEG_BACKEND_GPU_HYBRID , may not be able to correctly decode jpeg bitstreams which have out of bound run length codes. 7. Notices  7.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html", "parent_url": "https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html", "content_type": "text/html", "text": "Inline PTX Assembly 1. Using Inline PTX Assembly in CUDA 1.1. Assembler (ASM) Statements 1.1.1. Parameters 1.1.2. Constraints 1.2. Pitfalls 1.2.1. Namespace Conflicts 1.2.2. Memory Space Conflicts 1.2.3. Incorrect Optimization 1.2.4. Incorrect PTX 1.3. Error Checking 2. Notices 2.1. Notice 2.2. OpenCL 2.3. Trademarks Inline PTX Assembly in CUDA » 1. Using Inline PTX Assembly in CUDA v12.5 | PDF | Archive Inline PTX Assembly in CUDA The reference guide for inlining PTX (parallel thread execution) assembly statements into CUDA. 1. Using Inline PTX Assembly in CUDA  The NVIDIA ® CUDA ® programming environment provides a parallel thread execution (PTX) instruction set architecture (ISA) for using the GPU as a data-parallel computing device. For more information on the PTX ISA, refer to the latest version of the PTX ISA reference document . This application note describes how to inline PTX assembly language statements into CUDA code. 1.1. Assembler (ASM) Statements  Assembler statements, asm() , provide a way to insert arbitrary PTX code into your CUDA program. A simple example is: asm ( \"membar.gl;\" ); This inserts a PTX membar.gl into your generated PTX code at the point of the asm() statement. 1.1.1. Parameters  An asm() statement becomes more complicated, and more useful, when we pass values in and out of the asm. The basic syntax is as follows: asm ( \"template-string\" : \"constraint\" ( output ) : \"constraint\" ( input )); where you can have multiple input or output operands separated by commas. The template string contains PTX instructions with references to the operands. Multiple PTX instructions can be given by separating them with semicolons. A simple example is as follows: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); Each %n in the template string is an index into the following list of operands, in text order. So %0 refers to the first operand, %1 to the second operand, and so on. Since the output operands are always listed ahead of the input operands, they are assigned the smallest indices. This example is conceptually equivalent to the following: add . s32 i , j , k ; Note that the numbered references in the string can be in arbitrary order. The following is equivalent to the above example: asm ( \"add.s32 %0, %2, %1;\" : \"=r\" ( i ) : \"r\" ( k ), \"r\" ( j )); You can also repeat a reference, e.g.: asm ( \"add.s32 %0, %1, %1;\" : \"=r\" ( i ) : \"r\" ( k )); is conceptually add . s32 i , k , k ; If there is no input operand, you can drop the final colon, e.g.: asm ( \"mov.s32 %0, 2;\" : \"=r\" ( i )); If there is no output operand, the colon separators are adjacent, e.g.: asm ( \"mov.s32 r1, %0;\" :: \"r\" ( i )); If you want the % in a ptx instruction, then you should escape it with double %% , e.g.: asm ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x )); The above was simplified to explain the ordering of the string % references. In reality, the operand values are passed via whatever mechanism the constraint specifies. The full list of constraints will be explained later, but the “r” constraint refers to a 32bit integer register. So the earlier example asm() statement: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); produces the following code sequence in the output generated by the compiler: ld . s32 r1 , [ j ]; ld . s32 r2 , [ k ]; add . s32 r3 , r1 , r2 ; st . s32 [ i ], r3 ; This is where the distinction between input and output operands becomes important. The input operands are loaded into registers before the asm() statement, then the result register is stored to the output operand. The “=” modifier in “=r” specifies that the register is written to. There is also available a “+” modifier that specifies the register is both read and written, e.g.: asm ( \"add.s32 %0, %0, %1;\" : \"+r\" ( i ) : \"r\" ( j )); Multiple instructions can be combined into a single asm() statement; basically, anything legal can be put into the asm string. Multiple instructions can be split across multiple lines by making use of C/C++’s implicit string concatenation. Both C++ style line end comments “//” and classical C-style comments “/**/” can be interspersed with these strings. To generate readable output in the PTX intermediate file it is best practice to terminate each instruction string except the last one with “nt”. For example, a cube routine could be written as: __device__ int cube ( int x ) { int y ; asm ( \".reg .u32 t1; \\n\\t \" // temp reg t1 \" mul.lo.u32 t1, %1, %1; \\n\\t \" // t1 = x * x \" mul.lo.u32 %0, t1, %1;\" // y = t1 * x : \"=r\" ( y ) : \"r\" ( x )); return y ; } If an output operand is conditionally updated by the asm instructions, then the “+” modifier should be used. There is an implicit use of the output operand in such a case. For example, __device__ int cond ( int x ) { int y = 0 ; asm ( \"{ \\n\\t \" \" .reg .pred %p; \\n\\t \" \" setp.eq.s32 %p, %1, 34; \\n\\t \" // x == 34? \" @%p mov.s32 %0, 1; \\n\\t \" // set y to 1 if true \"}\" // conceptually y = (x==34)?1:y : \"+r\" ( y ) : \"r\" ( x )); return y ; } 1.1.2. Constraints  There is a separate constraint letter for each PTX register type: \"h\" = . u16 reg \"r\" = . u32 reg \"l\" = . u64 reg \"f\" = . f32 reg \"d\" = . f64 reg Example: asm ( \"cvt.f32.s64 %0, %1;\" : \"=f\" ( x ) : \"l\" ( y )); generates: ld . s64 rd1 , [ y ]; cvt . f32 . s64 f1 , rd1 ; st . f32 [ x ], f1 ; The constraint \"n\" may be used for immediate integer operands with a known value. Example: asm ( \"add.u32 %0, %0, %1;\" : \"=r\" ( x ) : \"n\" ( 42 )); generates: add . u32 r1 , r1 , 42 ; The constraint \"C\" can be used for operand of type ‘array of const char’, where the array contents are known at compile time.\nIt is intended to allow customization of PTX instruction modes based on compile time computation (see examples). Here is the specification\nfor the \"C\" constraint: 'C' ( constant - expression ) The constant-expression is evaluated during compilation and shall generate the address of a variable V , where: V has static storage duration . V has type ‘array of const char’. V is constant-initialized . If V is a static class member, then V ’s initializing declaration is the declaration within the class. During translation, the compiler will replace a reference to the operand within the Assembler Template with the contents of V ’s initializer, except for the last trailing zero.\nNo constraint modifiers are allowed for this constraint. This constraint can only be used in device code. (terms in italics are C++ standard terms and/or terms from the GNU inline asm specification). Here’s an example of the use of C constraint to generate different PTX instruction modes based on compile time computation: constexpr int mode_rz = 0 ; constexpr int mode_rn = 1 ; template < int mode > struct helper ; template <> struct helper < mode_rz > { static constexpr const char mode [] = \".rz\" ; }; template <> struct helper < mode_rn > { static constexpr const char mode [] = \".rn\" ; }; template < int rounding_mode > __device__ float compute_add ( float a , float b ) { float result ; asm ( \"add.f32%1 %0,%2,%3;\" : \"=f\" ( result ) : \"C\" ( helper < rounding_mode >:: mode ), \"f\" ( a ), \"f\" ( b )); return result ; } __global__ void kern ( float * result , float a , float b ) { * result ++ = compute_add < mode_rn > ( a , b ); // generates add.f32.rn * result = compute_add < mode_rz > ( a , b ); // generates add.f32.rz } Other examples (compile in C++17 or later dialect): struct S1 { static constexpr char buf1 [] = \"Jumped\" ; static constexpr char buf2 [] = { 'O' , 'v' , 'e' , 'r' , 0 }; }; template < const char * p1 , const char * p2 , const char * p3 > __device__ void doit () { asm volatile ( \"%0 %1 %2\" : : \"C\" ( p1 ), \"C\" ( p2 ), \"C\" ( p3 )); } struct S2 { static const char buf []; }; const char S2 :: buf [] = \"this\" ; const char buf3 [] = \"Jumped\" ; extern const char buf4 []; __global__ void foo () { static const char v1 [] = \"The\" ; static constexpr char v2 [] = \"Quick\" ; static const char v3 [] = { 'B' , 'r' , 'o' , 'w' , 'n' , 0 }; static constexpr char v4 [] = { 'F' , 'o' , 'x' , 0 }; //OK: generates 'The Quick Brown Fox Jumped Over' in PTX asm volatile ( \"%0 %1 %2 %3 %4 %5\" : : \"C\" ( v1 ) , \"C\" ( v2 ), \"C\" ( v3 ), \"C\" ( v4 ), \"C\" ( S1 :: buf1 ), \"C\" ( S1 :: buf2 ) ); //OK: generates 'Brown Fox Jumped' in PTX doit < v3 , v4 , buf3 > (); //error cases const char n1 [] = \"hi\" ; //error: argument to \"C\" constraint is not a constant expression asm volatile ( \"%0\" :: \"C\" ( n1 )); //error: S2::buf was not initialized at point of declaration asm volatile ( \"%0\" :: \"C\" ( S2 :: buf )); //error: buf4 was not initialized asm volatile ( \"%0\" :: \"C\" ( buf4 )); } There is no constraint letter for 8-bit wide PTX registers. PTX instructions types accepting 8-bit wide types permit operands to be wider than the instruction-type size . Example: __device__ void copy_u8 ( char * in , char * out ) { int d ; asm ( \"ld.u8 %0, [%1];\" : \"=r\" ( d ) : \"l\" ( in )); * out = d ; } generates: ld . u8 r1 , [ rd1 ]; st . u8 [ rd2 ], r1 ; The behavior of using a constraint string that is not one of those specified above is undefined. 1.2. Pitfalls  Although asm() statements are very flexible and powerful, you may encounter some pitfalls—these are listed in this section. 1.2.1. Namespace Conflicts  If the cube function (described before) is called and inlined multiple times in the code, it generates an error about duplicate definitions of the temp register t1. To avoid this error you need to: not inline the cube function, or, nest the t1 use inside {} so that it has a separate scope for each invocation, e.g.: __device__ int cube ( int x ) { int y ; asm ( \"{ \\n\\t \" // use braces for local scope \" reg .u32 t1; \\n\\t \" // temp reg t1, \" mul.lo.u32 t1, %1, %1; \\n\\t \" // t1 = x * x \" mul.lo.u32 %0, t1, %1; \\n\\t \" // y = t1 * x \"}\" : \"=r\" ( y ) : \"r\" ( x )); return y ; } Note that you can similarly use braces for local labels inside the asm() statement. 1.2.2. Memory Space Conflicts  Since asm() statements have no way of knowing what memory space a register is in, the user must make sure that the appropriate PTX instruction is used. For sm_20 and greater, any pointer argument to an asm() statement is passed as a generic address. 1.2.3. Incorrect Optimization  The compiler assumes that an asm() statement has no side effects except to change the output operands. To ensure that the asm is not deleted or moved during generation of PTX, you should use the volatile keyword, e.g.: asm volatile ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x )); Normally any memory that is written to will be specified as an out operand, but if there is a hidden side effect on user memory (for example, indirect access of a memory location via an operand), or if you want to stop any memory optimizations around the asm() statement performed during generation of PTX, you can add a “memory” clobbers specification after a 3rd colon, e.g.: asm volatile ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x ) :: \"memory\" ); asm ( \"st.u32 [%0], %1;\" :: \"l\" ( p ), \"r\" ( x ) : \"memory\" ); 1.2.4. Incorrect PTX  The compiler front end does not parse the asm() statement template string and does not know what it means or even whether it is valid PTX input. So if there are any errors in the string it will not show up until ptxas . For example, if you pass a value with an “r” constraint but use it in an add.f64 you will get a parse error from ptxas. Similarly, operand modifiers are not supported. For example, in asm ( \"mov.u32 %0, %n1;\" : \"=r\" ( n ) : \"r\" ( 1 )); the ‘n’ modifier in “%n1” is not supported and will be passed to ptxas , where it can cause undefined behavior. Refer to the document nvcc.pdf for further compiler related details. 1.3. Error Checking  The following are some of the error checks that the compiler will do on inlinePTXasm: Multiple constraint letters for a single asm operand are not allowed, e.g.: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"rf\" ( j ), \"r\" ( k )); error: an asm operand may specify only one constraint letter in a __device__/__global__ function Only scalar variables are allowed as asm operands. Specifically aggregates like ‘struct’ type variables are not allowed, e.g. int4 i4 ; asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i4 ) : \"r\" ( j ), \"r\" ( k )); error: an asm operand must have scalar type The type and size implied by a PTX asm constraint must match that of the associated operand. Example where size does not match: For ‘char’ type variable “ci”, asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( ci ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(1) does not match type/size implied by constraint ‘r’ In order to use ‘char’ type variables “ci”, “cj”, and “ck” in the above asm statement, code segment similar to the following may be used, int temp = ci ; asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( temp ) : \"r\" (( int ) cj ), \"r\" (( int ) ck )); ci = temp ; Another example where type does not match: For ‘float’ type variable “fi”, asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( fi ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(4) does not match type/size implied by constraint ‘r’ 2. Notices  2.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-driver-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-driver-api/index.html", "content_type": "text/html", "text": "CUDA Driver API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 CUDA Driver API 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Data types used by CUDA driver 6.2. Error Handling 6.3. Initialization 6.4. Version Management 6.5. Device Management 6.6. Device Management [DEPRECATED] 6.7. Primary Context Management 6.8. Context Management 6.9. Context Management [DEPRECATED] 6.10. Module Management 6.11. Module Management [DEPRECATED] 6.12. Library Management 6.13. Memory Management 6.14. Virtual Memory Management 6.15. Stream Ordered Memory Allocator 6.16. Multicast Object Management 6.17. Unified Addressing 6.18. Stream Management 6.19. Event Management 6.20. External Resource Interoperability 6.21. Stream Memory Operations 6.22. Execution Control 6.23. Execution Control [DEPRECATED] 6.24. Graph Management 6.25. Occupancy 6.26. Texture Reference Management [DEPRECATED] 6.27. Surface Reference Management [DEPRECATED] 6.28. Texture Object Management 6.29. Surface Object Management 6.30. Tensor Map Object Managment 6.31. Peer Context Memory Access 6.32. Graphics Interoperability 6.33. Driver Entry Point Access 6.34. Coredump Attributes Control API 6.35. Green Contexts 6.36. Profiler Control [DEPRECATED] 6.37. Profiler Control 6.38. OpenGL Interoperability 6.38.1. OpenGL Interoperability [DEPRECATED] 6.39. Direct3D 9 Interoperability 6.39.1. Direct3D 9 Interoperability [DEPRECATED] 6.40. Direct3D 10 Interoperability 6.40.1. Direct3D 10 Interoperability [DEPRECATED] 6.41. Direct3D 11 Interoperability 6.41.1. Direct3D 11 Interoperability [DEPRECATED] 6.42. VDPAU Interoperability 6.43. EGL Interoperability 7. Data Structures 7.1. CUaccessPolicyWindow_v1 7.2. CUarrayMapInfo_v1 7.3. CUasyncNotificationInfo 7.4. CUctxCigParam 7.5. CUctxCreateParams 7.6. CUDA_ARRAY3D_DESCRIPTOR_v2 7.7. CUDA_ARRAY_DESCRIPTOR_v2 7.8. CUDA_ARRAY_MEMORY_REQUIREMENTS_v1 7.9. CUDA_ARRAY_SPARSE_PROPERTIES_v1 7.10. 7.11. CUDA_CHILD_GRAPH_NODE_PARAMS 7.12. CUDA_CONDITIONAL_NODE_PARAMS 7.13. CUDA_EVENT_RECORD_NODE_PARAMS 7.14. CUDA_EVENT_WAIT_NODE_PARAMS 7.15. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1 7.16. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2 7.17. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1 7.18. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2 7.19. CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1 7.20. CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1 7.21. CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1 7.22. CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1 7.23. CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1 7.24. CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1 7.25. CUDA_GRAPH_INSTANTIATE_PARAMS 7.26. CUDA_HOST_NODE_PARAMS_v1 7.27. CUDA_HOST_NODE_PARAMS_v2 7.28. CUDA_KERNEL_NODE_PARAMS_v1 7.29. CUDA_KERNEL_NODE_PARAMS_v2 7.30. CUDA_KERNEL_NODE_PARAMS_v3 7.31. CUDA_LAUNCH_PARAMS_v1 7.32. CUDA_MEM_ALLOC_NODE_PARAMS_v1 7.33. CUDA_MEM_ALLOC_NODE_PARAMS_v2 7.34. CUDA_MEM_FREE_NODE_PARAMS 7.35. CUDA_MEMCPY2D_v2 7.36. CUDA_MEMCPY3D_PEER_v1 7.37. CUDA_MEMCPY3D_v2 7.38. CUDA_MEMCPY_NODE_PARAMS 7.39. CUDA_MEMSET_NODE_PARAMS_v1 7.40. CUDA_MEMSET_NODE_PARAMS_v2 7.41. CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1 7.42. CUDA_RESOURCE_DESC_v1 7.43. CUDA_RESOURCE_VIEW_DESC_v1 7.44. CUDA_TEXTURE_DESC_v1 7.45. CUdevprop_v1 7.46. CUdevResource 7.47. CUdevSmResource 7.48. CUeglFrame_v1 7.49. CUexecAffinityParam_v1 7.50. CUexecAffinitySmCount_v1 7.51. CUgraphEdgeData 7.52. CUgraphExecUpdateResultInfo_v1 7.53. CUgraphNodeParams 7.54. CUipcEventHandle_v1 7.55. CUipcMemHandle_v1 7.56. CUlaunchAttribute 7.57. CUlaunchAttributeValue 7.58. CUlaunchConfig 7.59. CUlaunchMemSyncDomainMap 7.60. CUmemAccessDesc_v1 7.61. CUmemAllocationProp_v1 7.62. CUmemFabricHandle_v1 7.63. CUmemLocation_v1 7.64. CUmemPoolProps_v1 7.65. CUmemPoolPtrExportData_v1 7.66. CUmulticastObjectProp_v1 7.67. CUstreamBatchMemOpParams_v1 7.68. CUtensorMap 8. Data Fields 9. Deprecated List Search Results CUDA Driver API\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback Table of Contents 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Data types used by CUDA driver 6.2. Error Handling 6.3. Initialization 6.4. Version Management 6.5. Device Management 6.6. Device Management [DEPRECATED] 6.7. Primary Context Management 6.8. Context Management 6.9. Context Management [DEPRECATED] 6.10. Module Management 6.11. Module Management [DEPRECATED] 6.12. Library Management 6.13. Memory Management 6.14. Virtual Memory Management 6.15. Stream Ordered Memory Allocator 6.16. Multicast Object Management 6.17. Unified Addressing 6.18. Stream Management 6.19. Event Management 6.20. External Resource Interoperability 6.21. Stream Memory Operations 6.22. Execution Control 6.23. Execution Control [DEPRECATED] 6.24. Graph Management 6.25. Occupancy 6.26. Texture Reference Management [DEPRECATED] 6.27. Surface Reference Management [DEPRECATED] 6.28. Texture Object Management 6.29. Surface Object Management 6.30. Tensor Map Object Managment 6.31. Peer Context Memory Access 6.32. Graphics Interoperability 6.33. Driver Entry Point Access 6.34. Coredump Attributes Control API 6.35. Green Contexts 6.36. Profiler Control [DEPRECATED] 6.37. Profiler Control 6.38. OpenGL Interoperability 6.38.1. OpenGL Interoperability [DEPRECATED] 6.39. Direct3D 9 Interoperability 6.39.1. Direct3D 9 Interoperability [DEPRECATED] 6.40. Direct3D 10 Interoperability 6.40.1. Direct3D 10 Interoperability [DEPRECATED] 6.41. Direct3D 11 Interoperability 6.41.1. Direct3D 11 Interoperability [DEPRECATED] 6.42. VDPAU Interoperability 6.43. EGL Interoperability 7. Data Structures 7.1. CUaccessPolicyWindow_v1 7.2. CUarrayMapInfo_v1 7.3. CUasyncNotificationInfo 7.4. CUctxCigParam 7.5. CUctxCreateParams 7.6. CUDA_ARRAY3D_DESCRIPTOR_v2 7.7. CUDA_ARRAY_DESCRIPTOR_v2 7.8. CUDA_ARRAY_MEMORY_REQUIREMENTS_v1 7.9. CUDA_ARRAY_SPARSE_PROPERTIES_v1 7.10. 7.11. CUDA_CHILD_GRAPH_NODE_PARAMS 7.12. CUDA_CONDITIONAL_NODE_PARAMS 7.13. CUDA_EVENT_RECORD_NODE_PARAMS 7.14. CUDA_EVENT_WAIT_NODE_PARAMS 7.15. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1 7.16. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2 7.17. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1 7.18. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2 7.19. CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1 7.20. CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1 7.21. CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1 7.22. CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1 7.23. CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1 7.24. CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1 7.25. CUDA_GRAPH_INSTANTIATE_PARAMS 7.26. CUDA_HOST_NODE_PARAMS_v1 7.27. CUDA_HOST_NODE_PARAMS_v2 7.28. CUDA_KERNEL_NODE_PARAMS_v1 7.29. CUDA_KERNEL_NODE_PARAMS_v2 7.30. CUDA_KERNEL_NODE_PARAMS_v3 7.31. CUDA_LAUNCH_PARAMS_v1 7.32. CUDA_MEM_ALLOC_NODE_PARAMS_v1 7.33. CUDA_MEM_ALLOC_NODE_PARAMS_v2 7.34. CUDA_MEM_FREE_NODE_PARAMS 7.35. CUDA_MEMCPY2D_v2 7.36. CUDA_MEMCPY3D_PEER_v1 7.37. CUDA_MEMCPY3D_v2 7.38. CUDA_MEMCPY_NODE_PARAMS 7.39. CUDA_MEMSET_NODE_PARAMS_v1 7.40. CUDA_MEMSET_NODE_PARAMS_v2 7.41. CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1 7.42. CUDA_RESOURCE_DESC_v1 7.43. CUDA_RESOURCE_VIEW_DESC_v1 7.44. CUDA_TEXTURE_DESC_v1 7.45. CUdevprop_v1 7.46. CUdevResource 7.47. CUdevSmResource 7.48. CUeglFrame_v1 7.49. CUexecAffinityParam_v1 7.50. CUexecAffinitySmCount_v1 7.51. CUgraphEdgeData 7.52. CUgraphExecUpdateResultInfo_v1 7.53. CUgraphNodeParams 7.54. CUipcEventHandle_v1 7.55. CUipcMemHandle_v1 7.56. CUlaunchAttribute 7.57. CUlaunchAttributeValue 7.58. CUlaunchConfig 7.59. CUlaunchMemSyncDomainMap 7.60. CUmemAccessDesc_v1 7.61. CUmemAllocationProp_v1 7.62. CUmemFabricHandle_v1 7.63. CUmemLocation_v1 7.64. CUmemPoolProps_v1 7.65. CUmemPoolPtrExportData_v1 7.66. CUmulticastObjectProp_v1 7.67. CUstreamBatchMemOpParams_v1 7.68. CUtensorMap 8. Data Fields 9. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/index.html", "content_type": "text/html", "text": "CUDA Math API Reference Manual 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices CUDA Math API Reference Manual » CUDA Math API Reference Manual v12.5 | PDF | Archive CUDA Math API Reference Manual  CUDA mathematical functions are always available in device code. Host implementations of the common mathematical functions are mapped in a platform-specific way to standard math library functions, provided by the host compiler and respective host libm where available. Some functions, not available with the host compilers, are implemented in crt/math_functions.hpp header file. For example, see erfinv() . Other, less common functions, like rhypot() , cyl_bessel_i0() are only available in device code. CUDA Math device functions are no-throw for well-formed CUDA programs. Note that many floating-point and integer functions names are overloaded for different argument types. For example, the log() function has the following prototypes: double log ( double x ); float log ( float x ); float logf ( float x ); Note also that due to implementation constraints, certain math functions from std:: namespace may be callable in device code even via explicitly qualified std:: names. However, such use is discouraged, since this capability is unsupported, unverified, undocumented, not portable, and may change without notice. 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/nvblas/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvblas/index.html", "content_type": "text/html", "text": "NVBLAS 1. Introduction 2. NVBLAS Overview 3. GPU Accelerated Routines 4. BLAS Symbols Interception 5. Device Memory Support 6. Security Precaution 7. Configuration 7.1. NVBLAS_CONFIG_FILE Environment Variable 7.2. Configuration Keywords 7.2.1. NVBLAS_LOGFILE 7.2.2. NVBLAS_TRACE_LOG_ENABLED 7.2.3. NVBLAS_CPU_BLAS_LIB 7.2.4. NVBLAS_GPU_LIST 7.2.5. NVBLAS_TILE_DIM 7.2.6. NVBLAS_GPU_DISABLED_<BLAS_FUNC_NAME> 7.2.7. NVBLAS_CPU_RATIO_<BLAS_FUNC_NAME> 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED 7.2.9. Configuration File Example 8. NVBLAS Installation 9. Usage 10. Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks NVBLAS » 1. Introduction v12.5 | PDF | Archive NVBLAS The User guide for NVBLAS, drop-in BLAS replacement, multi-GPUs accelerated 1. Introduction  The NVBLAS Library is a GPU-accelerated Libary that implements BLAS (Basic Linear Algebra Subprograms). It can accelerate most BLAS Level-3 routines by dynamically routing BLAS calls to one or more NVIDIA GPUs present in the system, when the charateristics of the call make it speed up on a GPU. 2. NVBLAS Overview  The NVBLAS Library is built on top of the cuBLAS Library using only the CUBLASXT API (refer to the CUBLASXT API section of the cuBLAS Documentation for more details). NVBLAS also requires the presence of a CPU BLAS lirbary on the system. Currently NVBLAS intercepts only compute intensive BLAS Level-3 calls (see table below). Depending on the charateristics of those BLAS calls, NVBLAS will redirect the calls to the GPUs present in the system or to CPU. That decision is based on a simple heuristic that estimates if the BLAS call will execute for long enough to amortize the PCI transfers of the input and output data to the GPU. Because NVBLAS does not support all standard BLAS routines, it might be necessary to associate it with an existing full BLAS Library. Please refer to the Usage section for more details. 3. GPU Accelerated Routines  NVBLAS offloads only the compute-intensive BLAS3 routines which have the best potential for acceleration on GPUs. The following table shows the currently supported routines: Routine Types Operation gemm S,D,C,Z Multiplication of 2 matrices syrk S,D,C,Z Symmetric rank-k update herk C,Z Hermitian rank-k update syr2k S,D,C,Z Symmetric rank-2k update her2k C,Z Hermitian rank-2k update trsm S,D,C,Z Triangular solve with multiple right-hand sides trmm S,D,C,Z Triangular matrix-matrix multiplication symm S,D,C,Z Symmetric matrix-matrix multiplication hemm C,Z Hermitian matrix-matrix multiplication 4. BLAS Symbols Interception  Standard BLAS Library implementations usually expose multiple symbols for the same routines. Let’s say func is a BLAS routine name, func_ or/and func are usually defined as extern symbols. Some BLAS Libraries might also expose some symbols with a proprietary appended prefix. NVBLAS intercepts only the symbols func_ and func . The user needs to make sure that the application intended to be GPU-accelerated by NVBLAS actually calls those defined symbols. Any other symbols will not be intercepted and the original BLAS routine will be executed for those cases. 5. Device Memory Support  Starting with Release 8.0, data can be located on any GPU device, even on GPU devices that are not configured to be part of the computation. When any of the data is located on a GPU, the computation will be exclusively done on GPU whatever the size of the problem. Also, this feature has to be used with caution: the user has to be sure that the BLAS call will indeed be intercepted by NVBLAS, otherwise it will result in a crash when the CPU BLAS tries to execute it. 6. Security Precaution  Because the NVBLAS Library relies on a symbols interception mechanism, it is essential to make sure it has not been compromised. In that regard, NVBLAS should never be used from a process running at elevated privileges, such as Administrator on Windows or root on Linux. 7. Configuration  Because NVBLAS is a drop-in replacement of BLAS, it must be configured through an ASCII text file that describes how many and which GPUs can participate in the intercepted BLAS calls. The configuration file is parsed at the time of the loading of the library. The format of the configuration file is based on keywords optionally followed by one or more user-defined parameters. At most one keyword per line is allowed. Blank lines or lines beginning with the character # are ignored. 7.1. NVBLAS_CONFIG_FILE Environment Variable  The location and name of the configuration file must be defined by the environment variable NVBLAS_CONFIG_FILE . By default, if NVBLAS_CONFIG_FILE is not defined, NVBLAS will try to open the file nvblas.conf in the current directory. For a safe use of NVBLAS, the configuration file should have have restricted write permissions. 7.2. Configuration Keywords  The configuration keywords syntax is described in the following subsections. 7.2.1. NVBLAS_LOGFILE  This keyword defines the file where NVBLAS should print status and error messages. By default, if not defined, the standard error output file (eg. stderr) will be used. It is advised to define this keyword early in the configuration to capture errors in parsing that file itself. 7.2.2. NVBLAS_TRACE_LOG_ENABLED  When this keyword is defined, every intercepted BLAS calls will be logged into the NVBLAS_LOGFILE. This feature, even though intrusive, can be useful for debugging purposes. 7.2.3. NVBLAS_CPU_BLAS_LIB  This keyword defines the CPU BLAS dynamic library file (for example, .so file on Linux or .dll on Windows) that NVBLAS should open to find the CPU BLAS symbols definitions. This keyword must be defined for NVBLAS to work. Because CPU Blas libraries are often composed of multiple files, even though this keyword is set to the full path to the main file of the CPU library, it might still be necessary to define the right path to find the rest of the library files in the environment of your system. On Linux, this can be done by setting the environment variable LD_LIBRARY_PATH whereas on Windows, this can be done by setting the environment variable PATH . For a safe use of NVBLAS, the following precautions are strongly advised: The CPU BLAS Library should be located where ordinary users do not have write permissions. The path specified should be absolute, not relative. 7.2.4. NVBLAS_GPU_LIST  This keyword defines the list of GPUs that should participate in the computation of the intercepted BLAS calls. If not defined, only GPU device 0 is used, since that is normally the most compute-capable GPU installed in the system. This keyword can be set to a list of device numbers separated by blank characters. Also the following wildcard keywords are also accepted for simplicity : Keyword Meaning ALL All compute-capable GPUs detected on the system will be used by NVBLAS ALL0 GPU device 0, AND all others GPUs detected that have the same compute-capabilities as device 0 will be used by NVBLAS Note In the current release of CUBLAS, the CUBLASXT API supports two GPUs if they are on the same board such as Tesla K10 or GeForce GTX690 and one GPU otherwise. Because NVBLAS is built on top of the CUBLASXT API, NVBLAS has the same restriction. If access to more GPUs devices is needed, details of the licensing are described at cublasXt . 7.2.5. NVBLAS_TILE_DIM  This keyword defines the tile dimension that should be used to divide the matrices involved in the computation. This definition maps directly to a call of the cublasXt API routine cublasXtSetBlockDim . Refer to cuBLAS documentation to understand the tradeoffs associated with setting this to a larger or a smaller value. 7.2.6. NVBLAS_GPU_DISABLED_<BLAS_FUNC_NAME>  This keyword, appended with the name of a BLAS routine disables NVBLAS from running a specified routine on the GPU. This feature is intended mainly for debugging purposes. By default, all supported BLAS routines are enabled. 7.2.7. NVBLAS_CPU_RATIO_<BLAS_FUNC_NAME>  This keyword, appended with the name of ta BLAS routine defines the ratio of the workload that should remain on the CPU in the event that the NVBLAS decides to offload work for that routine on the GPU. This functionality is directly mapped to the cublasXt API routine cublasXtSetCpuRatio . By default, the ratio is defined to zero for all routines. Please refer to the cuBLAS documentation for details and for the list of routines which support this feature. 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED  This keyword enables the Pinning Memory mode. This functionality is directly mapped to the cublasXt API routine cublasXtSetPinningMemMode . If this keyowrd is not present in the configuration file, the Pinning Memory mode will be set to CUBLASXT_PINNING_DISABLED . Note There are some restrictions to use this feature as specified in the cuBLAS documentation of the underlying routine cublasXtSetPinningMemMode . Specifically when NVBLAS is used in a multi-threaded applications, this option should not be used if there is a chance that matrices used by different threads overlaps while calling NVBLAS. Please refer to the cuBLAS Documentation of the routine `cublasXtSetPinningMemMode < https://docs.nvidia.com/cuda/cublas/index.html#cublasxt_setPinningMemMode >`__ for details. 7.2.9. Configuration File Example  The following example shows a typical NVBLAS configuration file : # This is the configuration file to use NVBLAS Library\n# Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file.\n# By default, if NVBLAS_CONFIG_FILE is not defined,\n# NVBLAS Library will try to open the file \"nvblas.conf\" in its current directory\n# Example : NVBLAS_CONFIG_FILE  /home/cuda_user/my_nvblas.conf\n# The config file should have restricted write permissions accesses\n\n# Specify which output log file (default is stderr)\nNVBLAS_LOGFILE  nvblas.log\n\n# Enable trace log of every intercepted BLAS calls\nNVBLAS_TRACE_LOG_ENABLED\n\n#Put here the CPU BLAS fallback Library of your choice\n#It is strongly advised to use full path to describe the location of the CPU Library\nNVBLAS_CPU_BLAS_LIB  /usr/lib/libopenblas.so\n#NVBLAS_CPU_BLAS_LIB  <mkl_path_installtion>/libmkl_rt.so\n\n# List of GPU devices Id to participate to the computation\n# Use ALL if you want all your GPUs to contribute\n# Use ALL0, if you want all your GPUs of the same type as device 0 to contribute\n# However, NVBLAS consider that all GPU have the same performance and PCI bandwidth\n# By default if no GPU are listed, only device 0 will be used\n\n#NVBLAS_GPU_LIST 0 2 4\n#NVBLAS_GPU_LIST ALL\nNVBLAS_GPU_LIST ALL0\n\n# Tile Dimension\nNVBLAS_TILE_DIM 2048\n\n# Autopin Memory\nNVBLAS_AUTOPIN_MEM_ENABLED\n\n#List of BLAS routines that are prevented from running on GPU (use for debugging purpose\n# The current list of BLAS routines supported by NVBLAS are\n# GEMM, SYRK, HERK, TRSM, TRMM, SYMM, HEMM, SYR2K, HER2K\n\n#NVBLAS_GPU_DISABLED_SGEMM\n#NVBLAS_GPU_DISABLED_DGEMM\n#NVBLAS_GPU_DISABLED_CGEMM\n#NVBLAS_GPU_DISABLED_ZGEMM\n\n# Computation can be optionally hybridized between CPU and GPU\n# By default, GPU-supported BLAS routines are ran fully on GPU\n# The option NVBLAS_CPU_RATIO_<BLAS_ROUTINE> give the ratio [0,1]\n# of the amount of computation that should be done on CPU\n# CAUTION : this option should be used wisely because it can actually\n# significantly reduced the overall performance if too much work is given to CPU\n\n#NVBLAS_CPU_RATIO_CGEMM 0.07 8. NVBLAS Installation  The NVBLAS Library is part of the CUDA Toolkit, and will be installed along all the other CUDA libraries. It is available on 64-bit operating systems. NVBLAS Library is built on top of cuBLAS, so the cuBLAS library needs to be accessible by NVBLAS. 9. Usage  To use the NVBLAS Library, the user application must be relinked against NVBLAS in addition to the original CPU Blas (technically only NVBLAS is needed unless some BLAS routines not supported by NVBLAS are used by the application). To be sure that the linker links against the exposed symbols of NVBLAS and not the ones from the CPU BLAS, the NVBLAS Library needs to be put before the CPU BLAS on the linkage command line. On Linux, an alternative way to use NVBLAS Library is to use the LD_PRELOAD environment variable; this technique has the advantage of avoiding the relinkage step. However, the user should avoid defining that environment variable globally because it will cause the NVBLAS library to be loaded by every shell command executed on the system, thus leading to a lack of responsiveness of the system. Finally, mathematical tools and libraries often offer the opportunity to specify the BLAS Library to be used through an environment variable or a configuration file. Because NVBLAS does not support all the standard BLAS routines, it might be necessary to pair NVBLAS with a full BLAS library, even though your application only calls supported NVBLAS routines. Fortunately, those tools and libraries usually offer a way to specify multiple BLAS Libraries. Please refer to the documentation of the appropriate tools and libraries for details. 10. Notices  10.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html", "parent_url": "https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html", "content_type": "text/html", "text": "PTX Interoperability 1. Introduction 2. Data Representation 2.1. Fundamental Types 2.2. Aggregates and Unions 2.3. Bit Fields 2.4. Texture, Sampler, and Surface Types 3. Function Calling Sequence 3.1. Registers 3.2. Stack Frame 3.3. Parameter Passing 4. System Calls 5. Debug Information 5.1. Generation of Debug Information 5.2. CUDA-Specific DWARF Definitions 6. Example 7. C++ 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks PTX Interoperability » 1. Introduction v12.5 | PDF | Archive PTX Writer’s Guide to Interoperability The guide to writing ABI-compliant PTX. 1. Introduction  This document defines the Application Binary Interface (ABI) for the CUDA ® architecture when generating PTX. By following the ABI, external developers can generate compliant PTX code that can be linked with other code. PTX is a low-level parallel-thread-execution virtual machine and ISA (Instruction Set Architecture). PTX can be output from multiple tools or written directly by developers. PTX is meant to be GPU-architecture independent, so that the same code can be reused for different GPU architectures. For more information on PTX, refer to the latest version of the PTX ISA reference document . There are multiple CUDA architecture families, each with their own ISA; e.g. SM 5.x is the Maxwell family, SM 6.x is the Pascal family. This document describes the high-level ABI for all architectures. Programs conforming to an ABI are expected to be executed on the appropriate architecture GPU, and can assume that instructions from that ISA are available. 2. Data Representation  2.1. Fundamental Types  The below table shows the native scalar PTX types that are supported. Any PTX producer must use these sizes and alignments in order for its PTX to be compatible with PTX generated by other producers. PTX also supports native vector types, which are discussed in Aggregates and Unions . The sizes of types are defined by the host. For example, pointer size and long int size are dictated by the hosts ABI. PTX has an .address_size directive that specifies the address size used throughout the PTX code. The size of pointers is 32 bits on a 32-bit host or 64 bits on a 64-bit host. However, addresses of the local and shared memory spaces are always 32 bits in size. During separate compilation we store info about the host platform in each object file. The linker will fail to link object files generated for incompatible host platforms. PTX Type Size (bytes) Align (bytes) Hardware Representation .b8 1 1 untyped byte .b16 2 2 untyped halfword .b32 4 4 untyped word .b64 8 8 untyped doubleword .s8 1 1 signed integral byte .s16 2 2 signed integral halfword .s32 4 4 signed integral word .s64 8 8 signed integral doubleword .u8 1 1 unsigned integral byte .u16 2 2 unsigned integral halfword .u32 4 4 unsigned integral word .u64 8 8 unsigned integral doubleword .f16 2 2 IEEE half precision .f32 4 4 IEEE single precision .f64 8 8 IEEE double precision 2.2. Aggregates and Unions  Beyond the scalar types, PTX also supports native-vector types of these scalar types, with both its vector syntax and its byte-array syntax. For scalar types with a size no greater than four bytes, vector types with 1, 2, 3, and 4 elements exist; for all other types, only 1 and 2 element vector types exist. All aggregates and unions can be supported in PTX with its byte-array syntax. The following are the size-and-alignment rules for all aggregates and unions. For a non-native-vector type, an entire aggregate or union is aligned on the same boundary as its most strictly aligned member. This rule is not followed if the alignments are defined by the input language. For example, in OpenCL built-in vector data types have their alignment set to the size of the built-in data type in bytes. For a native vector type – discussed at the start of this section – the alignment is defined as follows. (For the definitions below, the native vector has n elements and has an element type t.) For a vector with an odd number of elements, its alignment is the same as its member: alignof(t). For a vector with an even number of elements, its alignment is set to number of elements times the alignment of its member: n*alignof(t). Each member is assigned to the lowest available offset with the appropriate alignment. This may require internal padding, depending on the previous member. The size of an aggregate or union, if necessary, is increased to make it a multiple of the alignment of the aggregate or union. This may require tail padding, depending on the last member. 2.3. Bit Fields  C structure and union definitions may have bit fields that define integral objects with a specified number of bits. Bit Field Type Width w Range signed char 1 to 8 -2 w-1 to 2 w-1 - 1 unsigned char 1 to 8 0 to 2 w - 1 signed short 1 to 16 -2 w-1 to 2 w-1 - 1 unsigned short 1 to 16 0 to 2 w - 1 signed int 1 to 32 -2 w-1 to 2 w-1 - 1 unsigned int 1 to 32 0 to 2 w - 1 signed long long 1 to 64 -2 w-1 to 2 w-1 - 1 unsigned long long 1 to 64 0 to 2 w - 1 Current GPUs only support little-endian memory, so the below assumes little-endian layout. The following are rules that apply to bit fields. Plain bit fields (neither signed nor unsigned is specified) are treated as signed. When no type is provided (e.g., signed : 6 is specified), the type defaults to int. Bit fields obey the same size and alignment rules as other structure and union members, with the following modifications. Bit fields are allocated in memory from right to left (least to more significant) for little endian. A bit field must entirely reside in a storage unit appropriate for its declared type. A bit field should never cross its unit boundary. Bit fields may share a storage unit with other structure and union members, including members that are not bit fields, as long as there is enough space within the storage unit. Unnamed bit fields do not affect the alignment of a structure or union. Zero-length bit fields force the alignment of the following member of a structure to the next alignment boundary corresponding to the bit-field type. An unnamed, zero-length bit field will not force the external alignment of the structure to that boundary. If an unnamed, zero-length bit field has a stricter alignment than the external alignment, there is no guarantee that the stricter alignment will be maintained when the structure or union gets allocated to memory. The following figures contain examples of bit fields. Figure 1 shows the byte offsets (upper corners) and the bit numbers (lower corners) that are used in the examples. The remaining figures show different bit-field examples. Bit Numbering  Bit-field Allocation  Boundary Alignment  Storage Unit Sharing  Union Allocation  Unnamed Bit Fields  2.4. Texture, Sampler, and Surface Types  Texture, sampler and surface types are used to define references to texture and surface memory. The CUDA architecture provides hardware and instructions to efficiently read data from texture or surface memory as opposed to global memory. References to textures are bound through runtime functions to device read-only regions of memory, called a texture memory, before they can be used by a kernel. A texture reference has several attributes e.g. normalized mode, addressing mode, and texture filtering etc. A sampler reference can be used to sample a texture when read in a kernel. A surface reference is used to read or write data from and to the surface memory. It also has various attributes similar to a texture. At the PTX level objects that access texture or surface memory are referred to as opaque objects. Textures are expressed by either a .texref or .samplerref type and surfaces are expressed by the .surfref type. The data of opaque objects can be accessed by specific instructions (TEX for .texref/.samplerref and SULD/SUST for .surfref). The attributes of opaque objects are implemented by allocating a descriptor in memory which is populated by the driver. PTX TXQ/SUQ instructions get translated into memory reads of fields of the descriptor. The internal format of the descriptor varies with each architecture and should not be relied on by the user. The data and the attributes of an opaque object may be accessed directly if the texture or surface reference is known at compile time or indirectly. If the reference is not known during compile time all information required to read data and attributes is contained in a .b64 value called the handle. The handle can be used to pass and return oqaque object references to and from functions as well as to reference external textures, samplers and surfaces. 3. Function Calling Sequence  This section describes the PTX-level function calling sequence, including register usage, stack-frame layout, and parameter passing. The PTX-level function calling sequence describes what gets represented in PTX to enable function calls. There is an abstraction at this level. Most of the details associated with the function calling sequence are handled at the SASS level. PTX versions earlier than 2.0 do not conform to the ABI defined in this document, and cannot perform ABI compatible function calls. For the calling convention to work PTX version 2.0 or greater must be used. 3.1. Registers  At the PTX level, the registers that are specified are virtual. Register allocation occurs during PTX-to-SASS translation. The PTX-to-SASS translation also converts parameters and return values to physical registers or stack locations. 3.2. Stack Frame  The PTX level has no concept of the software stack. Manipulation of the stack is completely defined at the SASS level, and gets allocated during the PTX-to-SASS translation process. 3.3. Parameter Passing  At the PTX level, all parameters and return values present in a device function use the parameter state space (.param). The below table contains the rules for handling parameters and return values that are defined at the source level. For each source-level type, the corresponding PTX-level type that should be used is provided. Source Type Size in Bits PTX Type Integral types 8 to 32 (A) .u32 (if unsigned) or .s32 (if signed) Integral types 64 .u64 (if unsigned) or .s64 (if signed) Pointers (B) 32 .u32 Pointers (B) 64 .u64 Floating-point types (C) 32 .f32 Floating-point types (C) 64 .f64 Aggregates or unions Any size .align align .b8 name [ size ] Where align is overall aggregate-or-union alignment in bytes (D), name is variable name associated with aggregate or union, and size is the aggregate-or-union size in bytes. Handles (E) 64 .b64 (assigned from .texref, .sampleref, .surfref) NOTES: Values shorter than 32-bits are sign extended or zero extended, depending on whether they are signed or unsigned types. Unless the memory type is specified in the function declaration, all pointers passed at the PTX level must use a generic address. 16-bit floating-point types are only used for storage. Therefore, they cannot be used for parameters or return values. The alignment must be 1, 2, 4, 8, 16, 32, 64, or 128 bytes. The PTX built-in opaque types such as texture, sampler, and surface types are can be passed into functions as parameters and be returned by them through 64-bit handles. The handle contains the necessary information to access the actual data from the texture or surface memory as well as the attributes of the object stored in its type descriptor. See section Texture, Sampler, and Surface Types for more information on handles. 4. System Calls  System calls are calls into the driver operating system code. In PTX they look like regular calls, but the function definition is not given. A prototype must be provided in the PTX file, but the implementation of the function is provided by the driver. The prototype for the vprintf system call is: . extern . func (. param . s32 status ) vprintf (. param t1 format , . param t2 valist ) The following are the definitions for the vprintf parameters and return value. status : The status value that is returned by vprintf. format : A pointer to the format specifier input. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. valist : A pointer to the valist input. For 32-bit addresses, type t2 is .b32. For 64-bit addresses, type t2 is .b64. A call to vprintf using 32-bit addresses looks like: cvta . global . b32 % r2 , _fmt ; st . param . b32 [ param0 ], % r2 ; cvta . local . b32 % r3 , _valist_array ; st . param . b32 [ param1 ], % r3 ; call . uni ( _ ), vprintf , ( param0 , param1 ); For this code, _fmt is the format string in global memory, and _valist_array is the valist of arguments. Note that any pointers must be converted to generic space. The vprintf syscall is emitted as part of the printf function defined in “stdio.h”. The prototype for the malloc system call is: . extern . func (. param t1 ptr ) malloc (. param t2 size ) The following are the definitions for the malloc parameters and return value. ptr : The pointer to the memory that was allocated by malloc. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. size : The size of memory needed from malloc. This size is defined by the type size_t. When size_t is 32 bits, type t2 is .b32. When size_t is 64 bits, type t2 is .b64. The prototype for the free system call is: . extern . func free (. param t1 ptr ) The following is the definition for the free parameter. ptr : The pointer to the memory that should be freed. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. The malloc and free system calls are emitted as part of the malloc and free functions defined in “malloc.h”. In order to support assert, the PTX function call __assertfail is used whenever the assert expression produces a false value. The prototype for the __assertfail system call is: . extern . func __assertfail (. param t1 message , . param t1 file , . param . b32 line , . param t1 function , . param t2 charSize ) The following are the definitions for the __assertfail parameters. message : The pointer to the string that should be output. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. file : The pointer to the file name string associated with the assert. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. line : The line number associated with the assert. function : The pointer to the function name string associated with the assert. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. charSize : The size in bytes of the characters contained in the __assertfail parameter strings. The only supported character size is 1. The character size is defined by the type size_t. When size_t is 32 bits, type t2 is .b32. When size_t is 64 bits, type t2 is .b64. The __assertfail system call is emitted as part of the assert macro defined in “assert.h”. 5. Debug Information  Debug information is encoded in DWARF (Debug With Arbitrary Record Format). 5.1. Generation of Debug Information  The responsibility for generating debug information is split between the PTX producer and the PTX-to-SASS backend. The PTX producer is responsible for emitting binary DWARF into the PTX file, using the .section and .b8-.b16-.b32-and-.b64 directives in PTX. This should contain the .debug_info and .debug_abbrev sections, and possibly optional sections .debug_pubnames and .debug_aranges. These sections are standard DWARF2 sections that refer to labels and registers in the PTX. The PTX-to-SASS backend is responsible for generating the .debug_line section from the .file and .loc directives in the PTX file. This section maps source lines to SASS addresses. The backend also generates the .debug_frame section. 5.2. CUDA-Specific DWARF Definitions  In order to support debugging of multiple memory segments, address class codes are defined to reflect the memory space of variables. The address-class values are emitted as the DW_AT_address_class attribute for all variable and parameter Debugging Information Entries. The address class codes are defined in the below table. Code Value Description ADDR_code_space 1 Code storage ADDR_reg_space 2 Register storage ADDR_sreg_space 3 Special register storage ADDR_const_space 4 Constant storage ADDR_global_space 5 Global storage ADDR_local_space 6 Local storage ADDR_param_space 7 Parameter storage ADDR_shared_space 8 Shared storage ADDR_surf_space 9 Surface storage ADDR_tex_space 10 Texture storage ADDR_tex_sampler_space 11 Texture sampler storage ADDR_generic_space 12 Generic-address storage 6. Example  The following is example PTX with debug information for implementing the following program that makes a call: __device__ __noinline__ int foo ( int i , int j ) { return i + j ; } __global__ void test ( int * p ) { * p = foo ( 1 , 2 ); } The resulting PTX would be something like: . version 4.2 . target sm_20 , debug . address_size 64 . file 1 \"call_example.cu\" . visible . func (. param . b32 func_retval0 ) // return value _Z3fooii ( . param . b32 _Z3fooii_param_0 , // parameter \"i\" . param . b32 _Z3fooii_param_1 ) // parameter \"j\" { . reg . s32 % r < 4 > ; . loc 1 1 1 // following instructions are for line 1 func_begin0 : ld . param . u32 % r1 , [ _Z3fooii_param_0 ]; // load 1st param ld . param . u32 % r2 , [ _Z3fooii_param_1 ]; // load 2nd param . loc 1 3 1 // following instructions are for line 3 add . s32 % r3 , % r1 , % r2 ; st . param . b32 [ func_retval0 + 0 ], % r3 ; // store return value ret ; func_end0 : } . visible . entry _Z4testPi ( . param . u64 _Z4testPi_param_0 ) // parameter *p { . reg . s32 % r < 4 > ; . reg . s64 % rd < 2 > ; . loc 1 6 1 func_begin1 : ld . param . u64 % rd1 , [ _Z4testPi_param_0 ]; // load *p mov . u32 % r1 , 1 ; mov . u32 % r2 , 2 ; . loc 1 8 9 . param . b32 param0 ; st . param . b32 [ param0 + 0 ], % r1 ; // store 1 . param . b32 param1 ; st . param . b32 [ param1 + 0 ], % r2 ; // store 2 . param . b32 retval0 ; call . uni ( retval0 ), _Z3fooii , ( param0 , param1 ); // call foo ld . param . b32 % r3 , [ retval0 + 0 ]; // get return value st . u32 [ % rd1 ], % r3 ; // *p = return value . loc 1 9 2 ret ; func_end1 : } . section . debug_info { . b32 262 . b8 2 , 0 . b32 . debug_abbrev . b8 8 , 1 , 108 , 103 , 101 , 110 , 102 , 101 , 58 , 32 , 69 , 68 , 71 , 32 , 52 , 46 , 57 . b8 0 , 4 , 99 , 97 , 108 , 108 , 49 , 46 , 99 , 117 , 0 . b64 0 . b32 . debug_line // the .debug_line section will be created by ptxas from the .loc . b8 47 , 104 , 111 , 109 , 101 , 47 , 109 , 109 , 117 , 114 , 112 , 104 , 121 , 47 , 116 . b8 101 , 115 , 116 , 0 , 2 , 95 , 90 , 51 , 102 , 111 , 111 , 105 , 105 , 0 , 95 , 90 . b8 51 , 102 , 111 , 111 , 105 , 105 , 0 . b32 1 , 1 , 164 . b8 1 . b64 func_begin0 // start and end location of foo . b64 func_end0 . b8 1 , 156 , 3 , 105 , 0 . b32 1 , 1 , 164 . b8 5 , 144 , 177 , 228 , 149 , 1 , 2 , 3 , 106 , 0 . b32 1 , 1 , 164 . b8 5 , 144 , 178 , 228 , 149 , 1 , 2 , 0 , 4 , 105 , 110 , 116 , 0 , 5 . b32 4 . b8 2 , 95 , 90 , 52 , 116 , 101 , 115 , 116 , 80 , 105 , 0 , 95 , 90 , 52 , 116 , 101 . b8 115 , 116 , 80 , 105 , 0 . b32 1 , 6 , 253 . b8 1 . b64 func_begin1 // start and end location of test . b64 func_end1 . b8 1 , 156 , 3 , 112 , 0 . b32 1 , 6 , 259 . b8 9 , 3 . b64 _Z4testPi_param_0 . b8 7 , 0 , 5 , 118 , 111 , 105 , 100 , 0 , 6 . b32 164 . b8 12 , 0 } . section . debug_abbrev { . b8 1 , 17 , 1 , 37 , 8 , 19 , 11 , 3 , 8 , 17 , 1 , 16 , 6 , 27 , 8 , 0 , 0 , 2 , 46 , 1 , 135 . b8 64 , 8 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 63 , 12 , 17 , 1 , 18 , 1 , 64 , 10 , 0 , 0 . b8 3 , 5 , 0 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 2 , 10 , 51 , 11 , 0 , 0 , 4 , 36 , 0 , 3 . b8 8 , 62 , 11 , 11 , 6 , 0 , 0 , 5 , 59 , 0 , 3 , 8 , 0 , 0 , 6 , 15 , 0 , 73 , 19 , 51 , 11 . b8 0 , 0 , 0 } . section . debug_pubnames { . b32 41 . b8 2 , 0 . b32 . debug_info . b32 262 , 69 . b8 95 , 90 , 51 , 102 , 111 , 111 , 105 , 105 , 0 . b32 174 . b8 95 , 90 , 52 , 116 , 101 , 115 , 116 , 80 , 105 , 0 . b32 0 } 7. C++  The C++ implementation for device functions follows the Itanium C++ ABI. However, not everything in C++ is supported. In particular, the following are not supported in device code. Exceptions and try/catch blocks RTTI STL library Global constructors and destructors Virtual functions and classes across host and device (i.e., vtables cannot be used across host and device) There are also a few C features that are not currently supported: stdio other than printf 8. Notices  8.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ada-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/ada-tuning-guide/index.html", "content_type": "text/html", "text": "NVIDIA Ada GPU Architecture Tuning Guide 1. NVIDIA Ada GPU Architecture Tuning Guide 1.1. NVIDIA Ada GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ada GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Improved Tensor Core Operations 1.4.1.3. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased L2 capacity 1.4.2.2. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Ada Tuning Guide » 1. NVIDIA Ada GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ada GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ada GPU Architecture. 1. NVIDIA Ada GPU Architecture Tuning Guide  1.1. NVIDIA Ada GPU Architecture  The NVIDIA ® Ada GPU architecture is NVIDIA’s latest architecture for CUDA ® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ada GPU Architecture. 1.4. NVIDIA Ada GPU Architecture Tuning  1.4.1. Streaming Multiprocessor  The NVIDIA Ada GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM is 48, remaining the same compared to compute capability 8.6 GPUs, and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 24. The shared memory capacity per SM is 100 KB. The maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on compute capability 8.6 GPUs without changes to their application. 1.4.1.2. Improved Tensor Core Operations  The NVIDIA Ada GPU architecture includes new Ada Fourth Generation Tensor Cores featuring the Hopper FP8 Transformer Engine. 1.4.1.3. Improved FP32 throughput  Devices of compute capability 8.9 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run as-is on 8.9, it is recommended to compile explicitly for 8.9 to benefit from the increased FP32 throughput. 1.4.2. Memory System  1.4.2.1. Increased L2 capacity  The NVIDIA Ada GPU architecture increases the capacity of the L2 cache to 98304 KB in AD102, 16x larger than GA102. The NVIDIA Ada GPU architecture allows CUDA users to control the persistence of data in the L2 cache. For more information on the persistence of data in the L2 cache, refer to the section on managing the L2 cache in the CUDA C++ Programming Guide . 1.4.2.2. Unified Shared Memory/L1/Texture Cache  NVIDIA Ada architecture features a unified L1 cache, texture cache, and shared memory similar to that of the NVIDIA Ampere architecture. The combined L1 cache capacity is 128 KB. In the NVIDIA Ada GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA Ada GPU architecture supports shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 2. Revision History  Version 1.0 Initial Public Release Added support for compute capability 8.9 1 Throughout this guide, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.0 and 8.6, NVIDIA Ada refers to devices of compute capability 8.9. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cublas/index.html", "parent_url": "https://docs.nvidia.com/cuda/cublas/index.html", "content_type": "text/html", "text": "cuBLAS 1. Introduction 1.1. Data Layout 1.2. New and Legacy cuBLAS API 1.3. Example Code 2. Using the cuBLAS API 2.1. General Description 2.1.1. Error Status 2.1.2. cuBLAS Context 2.1.3. Thread Safety 2.1.4. Results Reproducibility 2.1.5. Scalar Parameters 2.1.6. Parallelism with Streams 2.1.7. Batching Kernels 2.1.8. Cache Configuration 2.1.9. Static Library Support 2.1.10. GEMM Algorithms Numerical Behavior 2.1.11. Tensor Core Usage 2.1.12. CUDA Graphs Support 2.1.13. 64-bit Integer Interface 2.2. cuBLAS Datatypes Reference 2.2.1. cublasHandle_t 2.2.2. cublasStatus_t 2.2.3. cublasOperation_t 2.2.4. cublasFillMode_t 2.2.5. cublasDiagType_t 2.2.6. cublasSideMode_t 2.2.7. cublasPointerMode_t 2.2.8. cublasAtomicsMode_t 2.2.9. cublasGemmAlgo_t 2.2.10. cublasMath_t 2.2.11. cublasComputeType_t 2.3. CUDA Datatypes Reference 2.3.1. cudaDataType_t 2.3.2. libraryPropertyType_t 2.4. cuBLAS Helper Function Reference 2.4.1. cublasCreate() 2.4.2. cublasDestroy() 2.4.3. cublasGetVersion() 2.4.4. cublasGetProperty() 2.4.5. cublasGetStatusName() 2.4.6. cublasGetStatusString() 2.4.7. cublasSetStream() 2.4.8. cublasSetWorkspace() 2.4.9. cublasGetStream() 2.4.10. cublasGetPointerMode() 2.4.11. cublasSetPointerMode() 2.4.12. cublasSetVector() 2.4.13. cublasGetVector() 2.4.14. cublasSetMatrix() 2.4.15. cublasGetMatrix() 2.4.16. cublasSetVectorAsync() 2.4.17. cublasGetVectorAsync() 2.4.18. cublasSetMatrixAsync() 2.4.19. cublasGetMatrixAsync() 2.4.20. cublasSetAtomicsMode() 2.4.21. cublasGetAtomicsMode() 2.4.22. cublasSetMathMode() 2.4.23. cublasGetMathMode() 2.4.24. cublasSetSmCountTarget() 2.4.25. cublasGetSmCountTarget() 2.4.26. cublasLoggerConfigure() 2.4.27. cublasGetLoggerCallback() 2.4.28. cublasSetLoggerCallback() 2.5. cuBLAS Level-1 Function Reference 2.5.1. cublasI<t>amax() 2.5.2. cublasI<t>amin() 2.5.3. cublas<t>asum() 2.5.4. cublas<t>axpy() 2.5.5. cublas<t>copy() 2.5.6. cublas<t>dot() 2.5.7. cublas<t>nrm2() 2.5.8. cublas<t>rot() 2.5.9. cublas<t>rotg() 2.5.10. cublas<t>rotm() 2.5.11. cublas<t>rotmg() 2.5.12. cublas<t>scal() 2.5.13. cublas<t>swap() 2.6. cuBLAS Level-2 Function Reference 2.6.1. cublas<t>gbmv() 2.6.2. cublas<t>gemv() 2.6.3. cublas<t>ger() 2.6.4. cublas<t>sbmv() 2.6.5. cublas<t>spmv() 2.6.6. cublas<t>spr() 2.6.7. cublas<t>spr2() 2.6.8. cublas<t>symv() 2.6.9. cublas<t>syr() 2.6.10. cublas<t>syr2() 2.6.11. cublas<t>tbmv() 2.6.12. cublas<t>tbsv() 2.6.13. cublas<t>tpmv() 2.6.14. cublas<t>tpsv() 2.6.15. cublas<t>trmv() 2.6.16. cublas<t>trsv() 2.6.17. cublas<t>hemv() 2.6.18. cublas<t>hbmv() 2.6.19. cublas<t>hpmv() 2.6.20. cublas<t>her() 2.6.21. cublas<t>her2() 2.6.22. cublas<t>hpr() 2.6.23. cublas<t>hpr2() 2.6.24. cublas<t>gemvBatched() 2.6.25. cublas<t>gemvStridedBatched() 2.7. cuBLAS Level-3 Function Reference 2.7.1. cublas<t>gemm() 2.7.2. cublas<t>gemm3m() 2.7.3. cublas<t>gemmBatched() 2.7.4. cublas<t>gemmStridedBatched() 2.7.5. cublas<t>gemmGroupedBatched() 2.7.6. cublas<t>symm() 2.7.7. cublas<t>syrk() 2.7.8. cublas<t>syr2k() 2.7.9. cublas<t>syrkx() 2.7.10. cublas<t>trmm() 2.7.11. cublas<t>trsm() 2.7.12. cublas<t>trsmBatched() 2.7.13. cublas<t>hemm() 2.7.14. cublas<t>herk() 2.7.15. cublas<t>her2k() 2.7.16. cublas<t>herkx() 2.8. BLAS-like Extension 2.8.1. cublas<t>geam() 2.8.2. cublas<t>dgmm() 2.8.3. cublas<t>getrfBatched() 2.8.4. cublas<t>getrsBatched() 2.8.5. cublas<t>getriBatched() 2.8.6. cublas<t>matinvBatched() 2.8.7. cublas<t>geqrfBatched() 2.8.8. cublas<t>gelsBatched() 2.8.9. cublas<t>tpttr() 2.8.10. cublas<t>trttp() 2.8.11. cublas<t>gemmEx() 2.8.12. cublasGemmEx() 2.8.13. cublasGemmBatchedEx() 2.8.14. cublasGemmStridedBatchedEx() 2.8.15. cublasGemmGroupedBatchedEx() 2.8.16. cublasCsyrkEx() 2.8.17. cublasCsyrk3mEx() 2.8.18. cublasCherkEx() 2.8.19. cublasCherk3mEx() 2.8.20. cublasNrm2Ex() 2.8.21. cublasAxpyEx() 2.8.22. cublasDotEx() 2.8.23. cublasRotEx() 2.8.24. cublasScalEx() 3. Using the cuBLASLt API 3.1. General Description 3.1.1. Problem Size Limitations 3.1.2. Heuristics Cache 3.1.3. cuBLASLt Logging 3.1.4. 8-bit Floating Point Data Types (FP8) Usage 3.1.5. Disabling CPU Instructions 3.1.6. Atomics Synchronization 3.2. cuBLASLt Code Examples 3.3. cuBLASLt Datatypes Reference 3.3.1. cublasLtClusterShape_t 3.3.2. cublasLtEpilogue_t 3.3.3. cublasLtHandle_t 3.3.4. cublasLtLoggerCallback_t 3.3.5. cublasLtMatmulAlgo_t 3.3.6. cublasLtMatmulAlgoCapAttributes_t 3.3.7. cublasLtMatmulAlgoConfigAttributes_t 3.3.8. cublasLtMatmulDesc_t 3.3.9. cublasLtMatmulDescAttributes_t 3.3.10. cublasLtMatmulHeuristicResult_t 3.3.11. cublasLtMatmulInnerShape_t 3.3.12. cublasLtMatmulPreference_t 3.3.13. cublasLtMatmulPreferenceAttributes_t 3.3.14. cublasLtMatmulSearch_t 3.3.15. cublasLtMatmulTile_t 3.3.16. cublasLtMatmulStages_t 3.3.17. cublasLtNumericalImplFlags_t 3.3.18. cublasLtMatrixLayout_t 3.3.19. cublasLtMatrixLayoutAttribute_t 3.3.20. cublasLtMatrixTransformDesc_t 3.3.21. cublasLtMatrixTransformDescAttributes_t 3.3.22. cublasLtOrder_t 3.3.23. cublasLtPointerMode_t 3.3.24. cublasLtPointerModeMask_t 3.3.25. cublasLtReductionScheme_t 3.4. cuBLASLt API Reference 3.4.1. cublasLtCreate() 3.4.2. cublasLtDestroy() 3.4.3. cublasLtDisableCpuInstructionsSetMask() 3.4.4. cublasLtGetCudartVersion() 3.4.5. cublasLtGetProperty() 3.4.6. cublasLtGetStatusName() 3.4.7. cublasLtGetStatusString() 3.4.8. cublasLtHeuristicsCacheGetCapacity() 3.4.9. cublasLtHeuristicsCacheSetCapacity() 3.4.10. cublasLtGetVersion() 3.4.11. cublasLtLoggerSetCallback() 3.4.12. cublasLtLoggerSetFile() 3.4.13. cublasLtLoggerOpenFile() 3.4.14. cublasLtLoggerSetLevel() 3.4.15. cublasLtLoggerSetMask() 3.4.16. cublasLtLoggerForceDisable() 3.4.17. cublasLtMatmul() 3.4.18. cublasLtMatmulAlgoCapGetAttribute() 3.4.19. cublasLtMatmulAlgoCheck() 3.4.20. cublasLtMatmulAlgoConfigGetAttribute() 3.4.21. cublasLtMatmulAlgoConfigSetAttribute() 3.4.22. cublasLtMatmulAlgoGetHeuristic() 3.4.23. cublasLtMatmulAlgoGetIds() 3.4.24. cublasLtMatmulAlgoInit() 3.4.25. cublasLtMatmulDescCreate() 3.4.26. cublasLtMatmulDescInit() 3.4.27. cublasLtMatmulDescDestroy() 3.4.28. cublasLtMatmulDescGetAttribute() 3.4.29. cublasLtMatmulDescSetAttribute() 3.4.30. cublasLtMatmulPreferenceCreate() 3.4.31. cublasLtMatmulPreferenceInit() 3.4.32. cublasLtMatmulPreferenceDestroy() 3.4.33. cublasLtMatmulPreferenceGetAttribute() 3.4.34. cublasLtMatmulPreferenceSetAttribute() 3.4.35. cublasLtMatrixLayoutCreate() 3.4.36. cublasLtMatrixLayoutInit() 3.4.37. cublasLtMatrixLayoutDestroy() 3.4.38. cublasLtMatrixLayoutGetAttribute() 3.4.39. cublasLtMatrixLayoutSetAttribute() 3.4.40. cublasLtMatrixTransform() 3.4.41. cublasLtMatrixTransformDescCreate() 3.4.42. cublasLtMatrixTransformDescInit() 3.4.43. cublasLtMatrixTransformDescDestroy() 3.4.44. cublasLtMatrixTransformDescGetAttribute() 3.4.45. cublasLtMatrixTransformDescSetAttribute() 4. Using the cuBLASXt API 4.1. General description 4.1.1. Tiling design approach 4.1.2. Hybrid CPU-GPU computation 4.1.3. Results reproducibility 4.2. cuBLASXt API Datatypes Reference 4.2.1. cublasXtHandle_t 4.2.2. cublasXtOpType_t 4.2.3. cublasXtBlasOp_t 4.2.4. cublasXtPinningMemMode_t 4.3. cuBLASXt API Helper Function Reference 4.3.1. cublasXtCreate() 4.3.2. cublasXtDestroy() 4.3.3. cublasXtDeviceSelect() 4.3.4. cublasXtSetBlockDim() 4.3.5. cublasXtGetBlockDim() 4.3.6. cublasXtSetCpuRoutine() 4.3.7. cublasXtSetCpuRatio() 4.3.8. cublasXtSetPinningMemMode() 4.3.9. cublasXtGetPinningMemMode() 4.4. cuBLASXt API Math Functions Reference 4.4.1. cublasXt<t>gemm() 4.4.2. cublasXt<t>hemm() 4.4.3. cublasXt<t>symm() 4.4.4. cublasXt<t>syrk() 4.4.5. cublasXt<t>syr2k() 4.4.6. cublasXt<t>syrkx() 4.4.7. cublasXt<t>herk() 4.4.8. cublasXt<t>her2k() 4.4.9. cublasXt<t>herkx() 4.4.10. cublasXt<t>trsm() 4.4.11. cublasXt<t>trmm() 4.4.12. cublasXt<t>spmm() 5. Using the cuBLASDx API 6. Using the cuBLAS Legacy API 6.1. Error Status 6.2. Initialization and Shutdown 6.3. Thread Safety 6.4. Memory Management 6.5. Scalar Parameters 6.6. Helper Functions 6.7. Level-1,2,3 Functions 6.8. Converting Legacy to the cuBLAS API 6.9. Examples 7. cuBLAS Fortran Bindings 8. Interaction with Other Libraries and Tools 8.1. nvprune 9. Acknowledgements 10. Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks cuBLAS » 1. Introduction v12.5 | PDF | Archive cuBLAS The API Reference guide for cuBLAS, the CUDA Basic Linear Algebra Subroutine library. 1. Introduction  The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA®CUDA™ runtime. It allows the user to access the computational resources of NVIDIA Graphics Processing Unit (GPU). The cuBLAS Library exposes four sets of APIs: The cuBLAS API , which is simply called cuBLAS API in this document (starting with CUDA 6.0), The cuBLASXt API (starting with CUDA 6.0), and The cuBLASLt API (starting with CUDA 10.1) The cuBLASDx API (not shipped with the CUDA Toolkit) To use the cuBLAS API, the application must allocate the required matrices and vectors in the GPU memory space, fill them with data, call the sequence of desired cuBLAS functions, and then upload the results from the GPU memory space back to the host. The cuBLAS API also provides helper functions for writing and retrieving data from the GPU. To use the cuBLASXt API, the application may have the data on the Host or any of the devices involved in the computation, and the Library will take care of dispatching the operation to, and transferring the data to, one or multiple GPUs present in the system, depending on the user request. The cuBLASLt is a lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API. This library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. After a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data. 1.1. Data Layout  For maximum compatibility with existing Fortran environments, the cuBLAS library uses column-major storage, and 1-based indexing. Since C and C++ use row-major storage, applications written in these languages can not use the native array semantics for two-dimensional arrays. Instead, macros or inline functions should be defined to implement matrices on top of one-dimensional arrays. For Fortran code ported to C in mechanical fashion, one may chose to retain 1-based indexing to avoid the need to transform loops. In this case, the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) Here, ld refers to the leading dimension of the matrix, which in the case of column-major storage is the number of rows of the allocated matrix (even if only a submatrix of it is being used). For natively written C and C++ code, one would most likely choose 0-based indexing, in which case the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2C(i,j,ld) (((j)*(ld))+(i)) 1.2. New and Legacy cuBLAS API  Starting with version 4.0, the cuBLAS Library provides a new API, in addition to the existing legacy API. This section discusses why a new API is provided, the advantages of using it, and the differences with the existing legacy API. Warning The legacy cuBLAS API is deprecated and will be removed in future release. The new cuBLAS library API can be used by including the header file cublas_v2.h . It has the following features that the legacy cuBLAS API does not have: The handle to the cuBLAS library context is initialized using the function and is explicitly passed to every subsequent library function call. This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs. This also allows the cuBLAS APIs to be reentrant. The scalars \\(\\alpha\\) and \\(\\beta\\) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host. This change allows library functions to execute asynchronously using streams even when \\(\\alpha\\) and \\(\\beta\\) are generated by a previous kernel. When a library routine returns a scalar result, it can be returned by reference on the host or the device, instead of only being allowed to be returned by value only on the host. This change allows library routines to be called asynchronously when the scalar result is generated and returned by reference on the device resulting in maximum parallelism. The error status cublasStatus_t is returned by all cuBLAS library function calls. This change facilitates debugging and simplifies software development. Note that cublasStatus was renamed cublasStatus_t to be more consistent with other types in the cuBLAS library. The cublasAlloc() and cublasFree() functions have been deprecated. This change removes these unnecessary wrappers around cudaMalloc() and cudaFree() , respectively. The function cublasSetKernelStream() was renamed cublasSetStream() to be more consistent with the other CUDA libraries. The legacy cuBLAS API, explained in more detail in Using the cuBLAS Legacy API , can be used by including the header file cublas.h . Since the legacy API is identical to the previously released cuBLAS library API, existing applications will work out of the box and automatically use this legacy API without any source code changes. The current and the legacy cuBLAS APIs cannot be used simultaneously in a single translation unit: including both cublas.h and cublas_v2.h header files will lead to compilation errors due to incompatible symbol redeclarations. In general, new applications should not use the legacy cuBLAS API, and existing applications should convert to using the new API if it requires sophisticated and optimal stream parallelism, or if it calls cuBLAS routines concurrently from multiple threads. For the rest of the document, the new cuBLAS Library API will simply be referred to as the cuBLAS Library API. As mentioned earlier the interfaces to the legacy and the cuBLAS library APIs are the header file cublas.h and cublas_v2.h , respectively. In addition, applications using the cuBLAS library need to link against: The DSO cublas.so for Linux, The DLL cublas.dll for Windows, or The dynamic library cublas.dylib for Mac OS X. Note The same dynamic library implements both the new and legacy cuBLAS APIs. 1.3. Example Code  For sample code references please see the two examples below. They show an application written in C using the cuBLAS library API with two indexing styles (Example 1. “Application Using C and cuBLAS: 1-based indexing” and Example 2. “Application Using C and cuBLAS: 0-based Indexing”). //Example 1. Application Using C and cuBLAS: 1-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include <cuda_runtime.h> #include \"cublas_v2.h\" #define M 6 #define N 5 #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) static __inline__ void modify ( cublasHandle_t handle , float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( handle , n - q + 1 , & alpha , & m [ IDX2F ( p , q , ldm )], ldm ); cublasSscal ( handle , ldm - p + 1 , & beta , & m [ IDX2F ( p , q , ldm )], 1 ); } int main ( void ){ cudaError_t cudaStat ; cublasStatus_t stat ; cublasHandle_t handle ; int i , j ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { a [ IDX2F ( i , j , M )] = ( float )(( i -1 ) * N + j ); } } cudaStat = cudaMalloc (( void ** ) & devPtrA , M * N * sizeof ( * a )); if ( cudaStat != cudaSuccess ) { printf ( \"device memory allocation failed\" ); free ( a ); return EXIT_FAILURE ; } stat = cublasCreate ( & handle ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"CUBLAS initialization failed \\n \" ); free ( a ); cudaFree ( devPtrA ); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } modify ( handle , devPtrA , M , N , 2 , 3 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } cudaFree ( devPtrA ); cublasDestroy ( handle ); for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2F ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } //Example 2. Application Using C and cuBLAS: 0-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include <cuda_runtime.h> #include \"cublas_v2.h\" #define M 6 #define N 5 #define IDX2C(i,j,ld) (((j)*(ld))+(i)) static __inline__ void modify ( cublasHandle_t handle , float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( handle , n - q , & alpha , & m [ IDX2C ( p , q , ldm )], ldm ); cublasSscal ( handle , ldm - p , & beta , & m [ IDX2C ( p , q , ldm )], 1 ); } int main ( void ){ cudaError_t cudaStat ; cublasStatus_t stat ; cublasHandle_t handle ; int i , j ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { a [ IDX2C ( i , j , M )] = ( float )( i * N + j + 1 ); } } cudaStat = cudaMalloc (( void ** ) & devPtrA , M * N * sizeof ( * a )); if ( cudaStat != cudaSuccess ) { printf ( \"device memory allocation failed\" ); free ( a ); return EXIT_FAILURE ; } stat = cublasCreate ( & handle ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"CUBLAS initialization failed \\n \" ); free ( a ); cudaFree ( devPtrA ); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } modify ( handle , devPtrA , M , N , 1 , 2 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } cudaFree ( devPtrA ); cublasDestroy ( handle ); for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2C ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } 2. Using the cuBLAS API  2.1. General Description  This section describes how to use the cuBLAS library API. 2.1.1. Error Status  All cuBLAS library function calls return the error status cublasStatus_t . 2.1.2. cuBLAS Context  The application must initialize a handle to the cuBLAS library context by calling the cublasCreate() function. Then, the handle is explicitly passed to every subsequent library function call. Once the application finishes using the library, it must call the function cublasDestroy() to release the resources associated with the cuBLAS library context. This approach allows the user to explicitly control the library setup when using multiple host threads and multiple GPUs. For example, the application can use cudaSetDevice() to associate different devices with different host threads and in each of those host threads it can initialize a unique handle to the cuBLAS library context, which will use the particular device associated with that host thread. Then, the cuBLAS library function calls made with different handles will automatically dispatch the computation to different devices. The device associated with a particular cuBLAS context is assumed to remain unchanged between the corresponding cublasCreate() and cublasDestroy() calls. In order for the cuBLAS library to use a different device in the same host thread, the application must set the new device to be used by calling cudaSetDevice() and then create another cuBLAS context, which will be associated with the new device, by calling cublasCreate() . A cuBLAS library context is tightly coupled with the CUDA context that is current at the time of the cublasCreate() call. An application that uses multiple CUDA contexts is required to create a cuBLAS context per CUDA context and make sure the former never outlives the latter. 2.1.3. Thread Safety  The library is thread safe and its functions can be called from multiple host threads, even with the same handle. When multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cuBLAS calls in all threads. It is even more true for the destruction of the handle. So it is not recommended that multiple thread share the same cuBLAS handle. 2.1.4. Results Reproducibility  By design, all cuBLAS API routines from a given toolkit version, generate the same bit-wise results at every run when executed on GPUs with the same architecture and the same number of SMs. However, bit-wise reproducibility is not guaranteed across toolkit versions because the implementation might differ due to some implementation changes. This guarantee holds when a single CUDA stream is active only. If multiple concurrent streams are active, the library may optimize total performance by picking different internal implementations. Note The non-deterministic behavior of multi-stream execution is due to library optimizations in selecting internal workspace for the routines running in parallel streams. To avoid this effect user can either: provide a separate workspace for each used stream using the cublasSetWorkspace() function, or have one cuBLAS handle per stream, or use cublasLtMatmul() instead of GEMM-family of functions and provide user owned workspace, or set a debug environment variable CUBLAS_WORKSPACE_CONFIG to :16:8 (may limit overall performance) or :4096:8 (will increase library footprint in GPU memory by approximately 24MiB). Any of those settings will allow for deterministic behavior even with multiple concurrent streams sharing a single cuBLAS handle. This behavior is expected to change in a future release. For some routines such as cublas<t>symv and cublas<t>hemv , an alternate significantly faster routine can be chosen using the routine cublasSetAtomicsMode() . In that case, the results are not guaranteed to be bit-wise reproducible because atomics are used for the computation. 2.1.5. Scalar Parameters  There are two categories of the functions that use scalar parameters : Functions that take alpha and/or beta parameters by reference on the host or the device as scaling factors, such as gemm . Functions that return a scalar result on the host or the device such as amax() , amin , asum() , rotg() , rotmg() , dot() and nrm2() . For the functions of the first category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , the scalar parameters alpha and/or beta can be on the stack or allocated on the heap, shouldn’t be placed in managed memory. Underneath, the CUDA kernels related to those functions will be launched with the value of alpha and/or beta . Therefore if they were allocated on the heap, they can be freed just after the return of the call even though the kernel launch is asynchronous. When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , alpha and/or beta must be accessible on the device and their values should not be modified until the kernel is done. Note that since cudaFree() does an implicit cudaDeviceSynchronize() , cudaFree() can still be called on alpha and/or beta just after the call but it would defeat the purpose of using this pointer mode in that case. For the functions of the second category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , these functions block the CPU, until the GPU has completed its computation and the results have been copied back to the Host. When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , these functions return immediately. In this case, similar to matrix and vector results, the scalar result is ready only when execution of the routine on the GPU has completed. This requires proper synchronization in order to read the result from the host. In either case, the pointer mode CUBLAS_POINTER_MODE_DEVICE allows the library functions to execute completely asynchronously from the Host even when alpha and/or beta are generated by a previous kernel. For example, this situation can arise when iterative methods for solution of linear systems and eigenvalue problems are implemented using the cuBLAS library. 2.1.6. Parallelism with Streams  If the application uses the results computed by multiple independent tasks, CUDA™ streams can be used to overlap the computation performed in these tasks. The application can conceptually associate each stream with each task. In order to achieve the overlap of computation between the tasks, the user should create CUDA™ streams using the function cudaStreamCreate() and set the stream to be used by each individual cuBLAS library routine by calling cublasSetStream() just before calling the actual cuBLAS routine. Note that cublasSetStream() resets the user-provided workspace to the default workspace pool; see cublasSetWorkspace() . Then, the computation performed in separate streams would be overlapped automatically when possible on the GPU. This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work. We recommend using the new cuBLAS API with scalar parameters and results passed by reference in the device memory to achieve maximum overlap of the computation when using streams. A particular application of streams, batching of multiple small kernels, is described in the following section. 2.1.7. Batching Kernels  In this section, we explain how to use streams to batch the execution of small kernels. For instance, suppose that we have an application where we need to make many small independent matrix-matrix multiplications with dense matrices. It is clear that even with millions of small independent matrices we will not be able to achieve the same GFLOPS rate as with a one large matrix. For example, a single \\(n \\times n\\) large matrix-matrix multiplication performs \\(n^{3}\\) operations for \\(n^{2}\\) input size, while 1024 \\(\\frac{n}{32} \\times \\frac{n}{32}\\) small matrix-matrix multiplications perform \\(1024\\left( \\frac{n}{32} \\right)^{3} = \\frac{n^{3}}{32}\\) operations for the same input size. However, it is also clear that we can achieve a significantly better performance with many small independent matrices compared with a single small matrix. The architecture family of GPUs allows us to execute multiple kernels simultaneously. Hence, in order to batch the execution of independent kernels, we can run each of them in a separate stream. In particular, in the above example we could create 1024 CUDA™ streams using the function cudaStreamCreate() , then preface each call to cublas<t>gemm() with a call to cublasSetStream() with a different stream for each of the matrix-matrix multiplications (note that cublasSetStream() resets user-provided workspace to the default workspace pool, see cublasSetWorkspace() ). This will ensure that when possible the different computations will be executed concurrently. Although the user can create many streams, in practice it is not possible to have more than 32 concurrent kernels executing at the same time. 2.1.8. Cache Configuration  On some devices, L1 cache and shared memory use the same hardware resources. The cache configuration can be set directly with the CUDA Runtime function cudaDeviceSetCacheConfig. The cache configuration can also be set specifically for some functions using the routine cudaFuncSetCacheConfig. Please refer to the CUDA Runtime API documentation for details about the cache configuration settings. Because switching from one configuration to another can affect kernels concurrency, the cuBLAS Library does not set any cache configuration preference and relies on the current setting. However, some cuBLAS routines, especially Level-3 routines, rely heavily on shared memory. Thus the cache preference setting might affect adversely their performance. 2.1.9. Static Library Support  The cuBLAS Library is also delivered in a static form as libcublas_static.a on Linux. The static cuBLAS library and all other static math libraries depend on a common thread abstraction layer library called libculibos.a . For example, on Linux, to compile a small application using cuBLAS, against the dynamic library, the following command can be used: nvcc myCublasApp . c - lcublas - o myCublasApp Whereas to compile against the static cuBLAS library, the following command must be used: nvcc myCublasApp . c - lcublas_static - lculibos - o myCublasApp It is also possible to use the native Host C++ compiler. Depending on the Host operating system, some additional libraries like pthread or dl might be needed on the linking line. The following command on Linux is suggested : g ++ myCublasApp . c - lcublas_static - lculibos - lcudart_static - lpthread - ldl - I < cuda - toolkit - path >/ include - L < cuda - toolkit - path >/ lib64 - o myCublasApp Note that in the latter case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. Starting with release 11.2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library. 2.1.10. GEMM Algorithms Numerical Behavior  Some GEMM algorithms split the computation along the dimension K to increase the GPU occupancy, especially when the dimension K is large compared to dimensions M and N. When this type of algorithm is chosen by the cuBLAS heuristics or explicitly by the user, the results of each split is summed deterministically into the resulting matrix to get the final result. For the routines cublas<t>gemmEx and cublasGemmEx() , when the compute type is greater than the output type, the sum of the split chunks can potentially lead to some intermediate overflows thus producing a final resulting matrix with some overflows. Those overflows might not have occurred if all the dot products had been accumulated in the compute type before being converted at the end in the output type. This computation side-effect can be easily exposed when the computeType is CUDA_R_32F and Atype, Btype and Ctype are in CUDA_R_16F. This behavior can be controlled using the compute precision mode CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION with cublasSetMathMode() 2.1.11. Tensor Core Usage  Tensor cores were first introduced with Volta GPUs (compute capability 7.0 and above) and significantly accelerate matrix multiplications. Starting with cuBLAS version 11.0.0, the library may automatically make use of Tensor Core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cuBLAS (see cublasSetMathMode() , cublasMath_t ). It should be noted that the library will pick a Tensor Core enabled implementation wherever it determines that it would provide the best performance. The best performance when using Tensor Cores can be achieved when the matrix dimensions and pointers meet certain memory alignment requirements. Specifically, all of the following conditions must be satisfied to get the most performance out of Tensor Cores: ((op_A == CUBLAS_OP_N ? m : k) * AtypeSize) % 16 == 0 ((op_B == CUBLAS_OP_N ? k : n) * BtypeSize) % 16 == 0 (m * CtypeSize) % 16 == 0 (lda * AtypeSize) % 16 == 0 (ldb * BtypeSize) % 16 == 0 (ldc * CtypeSize) % 16 == 0 intptr_t(A) % 16 == 0 intptr_t(B) % 16 == 0 intptr_t(C) % 16 == 0 To conduct matrix multiplication with FP8 types (see 8-bit Floating Point Data Types (FP8) Usage ), you must ensure that your matrix dimensions and pointers meet the optimal requirements listed above.  Aside from FP8, there are no longer any restrictions on matrix dimensions and memory alignments to use Tensor Cores (starting with cuBLAS version 11.0.0). 2.1.12. CUDA Graphs Support  cuBLAS routines can be captured in CUDA Graph stream capture without restrictions in most situations. The exception are routines that output results into host buffers (e.g. cublas<t>dot while pointer mode CUBLAS_POINTER_MODE_HOST is configured), as it enforces synchronization. For input coefficients (such as alpha , beta ) behavior depends on the pointer mode setting: In the case of CUBLAS(LT)_POINTER_MODE_HOST , coefficient values are captured in the graph. In the case of pointer modes with device pointers, coefficient value is accessed using the device pointer at the time of graph execution. Note When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync . However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.13. 64-bit Integer Interface  cuBLAS version 12 introduced 64-bit integer capable functions. Each 64-bit integer function is equivalent to a 32-bit integer function with the following changes: The function name has _64 suffix. The dimension (problem size) data type changed from int to int64_t . Examples of dimension: m , n , and k . The leading dimension data type changed from int to int64_t . Examples of leading dimension: lda , ldb , and ldc . The vector increment data type changed from int to int64_t . Examples of vector increment: incx and incy . For example, consider the following 32-bit integer functions: cublasStatus_t cublasSetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ); cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ); cublasStatus_t cublasSsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * A , int lda ); The equivalent 64-bit integer functions are: cublasStatus_t cublasSetMatrix_64 ( int64_t rows , int64_t cols , int64_t elemSize , const void * A , int64_t lda , void * B , int64_t ldb ); cublasStatus_t cublasIsamax_64 ( cublasHandle_t handle , int64_t n , const float * x , int64_t incx , int64_t * result ); cublasStatus_t cublasSsyr_64 ( cublasHandle_t handle , cublasFillMode_t uplo , int64_t n , const float * alpha , const float * x , int64_t incx , float * A , int64_t lda ); Not every function has a 64-bit integer equivalent. For instance, cublasSetMathMode() doesn’t have any arguments that could meaningfully be int64_t . For documentation brevity, the 64-bit integer APIs are not explicitly listed, but only mentioned that they exist for the relevant functions. 2.2. cuBLAS Datatypes Reference  2.2.1. cublasHandle_t  The cublasHandle_t type is a pointer type to an opaque structure holding the cuBLAS library context. The cuBLAS library context must be initialized using cublasCreate() and the returned handle must be passed to all subsequent library function calls. The context should be destroyed at the end using cublasDestroy() . 2.2.2. cublasStatus_t  The type is used for function status returns. All cuBLAS library functions return their status, which can have the following values. Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The cuBLAS library was not initialized. This is usually caused by the lack of a prior cublasCreate() call, an error in the CUDA Runtime API called by the cuBLAS routine, or an error in the hardware setup. To correct: call cublasCreate() before the function call; and check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. CUBLAS_STATUS_ALLOC_FAILED Resource allocation failed inside the cuBLAS library. This is usually caused by a cudaMalloc() failure. To correct: prior to the function call, deallocate previously allocated memory as much as possible. CUBLAS_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. CUBLAS_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by compute capability lower than 5.0. To correct: compile and run the application on a device with appropriate compute capability. CUBLAS_STATUS_MAPPING_ERROR An access to GPU memory space failed, which is usually caused by a failure to bind a texture. To correct: before the function call, unbind any previously bound textures. CUBLAS_STATUS_EXECUTION_FAILED The GPU program failed to execute. This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons. To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. CUBLAS_STATUS_INTERNAL_ERROR An internal cuBLAS operation failed. This error is usually caused by a cudaMemcpyAsync() failure. To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion. CUBLAS_STATUS_NOT_SUPPORTED The functionality requested is not supported. CUBLAS_STATUS_LICENSE_ERROR The functionality requested requires some license and an error was detected when trying to check the current licensing. This error can happen if the license is not present or is expired or if the environment variable NVIDIA_LICENSE_FILE is not set properly. 2.2.3. cublasOperation_t  The cublasOperation_t type indicates which operation needs to be performed with the dense matrix. Its values correspond to Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_OP_N The non-transpose operation is selected. CUBLAS_OP_T The transpose operation is selected. CUBLAS_OP_C The conjugate transpose operation is selected. 2.2.4. cublasFillMode_t  The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function. Its values correspond to Fortran characters L or l (lower) and U or u (upper) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_FILL_MODE_LOWER The lower part of the matrix is filled. CUBLAS_FILL_MODE_UPPER The upper part of the matrix is filled. CUBLAS_FILL_MODE_FULL The full matrix is filled. 2.2.5. cublasDiagType_t  The type indicates whether the main diagonal of the dense matrix is unity and consequently should not be touched or modified by the function. Its values correspond to Fortran characters ‘N’ or ‘n’ (non-unit) and ‘U’ or ‘u’ (unit) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_DIAG_NON_UNIT The matrix diagonal has non-unit elements. CUBLAS_DIAG_UNIT The matrix diagonal has unit elements. 2.2.6. cublasSideMode_t  The type indicates whether the dense matrix is on the left or right side in the matrix equation solved by a particular function. Its values correspond to Fortran characters ‘L’ or ‘l’ (left) and ‘R’ or ‘r’ (right) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_SIDE_LEFT The matrix is on the left side in the equation. CUBLAS_SIDE_RIGHT The matrix is on the right side in the equation. 2.2.7. cublasPointerMode_t  The cublasPointerMode_t type indicates whether the scalar values are passed by reference on the host or device. It is important to point out that if several scalar values are present in the function call, all of them must conform to the same single pointer mode. The pointer mode can be set and retrieved using cublasSetPointerMode() and cublasGetPointerMode() routines, respectively. Value Meaning CUBLAS_POINTER_MODE_HOST The scalars are passed by reference on the host. CUBLAS_POINTER_MODE_DEVICE The scalars are passed by reference on the device. 2.2.8. cublasAtomicsMode_t  The type indicates whether cuBLAS routines which has an alternate implementation using atomics can be used. The atomics mode can be set and queried using cublasSetAtomicsMode() and cublasGetAtomicsMode() and routines, respectively. Value Meaning CUBLAS_ATOMICS_NOT_ALLOWED The usage of atomics is not allowed. CUBLAS_ATOMICS_ALLOWED The usage of atomics is allowed. 2.2.9. cublasGemmAlgo_t  cublasGemmAlgo_t type is an enumerant to specify the algorithm for matrix-matrix multiplication on GPU architectures up to sm_75 . On sm_80 and newer GPU architectures, this enumarant has no effect. cuBLAS has the following algorithm options: Value Meaning CUBLAS_GEMM_DEFAULT Apply Heuristics to select the GEMM algorithm CUBLAS_GEMM_ALGO0 to CUBLAS_GEMM_ALGO23 Explicitly choose an Algorithm [0,23]. Note: Doesn’t have effect on NVIDIA Ampere architecture GPUs and newer. CUBLAS_GEMM_DEFAULT_TENSOR_OP [DEPRECATED] This mode is deprecated and will be removed in a future release. Apply Heuristics to select the GEMM algorithm, while allowing use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). CUBLAS_GEMM_ALGO0_TENSOR_OP to CUBLAS_GEMM_ALGO15_TENSOR_OP [DEPRECATED] Those values are deprecated and will be removed in a future release. Explicitly choose a Tensor core GEMM Algorithm [0,15]. Allows use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). Note: Doesn’t have effect on NVIDIA Ampere architecture GPUs and newer. 2.2.10. cublasMath_t  cublasMath_t enumerate type is used in cublasSetMathMode() to choose compute precision modes as defined in the following table. Since this setting does not directly control the use of Tensor Cores, the mode CUBLAS_TENSOR_OP_MATH is being deprecated, and will be removed in a future release. Value Meaning CUBLAS_DEFAULT_MATH This is the default and highest-performance mode that uses compute and intermediate storage precisions with at least the same number of mantissa and exponent bits as requested. Tensor Cores will be used whenever possible. CUBLAS_PEDANTIC_MATH This mode uses the prescribed precision and standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging. This mode might not be as performant as the other modes. CUBLAS_TF32_TENSOR_OP_MATH Enable acceleration of single-precision routines using TF32 tensor cores. CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION Forces any reductions during matrix multiplications to use the accumulator type (that is, compute type) and not the output type in case of mixed precision routines where output type precision is less than the compute type precision. This is a flag that can be set (using a bitwise or operation) alongside any of the other values. CUBLAS_TENSOR_OP_MATH [DEPRECATED] This mode is deprecated and will be removed in a future release. Allows the library to use Tensor Core operations whenever possible. For single precision GEMM routines cuBLAS will use the CUBLAS_COMPUTE_32F_FAST_16F compute type. 2.2.11. cublasComputeType_t  cublasComputeType_t enumerate type is used in cublasGemmEx() and cublasLtMatmul() (including all batched and strided batched variants) to choose compute precision modes as defined below. Value Meaning CUBLAS_COMPUTE_16F This is the default and highest-performance mode for 16-bit half precision floating point and all compute and intermediate storage precisions with at least 16-bit half precision. Tensor Cores will be used whenever possible. CUBLAS_COMPUTE_16F_PEDANTIC This mode uses 16-bit half precision floating point standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging. This mode might not be as performant as the other modes since it disables use of tensor cores. CUBLAS_COMPUTE_32F This is the default 32-bit single precision floating point and uses compute and intermediate storage precisions of at least 32-bits. CUBLAS_COMPUTE_32F_PEDANTIC Uses 32-bit single precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M). CUBLAS_COMPUTE_32F_FAST_16F Allows the library to use Tensor Cores with automatic down-conversion and 16-bit half-precision compute for 32-bit input and output matrices. CUBLAS_COMPUTE_32F_FAST_16BF Allows the library to use Tensor Cores with automatic down-convesion and bfloat16 compute for 32-bit input and output matrices. See Alternate Floating Point section for more details on bfloat16. CUBLAS_COMPUTE_32F_FAST_TF32 Allows the library to use Tensor Cores with TF32 compute for 32-bit input and output matrices. See Alternate Floating Point section for more details on TF32 compute. CUBLAS_COMPUTE_64F This is the default 64-bit double precision floating point and uses compute and intermediate storage precisions of at least 64-bits. CUBLAS_COMPUTE_64F_PEDANTIC Uses 64-bit double precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M). CUBLAS_COMPUTE_32I This is the default 32-bit integer mode and uses compute and intermediate storage precisions of at least 32-bits. CUBLAS_COMPUTE_32I_PEDANTIC Uses 32-bit integer arithmetic for all phases of calculations. Note Setting the environment variable NVIDIA_TF32_OVERRIDE = 0 will override any defaults or programmatic configuration of NVIDIA libraries, and consequently, cuBLAS will not accelerate FP32 computations with TF32 tensor cores. 2.3. CUDA Datatypes Reference  The chapter describes types shared by multiple CUDA Libraries and defined in the header file library_types.h . 2.3.1. cudaDataType_t  The cudaDataType_t type is an enumerant to specify the data precision. It is used when the data reference does not carry the type itself (e.g void *) For example, it is used in the routine cublasSgemmEx() . Value Meaning CUDA_R_16F the data type is a 16-bit real half precision floating-point CUDA_C_16F the data type is a 32-bit structure comprised of two half precision floating-points representing a complex number. CUDA_R_16BF the data type is a 16-bit real bfloat16 floating-point CUDA_C_16BF the data type is a 32-bit structure comprised of two bfloat16 floating-points representing a complex number. CUDA_R_32F the data type is a 32-bit real single precision floating-point CUDA_C_32F the data type is a 64-bit structure comprised of two single precision floating-points representing a complex number. CUDA_R_64F the data type is a 64-bit real double precision floating-point CUDA_C_64F the data type is a 128-bit structure comprised of two double precision floating-points representing a complex number. CUDA_R_8I the data type is a 8-bit real signed integer CUDA_C_8I the data type is a 16-bit structure comprised of two 8-bit signed integers representing a complex number. CUDA_R_8U the data type is a 8-bit real unsigned integer CUDA_C_8U the data type is a 16-bit structure comprised of two 8-bit unsigned integers representing a complex number. CUDA_R_32I the data type is a 32-bit real signed integer CUDA_C_32I the data type is a 64-bit structure comprised of two 32-bit signed integers representing a complex number. CUDA_R_8F_E4M3 the data type is an 8-bit real floating point in E4M3 format CUDA_R_8F_E5M2 the data type is an 8-bit real floating point in E5M2 format 2.3.2. libraryPropertyType_t  The libraryPropertyType_t is used as a parameter to specify which property is requested when using the routine cublasGetProperty() Value Meaning MAJOR_VERSION enumerant to query the major version MINOR_VERSION enumerant to query the minor version PATCH_LEVEL number to identify the patch level 2.4. cuBLAS Helper Function Reference  2.4.1. cublasCreate()  cublasStatus_t cublasCreate ( cublasHandle_t * handle ) This function initializes the cuBLAS library and creates a handle to an opaque structure holding the cuBLAS library context. It allocates hardware resources on the host and device and must be called prior to making any other cuBLAS library calls. The cuBLAS library context is tied to the current CUDA device. To use the library on multiple devices, one cuBLAS handle needs to be created for each device. Furthermore, for a given device, multiple cuBLAS handles with different configurations can be created. Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. For multi-threaded applications that use the same device from different threads, the recommended programming model is to create one cuBLAS handle per thread and use that cuBLAS handle for the entire life of the thread. Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_NOT_INITIALIZED the CUDA™ Runtime initialization failed CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_INVALID_VALUE handle == NULL 2.4.2. cublasDestroy()  cublasStatus_t cublasDestroy ( cublasHandle_t handle ) This function releases hardware resources used by the cuBLAS library. This function is usually the last call with a particular handle to the cuBLAS library. Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.3. cublasGetVersion()  cublasStatus_t cublasGetVersion ( cublasHandle_t handle , int * version ) This function returns the version number of the cuBLAS library. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the provided storage for library version number is not initialized (NULL) Note This function can be safely called with the handle set to NULL.  This allows users to get the version of the library without a handle.  Another way to do this is with cublasGetProperty() . 2.4.4. cublasGetProperty()  cublasStatus_t cublasGetProperty ( libraryPropertyType type , int * value ) This function returns the value of the requested property in memory pointed to by value. Refer to libraryPropertyType for supported types. Return Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully CUBLAS_STATUS_INVALID_VALUE Invalid type value If invalid type value or value == NULL 2.4.5. cublasGetStatusName()  const char * cublasGetStatusName ( cublasStatus_t status ) This function returns the string representation of a given status. Return Value Meaning NULL-terminated string The string representation of the status 2.4.6. cublasGetStatusString()  const char * cublasGetStatusString ( cublasStatus_t status ) This function returns the description string for a given status. Return Value Meaning NULL-terminated string The description of the status 2.4.7. cublasSetStream()  cublasStatus_t cublasSetStream ( cublasHandle_t handle , cudaStream_t streamId ) This function sets the cuBLAS library stream, which will be used to execute all subsequent calls to the cuBLAS library functions. If the cuBLAS library stream is not set, all kernels use the default NULL stream. In particular, this routine can be used to change the stream between kernel launches and then to reset the cuBLAS library stream back to NULL . Additionally this function unconditionally resets the cuBLAS library workspace back to the default workspace pool (see cublasSetWorkspace() ). Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.8. cublasSetWorkspace()  cublasStatus_t cublasSetWorkspace ( cublasHandle_t handle , void * workspace , size_t workspaceSizeInBytes ) This function sets the cuBLAS library workspace to a user-owned device buffer, which will be used to execute all subsequent calls to the cuBLAS library functions (on the currently set stream). If the cuBLAS library workspace is not set, all kernels will use the default workspace pool allocated during the cuBLAS context creation. In particular, this routine can be used to change the workspace between kernel launches. The workspace pointer has to be aligned to at least 256 bytes, otherwise CUBLAS_STATUS_INVALID_VALUE error is returned. The cublasSetStream() function unconditionally resets the cuBLAS library workspace back to the default workspace pool. Calling this function, including with workspaceSizeInBytes equal to 0, will prevent the cuBLAS library from utilizing the default workspace. Too small workspaceSizeInBytes may cause some routines to fail with CUBLAS_STATUS_ALLOC_FAILED error returned or cause large regressions in performance. Workspace size equal to or larger than 16KiB is enough to prevent CUBLAS_STATUS_ALLOC_FAILED error, while a larger workspace can provide performance benefits for some routines. Note If the stream set by cublasSetStream() is cudaStreamPerThread and there are multiple threads using the same cuBLAS library handle, then users must manually manage synchronization to avoid possible race conditions in the user provided workspace. Alternatively, users may rely on the default workspace pool which safely guards against race conditions. The table below shows the recommended size of user-provided workspace.\nThis is based on the cuBLAS default workspace pool size which is GPU architecture dependent. GPU Architecture Recommended workspace size NVIDIA Hopper Architecture 32 MiB Other 4 MiB The possible error values returned by this function and their meanings are listed below. Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the workspace pointer wasn’t aligned to at least 256 bytes 2.4.9. cublasGetStream()  cublasStatus_t cublasGetStream ( cublasHandle_t handle , cudaStream_t * streamId ) This function gets the cuBLAS library stream, which is being used to execute all calls to the cuBLAS library functions. If the cuBLAS library stream is not set, all kernels use the default NULL stream. Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was returned successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE streamId == NULL 2.4.10. cublasGetPointerMode()  cublasStatus_t cublasGetPointerMode ( cublasHandle_t handle , cublasPointerMode_t * mode ) This function obtains the pointer mode used by the cuBLAS library. Please see the section on the cublasPointerMode_t type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was obtained successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode == NULL 2.4.11. cublasSetPointerMode()  cublasStatus_t cublasSetPointerMode ( cublasHandle_t handle , cublasPointerMode_t mode ) This function sets the pointer mode used by the cuBLAS library. The default is for the values to be passed by reference on the host. Please see the section on the cublasPointerMode_t type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode is not CUBLAS_POINTER_MODE_HOST or CUBLAS_POINTER_MODE_DEVICE 2.4.12. cublasSetVector()  cublasStatus_t cublasSetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface . This function copies n elements from a vector x in host memory space to a vector y in GPU memory space. Elements in both vectors are assumed to have a size of elemSize bytes. The storage spacing between consecutive elements is given by incx for the source vector x and by incy for the destination vector y . Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that matrix. Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.13. cublasGetVector()  cublasStatus_t cublasGetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface . This function copies n elements from a vector x in GPU memory space to a vector y in host memory space. Elements in both vectors are assumed to have a size of elemSize bytes. The storage spacing between consecutive elements is given by incx for the source vector and incy for the destination vector y . Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that matrix. Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.14. cublasSetMatrix()  cublasStatus_t cublasSetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ) This function supports the 64-bit Integer Interface . This function copies a tile of rows x cols elements from a matrix A in host memory space to a matrix B in GPU memory space. It is assumed that each element requires storage of elemSize bytes and that both matrices are stored in column-major format, with the leading dimension of the source matrix A and destination matrix B given in lda and ldb , respectively. The leading dimension indicates the number of rows of the allocated matrix, even if only a submatrix of it is being used. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.15. cublasGetMatrix()  cublasStatus_t cublasGetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ) This function supports the 64-bit Integer Interface . This function copies a tile of rows x cols elements from a matrix A in GPU memory space to a matrix B in host memory space. It is assumed that each element requires storage of elemSize bytes and that both matrices are stored in column-major format, with the leading dimension of the source matrix A and destination matrix B given in lda and ldb , respectively. The leading dimension indicates the number of rows of the allocated matrix, even if only a submatrix of it is being used. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.16. cublasSetVectorAsync()  cublasStatus_t cublasSetVectorAsync ( int n , int elemSize , const void * hostPtr , int incx , void * devicePtr , int incy , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasSetVector() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.17. cublasGetVectorAsync()  cublasStatus_t cublasGetVectorAsync ( int n , int elemSize , const void * devicePtr , int incx , void * hostPtr , int incy , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasGetVector() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.18. cublasSetMatrixAsync()  cublasStatus_t cublasSetMatrixAsync ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasSetMatrix() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.19. cublasGetMatrixAsync()  cublasStatus_t cublasGetMatrixAsync ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasGetMatrix() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.20. cublasSetAtomicsMode()  cublasStatus_t cublasSetAtomicsMode ( cublasHandlet handle , cublasAtomicsMode_t mode ) Some routines like cublas<t>symv and cublas<t>hemv have an alternate implementation that use atomics to cumulate results. This implementation is generally significantly faster but can generate results that are not strictly identical from one run to the others. Mathematically, those different results are not significant but when debugging those differences can be prejudicial. This function allows or disallows the usage of atomics in the cuBLAS library for all routines which have an alternate implementation. When not explicitly specified in the documentation of any cuBLAS routine, it means that this routine does not have an alternate implementation that use atomics. When atomics mode is disabled, each cuBLAS routine should produce the same results from one run to the other when called with identical parameters on the same Hardware. The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED . Please see the section on the type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.21. cublasGetAtomicsMode()  cublasStatus_t cublasGetAtomicsMode ( cublasHandle_t handle , cublasAtomicsMode_t * mode ) This function queries the atomic mode of a specific cuBLAS context. The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED . Please see the section on the type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was queried successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the argument mode is a NULL pointer 2.4.22. cublasSetMathMode()  cublasStatus_t cublasSetMathMode ( cublasHandle_t handle , cublasMath_t mode ) The cublasSetMathMode() function enables you to choose the compute precision modes as defined by cublasMath_t . Users are allowed to set the compute precision mode as a logical combination of them (except the deprecated CUBLAS_TENSOR_OP_MATH ). For example, cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION) . Please note that the default math mode is CUBLAS_DEFAULT_MATH . For matrix and compute precisions allowed for cublasGemmEx() and cublasLtMatmul() APIs and their strided variants please refer to: cublasGemmEx() , cublasGemmBatchedEx() , cublasGemmStridedBatchedEx() , and cublasLtMatmul() . Return Value Meaning CUBLAS_STATUS_SUCCESS the math mode was set successfully. CUBLAS_STATUS_INVALID_VALUE an invalid value for mode was specified. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.23. cublasGetMathMode()  cublasStatus_t cublasGetMathMode ( cublasHandle_t handle , cublasMath_t * mode ) This function returns the math mode used by the library routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the math type was returned successfully. CUBLAS_STATUS_INVALID_VALUE if mode is NULL. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.24. cublasSetSmCountTarget()  cublasStatus_t cublasSetSmCountTarget ( cublasHandle_t handle , int smCountTarget ) The cublasSetSmCountTarget() function allows overriding the number of multiprocessors available to the library during kernels execution. This option can be used to improve the library performance when cuBLAS routines are known to run concurrently with other work on different CUDA streams. E.g. a NVIDIA A100 GPU has 108 SM and there is a concurrent kenrel running with grid size of 8, one can use cublasSetSmCountTarget() with value 100 to override the library heuristics to optimize for running on 100 multiprocessors. When set to 0 the library returns to its default behavior. The input value should not exceed the device’s multiprocessor count, which can be obtained using cudaDeviceGetAttribute . Negative values are not accepted. The user must ensure thread safety when modifying the library handle with this routine similar to when using cublasSetStream() , etc. Return Value Meaning CUBLAS_STATUS_SUCCESS SM count target was set successfully. CUBLAS_STATUS_INVALID_VALUE the value of smCountTarget outside of the allowed range. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.25. cublasGetSmCountTarget()  cublasStatus_t cublasGetSmCountTarget ( cublasHandle_t handle , int * smCountTarget ) This function obtains the value previously programmed to the library handle. Return Value Meaning CUBLAS_STATUS_SUCCESS SM count target was set successfully. CUBLAS_STATUS_INVALID_VALUE smCountTarget is NULL. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.26. cublasLoggerConfigure()  cublasStatus_t cublasLoggerConfigure ( int logIsOn , int logToStdOut , int logToStdErr , const char * logFileName ) This function configures logging during runtime. Besides this type of configuration, it is possible to configure logging with special environment variables which will be checked by libcublas: CUBLAS_LOGINFO_DBG - Setup env. variable to “1” means turn on logging (by default logging is off). CUBLAS_LOGDEST_DBG - Setup env. variable encodes how to log. “stdout”, “stderr” means to output log messages to stdout or stderr, respectively. In the other case, its specifies “filename” of file. Parameters logIsOn Input . Turn on/off logging completely. By default is off, but is turned on by calling cublasSetLoggerCallback() to user defined callback function. logToStdOut Input . Turn on/off logging to standard output I/O stream. By default is off. logToStdErr Input . Turn on/off logging to standard error I/O stream. By default is off. logFileName Input . Turn on/off logging to file in filesystem specified by it’s name. cublasLoggerConfigure() copies the content of logFileName . You should provide null pointer if you are not interested in this type of logging. Returns CUBLAS_STATUS_SUCCESS Success. 2.4.27. cublasGetLoggerCallback()  cublasStatus_t cublasGetLoggerCallback ( cublasLogCallback * userCallback ) This function retrieves function pointer to previously installed custom user defined callback function via cublasSetLoggerCallback() or zero otherwise. Parameters userCallback Output . Pointer to user defined callback function. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE userCallback is NULL 2.4.28. cublasSetLoggerCallback()  cublasStatus_t cublasSetLoggerCallback ( cublasLogCallback userCallback ) This function installs a custom user defined callback function via cublas C public API. Parameters userCallback Input . Pointer to user defined callback function. Returns CUBLAS_STATUS_SUCCESS Success. 2.5. cuBLAS Level-1 Function Reference  In this chapter we describe the Level-1 Basic Linear Algebra Subprograms (BLAS1) functions that perform scalar and vector based operations. We will use abbreviations < type > for type and < t > for the corresponding short type to make a more concise and clear presentation of the implemented functions. Unless otherwise specified < type > and < t > have the following meanings: <type> <t> Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision When the parameters and returned values of the function differ, which sometimes happens for complex input, the <t> can also have the following meanings Sc , Cs , Dz and Zd . The abbreviation \\(\\mathbf{Re}(\\cdot)\\) and \\(\\mathbf{Im}(\\cdot)\\) will stand for the real and imaginary part of a number, respectively. Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. Also, the \\(\\bar{\\alpha}\\) will denote the complex conjugate of \\(\\alpha\\) . In general throughout the documentation, the lower case Greek symbols \\(\\alpha\\) and \\(\\beta\\) will denote scalars, lower case English letters in bold type \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) will denote vectors and capital English letters \\(A\\) , \\(B\\) and \\(C\\) will denote matrices. 2.5.1. cublasI<t>amax()  cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamax ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamax ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamax ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface . This function finds the (smallest) index of the element of the maximum magnitude. Hence, the result is the first \\(i\\) such that \\(\\left| \\mathbf{Im}\\left( {x\\lbrack j\\rbrack} \\right) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j\\rbrack} \\right) \\right|\\) is maximum for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{ incx}\\) . Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: isamax , idamax , icamax , izamax 2.5.2. cublasI<t>amin()  cublasStatus_t cublasIsamin ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamin ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamin ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamin ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface . This function finds the (smallest) index of the element of the minimum magnitude. Hence, the result is the first \\(i\\) such that \\(\\left| \\mathbf{Im}\\left( {x\\lbrack j\\rbrack} \\right) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j\\rbrack} \\right) \\right|\\) is minimum for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: isamin 2.5.3. cublas<t>asum()  cublasStatus_t cublasSasum ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDasum ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScasum ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDzasum ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface . This function computes the sum of the absolute values of the elements of vector x . Hence, the result is \\(\\left. \\sum_{i = 1}^{n} \\middle| \\mathbf{Im}\\left( {x\\lbrack j\\rbrack} \\right) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j\\rbrack} \\right) \\right|\\) where \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) . Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0.0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: sasum , dasum , scasum , dzasum 2.5.4. cublas<t>axpy()  cublasStatus_t cublasSaxpy ( cublasHandle_t handle , int n , const float * alpha , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDaxpy ( cublasHandle_t handle , int n , const double * alpha , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCaxpy ( cublasHandle_t handle , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZaxpy ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function multiplies the vector x by the scalar \\(\\alpha\\) and adds it to the vector y overwriting the latest vector with the result. Hence, the performed operation is \\(\\mathbf{y}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack k\\rbrack + \\mathbf{y}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. alpha host or device input <type> scalar used for multiplication. n input number of elements in the vector x and y . x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.5.5. cublas<t>copy()  cublasStatus_t cublasScopy ( cublasHandle_t handle , int n , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDcopy ( cublasHandle_t handle , int n , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCcopy ( cublasHandle_t handle , int n , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZcopy ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function copies the vector x into the vector y . Hence, the performed operation is \\(\\mathbf{y}\\lbrack j\\rbrack = \\mathbf{x}\\lbrack k\\rbrack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x and y . x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device output <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: scopy , dcopy , ccopy , zcopy 2.5.6. cublas<t>dot()  cublasStatus_t cublasSdot ( cublasHandle_t handle , int n , const float * x , int incx , const float * y , int incy , float * result ) cublasStatus_t cublasDdot ( cublasHandle_t handle , int n , const double * x , int incx , const double * y , int incy , double * result ) cublasStatus_t cublasCdotu ( cublasHandle_t handle , int n , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * result ) cublasStatus_t cublasCdotc ( cublasHandle_t handle , int n , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * result ) cublasStatus_t cublasZdotu ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * result ) cublasStatus_t cublasZdotc ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * result ) This function supports the 64-bit Integer Interface . This function computes the dot product of vectors x and y . Hence, the result is \\(\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack k\\rbrack \\times \\mathbf{y}\\lbrack j\\rbrack} \\right)\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . result host or device output the resulting dot product, which is 0.0 if n<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sdot , ddot , cdotu , cdotc , zdotu , zdotc 2.5.7. cublas<t>nrm2()  cublasStatus_t cublasSnrm2 ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDnrm2 ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScnrm2 ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDznrm2 ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface . This function computes the Euclidean norm of the vector x . The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \\(\\sqrt{\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack j\\rbrack \\times \\mathbf{x}\\lbrack j\\rbrack} \\right)}\\) where \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) in exact arithmetic. Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with n elements. incx input stride between consecutive elements of x . result host or device output the resulting norm, which is 0.0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: snrm2 , dnrm2 , scnrm2 , dznrm2 2.5.8. cublas<t>rot()  cublasStatus_t cublasSrot ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * c , const float * s ) cublasStatus_t cublasDrot ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * c , const double * s ) cublasStatus_t cublasCrot ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy , const float * c , const cuComplex * s ) cublasStatus_t cublasCsrot ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy , const float * c , const float * s ) cublasStatus_t cublasZrot ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy , const double * c , const cuDoubleComplex * s ) cublasStatus_t cublasZdrot ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy , const double * c , const double * s ) This function supports the 64-bit Integer Interface . This function applies Givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \\(G = \\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\) to vectors x and y . Hence, the result is \\(\\mathbf{x}\\lbrack k\\rbrack = c \\times \\mathbf{x}\\lbrack k\\rbrack + s \\times \\mathbf{y}\\lbrack j\\rbrack\\) and \\(\\mathbf{y}\\lbrack j\\rbrack = - s \\times \\mathbf{x}\\lbrack k\\rbrack + c \\times \\mathbf{y}\\lbrack j\\rbrack\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . c host or device input cosine element of the rotation matrix. s host or device input sine element of the rotation matrix. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.5.9. cublas<t>rotg()  cublasStatus_t cublasSrotg ( cublasHandle_t handle , float * a , float * b , float * c , float * s ) cublasStatus_t cublasDrotg ( cublasHandle_t handle , double * a , double * b , double * c , double * s ) cublasStatus_t cublasCrotg ( cublasHandle_t handle , cuComplex * a , cuComplex * b , float * c , cuComplex * s ) cublasStatus_t cublasZrotg ( cublasHandle_t handle , cuDoubleComplex * a , cuDoubleComplex * b , double * c , cuDoubleComplex * s ) This function supports the 64-bit Integer Interface . This function constructs the Givens rotation matrix \\(G = \\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\) that zeros out the second entry of a \\(2 \\times 1\\) vector \\(\\left( {a,b} \\right)^{T}\\) . Then, for real numbers we can write \\(\\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\begin{pmatrix}\na \\\\\nb \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nr \\\\\n0 \\\\\n\\end{pmatrix}\\) where \\(c^{2} + s^{2} = 1\\) and \\(r = a^{2} + b^{2}\\) . The parameters \\(a\\) and \\(b\\) are overwritten with \\(r\\) and \\(z\\) , respectively. The value of \\(z\\) is such that \\(c\\) and \\(s\\) may be recovered using the following rules: \\(\\left( {c,s} \\right) = \\begin{cases}\n\\left( {\\sqrt{1 - z^{2}},z} \\right) & {\\text{ if }\\left| z \\middle| < 1 \\right.} \\\\\n\\left( {0.0,1.0} \\right) & {\\text{ if }\\left| z \\middle| = 1 \\right.} \\\\\n\\left( 1/z,\\sqrt{1 - z^{2}} \\right) & {\\text{ if }\\left| z \\middle| > 1 \\right.} \\\\\n\\end{cases}\\) For complex numbers we can write \\(\\begin{pmatrix}\nc & s \\\\\n{- \\bar{s}} & c \\\\\n\\end{pmatrix}\\begin{pmatrix}\na \\\\\nb \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nr \\\\\n0 \\\\\n\\end{pmatrix}\\) where \\(c^{2} + \\left( {\\bar{s} \\times s} \\right) = 1\\) and \\(r = \\frac{a}{|a|} \\times \\parallel \\left( {a,b} \\right)^{T} \\parallel_{2}\\) with \\(\\parallel \\left( {a,b} \\right)^{T} \\parallel_{2} = \\sqrt{\\left| a|^{2} + \\middle| b|^{2} \\right.}\\) for \\(a \\neq 0\\) and \\(r = b\\) for \\(a = 0\\) . Finally, the parameter \\(a\\) is overwritten with \\(r\\) on exit. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. a host or device in/out <type> scalar that is overwritten with \\(r\\) . b host or device in/out <type> scalar that is overwritten with \\(z\\) . c host or device output cosine element of the rotation matrix. s host or device output sine element of the rotation matrix. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotg , drotg , crotg , zrotg 2.5.10. cublas<t>rotm()  cublasStatus_t cublasSrotm ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * param ) cublasStatus_t cublasDrotm ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * param ) This function supports the 64-bit Integer Interface . This function applies the modified Givens transformation \\(H = \\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) to vectors x and y . Hence, the result is \\(\\mathbf{x}\\lbrack k\\rbrack = h_{11} \\times \\mathbf{x}\\lbrack k\\rbrack + h_{12} \\times \\mathbf{y}\\lbrack j\\rbrack\\) and \\(\\mathbf{y}\\lbrack j\\rbrack = h_{21} \\times \\mathbf{x}\\lbrack k\\rbrack + h_{22} \\times \\mathbf{y}\\lbrack j\\rbrack\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. The elements , , and of matrix \\(H\\) are stored in param[1] , param[2] , param[3] and param[4] , respectively. The flag=param[0] defines the following predefined values for the matrix \\(H\\) entries flag=-1.0 flag= 0.0 flag= 1.0 flag=-2.0 \\(\\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & h_{12} \\\\\nh_{21} & {1.0} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\nh_{11} & {1.0} \\\\\n{- 1.0} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & {0.0} \\\\\n{0.0} & {1.0} \\\\\n\\end{pmatrix}\\) Notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . param host or device input <type> vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\(H\\) . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotm , drotm 2.5.11. cublas<t>rotmg()  cublasStatus_t cublasSrotmg ( cublasHandle_t handle , float * d1 , float * d2 , float * x1 , const float * y1 , float * param ) cublasStatus_t cublasDrotmg ( cublasHandle_t handle , double * d1 , double * d2 , double * x1 , const double * y1 , double * param ) This function supports the 64-bit Integer Interface . This function constructs the modified Givens transformation \\(H = \\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) that zeros out the second entry of a \\(2 \\times 1\\) vector \\(\\left( {\\sqrt{d1}*x1,\\sqrt{d2}*y1} \\right)^{T}\\) . The flag=param[0] defines the following predefined values for the matrix \\(H\\) entries flag=-1.0 flag= 0.0 flag= 1.0 flag=-2.0 \\(\\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & h_{12} \\\\\nh_{21} & {1.0} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\nh_{11} & {1.0} \\\\\n{- 1.0} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & {0.0} \\\\\n{0.0} & {1.0} \\\\\n\\end{pmatrix}\\) Notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. d1 host or device in/out <type> scalar that is overwritten on exit. d2 host or device in/out <type> scalar that is overwritten on exit. x1 host or device in/out <type> scalar that is overwritten on exit. y1 host or device input <type> scalar. param host or device output <type> vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\(H\\) . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotmg , drotmg 2.5.12. cublas<t>scal()  cublasStatus_t cublasSscal ( cublasHandle_t handle , int n , const float * alpha , float * x , int incx ) cublasStatus_t cublasDscal ( cublasHandle_t handle , int n , const double * alpha , double * x , int incx ) cublasStatus_t cublasCscal ( cublasHandle_t handle , int n , const cuComplex * alpha , cuComplex * x , int incx ) cublasStatus_t cublasCsscal ( cublasHandle_t handle , int n , const float * alpha , cuComplex * x , int incx ) cublasStatus_t cublasZscal ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , cuDoubleComplex * x , int incx ) cublasStatus_t cublasZdscal ( cublasHandle_t handle , int n , const double * alpha , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function scales the vector x by the scalar \\(\\alpha\\) and overwrites it with the result. Hence, the performed operation is \\(\\mathbf{x}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. alpha host or device input <type> scalar used for multiplication. n input number of elements in the vector x . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. :class: table-no-stripes  Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 2.5.13. cublas<t>swap()  cublasStatus_t cublasSswap ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy ) cublasStatus_t cublasDswap ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy ) cublasStatus_t cublasCswap ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZswap ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function interchanges the elements of vector x and y . Hence, the performed operation is \\(\\left. \\mathbf{y}\\lbrack j\\rbrack\\Leftrightarrow\\mathbf{x}\\lbrack k\\rbrack \\right.\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x and y . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sswap , dswap , cswap , zswap 2.6. cuBLAS Level-2 Function Reference  In this chapter we describe the Level-2 Basic Linear Algebra Subprograms (BLAS2) functions that perform matrix-vector operations. 2.6.1. cublas<t>gbmv()  cublasStatus_t cublasSgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the banded matrix-vector multiplication \\(\\mathbf{y} = \\alpha\\text{ op}(A)\\mathbf{x} + \\beta\\mathbf{y}\\) where \\(A\\) is a banded matrix with \\(kl\\) subdiagonals and \\(ku\\) superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Also, for matrix \\(A\\) \\(\\text{ op}(A) = \\begin{cases}\nA & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_N}$} \\\\\nA^{T} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_T}$} \\\\\nA^{H} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_C}$} \\\\\n\\end{cases}\\) The banded matrix \\(A\\) is stored column by column, with the main diagonal stored in row \\(ku + 1\\) (starting in first position), the first superdiagonal stored in row \\(ku\\) (starting in second position), the first subdiagonal stored in row \\(ku + 2\\) (starting in first position), etc. So that in general, the element \\(A\\left( {i,j} \\right)\\) is stored in the memory location A(ku+1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\left\\lbrack {\\max\\left( {1,j - ku} \\right),\\min\\left( {m,j + kl} \\right)} \\right\\rbrack\\) . Also, the elements in the array \\(A\\) that do not conceptually correspond to the elements in the banded matrix (the top left \\(ku \\times ku\\) and bottom right \\(kl \\times kl\\) triangles) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A ) that is non- or (conj.) transpose. m input number of rows of matrix A . n input number of columns of matrix A . kl input number of subdiagonals of matrix A . ku input number of superdiagonals of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda>=kl+ku+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements if transa == CUBLAS_OP_N and m elements otherwise. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta == 0 then y does not have to be a valid input. y device in/out <type> vector with m elements if transa == CUBLAS_OP_N and n elements otherwise. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m, n, kl, ku < 0 or if lda < (kl+ku+1) or if incx, incy == 0 or if trans != CUBLAS_OP_N, CUBLAS_OP_T, CUBLAS_OP_C or alpha, beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgbmv , dgbmv , cgbmv , zgbmv 2.6.2. cublas<t>gemv()  cublasStatus_t cublasSgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication \\(\\textbf{y} = \\alpha\\text{ op}(A)\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(m \\times n\\) matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Also, for matrix \\(A\\) \\(\\text{ op}(A) = \\begin{cases}\nA & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_N}$} \\\\\nA^{T} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_T}$} \\\\\nA^{H} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_C}$} \\\\\n\\end{cases}\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A ) that is non- or (conj.) transpose. m input number of rows of matrix A . n input number of columns of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda >= max(1,m) . Before entry, the leading m by n part of the array A must contain the matrix of coefficients. Unchanged on exit. lda input leading dimension of two-dimensional array used to store matrix A . lda must be at least max(1,m) . x device input <type> vector at least (1+(n-1)*abs(incx)) elements if transa==CUBLAS_OP_N and at least (1+(m-1)*abs(incx)) elements otherwise. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector at least (1+(m-1)*abs(incy)) elements if transa==CUBLAS_OP_N and at least (1+(n-1)*abs(incy)) elements otherwise. incy input stride between consecutive elements of y The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 or incx,incy=0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemv , dgemv , cgemv , zgemv 2.6.3. cublas<t>ger()  cublasStatus_t cublasSger ( cublasHandle_t handle , int m , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * A , int lda ) cublasStatus_t cublasDger ( cublasHandle_t handle , int m , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * A , int lda ) cublasStatus_t cublasCgeru ( cublasHandle_t handle , int m , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasCgerc ( cublasHandle_t handle , int m , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZgeru ( cublasHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) cublasStatus_t cublasZgerc ( cublasHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the rank-1 update \\(A = \\begin{cases}\n{\\alpha\\mathbf{xy}^{T} + A} & \\text{if ger(),geru() is called} \\\\\n{\\alpha\\mathbf{xy}^{H} + A} & \\text{if gerc() is called} \\\\\n\\end{cases}\\) where \\(A\\) is a \\(m \\times n\\) matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. m input number of rows of matrix A . n input number of columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with m elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . A device in/out <type> array of dimension lda x n with lda >= max(1,m) . lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 if incx = 0 or incy = 0 or if alpha == NULL or lda < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sger , dger , cgeru , cgerc , zgeru , zgerc 2.6.4. cublas<t>sbmv()  cublasStatus_t cublasSsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric banded matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric banded matrix with \\(k\\) subdiagonals and superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the symmetric banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1, the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the symmetric banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k),j\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda >= k+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if alpha == NULL or beta == NULL or lda < (1 + k ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssbmv , dsbmv 2.6.5. cublas<t>spmv()  cublasStatus_t cublasSspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * AP , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * AP , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric packed matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\(A\\) . alpha host or device input <type> scalar used for multiplication. AP device input <type> array with \\(A\\) stored in packed format. x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device input <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sspmv , dspmv 2.6.6. cublas<t>spr()  cublasStatus_t cublasSspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * AP ) cublasStatus_t cublasDspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * AP ) This function supports the 64-bit Integer Interface . This function performs the packed symmetric rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{T} + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\(A\\) . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . AP device in/out <type> array with \\(A\\) stored in packed format. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sspr , dspr 2.6.7. cublas<t>spr2()  cublasStatus_t cublasSspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * AP ) cublasStatus_t cublasDspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * AP ) This function supports the 64-bit Integer Interface . This function performs the packed symmetric rank-2 update \\(A = \\alpha\\left( {\\textbf{x}\\textbf{y}^{T} + \\textbf{y}\\textbf{x}^{T}} \\right) + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\(A\\) . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . AP device in/out <type> array with \\(A\\) stored in packed format. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sspr2 , dspr2 2.6.8. cublas<t>symv()  cublasStatus_t cublasSsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , /* host or device pointer */ const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric matrix-vector multiplication. \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in lower or upper mode, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. This function has an alternate faster implementation using atomics that can be enabled with cublasSetAtomicsMode() . Please see the section on the function cublasSetAtomicsMode() for more details about the usage of atomics. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or lda < n CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymv , dsymv 2.6.9. cublas<t>syr()  cublasStatus_t cublasSsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * A , int lda ) cublasStatus_t cublasDsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * A , int lda ) cublasStatus_t cublasCsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{T} + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in column-major format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . A device in/out <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr , dsyr 2.6.10. cublas<t>syr2()  cublasStatus_t cublasSsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * A , int lda cublasStatus_t cublasDsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * A , int lda cublasStatus_t cublasCsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda cublasStatus_t cublasZsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda This function supports the 64-bit Integer Interface . This function performs the symmetric rank-2 update \\(A = \\alpha\\left( {\\textbf{x}\\textbf{y}^{T} + \\textbf{y}\\textbf{x}^{T}} \\right) + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . A device in/out <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if alpha == NULL or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr2 , dsyr2 2.6.11. cublas<t>tbmv()  cublasStatus_t cublasStbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular banded matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular banded matrix, and \\(\\mathbf{x}\\) is a vector. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k,j)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix . A device input <type> array of dimension lda x n , with lda>=k+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < (1 + k ) CUBLAS_STATUS_ALLOC_FAILED the allocation of internal scratch memory failed CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stbmv , dtbmv , ctbmv , ztbmv 2.6.12. cublas<t>tbsv()  cublasStatus_t cublasStbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the triangular banded linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular banded matrix, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(\\mathbf{x}\\) overwrites the right-hand-sides \\(\\mathbf{b}\\) on exit. No test for singularity or near-singularity is included in this function. If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k,j)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . A device input <type> array of dimension lda x n , with lda >= k+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < (1 + k ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stbsv , dtbsv , ctbsv , ztbsv 2.6.13. cublas<t>tpmv()  cublasStatus_t cublasStpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * AP , float * x , int incx ) cublasStatus_t cublasDtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * AP , double * x , int incx ) cublasStatus_t cublasCtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * AP , cuComplex * x , int incx ) cublasStatus_t cublasZtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * AP , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular packed matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular matrix stored in packed format, and \\(\\mathbf{x}\\) is a vector. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(A(i,j)\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . AP device input <type> array with \\(A\\) stored in packed format. x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or if uplo != CUBLAS_FILL_MODE_UPPER, CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N, CUBLAS_OP_T, CUBLAS_OP_C or diag != CUBLAS_DIAG_UNIT, CUBLAS_DIAG_NON_UNIT CUBLAS_STATUS_ALLOC_FAILED the allocation of internal scratch memory failed CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stpmv , dtpmv , ctpmv , ztpmv 2.6.14. cublas<t>tpsv()  cublasStatus_t cublasStpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * AP , float * x , int incx ) cublasStatus_t cublasDtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * AP , double * x , int incx ) cublasStatus_t cublasCtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * AP , cuComplex * x , int incx ) cublasStatus_t cublasZtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * AP , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the packed triangular linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular matrix stored in packed format, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(\\mathbf{x}\\) overwrites the right-hand-sides \\(\\mathbf{b}\\) on exit. No test for singularity or near-singularity is included in this function. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix are unity and should not be accessed. n input number of rows and columns of matrix A . AP device input <type> array with A stored in packed format. x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stpsv , dtpsv , ctpsv , ztpsv 2.6.15. cublas<t>trmv()  cublasStatus_t cublasStrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\(\\mathbf{x}\\) is a vector. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) (that is, non- or conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . A device input <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < max(1, n ) CUBLAS_STATUS_ALLOC_FAILED the allocation of internal scratch memory failed CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strmv , dtrmv , ctrmv , ztrmv 2.6.16. cublas<t>trsv()  cublasStatus_t cublasStrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the triangular linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(\\mathbf{x}\\) overwrites the right-hand-sides \\(\\mathbf{b}\\) on exit. No test for singularity or near-singularity is included in this function. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . A device input <type> array of dimension lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsv , dtrsv , ctrsv , ztrsv 2.6.17. cublas<t>hemv()  cublasStatus_t cublasChemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in lower or upper mode, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. This function has an alternate faster implementation using atomics that can be enabled with Please see the section on the for more details about the usage of atomics Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n , with lda>=max(1,n) . The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or lda < n CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chemv , zhemv 2.6.18. cublas<t>hbmv()  cublasStatus_t cublasChbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian banded matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian banded matrix with \\(k\\) subdiagonals and superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the Hermitian banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the Hermitian banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k),j\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x n , with lda>=k+1 . The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < ( k + 1) or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chbmv , zhbmv 2.6.19. cublas<t>hpmv()  cublasStatus_t cublasChpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian packed matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. AP device input <type> array with A stored in packed format. The imaginary parts of the diagonal elements are assumed to be zero. x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or incy == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chpmv , zhpmv 2.6.20. cublas<t>her()  cublasStatus_t cublasCher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in column-major format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . A device in/out <type> array of dimensions lda x n , with lda>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if lda < max(1, n ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher , zher 2.6.21. cublas<t>her2()  cublasStatus_t cublasCher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank-2 update \\(A = \\alpha\\textbf{x}\\textbf{y}^{H} + \\overset{ˉ}{\\alpha}\\textbf{y}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . A device in/out <type> array of dimension lda x n with lda>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or incy == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if lda < max(1, n ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher2, zher2 2.6.22. cublas<t>hpr()  cublasStatus_t cublasChpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * AP ) cublasStatus_t cublasZhpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface . This function performs the packed Hermitian rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . AP device in/out <type> array with A stored in packed format. The imaginary parts of the diagonal elements are assumed and set to zero. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chpr , zhpr 2.6.23. cublas<t>hpr2()  cublasStatus_t cublasChpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * AP ) cublasStatus_t cublasZhpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface . This function performs the packed Hermitian rank-2 update \\(A = \\alpha\\textbf{x}\\textbf{y}^{H} + \\overset{ˉ}{\\alpha}\\textbf{y}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . AP device in/out <type> array with A stored in packed format. The imaginary parts of the diagonal elements are assumed and set to zero. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or incy == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chpr2, zhpr2 2.6.24. cublas<t>gemvBatched()  cublasStatus_t cublasSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * const Aarray [], int lda , const float * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) cublasStatus_t cublasDgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * const Aarray [], int lda , const double * const xarray [], int incx , const double * beta , double * const yarray [], int incy , int batchCount ) cublasStatus_t cublasCgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * const Aarray [], int lda , const cuComplex * const xarray [], int incx , const cuComplex * beta , cuComplex * const yarray [], int incy , int batchCount ) cublasStatus_t cublasZgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * const Aarray [], int lda , const cuDoubleComplex * const xarray [], int incx , const cuDoubleComplex * beta , cuDoubleComplex * const yarray [], int incy , int batchCount ) #if defined(__cplusplus) cublasStatus_t cublasHSHgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * const Aarray [], int lda , const __half * const xarray [], int incx , const float * beta , __half * const yarray [], int incy , int batchCount ) cublasStatus_t cublasHSSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * const Aarray [], int lda , const __half * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) cublasStatus_t cublasTSTgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * const Aarray [], int lda , const __nv_bfloat16 * const xarray [], int incx , const float * beta , __nv_bfloat16 * const yarray [], int incy , int batchCount ) cublasStatus_t cublasTSSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * const Aarray [], int lda , const __nv_bfloat16 * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) #endif This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication of a batch of matrices and vectors. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors. The address of the input matrix and vector, and the output vector of each instance of the batch are read from arrays of pointers passed to the function by the caller. \\(\\textbf{y}\\lbrack i\\rbrack = \\alpha\\text{op}(A\\lbrack i\\rbrack)\\textbf{x}\\lbrack i\\rbrack + \\beta\\textbf{y}\\lbrack i\\rbrack,\\text{ for i} \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) is an array of pointers to matrice \\(A\\lbrack i\\rbrack\\) stored in column-major format with dimension \\(m \\times n\\) , and \\(\\textbf{x}\\) and \\(\\textbf{y}\\) are arrays of pointers to vectors. Also, for matrix \\(A\\lbrack i\\rbrack\\) , \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A\\lbrack i\\rbrack}^{T} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A\\lbrack i\\rbrack}^{H} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note \\(\\textbf{y}\\lbrack i\\rbrack\\) vectors must not overlap, i.e. the individual gemv operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemv in different CUDA streams, rather than use this API. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A[i] ) that is non- or (conj.) transpose. m input number of rows of matrix A[i] . n input number of columns of matrix A[i] . alpha host or device input <type> scalar used for multiplication. Aarray device input array of pointers to <type> array, with each array of dim. lda x n with lda>=max(1,m) . All pointers must meet certain alignment criteria. Please see below for details. lda input leading dimension of two-dimensional array used to store each matrix A[i] . xarray device input array of pointers to <type> array, with each dimension n if trans==CUBLAS_OP_N and m otherwise. All pointers must meet certain alignment criteria. Please see below for details. incx input stride of each one-dimensional array x[i]. beta host or device input <type> scalar used for multiplication. If beta == 0 , y does not have to be a valid input. yarray device in/out array of pointers to <type> array. It has dimensions m if trans==CUBLAS_OP_N and n otherwise. Vectors y[i] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. incy input stride of each one-dimensional array y[i]. batchCount input number of pointers contained in Aarray, xarray and yarray. If math mode enables fast math modes when using cublasSgemvBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k % 4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,batchCount<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.6.25. cublas<t>gemvStridedBatched()  cublasStatus_t cublasSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * A , int lda , long long int strideA , const float * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasDgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * A , int lda , long long int strideA , const double * x , int incx , long long int stridex , const double * beta , double * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasCgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * x , int incx , long long int stridex , const cuComplex * beta , cuComplex * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasZgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , long long int strideA , const cuDoubleComplex * x , int incx , long long int stridex , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasHSHgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * A , int lda , long long int strideA , const __half * x , int incx , long long int stridex , const float * beta , __half * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasHSSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * A , int lda , long long int strideA , const __half * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasTSTgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * A , int lda , long long int strideA , const __nv_bfloat16 * x , int incx , long long int stridex , const float * beta , __nv_bfloat16 * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasTSSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * A , int lda , long long int strideA , const __nv_bfloat16 * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication of a batch of matrices and vectors. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors. Input matrix A and vector x, and output vector y for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. Pointers to A matrix, x and y vectors for the first instance are passed to the function by the user along with offsets in number of elements - strideA, stridex and stridey that determine the locations of input matrices and vectors, and output vectors in future instances. \\(\\textbf{y} + i*{stridey} = \\alpha\\text{op}(A + i*{strideA})(\\textbf{x} + i*{stridex}) + \\beta(\\textbf{y} + i*{stridey}),\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) is an array of pointers to matrix stored in column-major format with dimension \\(A\\lbrack i\\rbrack\\) \\(m \\times n\\) , and \\(\\textbf{x}\\) and \\(\\textbf{y}\\) are arrays of pointers to vectors. Also, for matrix \\(A\\lbrack i\\rbrack\\) \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A\\lbrack i\\rbrack}^{T} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A\\lbrack i\\rbrack}^{H} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note \\(\\textbf{y}\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemv operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemv in different CUDA streams, rather than use this API. Note In the table below, we use A[i], x[i], y[i] as notation for A matrix, and x and y vectors in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, stridex, stridey away from A[i-1], x[i-1], y[i-1] . The unit for the offset is number of elements and must not be zero . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A[i] ) that is non- or (conj.) transpose. m input number of rows of matrix A[i] . n input number of columns of matrix A[i] . alpha host or device input <type> scalar used for multiplication. A device input <type>* pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x n with lda>=max(1,m) . lda input leading dimension of two-dimensional array used to store each matrix A[i] . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] x device input <type>* pointer to the x vector corresponding to the first instance of the batch, with each dimension n if trans==CUBLAS_OP_N and m otherwise. incx input stride of each one-dimensional array x[i]. stridex input Value of type long long int that gives the offset in number of elements between x[i] and x[i+1] beta host or device input <type> scalar used for multiplication. If beta == 0 , y does not have to be a valid input. y device in/out <type>* pointer to the y vector corresponding to the first instance of the batch, with each dimension m if trans==CUBLAS_OP_N and n otherwise. Vectors y[i] should not overlap; otherwise, undefined behavior is expected. incy input stride of each one-dimensional array y[i]. stridey input Value of type long long int that gives the offset in number of elements between y[i] and y[i+1] batchCount input number of GEMVs to perform in the batch. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,batchCount<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.7. cuBLAS Level-3 Function Reference  In this chapter we describe the Level-3 Basic Linear Algebra Subprograms (BLAS3) functions that perform matrix-matrix operations. 2.7.1. cublas<t>gemm()  cublasStatus_t cublasSgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) cublasStatus_t cublasHgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * A , int lda , const __half * B , int ldb , const __half * beta , __half * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or if alpha , beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_ARCH_MISMATCH in the case of cublasHgemm() the device does not support math in half precision. CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemm , dgemm , cgemm , zgemm 2.7.2. cublas<t>gemm3m()  cublasStatus_t cublasCgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the complex matrix-matrix multiplication, using Gauss complexity reduction algorithm. This can lead to an increase in performance up to 25% \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Note These 2 routines are only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A ) that is non- or (conj.) transpose. transb input Operation op( B ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A ) and C . n input Number of columns of matrix op( B ) and C . k input Number of columns of op( A ) and rows of op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input Leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input Leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input Leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If m , n , k < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or if alpha , beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capabilites lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: cgemm , zgemm 2.7.3. cublas<t>gemmBatched()  cublasStatus_t cublasHgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * const Aarray [], int lda , const __half * const Barray [], int ldb , const __half * beta , __half * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasSgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * const Aarray [], int lda , const float * const Barray [], int ldb , const float * beta , float * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasDgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * const Aarray [], int lda , const double * const Barray [], int ldb , const double * beta , double * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasCgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * const Aarray [], int lda , const cuComplex * const Barray [], int ldb , const cuComplex * beta , cuComplex * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasZgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * const Aarray [], int lda , const cuDoubleComplex * const Barray [], int ldb , const cuDoubleComplex * beta , cuDoubleComplex * const Carray [], int ldc , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication of a batch of matrices. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. \\(C\\lbrack i\\rbrack = \\alpha\\text{op}(A\\lbrack i\\rbrack)\\text{op}(B\\lbrack i\\rbrack) + \\beta C\\lbrack i\\rbrack,\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A[i] ) that is non- or (conj.) transpose. transb input operation op( B[i] ) that is non- or (conj.) transpose. m input number of rows of matrix op( A[i] ) and C[i] . n input number of columns of op( B[i] ) and C[i] . k input number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input <type> scalar used for multiplication. Aarray device input array of pointers to <type> array, with each array of dim. lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. All pointers must meet certain alignment criteria. Please see below for details. lda input leading dimension of two-dimensional array used to store each matrix A[i] . Barray device input array of pointers to <type> array, with each array of dim. ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise. All pointers must meet certain alignment criteria. Please see below for details. ldb input leading dimension of two-dimensional array used to store each matrix B[i] . beta host or device input <type> scalar used for multiplication. If beta == 0 , C does not have to be a valid input. Carray device in/out array of pointers to <type> array. It has dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. ldc input leading dimension of two-dimensional array used to store each matrix C[i] . batchCount input number of pointers contained in Aarray, Barray and Carray. If math mode enables fast math modes when using cublasSgemmBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k , batchCount < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_ARCH_MISMATCH cublasHgemmBatched() is only supported for GPU with architecture capabilities equal or greater than 5.3 2.7.4. cublas<t>gemmStridedBatched()  cublasStatus_t cublasHgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * A , int lda , long long int strideA , const __half * B , int ldb , long long int strideB , const __half * beta , __half * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasSgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * A , int lda , long long int strideA , const float * B , int ldb , long long int strideB , const float * beta , float * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasDgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , long long int strideA , const double * B , int ldb , long long int strideB , const double * beta , double * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasCgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * B , int ldb , long long int strideB , const cuComplex * beta , cuComplex * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasCgemm3mStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * B , int ldb , long long int strideB , const cuComplex * beta , cuComplex * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasZgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , long long int strideA , const cuDoubleComplex * B , int ldb , long long int strideB , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc , long long int strideC , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication of a batch of matrices. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. Pointers to A, B and C matrices for the first instance are passed to the function by the user along with offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances. \\(C + i*{strideC} = \\alpha\\text{op}(A + i*{strideA})\\text{op}(B + i*{strideB}) + \\beta(C + i*{strideC}),\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1] . The unit for the offset is number of elements and must not be zero . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A[i] ) that is non- or (conj.) transpose. transb input operation op( B[i] ) that is non- or (conj.) transpose. m input number of rows of matrix op( A[i] ) and C[i] . n input number of columns of op( B[i] ) and C[i] . k input number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input <type> scalar used for multiplication. A device input <type>* pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store each matrix A[i] . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] B device input <type>* pointer to the B matrix corresponding to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise. ldb input leading dimension of two-dimensional array used to store each matrix B[i] . strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] beta host or device input <type> scalar used for multiplication. If beta == 0 , C does not have to be a valid input. C device in/out <type>* pointer to the C matrix corresponding to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix C[i] . strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] batchCount input number of GEMMs to perform in the batch. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k , batchCount < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_ARCH_MISMATCH cublasHgemmStridedBatched() is only supported for GPU with architecture capabilities equal or greater than 5.3 2.7.5. cublas<t>gemmGroupedBatched()  cublasStatus_t cublasSgemmGroupedBatched ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const float alpha_array [], const float * const Aarray [], const int lda_array [], const float * const Barray [], const int ldb_array [], const float beta_array [], float * const Carray [], const int ldc_array [], int group_count , const int group_size []) cublasStatus_t cublasDgemmGroupedBatched ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const double alpha_array [], const double * const Aarray [], const int lda_array [], const double * const Barray [], const int ldb_array [], const double beta_array [], double * const Carray [], const int ldc_array [], int group_count , const int group_size []) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication on groups of matrices. A given group is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.  This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemm ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], lda_array [ i ], Barray [ idx ], ldb_array [ i ], beta_array [ i ], Carray [ idx ], ldc_array [ i ]); idx += 1 ; end end where \\(\\text{$\\mathrm{alpha\\_array}$}\\) and \\(\\text{$\\mathrm{beta\\_array}$}\\) are arrays of scaling factors, and \\(\\text{Aarray}\\) , \\(\\text{Barray}\\) and \\(\\text{Carray}\\) are arrays of pointers to matrices stored in column-major format.  For a given index, \\(\\text{idx}\\) , that is part of group \\(i\\) , the dimensions are: \\(\\text{op}(\\text{Aarray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{op}(\\text{Barray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{Carray}\\lbrack\\text{idx}\\rbrack\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) Note This API takes arrays of two different lengths.  The arrays of dimensions, leading dimensions, transpositions, and scaling factors are of length group_count and the arrays of matrices are of length problem_count where \\(\\text{$\\mathrm{problem\\_count}$} = \\sum_{i = 0}^{\\text{$\\mathrm{group\\_count}$} - 1} \\text{$\\mathrm{group\\_size}$}\\lbrack i\\rbrack\\) For matrix \\(A[\\text{idx}]\\) in group \\(i\\) \\(\\text{op}(A[\\text{idx}]) = \\left\\{ \\begin{matrix}\nA[\\text{idx}] & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA[\\text{idx}]^{T} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA[\\text{idx}]^{H} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B[\\text{idx}])\\) is defined similarly for matrix \\(B[\\text{idx}]\\) in group \\(i\\) . Note \\(C\\lbrack\\text{idx}\\rbrack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemmBatched in different CUDA streams, rather than use this API. Param. Memory In/out Meaning Array Length handle input handle to the cuBLAS library context. transa_array host input array containing the operations, op( A[idx] ), that is non- or (conj.) transpose for each group. group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj.) transpose for each group. group_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group. group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group. group_count k_array host input array containing the number of columns of op( A[idx] ) and rows of op( B[idx] ) for each group. group_count alpha_array host input array containing the <type> scalar used for multiplication for each group. group_count Aarray device input array of pointers to <type> array, with each array of dim. lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group. group_count Barray device input array of pointers to <type> array, with each array of dim. ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group. group_count beta_array host input array containing the <type> scalar used for multiplication for each group. group_count Carray device in/out array of pointers to <type> array. It has dimensions ldc[i] x n[i] with ldc[i]>=max(1,m[i]) . Matrices C[idx] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. problem_count ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group. group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group. group_count If math mode enables fast math modes when using cublasSgemmGroupedBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is required that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count < 0 or if m_array[i] , n_array[i] , k_array[i] , group_size[i] < 0 or if transa_array[i] , transb_array[i] != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda_array[i] < max(1, m_array[i] ) if transa_array[i] == CUBLAS_OP_N and lda_array[i] < max(1, k_array[i] ) otherwise or if ldb_array[i] < max(1, k_array[i] ) if transb_array[i] == CUBLAS_OP_N and ldb_array[i] < max(1, n_array[i] ) otherwise or if ldc_array[i] < max(1, m_array[i] ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_NOT_SUPPORTED the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE 2.7.6. cublas<t>symm()  cublasStatus_t cublasSsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a symmetric matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n < 0 or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or if ldc < max(1, m ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymm , dsymm , csymm , zsymm 2.7.7. cublas<t>syrk()  cublasStatus_t cublasSsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A. beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 2.7.8. cublas<t>syr2k()  cublasStatus_t cublasSsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank- \\(2k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\text{op}(B)\\text{op}(A)^{T}) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr2k , dsyr2k , csyr2k , zsyr2k 2.7.9. cublas<t>syrkx()  cublasStatus_t cublasSsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs a variation of the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrices \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric. A usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublas<t>dgmm . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk and ssyr2k , dsyr2k , csyr2k , zsyr2k 2.7.10. cublas<t>trmm()  cublasStatus_t cublasStrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * A , int lda , const float * B , int ldb , float * C , int ldc ) cublasStatus_t cublasDtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * A , int lda , const double * B , int ldb , double * C , int ldc ) cublasStatus_t cublasCtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , cuComplex * C , int ldc ) cublasStatus_t cublasZtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the triangular matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha\\text{op}(A)B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha B\\text{op}(A)} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrix, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Notice that in order to achieve better parallelism cuBLAS differs from the BLAS API only for this routine. The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLAS API assumes an out-of-place implementation (with results written into C). The application can obtain the in-place functionality of BLAS in the cuBLAS API by passing the address of the matrix B in place of the matrix C. No other overlapping in the input parameters is supported. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A sized accordingly. alpha host or device input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . C device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strmm , dtrmm , ctrmm , ztrmm 2.7.11. cublas<t>trsm()  cublasStatus_t cublasStrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * A , int lda , float * B , int ldb ) cublasStatus_t cublasDtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * A , int lda , double * B , int ldb ) cublasStatus_t cublasCtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , cuComplex * B , int ldb ) cublasStatus_t cublasZtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb ) This function supports the 64-bit Integer Interface . This function solves the triangular linear system with multiple right-hand-sides \\(\\left\\{ \\begin{matrix}\n{\\text{op}(A)X = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{X\\text{op}(A) = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\) and \\(B\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(X\\) overwrites the right-hand-sides \\(B\\) on exit. No test for singularity or near-singularity is included in this function. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of X . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A is sized accordingly. alpha host or device input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device in/out <type> array. It has dimensions ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if diag != CUBLAS_DIAG_NON_UNIT , CUBLAS_DIAG_UNIT or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsm , dtrsm , ctrsm , ztrsm 2.7.12. cublas<t>trsmBatched()  cublasStatus_t cublasStrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * const A [], int lda , float * const B [], int ldb , int batchCount ); cublasStatus_t cublasDtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * const A [], int lda , double * const B [], int ldb , int batchCount ); cublasStatus_t cublasCtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * const A [], int lda , cuComplex * const B [], int ldb , int batchCount ); cublasStatus_t cublasZtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * const A [], int lda , cuDoubleComplex * const B [], int ldb , int batchCount ); This function supports the 64-bit Integer Interface . This function solves an array of triangular linear systems with multiple right-hand-sides \\(\\left\\{ \\begin{matrix}\n{\\text{op}(A\\lbrack i\\rbrack)X\\lbrack i\\rbrack = \\alpha B\\lbrack i\\rbrack} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{X\\lbrack i\\rbrack\\text{op}(A\\lbrack i\\rbrack) = \\alpha B\\lbrack i\\rbrack} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\lbrack i\\rbrack\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\lbrack i\\rbrack\\) and \\(B\\lbrack i\\rbrack\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\lbrack i\\rbrack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A^{H}\\lbrack i\\rbrack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(X\\lbrack i\\rbrack\\) overwrites the right-hand-sides \\(B\\lbrack i\\rbrack\\) on exit. No test for singularity or near-singularity is included in this function. This function works for any sizes but is intended to be used for matrices of small sizes where the launch overhead is a significant factor. For bigger sizes, it might be advantageous to call batchCount times the regular cublas<t>trsm within a set of CUDA streams. The current implementation is limited to devices with compute capability above or equal 2.0. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A[i] is on the left or right of X[i] . uplo input indicates if matrix A[i] lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A[i] ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A[i] are unity and should not be accessed. m input number of rows of matrix B[i] , with matrix A[i] sized accordingly. n input number of columns of matrix B[i] , with matrix A[i] is sized accordingly. alpha host or device input <type> scalar used for multiplication, if alpha==0 then A[i] is not referenced and B[i] does not have to be a valid input. A device input array of pointers to <type> array, with each array of dim. lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A[i] . B device in/out array of pointers to <type> array, with each array of dim. ldb x n with ldb>=max(1,m) . Matrices B[i] should not overlap; otherwise, undefined behavior is expected. ldb input leading dimension of two-dimensional array used to store matrix B[i] . batchCount input number of pointers contained in A and B. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or batchCount < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if diag != CUBLAS_DIAG_NON_UNIT , CUBLAS_DIAG_UNIT or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or ldb < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsm , dtrsm , ctrsm , ztrsm 2.7.13. cublas<t>hemm()  cublasStatus_t cublasChemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZhemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a Hermitian matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or if ldc < max(1, m ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chemm , zhemm 2.7.14. cublas<t>herk()  cublasStatus_t cublasCherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk 2.7.15. cublas<t>her2k()  cublasStatus_t cublasCher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank- \\(2k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\overset{ˉ}{\\alpha}\\text{op}(B)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher2k , zher2k 2.7.16. cublas<t>herkx()  cublasStatus_t cublasCherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs a variation of the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian. An usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublas<t>dgmm . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input real scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk and cher2k , zher2k 2.8. BLAS-like Extension  This section describes the BLAS-extension functions that perform matrix-matrix operations. 2.8.1. cublas<t>geam()  cublasStatus_t cublasSgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const float * alpha , const float * A , int lda , const float * beta , const float * B , int ldb , float * C , int ldc ) cublasStatus_t cublasDgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const double * alpha , const double * A , int lda , const double * beta , const double * B , int ldb , double * C , int ldc ) cublasStatus_t cublasCgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , const cuComplex * B , int ldb , cuComplex * C , int ldc ) cublasStatus_t cublasZgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , const cuDoubleComplex * B , int ldb , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix addition/transposition \\(C = \\alpha\\text{op}(A) + \\beta\\text{op}(B)\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times n\\) , \\(\\text{op}(B)\\) \\(m \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . The operation is out-of-place if C does not overlap A or B. The in-place mode supports the following two operations, \\(C = \\alpha\\text{*}C + \\beta\\text{op}(B)\\) \\(C = \\alpha\\text{op}(A) + \\beta\\text{*}C\\) For in-place mode, if C = A , ldc = lda and transa = CUBLAS_OP_N . If C = B , ldc = ldb and transb = CUBLAS_OP_N . If the user does not meet above requirements, CUBLAS_STATUS_INVALID_VALUE is returned. The operation includes the following special cases: the user can reset matrix C to zero by setting *alpha=*beta=0 . the user can transpose matrix A by setting *alpha=1 and *beta=0 . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . alpha host or device input <type> scalar used for multiplication. If *alpha == 0 , A does not have to be a valid input. A device input <type> array of dimensions lda x n with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) if transb == CUBLAS_OP_N and ldb x m with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If *beta == 0 , B does not have to be a valid input. C device output <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if transa != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, n ) otherwise or if ldb < max(1, m ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or if A == C , (( CUBLAS_OP_N != transa ) || ( lda != ldc )) or if B == C , (( CUBLAS_OP_N != transb ) || ( ldb != ldc )) or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.2. cublas<t>dgmm()  cublasStatus_t cublasSdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const float * A , int lda , const float * x , int incx , float * C , int ldc ) cublasStatus_t cublasDdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const double * A , int lda , const double * x , int incx , double * C , int ldc ) cublasStatus_t cublasCdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const cuComplex * A , int lda , const cuComplex * x , int incx , cuComplex * C , int ldc ) cublasStatus_t cublasZdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{A \\times diag(X)} & {\\text{if }\\textsf{mode == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n{diag(X) \\times A} & {\\text{if }\\textsf{mode == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(m \\times n\\) . \\(X\\) is a vector of size \\(n\\) if mode == CUBLAS_SIDE_RIGHT and of size \\(m\\) if mode == CUBLAS_SIDE_LEFT . \\(X\\) is gathered from one-dimensional array x with stride incx . The absolute value of incx is the stride and the sign of incx is direction of the stride. If incx is positive, then we forward x from the first element. Otherwise, we backward x from the last element. The formula of X is \\(X\\lbrack j\\rbrack = \\left\\{ \\begin{matrix}\n{x\\lbrack j \\times incx\\rbrack} & {\\text{if }incx \\geq 0} \\\\\n{x\\lbrack(\\chi - 1) \\times |incx| - j \\times |incx|\\rbrack} & {\\text{if }incx < 0} \\\\\n\\end{matrix} \\right.\\) where \\(\\chi = m\\) if mode == CUBLAS_SIDE_LEFT and \\(\\chi = n\\) if mode == CUBLAS_SIDE_RIGHT . Example 1: if the user wants to perform \\(diag(diag(B)) \\times A\\) , then \\(incx = ldb + 1\\) where \\(ldb\\) is leading dimension of matrix B , either row-major or column-major. Example 2: if the user wants to perform \\(\\alpha \\times A\\) , then there are two choices, either cublas<t>geam() with *beta=0 and transa == CUBLAS_OP_N or cublas<t>dgmm() with incx=0 and x[0]=alpha . The operation is out-of-place. The in-place only works if lda = ldc . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. mode input left multiply if mode == CUBLAS_SIDE_LEFT or right multiply if mode == CUBLAS_SIDE_RIGHT m input number of rows of matrix A and C . n input number of columns of matrix A and C . A device input <type> array of dimensions lda x n with lda>=max(1,m) lda input leading dimension of two-dimensional array used to store the matrix A . x device input one-dimensional <type> array of size \\(|inc| \\times m\\) if mode == CUBLAS_SIDE_LEFT and \\(|inc| \\times n\\) if mode == CUBLAS_SIDE_RIGHT incx input stride of one-dimensional array x . C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if mode != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if lda < max(1, m ) or ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.3. cublas<t>getrfBatched()  cublasStatus_t cublasSgetrfBatched ( cublasHandle_t handle , int n , float * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasDgetrfBatched ( cublasHandle_t handle , int n , double * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasCgetrfBatched ( cublasHandle_t handle , int n , cuComplex * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasZgetrfBatched ( cublasHandle_t handle , int n , cuDoubleComplex * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format with dimensions nxn and leading dimension lda . This function performs the LU factorization of each Aarray[i] for i = 0, …, batchSize-1 by the following equation \\(\\text{P}\\text{*}{Aarray}\\lbrack i\\rbrack = L\\text{*}U\\) where P is a permutation matrix which represents partial pivoting with row interchanges. L is a lower triangular matrix with unit diagonal and U is an upper triangular matrix. Formally P is written by a product of permutation matrices Pj , for j = 1,2,...,n , say P = P1 * P2 * P3 * .... * Pn . Pj is a permutation matrix which interchanges two rows of vector x when performing Pj*x . Pj can be constructed by j element of PivotArray[i] by the following Matlab code // In Matlab PivotArray[i] is an array of base-1. // In C, PivotArray[i] is base-0. Pj = eye ( n ); swap Pj ( j , : ) and Pj ( PivotArray [ i ][ j ] , : ) L and U are written back to original matrix A , and diagonal elements of L are discarded. The L and U can be constructed by the following Matlab code // A is a matrix of nxn after getrf. L = eye ( n ); for j = 1 : n L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : n U ( i , i : n ) = A ( i , i : n ) end If matrix A(=Aarray[i]) is singular, getrf still works and the value of info(=infoArray[i]) reports first row index that LU factorization cannot proceed. If info is k , U(k,k) is zero. The equation P*A=L*U still holds, however L and U reconstruction needs different Matlab code as follows: // A is a matrix of nxn after getrf. // info is k, which means U(k,k) is zero. L = eye ( n ); for j = 1 : k -1 L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : k -1 U ( i , i : n ) = A ( i , i : n ) end for i = k : n U ( i , k : n ) = A ( i , k : n ) end This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>getrfBatched supports non-pivot LU factorization if PivotArray is NULL. cublas<t>getrfBatched supports arbitrary dimension. cublas<t>getrfBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of rows and columns of Aarray[i] . Aarray device input/output array of pointers to <type> array, with each array of dim. n x n with lda>=max(1,n) . Matrices Aarray[i] should not overlap; otherwise, undefined behavior is expected. lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . PivotArray device output array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If PivotArray is NULL, pivoting is disabled. infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of factorization of Aarray[i] . If info=0, the execution is successful. If info = -j, the j-th parameter had an illegal value. If info = k, U(k,k) is 0. The factorization has been completed, but U is exactly singular. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,batchSize,lda <0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgeqrf , dgeqrf , cgeqrf , zgeqrf 2.8.4. cublas<t>getrsBatched()  cublasStatus_t cublasSgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const float * const Aarray [], int lda , const int * devIpiv , float * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasDgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const double * const Aarray [], int lda , const int * devIpiv , double * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasCgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const cuComplex * const Aarray [], int lda , const int * devIpiv , cuComplex * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasZgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const cuDoubleComplex * const Aarray [], int lda , const int * devIpiv , cuDoubleComplex * const Barray [], int ldb , int * info , int batchSize ); This function solves an array of systems of linear equations of the form: \\(\\text{op}(A\\lbrack i \\rbrack) X\\lbrack i\\rbrack = B\\lbrack i\\rbrack\\) where \\(A\\lbrack i\\rbrack\\) is a matrix which has been LU factorized with pivoting, \\(X\\lbrack i\\rbrack\\) and \\(B\\lbrack i\\rbrack\\) are \\(n \\times {nrhs}\\) matrices. Also, for matrix \\(A\\) \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A^{H}\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>getrsBatched supports non-pivot LU factorization if devIpiv is NULL. cublas<t>getrsBatched supports arbitrary dimension. cublas<t>getrsBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows and columns of Aarray[i] . nrhs input number of columns of Barray[i] . Aarray device input array of pointers to <type> array, with each array of dim. n x n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . devIpiv device input array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If devIpiv is NULL, pivoting for all Aarray[i] is ignored. Barray device input/output array of pointers to <type> array, with each array of dim. n x nrhs with ldb>=max(1,n) . Matrices Barray[i] should not overlap; otherwise, undefined behavior is expected. ldb input leading dimension of two-dimensional array used to store each solution matrix Barray[i] . info host output If info=0, the execution is successful. If info = -j, the j-th parameter had an illegal value. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or nrhs < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) or ldb < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgeqrs , dgeqrs , cgeqrs , zgeqrs 2.8.5. cublas<t>getriBatched()  cublasStatus_t cublasSgetriBatched ( cublasHandle_t handle , int n , const float * const Aarray [], int lda , int * PivotArray , float * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasDgetriBatched ( cublasHandle_t handle , int n , const double * const Aarray [], int lda , int * PivotArray , double * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasCgetriBatched ( cublasHandle_t handle , int n , const cuComplex * const Aarray [], int lda , int * PivotArray , cuComplex * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasZgetriBatched ( cublasHandle_t handle , int n , const cuDoubleComplex * const Aarray [], int lda , int * PivotArray , cuDoubleComplex * const Carray [], int ldc , int * infoArray , int batchSize ); Aarray and Carray are arrays of pointers to matrices stored in column-major format with dimensions n*n and leading dimension lda and ldc respectively. This function performs the inversion of matrices A[i] for i = 0, …, batchSize-1 . Prior to calling cublas<t>getriBatched, the matrix A[i] must be factorized first using the routine cublas<t>getrfBatched. After the call of cublas<t>getrfBatched, the matrix pointing by Aarray[i] will contain the LU factors of the matrix A[i] and the vector pointing by (PivotArray+i) will contain the pivoting sequence. Following the LU factorization, cublas<t>getriBatched uses forward and backward triangular solvers to complete inversion of matrices A[i] for i = 0, …, batchSize-1 . The inversion is out-of-place, so memory space of Carray[i] cannot overlap memory space of Array[i]. Typically all parameters in cublas<t>getrfBatched would be passed into cublas<t>getriBatched. For example, // step 1: perform in-place LU decomposition, P*A = L*U. //      Aarray[i] is n*n matrix A[i] cublasDgetrfBatched ( handle , n , Aarray , lda , PivotArray , infoArray , batchSize ); //      check infoArray[i] to see if factorization of A[i] is successful or not. //      Array[i] contains LU factorization of A[i] // step 2: perform out-of-place inversion, Carray[i] = inv(A[i]) cublasDgetriBatched ( handle , n , Aarray , lda , PivotArray , Carray , ldc , infoArray , batchSize ); //      check infoArray[i] to see if inversion of A[i] is successful or not. The user can check singularity from either cublas<t>getrfBatched or cublas<t>getriBatched. This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. If cublas<t>getrfBatched is performed by non-pivoting, PivotArray of cublas<t>getriBatched should be NULL. cublas<t>getriBatched supports arbitrary dimension. cublas<t>getriBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of rows and columns of Aarray[i] . Aarray device input array of pointers to <type> array, with each array of dimension n*n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . PivotArray device output array of size n*batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If PivotArray is NULL, pivoting is disabled. Carray device output array of pointers to <type> array, with each array of dimension n*n with ldc>=max(1,n) . Matrices Carray[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix Carray[i] . infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of inversion of A[i] . If info=0, the execution is successful. If info = k, U(k,k) is 0. The U is exactly singular and the inversion failed. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or lda < 0 or ldc < 0 or batchSize < 0 or lda < n or ldc < n CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.6. cublas<t>matinvBatched()  cublasStatus_t cublasSmatinvBatched ( cublasHandle_t handle , int n , const float * const A [], int lda , float * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasDmatinvBatched ( cublasHandle_t handle , int n , const double * const A [], int lda , double * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasCmatinvBatched ( cublasHandle_t handle , int n , const cuComplex * const A [], int lda , cuComplex * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasZmatinvBatched ( cublasHandle_t handle , int n , const cuDoubleComplex * const A [], int lda , cuDoubleComplex * const Ainv [], int lda_inv , int * info , int batchSize ); A and Ainv are arrays of pointers to matrices stored in column-major format with dimensions n*n and leading dimension lda and lda_inv respectively. This function performs the inversion of matrices A[i] for i = 0, …, batchSize-1 . This function is a short cut of cublas<t>getrfBatched plus cublas<t>getriBatched . However it doesn’t work if n is greater than 32. If not, the user has to go through cublas<t>getrfBatched and cublas<t>getriBatched . If the matrix A[i] is singular, then info[i] reports singularity, the same as cublas<t>getrfBatched . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of rows and columns of A[i] . A device input array of pointers to <type> array, with each array of dimension n*n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store each matrix A[i] . Ainv device output array of pointers to <type> array, with each array of dimension n*n with lda_inv>=max(1,n) . Matrices Ainv[i] should not overlap; otherwise, undefined behavior is expected. lda_inv input leading dimension of two-dimensional array used to store each matrix Ainv[i] . info device output array of size batchSize that info[i] contains the information of inversion of A[i] . If info[i]=0, the execution is successful. If info[i]=k, U(k,k) is 0. The U is exactly singular and the inversion failed. batchSize input number of pointers contained in A. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or lda < 0 or lda_inv < 0 or batchSize < 0 or if lda < n or lda_inv < n or n > 32 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.7. cublas<t>geqrfBatched()  cublasStatus_t cublasSgeqrfBatched ( cublasHandle_t handle , int m , int n , float * const Aarray [], int lda , float * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasDgeqrfBatched ( cublasHandle_t handle , int m , int n , double * const Aarray [], int lda , double * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasCgeqrfBatched ( cublasHandle_t handle , int m , int n , cuComplex * const Aarray [], int lda , cuComplex * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasZgeqrfBatched ( cublasHandle_t handle , int m , int n , cuDoubleComplex * const Aarray [], int lda , cuDoubleComplex * const TauArray [], int * info , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format with dimensions m x n and leading dimension lda . TauArray is an array of pointers to vectors of dimension of at least max (1, min(m, n) . This function performs the QR factorization of each Aarray[i] for i = 0, ...,batchSize-1 using Householder reflections. Each matrix Q[i] is represented as a product of elementary reflectors and is stored in the lower part of each Aarray[i] as follows : Q[j] = H[j][1] H[j][2] . . . H[j](k), where k = min(m,n). Each H[j][i] has the form H[j][i] = I - tau[j] * v * v' where tau[j] is a real scalar, and v is a real vector with v(1:i-1) = 0 and v(i) = 1 ; v(i+1:m) is stored on exit in Aarray[j][i+1:m,i] , and tau in TauArray[j][i] . This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>geqrfBatched supports arbitrary dimension. cublas<t>geqrfBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. m input number of rows Aarray[i] . n input number of columns of Aarray[i] . Aarray device input array of pointers to <type> array, with each array of dim. m x n with lda>=max(1,m) . lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . TauArray device output array of pointers to <type> vector, with each vector of dim. max(1,min(m,n)) . info host output If info=0, the parameters passed to the function are valid If info<0, the parameter in postion -info is invalid batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or batchSize < 0 or lda < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgeqrf , dgeqrf , cgeqrf , zgeqrf 2.8.8. cublas<t>gelsBatched()  cublasStatus_t cublasSgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , float * const Aarray [], int lda , float * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasDgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , double * const Aarray [], int lda , double * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasCgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , cuComplex * const Aarray [], int lda , cuComplex * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasZgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , cuDoubleComplex * const Aarray [], int lda , cuDoubleComplex * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format. Carray is an array of pointers to matrices stored in column-major format. This function find the least squares solution of a batch of overdetermined systems: it solves the least squares problem described as follows : minimize || Carray [ i ] - Aarray [ i ] * Xarray [ i ] || , with i = 0 , ..., batchSize -1 On exit, each Aarray[i] is overwritten with their QR factorization and each Carray[i] is overwritten with the least square solution cublas<t>gelsBatched supports only the non-transpose operation and only solves over-determined systems (m >= n). cublas<t>gelsBatched only supports compute capability 2.0 or above. This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( Aarray[i] ) that is non- or (conj.) transpose. Only non-transpose operation is currently supported. m input number of rows of each Aarray[i] and Carray[i] if trans == CUBLAS_OP_N , numbers of columns of each Aarray[i] otherwise (not supported currently). n input number of columns of each Aarray[i] if trans == CUBLAS_OP_N , and number of rows of each Aarray[i] and Carray[i] otherwise (not supported currently). nrhs input number of columns of each Carray[i] . Aarray device input/output array of pointers to <type> array, with each array of dim. m x n with lda>=max(1,m) if trans == CUBLAS_OP_N , and n x m with lda>=max(1,n) otherwise (not supported currently). Matrices Aarray[i] should not overlap; otherwise, undefined behavior is expected. lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . Carray device input/output array of pointers to <type> array, with each array of dim. m x nrhs with ldc>=max(1,m) if trans == CUBLAS_OP_N , and n x nrhs with lda>=max(1,n) otherwise (not supported currently). Matrices Carray[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix Carray[i] . info host output If info=0, the parameters passed to the function are valid If info<0, the parameter in position -info is invalid devInfoArray device output optional array of integers of dimension batchsize. If non-null, every element devInfoArray[i] contain a value V with the following meaning: V = 0 : the i-th problem was sucessfully solved V > 0 : the V-th diagonal element of the Aarray[i] is zero. Aarray[i] does not have full rank. batchSize input number of pointers contained in Aarray and Carray The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or nrhs < 0 or batchSize < 0 or lda < max(1, m ) or ldc < max(1, m ) CUBLAS_STATUS_NOT_SUPPORTED the parameters m <n or trans is different from non-transpose. CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgels , dgels , cgels , zgels 2.8.9. cublas<t>tpttr()  cublasStatus_t cublasStpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * AP , float * A , int lda ); cublasStatus_t cublasDtpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * AP , double * A , int lda ); cublasStatus_t cublasCtpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * AP , cuComplex * A , int lda ); cublasStatus_t cublasZtpttr ( cublasHandle_t handle , cublasFillMode_t uplo int n , const cuDoubleComplex * AP , cuDoubleComplex * A , int lda ); This function performs the conversion from the triangular packed format to the triangular format If uplo == CUBLAS_FILL_MODE_LOWER then the elements of AP are copied into the lower triangular part of the triangular matrix A and the upper part of A is left untouched. If uplo == CUBLAS_FILL_MODE_UPPER then the elements of AP are copied into the upper triangular part of the triangular matrix A and the lower part of A is left untouched. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix AP contains lower or upper part of matrix A . n input number of rows and columns of matrix A . AP device input <type> array with \\(A\\) stored in packed format. A device output <type> array of dimensions lda x n , with lda>=max(1,n) . The opposite side of A is left untouched. lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stpttr , dtpttr , ctpttr , ztpttr 2.8.10. cublas<t>trttp()  cublasStatus_t cublasStrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * A , int lda , float * AP ); cublasStatus_t cublasDtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * A , int lda , double * AP ); cublasStatus_t cublasCtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * A , int lda , cuComplex * AP ); cublasStatus_t cublasZtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * AP ); This function performs the conversion from the triangular format to the triangular packed format If uplo == CUBLAS_FILL_MODE_LOWER then the lower triangular part of the triangular matrix A is copied into the array AP . If uplo == CUBLAS_FILL_MODE_UPPER then then the upper triangular part of the triangular matrix A is copied into the array AP . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates which matrix A lower or upper part is referenced. n input number of rows and columns of matrix A . A device input <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . AP device output <type> array with \\(A\\) stored in packed format. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strttp , dtrttp , ctrttp , ztrttp 2.8.11. cublas<t>gemmEx()  cublasStatus_t cublasSgemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const float * beta , void * C , cudaDataType_t Ctype , int ldc ) cublasStatus_t cublasCgemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const cuComplex * beta , void * C , cudaDataType_t Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemm . In this function the input matrices and output matrices can have a lower precision but the computation is still done in the type <t> . For example, in the type float for cublasSgemmEx() and in the type cuComplex for cublasCgemmEx() . \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input enumerant specifying the datatype of matrix A . lda input leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input enumerant specifying the datatype of matrix B . ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . Ctype input enumerant specifying the datatype of matrix C . ldc input leading dimension of a two-dimensional array used to store the matrix C . The matrix types combinations supported for cublasSgemmEx() are listed below: C A/B CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_8I CUDA_R_16BF CUDA_R_16F CUDA_R_32F The matrix types combinations supported for cublasCgemmEx() are listed below : C A/B CUDA_C_32F CUDA_C_8I CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ARCH_MISMATCH cublasCgemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0 CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters Atype , Btype and Ctype is not supported CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemm For more information about the numerical behavior of some GEMM algorithms, refer to the GEMM Algorithms Numerical Behavior section. 2.8.12. cublasGemmEx()  cublasStatus_t cublasGemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const void * beta , void * C , cudaDataType_t Ctype , int ldc , cublasComputeType_t computeType , cublasGemmAlgo_t algo ) #if defined(__cplusplus) cublasStatus_t cublasGemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType Atype , int lda , const void * B , cudaDataType Btype , int ldb , const void * beta , void * C , cudaDataType Ctype , int ldc , cudaDataType computeType , cublasGemmAlgo_t algo ) #endif This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemm that allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Supported combinations of arguments are listed further down in this section. Note The second variant of cublasGemmEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t . C applications would still compile with the updated function signature. This function is only supported on devices with compute capability 5.0 or later. \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A ) that is non- or (conj.) transpose. transb input Operation op( B ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A ) and C . n input Number of columns of matrix op( B ) and C . k input Number of columns of op( A ) and rows of op( B ). alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input Enumerant specifying the datatype of matrix B . ldb input Leading dimension of two-dimensional array used to store matrix B . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of a two-dimensional array used to store the matrix C . computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F Note CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC compute types are only supported with A, B being 4-byte aligned and lda, ldb being multiples of 4. For better performance, it is also recommended that IMMA kernels requirements for a regular data ordering listed here are met. The possible error values returned by this function and their meanings are listed in the following table. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or Atype or Btype or Ctype or algo is not supported CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. Starting with release 11.2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library. Also refer to: sgemm. For more information about the numerical behavior of some GEMM algorithms, refer to the GEMM Algorithms Numerical Behavior section. 2.8.13. cublasGemmBatchedEx()  cublasStatus_t cublasGemmBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * const Aarray [], cudaDataType_t Atype , int lda , const void * const Barray [], cudaDataType_t Btype , int ldb , const void * beta , void * const Carray [], cudaDataType_t Ctype , int ldc , int batchCount , cublasComputeType_t computeType , cublasGemmAlgo_t algo ) #if defined(__cplusplus) cublasStatus_t cublasGemmBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * const Aarray [], cudaDataType Atype , int lda , const void * const Barray [], cudaDataType Btype , int ldb , const void * beta , void * const Carray [], cudaDataType Ctype , int ldc , int batchCount , cudaDataType computeType , cublasGemmAlgo_t algo ) #endif This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemmBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrix arrays, the precision of computation and the GEMM algorithm to be run. Like cublas<t>gemmBatched , the batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. Supported combinations of arguments are listed further down in this section. Note The second variant of cublasGemmBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t . C applications would still compile with the updated function signature. \\(C\\lbrack i\\rbrack = \\alpha\\text{op}(A\\lbrack i\\rbrack)\\text{op}(B\\lbrack i\\rbrack) + \\beta C\\lbrack i\\rbrack,\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A[i] ) that is non- or (conj.) transpose. transb input Operation op( B[i] ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A[i] ) and C[i] . n input Number of columns of matrix op( B[i] ) and C[i] . k input Number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. Aarray device input Array of pointers to <Atype> array, with each array of dim. lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. All pointers must meet certain alignment criteria. Please see below for details. Atype input Enumerant specifying the datatype of Aarray . lda input Leading dimension of two-dimensional array used to store the matrix A[i] . Barray device input Array of pointers to <Btype> array, with each array of dim. ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. All pointers must meet certain alignment criteria. Please see below for details. Btype input Enumerant specifying the datatype of Barray . ldb input Leading dimension of two-dimensional array used to store matrix B[i] . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. If beta==0 , C[i] does not have to be a valid input. Carray device in/out Array of pointers to <Ctype> array. It has dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. Ctype input Enumerant specifying the datatype of Carray . ldc input Leading dimension of a two-dimensional array used to store each matrix C[i] . batchCount input Number of pointers contained in Aarray, Barray and Carray. computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F If Atype is CUDA_R_16F or CUDA_R_16BF , or computeType is any of the FAST options, or when math mode or algo enable fast math modes, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k%8==0 then ensure intptr_t(ptr) % 16 == 0 , if k%2==0 then ensure intptr_t(ptr) % 4 == 0 . Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4. For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal to or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or Atype or Btype or Ctype or algo or computeType is not supported CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. Also refer to: sgemm. 2.8.14. cublasGemmStridedBatchedEx()  cublasStatus_t cublasGemmStridedBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType_t Atype , int lda , long long int strideA , const void * B , cudaDataType_t Btype , int ldb , long long int strideB , const void * beta , void * C , cudaDataType_t Ctype , int ldc , long long int strideC , int batchCount , cublasComputeType_t computeType , cublasGemmAlgo_t algo ) #if defined(__cplusplus) cublasStatus_t cublasGemmStridedBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType Atype , int lda , long long int strideA , const void * B , cudaDataType Btype , int ldb , long long int strideB , const void * beta , void * C , cudaDataType Ctype , int ldc , long long int strideC , int batchCount , cudaDataType computeType , cublasGemmAlgo_t algo ) #endif This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemmStridedBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Like cublas<t>gemmStridedBatched , the batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. Pointers to A, B and C matrices for the first instance are passed to the function by the user along with the offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances. Note The second variant of cublasGemmStridedBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType_t instead of cublasComputeType_t . C applications would still compile with the updated function signature. \\(C + i*{strideC} = \\alpha\\text{op}(A + i*{strideA})\\text{op}(B + i*{strideB}) + \\beta(C + i*{strideC}),\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1] . The unit for the offset is number of elements and must not be zero . Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A[i] ) that is non- or (conj.) transpose. transb input Operation op( B[i] ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A[i] ) and C[i] . n input Number of columns of matrix op( B[i] ) and C[i] . k input Number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. A device input Pointer to <Atype> matrix, A, corresponds to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of A . lda input Leading dimension of two-dimensional array used to store the matrix A[i] . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] . B device input Pointer to <Btype> matrix, B, corresponds to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input Enumerant specifying the datatype of B . ldb input Leading dimension of two-dimensional array used to store matrix B[i] . strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. If beta==0 , C[i] does not have to be a valid input. C device in/out Pointer to <Ctype> matrix, C, corresponds to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. Ctype input Enumerant specifying the datatype of C . ldc input Leading dimension of a two-dimensional array used to store each matrix C[i] . strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] . batchCount input Number of GEMMs to perform in the batch. computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmStridedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4. For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or Atype or Btype or Ctype or algo or computeType is not supported CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU Also refer to: sgemm. 2.8.15. cublasGemmGroupedBatchedEx()  cublasStatus_t cublasGemmGroupedBatchedEx ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const void * alpha_array , const void * const Aarray [], cudaDataType_t Atype , const int lda_array [], const void * const Barray [], cudaDataType_t Btype , const int ldb_array [], const void * beta_array , void * const Carray [], cudaDataType_t Ctype , const int ldc_array [], int group_count , const int group_size [], cublasComputeType_t computeType ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication on groups of matrices. A given group is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.  This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemmEx ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], Atype , lda_array [ i ], Barray [ idx ], Btype , ldb_array [ i ], beta_array [ i ], Carray [ idx ], Ctype , ldc_array [ i ], computeType , CUBLAS_GEMM_DEFAULT ); idx += 1 ; end end where \\(\\text{$\\mathrm{alpha\\_array}$}\\) and \\(\\text{$\\mathrm{beta\\_array}$}\\) are arrays of scaling factors, and \\(\\text{Aarray}\\) , \\(\\text{Barray}\\) and \\(\\text{Carray}\\) are arrays of pointers to matrices stored in column-major format.  For a given index, \\(\\text{idx}\\) , that is part of group \\(i\\) , the dimensions are: \\(\\text{op}(\\text{Aarray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{op}(\\text{Barray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{Carray}\\lbrack\\text{idx}\\rbrack\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) Note This API takes arrays of two different lengths.  The arrays of dimensions, leading dimensions, transpositions, and scaling factors are of length group_count and the arrays of matrices are of length problem_count where \\(\\text{$\\mathrm{problem\\_count}$} = \\sum_{i = 0}^{\\text{$\\mathrm{group\\_count}$} - 1} \\text{$\\mathrm{group\\_size}$}\\lbrack i\\rbrack\\) For matrix \\(A[\\text{idx}]\\) in group \\(i\\) \\(\\text{op}(A[\\text{idx}]) = \\left\\{ \\begin{matrix}\nA[\\text{idx}] & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA[\\text{idx}]^{T} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA[\\text{idx}]^{H} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B[\\text{idx}])\\) is defined similarly for matrix \\(B[\\text{idx}]\\) in group \\(i\\) . Note \\(C\\lbrack\\text{idx}\\rbrack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublasGemmBatchedEx() in different CUDA streams, rather than use this API. Param. Memory In/out Meaning Array Length handle input handle to the cuBLAS library context. transa_array host input array containing the operations, op( A[idx] ), that is non- or (conj.) transpose for each group. group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj.) transpose for each group. group_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group. group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group. group_count k_array host input array containing the number of columns of op( A[idx] ) and rows of op( B[idx] ) for each group. group_count alpha_array host input array containing the <type> scalar used for multiplication for each group. group_count Aarray device input array of pointers to <type> array, with each array of dim. lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count Atype input Enumerant specifying the datatype of A . lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group. group_count Barray device input array of pointers to <type> array, with each array of dim. ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count Btype input Enumerant specifying the datatype of B . ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group. group_count beta_array host input array containing the <type> scalar used for multiplication for each group. group_count Carray device in/out array of pointers to <type> array. It has dimensions ldc[i] x n[i] with ldc[i]>=max(1,m[i]) . Matrices C[idx] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. problem_count Ctype input Enumerant specifying the datatype of C . ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group. group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group. group_count computeType input Enumerant specifying the computation type. cublasGemmGroupedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F If Atype is CUDA_R_16F or CUDA_R_16BF or if the computeType is any of the FAST options, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is required that they meet the following rule: if (k * AtypeSize) % 16 == 0 then ensure intptr_t(ptr) % 16 == 0 , if (k * AtypeSize) % 4 == 0 then ensure intptr_t(ptr) % 4 == 0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count < 0 or if m_array[i] , n_array[i] , k_array[i] , group_size[i] < 0 or if transa_array[i] , transb_array[i] != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda_array[i] < max(1, m_array[i] ) if transa_array[i] == CUBLAS_OP_N and lda_array[i] < max(1, k_array[i] ) otherwise or if ldb_array[i] < max(1, k_array[i] ) if transb_array[i] == CUBLAS_OP_N and ldb_array[i] < max(1, n_array[i] ) otherwise or if ldc_array[i] < max(1, m_array[i] ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_NOT_SUPPORTED the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE Atype or Btype or Ctype or computeType is not supported 2.8.16. cublasCsyrkEx()  cublasStatus_t cublasCsyrkEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCsyrk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input Operation op( A ) that is non- or transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A. beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCsyrkEx() are listed below: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 2.8.17. cublasCsyrk3mEx()  cublasStatus_t cublasCsyrk3mEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCsyrk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex . This routine is implemented using the Gauss complexity reduction algorithm which can lead to an increase in performance up to 25% This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input Operation op( A ) that is non- or transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A. beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCsyrk3mEx() are listed below : A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 2.8.18. cublasCherkEx()  cublasStatus_t cublasCherkEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCherk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input Operation op( A ) that is non- or (conj.) transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCherkEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: cherk 2.8.19. cublasCherk3mEx()  cublasStatus_t cublasCherk3mEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCherk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex . This routine is implemented using the Gauss complexity reduction algorithm which can lead to an increase in performance up to 25% This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input Operation op( A ) that is non- or (conj.) transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCherk3mEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: cherk 2.8.20. cublasNrm2Ex()  cublasStatus_t cublasNrm2Ex ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , void * result , cudaDataType resultType , cudaDataType executionType ) This function supports the 64-bit Integer Interface . This function is an API generalization of the routine cublas<t>nrm2 where input data, output data and compute type can be specified independently. This function computes the Euclidean norm of the vector x . The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \\(\\sqrt{\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack j\\rbrack \\times \\mathbf{x}\\lbrack j\\rbrack} \\right)}\\) where \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) in exact arithmetic. Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with n elements. xType input enumerant specifying the datatype of vector x . incx input stride between consecutive elements of x . result host or device output the resulting norm, which is 0.0 if n,incx<=0 . resultType input enumerant specifying the datatype of the result . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasNrm2Ex() are listed below : x result execution CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_R_64F CUDA_R_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType , resultType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE If xType or resultType or executionType is not supported or result == NULL For references please refer to: snrm2 , dnrm2 , scnrm2 , dznrm2 2.8.21. cublasAxpyEx()  cublasStatus_t cublasAxpyEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , const void * x , cudaDataType xType , int incx , void * y , cudaDataType yType , int incy , cudaDataType executiontype ); This function supports the 64-bit Integer Interface . This function is an API generalization of the routine cublas<t>axpy where input data, output data and compute type can be specified independently. This function multiplies the vector x by the scalar \\(\\alpha\\) and adds it to the vector y overwriting the latest vector with the result. Hence, the performed operation is \\(\\mathbf{y}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack k\\rbrack + \\mathbf{y}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. n input Number of elements in the vector x and y . alpha host or device input <type> scalar used for multiplication. alphaType input Enumerant specifying the datatype of scalar alpha . x device input <type> vector with n elements. xType input Enumerant specifying the datatype of vector x . incx input Stride between consecutive elements of x . y device in/out <type> vector with n elements. yType input Enumerant specifying the datatype of vector y . incy input Stride between consecutive elements of y . executionType input Enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasAxpyEx() are listed in the following table: alpha x y execution CUDA_R_32F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , and executionType is not supported. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. CUBLAS_STATUS_INVALID_VALUE alphaType or xType or yType or executionType is not supported. For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.8.22. cublasDotEx()  cublasStatus_t cublasDotEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); cublasStatus_t cublasDotcEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); These functions support the 64-bit Integer Interface . These functions are an API generalization of the routines cublas<t>dot and cublas<t>dotc where input data, output data and compute type can be specified independently. Note: cublas<t>dotc is dot product conjugated, cublas<t>dotu is dot product unconjugated. This function computes the dot product of vectors x and y . Hence, the result is \\(\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack k\\rbrack \\times \\mathbf{y}\\lbrack j\\rbrack} \\right)\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. n input Number of elements in the vectors x and y . x device input <type> vector with n elements. xType input Enumerant specifying the datatype of vector x . incx input Stride between consecutive elements of x . y device input <type> vector with n elements. yType input Enumerant specifying the datatype of vector y . incy input Stride between consecutive elements of y . result host or device output The resulting dot product, which is 0.0 if n<=0 . resultType input Enumerant specifying the datatype of the result . executionType input Enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasDotEx() and cublasDotcEx() are listed below: x y result execution CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ALLOC_FAILED The reduction buffer could not be allocated. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , resultType and executionType is not supported. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. CUBLAS_STATUS_INVALID_VALUE xType or yType or resultType or executionType is not supported. For references please refer to: sdot , ddot , cdotu , cdotc , zdotu , zdotc 2.8.23. cublasRotEx()  cublasStatus_t cublasRotEx ( cublasHandle_t handle , int n , void * x , cudaDataType xType , int incx , void * y , cudaDataType yType , int incy , const void * c , /* host or device pointer */ const void * s , cudaDataType csType , cudaDataType executiontype ); This function supports the 64-bit Integer Interface . This function is an extension to the routine cublas<t>rot where input data, output data, cosine/sine type, and compute type can be specified independently. This function applies Givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \\(G = \\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\) to vectors x and y . Hence, the result is \\(\\mathbf{x}\\lbrack k\\rbrack = c \\times \\mathbf{x}\\lbrack k\\rbrack + s \\times \\mathbf{y}\\lbrack j\\rbrack\\) and \\(\\mathbf{y}\\lbrack j\\rbrack = - s \\times \\mathbf{x}\\lbrack k\\rbrack + c \\times \\mathbf{y}\\lbrack j\\rbrack\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device in/out <type> vector with n elements. xType input enumerant specifying the datatype of vector x . incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. yType input enumerant specifying the datatype of vector y . incy input stride between consecutive elements of y . c host or device input cosine element of the rotation matrix. s host or device input sine element of the rotation matrix. csType input enumerant specifying the datatype of c and s . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasRotEx() are listed below : executionType xType / yType csType CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_R_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.8.24. cublasScalEx()  cublasStatus_t cublasScalEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , void * x , cudaDataType xType , int incx , cudaDataType executionType ); This function supports the 64-bit Integer Interface . This function scales the vector x by the scalar \\(\\alpha\\) and overwrites it with the result. Hence, the performed operation is \\(\\mathbf{x}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . alpha host or device input <type> scalar used for multiplication. alphaType input enumerant specifying the datatype of scalar alpha . x device in/out <type> vector with n elements. xType input enumerant specifying the datatype of vector x . incx input stride between consecutive elements of x . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasScalEx() are listed below : alpha x execution CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE alphaType or xType or executionType is not supported For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 3. Using the cuBLASLt API  3.1. General Description  The cuBLASLt library is a new lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API. This new library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. Once a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data. Note The cuBLASLt library does not guarantee the support of all possible sizes and configurations, however, since CUDA 12.2 update 2, the problem size limitations on m, n, and batch size have been largely resolved. The main focus of the library is to provide the most performant kernels, which might have some implied limitations. Some non-standard configurations may require a user to handle them manually, typically by decomposing the problem into smaller parts (see Problem Size Limitations ). 3.1.1. Problem Size Limitations  There are inherent problem size limitations that are a result of limitations in CUDA grid dimensions.  For example, many kernels do not support batch sizes greater than 65535 due to a limitation on the z dimension of a grid.  There are similar restriction on the m and n values for a given problem. In cases where a problem cannot be run by a single kernel, cuBLASLt will attempt to decompose the problem into multiple sub-problems and solve it by running the kernel on each sub-problem. There are some restrictions on cuBLASLt internal problem decomposition which are summarized below: Amax computations are not supported.  This means that CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER must be left unset (see cublasLtMatmulDescAttributes_t ) All matrix layouts must have CUBLASLT_MATRIX_LAYOUT_ORDER set to CUBLASLT_ORDER_COL (see cublasLtOrder_t ) cuBLASLt will not partition along the n dimension when CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_DRELU_BGRAD or CUBLASLT_EPILOGUE_DGELU_BGRAD (see cublasLtEpilogue_t ) To overcome these limitations, a user may want to partition the problem themself, launch kernels for each sub-problem, and compute any necessary reductions to combine the results. 3.1.2. Heuristics Cache  cuBLASLt uses heuristics to pick the most suitable matmul kernel for execution based on the problem sizes, GPU configuration, and other parameters. This requires performing some computations on the host CPU, which could take tens of microseconds. To overcome this overhead, it is recommended to query the heuristics once using cublasLtMatmulAlgoGetHeuristic() and then reuse the result for subsequent computations using cublasLtMatmul() . For the cases where querying heuristics once and then reusing them is not feasible, cuBLASLt implements a heuristics cache that maps matmul problems to kernels previously selected by heuristics. The heuristics cache uses an LRU-like eviction policy and is thread-safe. The user can control the heuristics cache capacity with the CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable or with the cublasLtHeuristicsCacheSetCapacity() function which has higher precedence. The capacity is measured in number of entries and might be rounded up to the nearest multiple of some factor for performance reasons. Each entry takes about 360 bytes but is subject to change. The default capacity is 8192 entries. Note Setting capacity to zero disables the cache completely. This can be useful for workloads that do not have a steady state and for which cache operations may have higher overhead than regular heuristics computations. Note The cache is not ideal for performance reasons, so it is sometimes necessary to increase its capacity 1.5x-2.x over the anticipated number of unique matmul problems to achieve a nearly perfect hit rate. See also: cublasLtHeuristicsCacheGetCapacity() , cublasLtHeuristicsCacheSetCapacity() . 3.1.3. cuBLASLt Logging  cuBLASLt logging mechanism can be enabled by setting the following environment variables before launching the target application: CUBLASLT_LOG_LEVEL=<level> , where <level> is one of the following levels: “0” - Off - logging is disabled (default) “1” - Error - only errors will be logged “2” - Trace - API calls that launch CUDA kernels will log their parameters and important information “3” - Hints - hints that can potentially improve the application’s performance “4” - Info - provides general information about the library execution, may contain details about heuristic status “5” - API Trace - API calls will log their parameter and important information CUBLASLT_LOG_MASK=<mask> , where <mask> is a combination of the following flags: “0” - Off “1” - Error “2” - Trace “4” - Hints “8” - Info “16” - API Trace For example, use CUBLASLT_LOG_MASK=5 to enable Error and Hints messages. CUBLASLT_LOG_FILE=<file_name> , where <file_name> is a path to a logging file.  File name may contain %i , that will be replaced with the process ID. For example <file_name>_%i.log . If CUBLASLT_LOG_FILE is not defined, the log messages are printed to stdout. Another option is to use the experimental cuBLASLt logging API. See: cublasLtLoggerSetCallback() , cublasLtLoggerSetFile() , cublasLtLoggerOpenFile() , cublasLtLoggerSetLevel() , cublasLtLoggerSetMask() , cublasLtLoggerForceDisable() 3.1.4. 8-bit Floating Point Data Types (FP8) Usage  FP8 was first introduced with Ada and Hopper GPUs (compute capability 8.9 and above) and is designed to further accelerate matrix multiplications. There are two types of FP8 available: CUDA_R_8F_E4M3 is designed to be accurate at a smaller dynamic range than half precision. The E4 and M3 represent a 4-bit exponent and a 3-bit mantissa respectively. For more details, see __nv__fp8__e4m3 . CUDA_R_8F_E5M2 is designed to be accurate at a similar dynamic range as half precision. The E5 and M2 represent a 5-bit exponent and a 2-bit mantissa respectively. For more information see __nv__fp8__e5m2 . Note Unless otherwise stated, FP8 refers to both CUDA_R_8F_E4M3 and CUDA_R_8F_E5M2 . In order to maintain accurate FP8 matrix multiplications, we define native compute FP8 matrix multiplication as follows: \\[D = scale_D \\cdot (\\alpha \\cdot scale_A \\cdot scale_B \\cdot \\text{op}(A) \\text{op}(B) + \\beta \\cdot scale_C \\cdot C)\\] where A, B, and C are input matrices, and scaleA, scaleB, scaleC, scaleD, alpha, and beta are input scalars. This differs from the other matrix multiplication routines because of this addition of scaling factors for each matrix. The scaleA, scaleB, and scaleC are used for de-quantization, and scaleD is used for quantization. Note that all the scaling factors are applied multiplicatively. This means that sometimes it is necessary to use a scaling factor or its reciprocal depending on the context in which it is applied. For more information on FP8, see cublasLtMatmul() and cublasLtMatmulDescAttributes_t . For FP8 matrix multiplications, epilogues and amaxD may be computed as follows: \\[\\begin{split}D_{temp}, Aux_{temp} & = \\mathop{Epilogue}(\\alpha \\cdot scale_A \\cdot scale_B \\cdot \\text{op}(A) \\text{op}(B) + \\beta \\cdot scale_C \\cdot C) \\\\\namax_{D} & = \\mathop{absmax}(D_{temp}) \\\\\namax_{Aux} & = \\mathop{absmax}(Aux_{temp}) \\\\\nD & = scale_D * D_{temp} \\\\\nAux & = scale_{Aux} * Aux_{temp} \\\\\\end{split}\\] Here Aux is an auxiliary output of an epilogue function like GELU, scaleAux is an optional scaling factor that can be applied to Aux, and amaxAux is the maximum absolute value in Aux before scaling. For more information, see attributes CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER in cublasLtMatmulDescAttributes_t . 3.1.5. Disabling CPU Instructions  As mentioned in the Heuristics Cache section, cuBLASLt heuristics perform some compute-intensive operations on the host CPU.\nTo speed-up the operations, the implementation detects CPU capabilities and may use special instructions, such as Advanced Vector Extensions (AVX) on x86-64 CPUs.\nHowever, in some rare cases this might be not desirable. For instance, using advanced instructions may result in CPU running at a lower frequency, which would affect performance of the other host code. The user can optionally instruct the cuBLASLt library to not use some CPU instructions with the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable or with the cublasLtDisableCpuInstructionsSetMask() function which has higher precedence.\nThe default mask is 0, meaning that there are no restrictions. Please check cublasLtDisableCpuInstructionsSetMask() for more information. 3.1.6. Atomics Synchronization  Atomics synchronization allows optimizing matmul workloads by enabling cublasLtMatmul() to have a producer or consumer relationship with another concurrently running kernel. This allows overlapping computation and communication with a finer granularity. Conceptually, matmul is provided with an array containing 32-bit integer counters, and then: In the consumer mode, either matrix A is partitioned into chunks by rows, or matrix B is partitioned into chunks by columns 1 . A chunk can be read from memory and used in computations only when the corresponding atomic counter reaches value of 0. The producer should execute a memory fence to ensure that the written value is visible to the concurrently running matmul kernel 2 . In the producer mode, the output matrix C (or D in the out-of-place mode), is partitioned by rows or columns, and after a chunk is computed, the corresponding atomic counter is set to 0. Each counter must be initialized to 1 before the matmul kernel runs. 1 The current implementation allows partitioning either the rows or the columns of the matrixes, but not both. Batched cases are not supported. 2 One possible implementation of a memory fence is cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope::thread_scope_device) (see cuda::atomic_thread_fence() for more details). The array of counters are passed to matmuls via the CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER compute descriptor attributes for the consumer and producer modes respectively 3 . The arrays must have a sufficient number of elements for all the chunks. 3 The current implementation allows to only enable either the producer or the consumer mode, but not both. Matmul will return an error if both input and output counter pointers to a non-NULL value. The number of chunks is controlled by CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS compute descriptor attributes. Both of these attributes must be set to a value greater than zero for the feature to be enabled. For the column-major layout, the number of chunks must satisfy: \\[\\begin{split}0 \\leq \\text{$\\mathrm{NUM\\_CHUNKS\\_ROWS}$} \\leq & \\mathop{\\text{floor}}\\left( \\frac{\\text{M}}{\\text{$\\mathrm{TILE\\_SIZE\\_M}$} * \\text{$\\mathrm{CLUSTER\\_SHAPE\\_M}$}} \\right) \\\\\n0 \\leq \\text{$\\mathrm{NUM\\_CHUNKS\\_COLS}$} \\leq & \\mathop{\\text{floor}}\\left( \\frac{\\text{N}}{\\text{$\\mathrm{TILE\\_SIZE\\_N}$} * \\text{$\\mathrm{CLUSTER\\_SHAPE\\_N}$}} \\right)\\end{split}\\] For row-major layout, M and N in tile size and cluster shape must be swapped. These restrictions mean that it is required to first query heuristic via cublasLtMatmulAlgoGetHeuristic() and inspect the result for tile and cluster shapes, and only then set the number of chunks. The pseudocode below shows the principles of operation: // The code below shows operation when partitioning over // rows assuming column-major layout and TN case. // // The case when partitioning is done over columns or // row-major case are handled in a similar fashion, // with the main difference being the offsets // computations. // // Note that the actual implementation does not // guarantee in which order the chunks are computed, // and may employ various optimizations to improve // overall performance. // // Here: //   - A, B, C -- input matrices in the column-major layout //   - lda -- leading dimension of matrix A //   - M, N, K -- the original problem dimensions //   - counters_in[] and counters_out[] -- the arrays of //     input and output atomic counters // for ( int i = 0 ; i < NUM_CHUNKS_ROWS ; i ++ ) { // Consumer: wait for the input counter to become 0 if ( consumer ) { while ( counters_in [ i ] != 0 ); // spin } // compute chunk dimensions chunk_m_begin = floor (( double ) M / NUM_CHUNKS_ROWS * i ); chunk_m_end = floor (( double ) M / NUM_CHUNKS_ROWS * ( i + 1 )); chunk_m = chunk_m_end - chunk_m_begin ; // Compute the current chunk matmul ( chunk_m , N , K , A [ chunk_m_begin * lda ], // A is col-major transposed B , // B is not partitioned C [ chunk_m_begin ] // C is col-major non-transposed ); // Producer: set the counter to 0 when done if ( producer ) { counters_out [ i ] = 0 ; // make the written value visible to the consumer kernel memory_fence (); } } It should be noted that, in general, CUDA programming model provides few kernel co-scheduling guarantees. Thus, use of this feature requires careful orchestration of producer and consumer kernels launch order and resource availability, as it easy to create a deadlock situation. A deadlock may occur in the following cases (this is not an exhaustive list): If a producer kernel cannot start because consumer kernel was launched first and is occupying some of SMs that are needed by the producer kernel to launch. It is strongly recommended to set CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET to carve out some SMs for non-matmul (typically communication) kernels to execute on. If cudaDeviceSynchronize() is called after consumer kernel starts but before the producer kernel does. When lazy module loading is enabled, and producer kernel cannot be loaded while the consumer kernel is running due to locking in the CUDA runtime library. Both kernels also must be loaded before they are run together to avoid this situation. Using CUDA Graphs is another way to avoid deadlocks due to lazy loading. Note This feature is aimed at advanced users and is only available on Hopper architecture for FP8 non-batched cases with fast accumulation mode enabled, and is considered to have beta quality due to the large number of restrictions on its use. 3.2. cuBLASLt Code Examples  Please visit https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLASLt for updated code examples. 3.3. cuBLASLt Datatypes Reference  3.3.1. cublasLtClusterShape_t  cublasLtClusterShape_t is an enumerated type used to configure thread block cluster dimensions. Thread block clusters add an optional hierarchical level and are made up of thread blocks. Similar to thread blocks, these can be one, two, or three-dimensional. See also Thread Block Clusters . Value Description CUBLASLT_CLUSTER_SHAPE_AUTO Cluster shape is automatically selected. CUBLASLT_CLUSTER_SHAPE_1x1x1 Cluster shape is 1 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_1x2x1 Cluster shape is 1 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_1x4x1 Cluster shape is 1 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_2x1x1 Cluster shape is 2 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_2x2x1 Cluster shape is 2 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_2x4x1 Cluster shape is 2 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_4x1x1 Cluster shape is 4 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_4x2x1 Cluster shape is 4 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_4x4x1 Cluster shape is 4 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_1x8x1 Cluster shape is 1 x 8 x 1. CUBLASLT_CLUSTER_SHAPE_8x1x1 Cluster shape is 8 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_2x8x1 Cluster shape is 2 x 8 x 1. CUBLASLT_CLUSTER_SHAPE_8x2x1 Cluster shape is 8 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_1x16x1 Cluster shape is 1 x 16 x 1. CUBLASLT_CLUSTER_SHAPE_16x1x1 Cluster shape is 16 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_1x3x1 Cluster shape is 1 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_1x5x1 Cluster shape is 1 x 5 x 1. CUBLASLT_CLUSTER_SHAPE_1x6x1 Cluster shape is 1 x 6 x 1. CUBLASLT_CLUSTER_SHAPE_1x7x1 Cluster shape is 1 x 7 x 1. CUBLASLT_CLUSTER_SHAPE_1x9x1 Cluster shape is 1 x 9 x 1. CUBLASLT_CLUSTER_SHAPE_1x10x1 Cluster shape is 1 x 10 x 1. CUBLASLT_CLUSTER_SHAPE_1x11x1 Cluster shape is 1 x 11 x 1. CUBLASLT_CLUSTER_SHAPE_1x12x1 Cluster shape is 1 x 12 x 1. CUBLASLT_CLUSTER_SHAPE_1x13x1 Cluster shape is 1 x 13 x 1. CUBLASLT_CLUSTER_SHAPE_1x14x1 Cluster shape is 1 x 14 x 1. CUBLASLT_CLUSTER_SHAPE_1x15x1 Cluster shape is 1 x 15 x 1. CUBLASLT_CLUSTER_SHAPE_2x3x1 Cluster shape is 2 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_2x5x1 Cluster shape is 2 x 5 x 1. CUBLASLT_CLUSTER_SHAPE_2x6x1 Cluster shape is 2 x 6 x 1. CUBLASLT_CLUSTER_SHAPE_2x7x1 Cluster shape is 2 x 7 x 1. CUBLASLT_CLUSTER_SHAPE_3x1x1 Cluster shape is 3 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_3x2x1 Cluster shape is 3 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_3x3x1 Cluster shape is 3 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_3x4x1 Cluster shape is 3 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_3x5x1 Cluster shape is 3 x 5 x 1. CUBLASLT_CLUSTER_SHAPE_4x3x1 Cluster shape is 4 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_5x1x1 Cluster shape is 5 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_5x2x1 Cluster shape is 5 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_5x3x1 Cluster shape is 5 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_6x1x1 Cluster shape is 6 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_6x2x1 Cluster shape is 6 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_7x1x1 Cluster shape is 7 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_7x2x1 Cluster shape is 7 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_9x1x1 Cluster shape is 9 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_10x1x1 Cluster shape is 10 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_11x1x1 Cluster shape is 11 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_12x1x1 Cluster shape is 12 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_13x1x1 Cluster shape is 13 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_14x1x1 Cluster shape is 14 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_15x1x1 Cluster shape is 15 x 1 x 1. 3.3.2. cublasLtEpilogue_t  The cublasLtEpilogue_t is an enum type to set the postprocessing options for the epilogue. Value Description CUBLASLT_EPILOGUE_DEFAULT = 1 No special postprocessing, just scale and quantize the results if necessary. CUBLASLT_EPILOGUE_RELU = 2 Apply ReLU point-wise transform to the results ( x := max(x, 0) ). CUBLASLT_EPILOGUE_RELU_AUX = CUBLASLT_EPILOGUE_RELU | 128 Apply ReLU point-wise transform to the results ( x := max(x, 0) ). This epilogue mode produces an extra output, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_BIAS = 4 Apply (broadcast) bias from the bias vector. Bias vector length must match matrix D rows, and it must be packed (such as stride between vector elements is 1). Bias vector is broadcast to all columns and added before applying the final postprocessing. CUBLASLT_EPILOGUE_RELU_BIAS = CUBLASLT_EPILOGUE_RELU | CUBLASLT_EPILOGUE_BIAS Apply bias and then ReLU transform. CUBLASLT_EPILOGUE_RELU_AUX_BIAS = CUBLASLT_EPILOGUE_RELU_AUX | CUBLASLT_EPILOGUE_BIAS Apply bias and then ReLU transform. This epilogue mode produces an extra output, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DRELU = 8 | 128 Apply ReLu gradient to matmul output. Store ReLu gradient in the output matrix. This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DRELU_BGRAD = CUBLASLT_EPILOGUE_DRELU | 16 Apply independently ReLu and Bias gradient to matmul output. Store ReLu gradient in the output matrix, and Bias gradient in the bias buffer (see CUBLASLT_MATMUL_DESC_BIAS_POINTER). This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_GELU = 32 Apply GELU point-wise transform to the results ( x := GELU(x) ). CUBLASLT_EPILOGUE_GELU_AUX = CUBLASLT_EPILOGUE_GELU | 128 Apply GELU point-wise transform to the results ( x := GELU(x) ). This epilogue mode outputs GELU input as a separate matrix (useful for training). See CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_GELU_BIAS = CUBLASLT_EPILOGUE_GELU | CUBLASLT_EPILOGUE_BIAS Apply Bias and then GELU transform 4 . CUBLASLT_EPILOGUE_GELU_AUX_BIAS = CUBLASLT_EPILOGUE_GELU_AUX | CUBLASLT_EPILOGUE_BIAS Apply Bias and then GELU transform 4 . This epilogue mode outputs GELU input as a separate matrix (useful for training). See CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DGELU = 64 | 128 Apply GELU gradient to matmul output. Store GELU gradient in the output matrix. This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DGELU_BGRAD = CUBLASLT_EPILOGUE_DGELU | 16 Apply independently GELU and Bias gradient to matmul output. Store GELU gradient in the output matrix, and Bias gradient in the bias buffer (see CUBLASLT_MATMUL_DESC_BIAS_POINTER). This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_BGRADA = 256 Apply Bias gradient to the input matrix A. The bias size corresponds to the number of rows of the matrix D. The reduction happens over the GEMM’s “k” dimension. Store Bias gradient in the bias buffer, see CUBLASLT_MATMUL_DESC_BIAS_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_BGRADB = 512 Apply Bias gradient to the input matrix B. The bias size corresponds to the number of columns of the matrix D. The reduction happens over the GEMM’s “k” dimension. Store Bias gradient in the bias buffer, see CUBLASLT_MATMUL_DESC_BIAS_POINTER of cublasLtMatmulDescAttributes_t . NOTES: 4 ( 1 , 2 ) GELU (Gaussian Error Linear Unit) is approximated by: \\({0.5}x\\left( 1 + \\text{tanh}\\left( \\sqrt{2/\\pi}\\left( x + {0.044715}x^{3} \\right) \\right) \\right)\\) 3.3.3. cublasLtHandle_t  The cublasLtHandle_t type is a pointer type to an opaque structure holding the cuBLASLt  library context. Use cublasLtCreate() to initialize the cuBLASLt library context and return a handle to an opaque structure holding the cuBLASLt library context, and use cublasLtDestroy() to destroy a previously created cuBLASLt library context descriptor and release the resources. Note cuBLAS handle ( cublasHandle_t ) encapsulates a cuBLASLt handle. Any valid cublasHandle_t can be used in place of cublasLtHandle_t with a simple cast. However, unlike a cuBLAS handle, a cuBLASLt handle is not tied to any particular CUDA context. 3.3.4. cublasLtLoggerCallback_t  cublasLtLoggerCallback_t is a callback function pointer type. A callback function can be set using cublasLtLoggerSetCallback() . Parameters : Parameter Memory Input / Output Description logLevel Output See cuBLASLt Logging . functionName Output The name of the API that logged this message. message Output The log message. 3.3.5. cublasLtMatmulAlgo_t  cublasLtMatmulAlgo_t is an opaque structure holding the description of the matrix multiplication algorithm. This structure can be trivially serialized and later restored for use with the same version of cuBLAS library to save on selecting the right configuration again. 3.3.6. cublasLtMatmulAlgoCapAttributes_t  cublasLtMatmulAlgoCapAttributes_t enumerates matrix multiplication algorithm capability attributes that can be retrieved from an initialized cublasLtMatmulAlgo_t descriptor using cublasLtMatmulAlgoCapGetAttribute() . Value Description Data Type CUBLASLT_ALGO_CAP_SPLITK_SUPPORT Support for split-K. Boolean (0 or 1) to express if split-K implementation is supported. 0 means no support, and supported otherwise. See CUBLASLT_ALGO_CONFIG_SPLITK_NUM of cublasLtMatmulAlgoConfigAttributes_t . int32_t CUBLASLT_ALGO_CAP_REDUCTION_SCHEME_MASK Mask to express the types of reduction schemes supported, see cublasLtReductionScheme_t . If the reduction scheme is not masked out then it is supported. For example: int isReductionSchemeComputeTypeSupported ? (reductionSchemeMask & CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE) == CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE ? 1 : 0; uint32_t CUBLASLT_ALGO_CAP_CTA_SWIZZLING_SUPPORT Support for CTA-swizzling. Boolean (0 or 1) to express if CTA-swizzling implementation is supported. 0 means no support, and 1 means supported value of 1; other values are reserved. See also CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING of cublasLtMatmulAlgoConfigAttributes_t . uint32_t CUBLASLT_ALGO_CAP_STRIDED_BATCH_SUPPORT Support strided batch. 0 means no support, supported otherwise. int32_t CUBLASLT_ALGO_CAP_OUT_OF_PLACE_RESULT_SUPPORT Support results out of place (D != C in D = alpha.A.B + beta.C). 0 means no support, supported otherwise. int32_t CUBLASLT_ALGO_CAP_UPLO_SUPPORT Syrk (symmetric rank k update)/herk (Hermitian rank k update) support (on top of regular gemm). 0 means no support, supported otherwise. int32_t CUBLASLT_ALGO_CAP_TILE_IDS The tile ids possible to use. See cublasLtMatmulTile_t . If no tile ids are supported then use CUBLASLT_MATMUL_TILE_UNDEFINED. Use cublasLtMatmulAlgoCapGetAttribute() with sizeInBytes = 0 to query the actual count. Array of uint32_t CUBLASLT_ALGO_CAP_STAGES_IDS The stages ids possible to use. See cublasLtMatmulStages_t . If no stages ids are supported then use CUBLASLT_MATMUL_STAGES_UNDEFINED. Use cublasLtMatmulAlgoCapGetAttribute() with sizeInBytes = 0 to query the actual count. Array of uint32_t CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX Custom option range is from 0 to CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX (inclusive). See CUBLASLT_ALGO_CONFIG_CUSTOM_OPTION of cublasLtMatmulAlgoConfigAttributes_t . int32_t CUBLASLT_ALGO_CAP_MATHMODE_IMPL Indicates whether the algorithm is using regular compute or tensor operations. 0 means regular compute, 1 means tensor operations.\nDEPRECATED int32_t CUBLASLT_ALGO_CAP_GAUSSIAN_IMPL Indicate whether the algorithm implements the Gaussian optimization of complex matrix multiplication. 0 means regular compute; 1 means Gaussian. See cublasMath_t .\nDEPRECATED int32_t CUBLASLT_ALGO_CAP_CUSTOM_MEMORY_ORDER Indicates whether the algorithm supports custom (not COL or ROW memory order). 0 means only COL and ROW memory order is allowed, non-zero means that algo might have different requirements. See cublasLtOrder_t . int32_t CUBLASLT_ALGO_CAP_POINTER_MODE_MASK Bitmask enumerating the pointer modes the algorithm supports. See cublasLtPointerModeMask_t . uint32_t CUBLASLT_ALGO_CAP_EPILOGUE_MASK Bitmask enumerating the kinds of postprocessing algorithm supported in the epilogue. See cublasLtEpilogue_t . uint32_t CUBLASLT_ALGO_CAP_LD_NEGATIVE Support for negative ld for all of the matrices. 0 means no support, supported otherwise. uint32_t CUBLASLT_ALGO_CAP_NUMERICAL_IMPL_FLAGS Details about algorithm’s implementation that affect it’s numerical behavior. See cublasLtNumericalImplFlags_t . uint64_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_A_BYTES Minimum alignment required for A matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_B_BYTES Minimum alignment required for B matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_C_BYTES Minimum alignment required for C matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_D_BYTES Minimum alignment required for D matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_ATOMIC_SYNC Support for synchronization via atomic counters. See Atomics Synchronization . int32_t 3.3.7. cublasLtMatmulAlgoConfigAttributes_t  cublasLtMatmulAlgoConfigAttributes_t is an enumerated type that contains the configuration attributes for cuBLASLt matrix multiply algorithms. The configuration attributes are algorithm-specific, and can be set. The attributes configuration of a given algorithm should agree with its capability attributes. Use cublasLtMatmulAlgoConfigGetAttribute() and cublasLtMatmulAlgoConfigSetAttribute() to get and set the attribute value of a matmul algorithm descriptor. Value Description Data Type CUBLASLT_ALGO_CONFIG_ID Read-only attribute. Algorithm index. See cublasLtMatmulAlgoGetIds() . Set by cublasLtMatmulAlgoInit() . int32_t CUBLASLT_ALGO_CONFIG_TILE_ID Tile id. See cublasLtMatmulTile_t . Default: CUBLASLT_MATMUL_TILE_UNDEFINED . uint32_t CUBLASLT_ALGO_CONFIG_STAGES_ID stages id, see cublasLtMatmulStages_t . Default: CUBLASLT_MATMUL_STAGES_UNDEFINED . uint32_t CUBLASLT_ALGO_CONFIG_SPLITK_NUM Number of K splits. If the number of K splits is greater than one, SPLITK_NUM parts of matrix multiplication will be computed in parallel. The results will be accumulated according to CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME . uint32_t CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME Reduction scheme to use when splitK value > 1. Default: CUBLASLT_REDUCTION_SCHEME_NONE . See cublasLtReductionScheme_t . uint32_t CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING Enable/Disable CTA swizzling. Change mapping from CUDA grid coordinates to parts of the matrices. Possible values: 0 and 1; other values reserved. uint32_t CUBLASLT_ALGO_CONFIG_CUSTOM_OPTION Custom option value. Each algorithm can support some custom options that don’t fit the description of the other configuration attributes. See the CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX of cublasLtMatmulAlgoCapAttributes_t for the accepted range for a specific case. uint32_t CUBLASLT_ALGO_CONFIG_INNER_SHAPE_ID Inner shape ID. Refer to cublasLtMatmulInnerShape_t. Default: CUBLASLT_MATMUL_INNER_SHAPE_UNDEFINED . uint16_t CUBLASLT_ALGO_CONFIG_CLUSTER_SHAPE_ID Cluster shape ID. Refer to cublasLtClusterShape_t. Default: CUBLASLT_CLUSTER_SHAPE_AUTO . uint16_t 3.3.8. cublasLtMatmulDesc_t  The cublasLtMatmulDesc_t is a pointer to an opaque structure holding the description of the matrix multiplication operation cublasLtMatmul() . A descriptor can be created by calling cublasLtMatmulDescCreate() and destroyed by calling cublasLtMatmulDescDestroy() . 3.3.9. cublasLtMatmulDescAttributes_t  cublasLtMatmulDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix multiply operation. Use cublasLtMatmulDescGetAttribute() and cublasLtMatmulDescSetAttribute() to get and set the attribute value of a matmul descriptor. Attribute Name Description Data Type CUBLASLT_MATMUL_DESC_COMPUTE_TYPE Compute type. Defines the data type used for multiply and accumulate operations, and the accumulator during the matrix multiplication. See cublasComputeType_t . int32_t CUBLASLT_MATMUL_DESC_SCALE_TYPE Scale type. Defines the data type of the scaling factors alpha and beta . The accumulator value and the value from matrix C are typically converted to scale type before final scaling. The value is then converted from scale type to the type of matrix D before storing in memory. Default value is aligned with CUBLASLT_MATMUL_DESC_COMPUTE_TYPE. See cudaDataType_t . int32_t CUBLASLT_MATMUL_DESC_POINTER_MODE Specifies alpha and beta are passed by reference, whether they are scalars on the host or on the device, or device vectors. Default value is: CUBLASLT_POINTER_MODE_HOST (i.e., on the host). See cublasLtPointerMode_t . int32_t CUBLASLT_MATMUL_DESC_TRANSA Specifies the type of transformation operation that should be performed on matrix A. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_TRANSB Specifies the type of transformation operation that should be performed on matrix B. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_TRANSC Specifies the type of transformation operation that should be performed on matrix C. Currently only CUBLAS_OP_N is supported. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_FILL_MODE Indicates whether the lower or upper part of the dense matrix was filled, and consequently should be used by the function. Default value is: CUBLAS_FILL_MODE_FULL .See cublasFillMode_t . int32_t CUBLASLT_MATMUL_DESC_EPILOGUE Epilogue function. See cublasLtEpilogue_t . Default value is: CUBLASLT_EPILOGUE_DEFAULT . uint32_t CUBLASLT_MATMUL_DESC_BIAS_POINTER Bias or Bias gradient vector pointer in the device memory. Input vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BIAS , CUBLASLT_EPILOGUE_RELU_BIAS , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_GELU_BIAS , CUBLASLT_EPILOGUE_GELU_AUX_BIAS . Output vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_DRELU_BGRAD , CUBLASLT_EPILOGUE_DGELU_BGRAD , CUBLASLT_EPILOGUE_BGRADA . Output vector with length that matches the number of columns of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BGRADB . Bias vector elements are the same type as alpha and beta (see CUBLASLT_MATMUL_DESC_SCALE_TYPE in this table) when matrix D datatype is CUDA_R_8I and same as matrix D datatype otherwise. See the datatypes table under cublasLtMatmul() for detailed mapping. Default value is: NULL. void * / const void * CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE Stride (in elements) to the next bias or bias gradient vector for strided batch operations.  The default value is 0. int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER Pointer for epilogue auxiliary buffer. Output vector for ReLu bit-mask in forward pass when CUBLASLT_EPILOGUE_RELU_AUX or CUBLASLT_EPILOGUE_RELU_AUX_BIAS epilogue is used. Input vector for ReLu bit-mask in backward pass when CUBLASLT_EPILOGUE_DRELU or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Output of GELU input matrix in forward pass when CUBLASLT_EPILOGUE_GELU_AUX_BIAS epilogue is used. Input of GELU input matrix for backward pass when CUBLASLT_EPILOGUE_DGELU or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue is used. For aux data type, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE . Routines that don’t dereference this pointer, like cublasLtMatmulAlgoGetHeuristic() depend on its value to determine expected pointer alignment. Requires setting the CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD attribute. void * / const void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD Leading dimension for epilogue auxiliary buffer. ReLu bit-mask matrix leading dimension in elements (i.e. bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU_BGRAD , or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Must be divisible by 128 and be no less than the number of rows in the output matrix. GELU input matrix leading dimension in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DGELU ,  or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used. Must be divisible by 8 and be no less than the number of rows in the output matrix. int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_BATCH_STRIDE Batch stride for epilogue auxiliary buffer. ReLu bit-mask matrix batch stride in elements (i.e. bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Must be divisible by 128. GELU input matrix batch stride in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used. Must be divisible by 8. Default value: 0. int64_t CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE Batch stride for alpha vector. Used together with CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST when matrix D’s CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT is greater than 1. If CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO is set then CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE must be set to 0 as this mode doesn’t support batched alpha vector. Default value: 0. int64_t CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET Number of SMs to target for parallel execution. Optimizes heuristics for execution on a different number of SMs when user expects a concurrent stream to be using some of the device resources. Default value: 0. int32_t CUBLASLT_MATMUL_DESC_A_SCALE_POINTER Device pointer to the scale factor value that converts data in matrix A to the compute data type range. The scaling factor must have the same type as the compute type. If not specified, or set to NULL, the scaling factor is assumed to be 1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL const void* CUBLASLT_MATMUL_DESC_B_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix B. Default value: NULL const void* CUBLASLT_MATMUL_DESC_C_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix C. Default value: NULL const void* CUBLASLT_MATMUL_DESC_D_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix D. Default value: NULL const void* CUBLASLT_MATMUL_DESC_AMAX_D_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the output matrix. The computed value has the same type as the compute type. If not specified, or set to NULL, the maximum absolute value is not computed. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE The type of the data that will be stored in CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . If unset (or set to the default value of -1), the data type is set to be the output matrix element data type (DType) with some exceptions: ReLu uses a bit-mask. For FP8 kernels with an output type (DType) of CUDA_R_8F_E4M3 , the data type can be set to a non-default value if: AType and BType are CUDA_R_8F_E4M3 . Bias Type is CUDA_R_16F . CType is CUDA_R_16BF or CUDA_R_16F CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_GELU_AUX When CType is CUDA_R_16BF , the data type may be set to CUDA_R_16BF or CUDA_R_8F_E4M3 . When CType is CUDA_R_16F , the data type may be set to CUDA_R_16F . Otherwise, the data type should be left unset or set to the default value of -1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER Device pointer to the scaling factor value to convert results from compute type data range to storage data range in the auxiliary matrix that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . The scaling factor value must have the same type as the compute type. If not specified, or set to NULL, the scaling factor is assumed to be 1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the buffer that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . The computed value has the same type as the compute type. If not specified, or set to NULL, the maximum absolute value is not computed. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE. Default value: NULL void * CUBLASLT_MATMUL_DESC_FAST_ACCUM Flag for managing FP8 fast accumulation mode. When enabled, problem execution might be faster but at the cost of lower accuracy because intermediate results will not periodically be promoted to a higher precision. Default value: 0 - fast accumulation mode is disabled int8_t CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE Type of the bias or bias gradient vector in the device memory. Bias case: see CUBLASLT_EPILOGUE_BIAS . If unset (or set to the default value of -1), the bias vector elements are the same type as the elements of the output matrix (Dtype) with the following exceptions: IMMA kernels with computeType= CUDA_R_32I and Ctype=CUDA_R_8I where the bias vector elements are the same type as alpha, beta ( CUBLASLT_MATMUL_DESC_SCALE_TYPE=CUDA_R_32F ) For FP8 kernels with an output type of CUDA_R_32F , CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 . See cublasLtMatmul() for more details. Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER Pointer to a device array of input atomic counters consumed by a matmul. When a counter reaches zero, computation of the corresponding chunk of the output tensor is allowed to start. Default: NULL. See Atomics Synchronization . int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER Pointer to a device array of output atomic counters produced by a matmul. A matmul kernel sets a counter to zero when the computations of the corresponding chunk of the output tensor have completed. All the counters must be initialized to 1 before a matmul kernel is run. Default: NULL. See Atomics Synchronization . int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS Number of atomic synchronization chunks in the row dimension of the output matrix D. Each chunk corresponds to a single atomic counter. Default: 0 (atomics synchronization disabled).  See Atomics Synchronization . int32_t CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS Number of atomic synchronization chunks in the column dimension of the output matrix D. Each chunk corresponds to a single atomic counter. Default: 0 (atomics synchronization disabled).  See Atomics Synchronization . int32_t 3.3.10. cublasLtMatmulHeuristicResult_t  cublasLtMatmulHeuristicResult_t is a descriptor that holds the configured matrix multiplication algorithm descriptor and its runtime properties. Member Description cublasLtMatmulAlgo_t algo Must be initialized with cublasLtMatmulAlgoInit() if the preference CUBLASLT_MATMUL_PERF_SEARCH_MODE is set to CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID. See cublasLtMatmulSearch_t . size_t workspaceSize; Actual size of workspace memory required. cublasStatus_t state; Result status. Other fields are valid only if, after call to cublasLtMatmulAlgoGetHeuristic() , this member is set to CUBLAS_STATUS_SUCCESS. float wavesCount; Waves count is a device utilization metric. A wavesCount value of 1.0f suggests that when the kernel is launched it will fully occupy the GPU. int reserved[4]; Reserved. 3.3.11. cublasLtMatmulInnerShape_t  cublasLtMatmulInnerShape_t is an enumerated type used to configure various aspects of the internal kernel design. This does not impact the CUDA grid size. Value Description CUBLASLT_MATMUL_INNER_SHAPE_UNDEFINED Inner shape is undefined. CUBLASLT_MATMUL_INNER_SHAPE_MMA884 Inner shape is MMA884. CUBLASLT_MATMUL_INNER_SHAPE_MMA1684 Inner shape is MMA1684. CUBLASLT_MATMUL_INNER_SHAPE_MMA1688 Inner shape is MMA1688. CUBLASLT_MATMUL_INNER_SHAPE_MMA16816 Inner shape is MMA16816. 3.3.12. cublasLtMatmulPreference_t  The cublasLtMatmulPreference_t is a pointer to an opaque structure holding the description of the preferences for cublasLtMatmulAlgoGetHeuristic() configuration. Use cublasLtMatmulPreferenceCreate() to create one instance of the descriptor and cublasLtMatmulPreferenceDestroy() to destroy a previously created descriptor and release the resources. 3.3.13. cublasLtMatmulPreferenceAttributes_t  cublasLtMatmulPreferenceAttributes_t is an enumerated type used to apply algorithm search preferences while fine-tuning the heuristic function. Use cublasLtMatmulPreferenceGetAttribute() and cublasLtMatmulPreferenceSetAttribute() to get and set the attribute value of a matmul preference descriptor. Value Description Data Type CUBLASLT_MATMUL_PREF_SEARCH_MODE Search mode. See cublasLtMatmulSearch_t . Default is CUBLASLT_SEARCH_BEST_FIT. uint32_t CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES Maximum allowed workspace memory. Default is 0 (no workspace memory allowed). uint64_t CUBLASLT_MATMUL_PREF_REDUCTION_SCHEME_MASK Reduction scheme mask. See cublasLtReductionScheme_t . Only algorithm configurations specifying CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME that is not masked out by this attribute are allowed. For example, a mask value of 0x03 will allow only INPLACE and COMPUTE_TYPE reduction schemes. Default is CUBLASLT_REDUCTION_SCHEME_MASK (i.e., allows all reduction schemes). uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_A_BYTES Minimum buffer alignment for matrix A (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix A, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_B_BYTES Minimum buffer alignment for matrix B (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix B, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_C_BYTES Minimum buffer alignment for matrix C (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix C, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_D_BYTES Minimum buffer alignment for matrix D (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix D, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MAX_WAVES_COUNT Maximum wave count. See cublasLtMatmulHeuristicResult_t ::wavesCount. Selecting a non-zero value will exclude algorithms that report device utilization higher than specified. Default is 0.0f. float CUBLASLT_MATMUL_PREF_IMPL_MASK Numerical implementation details mask. See cublasLtNumericalImplFlags_t . Filters heuristic result to only include algorithms that use the allowed implementations. default: uint64_t(-1) (allow everything) uint64_t 3.3.14. cublasLtMatmulSearch_t  cublasLtMatmulSearch_t is an enumerated type that contains the attributes for heuristics search type. Value Description Data Type CUBLASLT_SEARCH_BEST_FIT Request heuristics for the best algorithm for the given use case. CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID Request heuristics only for the pre-configured algo id. 3.3.15. cublasLtMatmulTile_t  cublasLtMatmulTile_t is an enumerated type used to set the tile size in rows x columns. See also CUTLASS: Fast Linear Algebra in CUDA C++ . Value Description CUBLASLT_MATMUL_TILE_UNDEFINED Tile size is undefined. CUBLASLT_MATMUL_TILE_8x8 Tile size is 8 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x16 Tile size is 8 rows x 16 columns. CUBLASLT_MATMUL_TILE_16x8 Tile size is 16 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x32 Tile size is 8 rows x 32 columns. CUBLASLT_MATMUL_TILE_16x16 Tile size is 16 rows x 16 columns. CUBLASLT_MATMUL_TILE_32x8 Tile size is 32 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x64 Tile size is 8 rows x 64 columns. CUBLASLT_MATMUL_TILE_16x32 Tile size is 16 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x16 Tile size is 32 rows x 16 columns. CUBLASLT_MATMUL_TILE_64x8 Tile size is 64 rows x 8 columns. CUBLASLT_MATMUL_TILE_32x32 Tile size is 32 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x64 Tile size is 32 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x32 Tile size is 64 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x128 Tile size is 32 rows x 128 columns. CUBLASLT_MATMUL_TILE_64x64 Tile size is 64 rows x 64 columns. CUBLASLT_MATMUL_TILE_128x32 Tile size is 128 rows x 32 columns. CUBLASLT_MATMUL_TILE_64x128 Tile size is 64 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x64 Tile size is 128 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x256 Tile size is 64 rows x 256 columns. CUBLASLT_MATMUL_TILE_128x128 Tile size is 128 rows x 128 columns. CUBLASLT_MATMUL_TILE_256x64 Tile size is 256 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x512 Tile size is 64 rows x 512 columns. CUBLASLT_MATMUL_TILE_128x256 Tile size is 128 rows x 256 columns. CUBLASLT_MATMUL_TILE_256x128 Tile size is 256 rows x 128 columns. CUBLASLT_MATMUL_TILE_512x64 Tile size is 512 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x96 Tile size is 64 rows x 96 columns. CUBLASLT_MATMUL_TILE_96x64 Tile size is 96 rows x 64 columns. CUBLASLT_MATMUL_TILE_96x128 Tile size is 96 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x160 Tile size is 128 rows x 160 columns. CUBLASLT_MATMUL_TILE_160x128 Tile size is 160 rows x 128 columns. CUBLASLT_MATMUL_TILE_192x128 Tile size is 192 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x192 Tile size is 128 rows x 192 columns. CUBLASLT_MATMUL_TILE_128x96 Tile size is 128 rows x 96 columns. 3.3.16. cublasLtMatmulStages_t  cublasLtMatmulStages_t is an enumerated type used to configure the size and number of shared memory buffers where input elements are staged. Number of staging buffers defines kernel’s pipeline depth. Value Description CUBLASLT_MATMUL_STAGES_UNDEFINED Stage size is undefined. CUBLASLT_MATMUL_STAGES_16x1 Stage size is 16, number of stages is 1. CUBLASLT_MATMUL_STAGES_16x2 Stage size is 16, number of stages is 2. CUBLASLT_MATMUL_STAGES_16x3 Stage size is 16, number of stages is 3. CUBLASLT_MATMUL_STAGES_16x4 Stage size is 16, number of stages is 4. CUBLASLT_MATMUL_STAGES_16x5 Stage size is 16, number of stages is 5. CUBLASLT_MATMUL_STAGES_16x6 Stage size is 16, number of stages is 6. CUBLASLT_MATMUL_STAGES_32x1 Stage size is 32, number of stages is 1. CUBLASLT_MATMUL_STAGES_32x2 Stage size is 32, number of stages is 2. CUBLASLT_MATMUL_STAGES_32x3 Stage size is 32, number of stages is 3. CUBLASLT_MATMUL_STAGES_32x4 Stage size is 32, number of stages is 4. CUBLASLT_MATMUL_STAGES_32x5 Stage size is 32, number of stages is 5. CUBLASLT_MATMUL_STAGES_32x6 Stage size is 32, number of stages is 6. CUBLASLT_MATMUL_STAGES_64x1 Stage size is 64, number of stages is 1. CUBLASLT_MATMUL_STAGES_64x2 Stage size is 64, number of stages is 2. CUBLASLT_MATMUL_STAGES_64x3 Stage size is 64, number of stages is 3. CUBLASLT_MATMUL_STAGES_64x4 Stage size is 64, number of stages is 4. CUBLASLT_MATMUL_STAGES_64x5 Stage size is 64, number of stages is 5. CUBLASLT_MATMUL_STAGES_64x6 Stage size is 64, number of stages is 6. CUBLASLT_MATMUL_STAGES_128x1 Stage size is 128, number of stages is 1. CUBLASLT_MATMUL_STAGES_128x2 Stage size is 128, number of stages is 2. CUBLASLT_MATMUL_STAGES_128x3 Stage size is 128, number of stages is 3. CUBLASLT_MATMUL_STAGES_128x4 Stage size is 128, number of stages is 4. CUBLASLT_MATMUL_STAGES_128x5 Stage size is 128, number of stages is 5. CUBLASLT_MATMUL_STAGES_128x6 Stage size is 128, number of stages is 6. CUBLASLT_MATMUL_STAGES_32x10 Stage size is 32, number of stages is 10. CUBLASLT_MATMUL_STAGES_8x4 Stage size is 8, number of stages is 4. CUBLASLT_MATMUL_STAGES_16x10 Stage size is 16, number of stages is 10. CUBLASLT_MATMUL_STAGES_8x5 Stage size is 8, number of stages is 5. CUBLASLT_MATMUL_STAGES_8x3 Stage size is 8, number of stages is 3. CUBLASLT_MATMUL_STAGES_8xAUTO Stage size is 8, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_16xAUTO Stage size is 16, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_32xAUTO Stage size is 32, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_64xAUTO Stage size is 64, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_128xAUTO Stage size is 128, number of stages is selected automatically. 3.3.17. cublasLtNumericalImplFlags_t  cublasLtNumericalImplFlags_t : a set of bit-flags that can be specified to select implementation details that may affect numerical behavior of algorithms. Flags below can be combined using the bit OR operator “|”. Value Description CUBLASLT_NUMERICAL_IMPL_FLAGS_FMA Specify that the implementation is based on [H,F,D]FMA (fused multiply-add) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_HMMA Specify that the implementation is based on HMMA (tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_IMMA Specify that the implementation is based on IMMA (integer tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_DMMA Specify that the implementation is based on DMMA (double precision tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_TENSOR_OP_MASK Mask to filter implementations using any of the above kinds of tensor operations. CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_TYPE_MASK Mask to filter implementation details about multiply-accumulate instructions used. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_16F Specify that the implementation’s inner dot product is using half precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32F Specify that the implementation’s inner dot product is using single precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_64F Specify that the implementation’s inner dot product is using double precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32I Specify that the implementation’s inner dot product is using 32 bit signed integer precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_TYPE_MASK Mask to filter implementation details about accumulator used. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16F Specify that the implementation’s inner dot product multiply-accumulate instruction is using half-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16BF Specify that the implementation’s inner dot product multiply-accumulate instruction is using bfloat16 inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_TF32 Specify that the implementation’s inner dot product multiply-accumulate instruction is using TF32 inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_32F Specify that the implementation’s inner dot product multiply-accumulate instruction is using single-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_64F Specify that the implementation’s inner dot product multiply-accumulate instruction is using double-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_8I Specify that the implementation’s inner dot product multiply-accumulate instruction is using 8-bit integer inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_INPUT_TYPE_MASK Mask to filter implementation details about accumulator input used. CUBLASLT_NUMERICAL_IMPL_FLAGS_GAUSSIAN Specify that the implementation applies Gauss complexity reduction algorithm to reduce arithmetic complexity of the complex matrix multiplication problem 3.3.18. cublasLtMatrixLayout_t  The cublasLtMatrixLayout_t is a pointer to an opaque structure holding the description of a matrix layout. Use cublasLtMatrixLayoutCreate() to create one instance of the descriptor and cublasLtMatrixLayoutDestroy() to destroy a previously created descriptor and release the resources. 3.3.19. cublasLtMatrixLayoutAttribute_t  cublasLtMatrixLayoutAttribute_t is a descriptor structure containing the attributes that define the details of the matrix operation. Use cublasLtMatrixLayoutGetAttribute() and cublasLtMatrixLayoutSetAttribute() to get and set the attribute value of a matrix layout descriptor. Attribute Name Description Data Type CUBLASLT_MATRIX_LAYOUT_TYPE Specifies the data precision type. See cudaDataType_t . uint32_t CUBLASLT_MATRIX_LAYOUT_ORDER Specifies the memory order of the data of the matrix. Default value is CUBLASLT_ORDER_COL. See cublasLtOrder_t . int32_t CUBLASLT_MATRIX_LAYOUT_ROWS Describes the number of rows in the matrix. Normally only values that can be expressed as int32_t are supported. uint64_t CUBLASLT_MATRIX_LAYOUT_COLS Describes the number of columns in the matrix. Normally only values that can be expressed as int32_t are supported. uint64_t CUBLASLT_MATRIX_LAYOUT_LD The leading dimension of the matrix. For CUBLASLT_ORDER_COL this is the stride (in elements) of matrix column. See also cublasLtOrder_t . Currently only non-negative values are supported. Must be large enough so that matrix memory locations are not overlapping (e.g., greater or equal to CUBLASLT_MATRIX_LAYOUT_ROWS in case of CUBLASLT_ORDER_COL). int64_t CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT Number of matmul operations to perform in the batch. Default value is 1. See also CUBLASLT_ALGO_CAP_STRIDED_BATCH_SUPPORT in cublasLtMatmulAlgoCapAttributes_t . int32_t CUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET Stride (in elements) to the next matrix for the strided batch operation. Default value is 0. When matrix type is planar-complex (CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0), batch stride is interpreted by cublasLtMatmul() in number of real valued sub-elements. E.g. for data of type CUDA_C_16F, offset of 1024B is encoded as a stride of value 512 (since each element of the real and imaginary matrices is a 2B (16bit) floating point type). NOTE: A bug in cublasLtMatrixTransform() causes it to interpret the batch stride for a planar-complex matrix as if it was specified in number of complex elements. Therefore an offset of 1024B must be encoded as stride value 256 when calling cublasLtMatrixTransform() (each complex element is 4B with real and imaginary values 2B each). This behavior is expected to be corrected in the next major cuBLAS version. int64_t CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET Stride (in bytes) to the imaginary plane for planar-complex layout. Default value is 0, indicating that the layout is regular (real and imaginary parts of complex numbers are interleaved in memory for each element). int64_t 3.3.20. cublasLtMatrixTransformDesc_t  The cublasLtMatrixTransformDesc_t is a pointer to an opaque structure holding the description of a matrix transformation operation. Use cublasLtMatrixTransformDescCreate() to create one instance of the descriptor and cublasLtMatrixTransformDescDestroy() to destroy a previously created descriptor and release the resources. 3.3.21. cublasLtMatrixTransformDescAttributes_t  cublasLtMatrixTransformDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix transform operation. Use cublasLtMatrixTransformDescGetAttribute() and cublasLtMatrixTransformDescSetAttribute() to set the attribute value of a matrix transform descriptor. Transform Attribute Name Description Data Type CUBLASLT_MATRIX_TRANSFORM_DESC_SCALE_TYPE Scale type. Inputs are converted to the scale type for scaling and summation, and results are then converted to the output type to store in the memory. For the supported data types see cudaDataType_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_POINTER_MODE Specifies the scalars alpha and beta are passed by reference whether on the host or on the device. Default value is: CUBLASLT_POINTER_MODE_HOST (i.e., on the host). See cublasLtPointerMode_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSA Specifies the type of operation that should be performed on the matrix A. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSB Specifies the type of operation that should be performed on the matrix B. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t 3.3.22. cublasLtOrder_t  cublasLtOrder_t is an enumerated type used to indicate the data ordering of the matrix. Value Data Order Description CUBLASLT_ORDER_COL Data is ordered in column-major format. The leading dimension is the stride (in elements) to the beginning of next column in memory. CUBLASLT_ORDER_ROW Data is ordered in row-major format. The leading dimension is the stride (in elements) to the beginning of next row in memory. CUBLASLT_ORDER_COL32 Data is ordered in column-major ordered tiles of 32 columns. The leading dimension is the stride (in elements) to the beginning of next group of 32-columns. For example, if the matrix has 33 columns and 2 rows, then the leading dimension must be at least (32) * 2 = 64. CUBLASLT_ORDER_COL4_4R2_8C Data is ordered in column-major ordered tiles of composite tiles with total 32 columns and 8 rows. A tile is composed of interleaved inner tiles of 4 columns within 4 even or odd rows in an alternating pattern. The leading dimension is the stride (in elements) to the beginning of the first 32 column x 8 row tile for the next 32-wide group of columns. For example, if the matrix has 33 columns and 1 row, the leading dimension must be at least (32 * 8) * 1 = 256. CUBLASLT_ORDER_COL32_2R_4R4 Data is ordered in column-major ordered tiles of composite tiles with total 32 columns ands 32 rows. Element offset within the tile is calculated as (((row%8)/2*4+row/8)*2+row%2)*32+col. Leading dimension is the stride (in elements) to the beginning of the first 32 column x 32 row tile for the next 32-wide group of columns. E.g. if matrix has 33 columns and 1 row, ld must be at least (32*32)*1 = 1024. 3.3.23. cublasLtPointerMode_t  cublasLtPointerMode_t is an enumerated type used to set the pointer mode for the scaling factors alpha and beta . Value Description CUBLASLT_POINTER_MODE_HOST = CUBLAS_POINTER_MODE_HOST Matches CUBLAS_POINTER_MODE_HOST, and the pointer targets a single value host memory. CUBLASLT_POINTER_MODE_DEVICE = CUBLAS_POINTER_MODE_DEVICE Matches CUBLAS_POINTER_MODE_DEVICE, and the pointer targets a single value device memory. CUBLASLT_POINTER_MODE_DEVICE_VECTOR = 2 Pointers target device memory vectors of length equal to the number of rows of matrix D. CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO = 3 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is zero. CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST = 4 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is a single value in host memory. 3.3.24. cublasLtPointerModeMask_t  cublasLtPointerModeMask_t is an enumerated type used to define and query the pointer mode capability. Value Description CUBLASLT_POINTER_MODE_MASK_HOST = 1 See CUBLASLT_POINTER_MODE_HOST in cublasLtPointerMode_t . CUBLASLT_POINTER_MODE_MASK_DEVICE = 2 See CUBLASLT_POINTER_MODE_DEVICE in cublasLtPointerMode_t . CUBLASLT_POINTER_MODE_MASK_DEVICE_VECTOR = 4 See CUBLASLT_POINTER_MODE_DEVICE_VECTOR in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_ZERO = 8 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_HOST = 16 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST in cublasLtPointerMode_t 3.3.25. cublasLtReductionScheme_t  cublasLtReductionScheme_t is an enumerated type used to specify a reduction scheme for the portions of the dot-product calculated in parallel (i.e., “split - K”). Value Description CUBLASLT_REDUCTION_SCHEME_NONE Do not apply reduction. The dot-product will be performed in one sequence. CUBLASLT_REDUCTION_SCHEME_INPLACE Reduction is performed “in place” using the output buffer, parts are added up in the output data type. Workspace is only used for counters that guarantee sequentiality. CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE Reduction done out of place in a user-provided workspace. The intermediate results are stored in the compute type in the workspace and reduced in a separate step. CUBLASLT_REDUCTION_SCHEME_OUTPUT_TYPE Reduction done out of place in a user-provided workspace. The intermediate results are stored in the output type in the workspace and reduced in a separate step. CUBLASLT_REDUCTION_SCHEME_MASK Allows all reduction schemes. 3.4. cuBLASLt API Reference  3.4.1. cublasLtCreate()  cublasStatus_t cublasLtCreate ( cublasLtHandle_t * lighthandle ) This function initializes the cuBLASLt library and creates a handle to an opaque structure holding the cuBLASLt library context. It allocates light hardware resources on the host and device, and must be called prior to making any other cuBLASLt library calls. The cuBLASLt library context is tied to the current CUDA device. To use the library on multiple devices, one cuBLASLt handle should be created for each device. Parameters: Parameter Memory Input / Output Description lightHandle Output Pointer to the allocated cuBLASLt handle for the created cuBLASLt context. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The allocation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The cuBLASLt library was not initialized. This usually happens: when cublasLtCreate() is not called first an error in the CUDA Runtime API called by the cuBLASLt routine, or an error in the hardware setup. CUBLAS_STATUS_ALLOC_FAILED Resource allocation failed inside the cuBLASLt library. This is usually caused by a cudaMalloc() failure. To correct: prior to the function call, deallocate the previously allocated memory as much as possible. CUBLAS_STATUS_INVALID_VALUE lighthandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.2. cublasLtDestroy()  cublasStatus_t cublasLtDestroy ( cublasLtHandle_t lightHandle ) This function releases hardware resources used by the cuBLASLt library. This function is usually the last call with a particular handle to the cuBLASLt library. Because cublasLtCreate() allocates some internal resources and the release of those resources by calling cublasLtDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the cuBLASLt handle to be destroyed. Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The cuBLASLt context was successfully destroyed. CUBLAS_STATUS_NOT_INITIALIZED The cuBLASLt library was not initialized. CUBLAS_STATUS_INVALID_VALUE lightHandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.3. cublasLtDisableCpuInstructionsSetMask()  unsigned cublasLtDisableCpuInstructionsSetMask ( unsigned mask ); Instructs cuBLASLt library to not use CPU instructions specified by the flags in the mask .\nThe function takes precedence over the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable. Parameters: mask – the flags combined with bitwise OR(|) operator that specify which CPU instructions should not be used. Supported flags: Value Description 0x1 x86-64 AVX512 ISA. Returns: the previous value of the mask . 3.4.4. cublasLtGetCudartVersion()  size_t cublasLtGetCudartVersion ( void ); This function returns the version number of the CUDA Runtime library. Parameters: None. Returns: size_t - The version number of the CUDA Runtime library. 3.4.5. cublasLtGetProperty()  cublasStatus_t cublasLtGetProperty ( libraryPropertyType type , int * value ); This function returns the value of the requested property by writing it to the memory location pointed to by the value parameter. Parameters : Parameter Memory Input / Output Description type Input Of the type libraryPropertyType , whose value is requested from the property. See libraryPropertyType_t . value Output Pointer to the host memory location where the requested information should be written. Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The requested libraryPropertyType information is successfully written at the provided address. CUBLAS_STATUS_INVALID_VALUE If invalid value of the type input argument or value == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.6. cublasLtGetStatusName()  const char * cublasLtGetStatusName ( cublasStatus_t status ); Returns the string representation of a given status. Parameters: cublasStatus_t - the status. Returns: const char* - the NULL-terminated string. 3.4.7. cublasLtGetStatusString()  const char * cublasLtGetStatusString ( cublasStatus_t status ); Returns the description string for a given status. Parameters: cublasStatus_t - the status. Returns: const char* - the NULL-terminated string. 3.4.8. cublasLtHeuristicsCacheGetCapacity()  cublasStatus_t cublasLtHeuristicsCacheGetCapacity ( size_t * capacity ); Returns the Heuristics Cache capacity. Parameters: Parameter Description capacity The pointer to the returned capacity value. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully written. CUBLAS_STATUS_INVALID_VALUE The capacity was successfully set. 3.4.9. cublasLtHeuristicsCacheSetCapacity()  cublasStatus_t cublasLtHeuristicsCacheSetCapacity ( size_t capacity ); Sets the Heuristics Cache capacity. Set the capacity to 0 to disable the heuristics cache. This function takes precedence over CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable. Parameters: Parameter Description capacity The desirable heuristics cache capacity. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully set. 3.4.10. cublasLtGetVersion()  size_t cublasLtGetVersion ( void ); This function returns the version number of cuBLASLt library. Parameters: None. Returns: size_t - The version number of cuBLASLt library. 3.4.11. cublasLtLoggerSetCallback()  cublasStatus_t cublasLtLoggerSetCallback ( cublasLtLoggerCallback_t callback ); Experimental: This function sets the logging callback function. Parameters : Parameter Memory Input / Output Description callback Input Pointer to a callback function. See cublasLtLoggerCallback_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the callback function was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.12. cublasLtLoggerSetFile()  cublasStatus_t cublasLtLoggerSetFile ( FILE * file ); Experimental: This function sets the logging output file. Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle. Parameters : Parameter Memory Input / Output Description file Input Pointer to an open file. File should have write permission. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging file was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.13. cublasLtLoggerOpenFile()  cublasStatus_t cublasLtLoggerOpenFile ( const char * logFile ); Experimental: This function opens a logging output file in the given path. Parameters : Parameter Memory Input / Output Description logFile Input Path of the logging output file. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging file was successfully opened. See cublasStatus_t for a complete list of valid return codes. 3.4.14. cublasLtLoggerSetLevel()  cublasStatus_t cublasLtLoggerSetLevel ( int level ); Experimental: This function sets the value of the logging level. Parameters : Parameter Memory Input / Output Description level Input Value of the logging level. See cuBLASLt Logging . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If the value was not a valid logging level. See cuBLASLt Logging . CUBLAS_STATUS_SUCCESS If the logging level was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.15. cublasLtLoggerSetMask()  cublasStatus_t cublasLtLoggerSetMask ( int mask ); Experimental: This function sets the value of the logging mask. Parameters : Parameter Memory Input / Output Description mask Input Value of the logging mask. See cuBLASLt Logging . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging mask was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.16. cublasLtLoggerForceDisable()  cublasStatus_t cublasLtLoggerForceDisable (); Experimental: This function disables logging for the entire run. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging was successfully disabled. See cublasStatus_t for a complete list of valid return codes. 3.4.17. cublasLtMatmul()  cublasStatus_t cublasLtMatmul ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t computeDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * B , cublasLtMatrixLayout_t Bdesc , const void * beta , const void * C , cublasLtMatrixLayout_t Cdesc , void * D , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , void * workspace , size_t workspaceSizeInBytes , cudaStream_t stream ); This function computes the matrix multiplication of matrices A and B to produce the output matrix D, according to the following operation: D = alpha*(A*B) + beta*(C), where A , B , and C are input matrices, and alpha and beta are input scalars. Note This function supports both in-place matrix multiplication ( C == D and Cdesc == Ddesc ) and out-of-place matrix multiplication ( C != D , both matrices must have the same data type, number of rows, number of columns, batch size, and memory order). In the out-of-place case, the leading dimension of C can be different from the leading dimension of D. Specifically the leading dimension of C can be 0 to achieve row or column broadcast. If Cdesc is omitted, this function assumes it to be equal to Ddesc . The workspace pointer must be aligned to at least a multiple of 256 bytes.\nThe recommendations on workspaceSizeInBytes are the same as mentioned in the cublasSetWorkspace() section. Datatypes Supported: cublasLtMatmul() supports the following computeType, scaleType, Atype/Btype, and Ctype. Footnotes can be found at the end of this section. Table 1. When A, B, C, and D are Regular Column- or Row-major Matrices  computeType scaleType Atype/Btype Ctype Bias Type 5 CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported. CUDA_R_32F CUDA_R_8I CUDA_R_8I Non-default epilogue not supported. CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUDA_R_8I CUDA_R_32F Non-default epilogue not supported. CUDA_R_16BF CUDA_R_32F CUDA_R_32F 5 CUDA_R_16F CUDA_R_32F CUDA_R_32F 5 CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_8I 6 CUDA_C_32F 6 Non-default epilogue not supported. CUDA_C_32F 6 CUDA_C_32F 6 CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_32F 6 CUDA_C_32F 6 Non-default epilogue not supported. CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F 5 CUDA_C_64F 6 CUDA_C_64F 6 CUDA_C_64F 6 Non-default epilogue not supported. To use IMMA kernels, one of the following sets of requirements, with the first being the preferred one, must be met: Using a regular data ordering: All matrix pointers must be 4-byte aligned. For even better performance, this condition should hold with 16 instead of 4. Leading dimensions of matrices A, B, C must be multiples of 4. Only the “TN” format is supported - A must be transposed and B non-transposed. Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST . With the latter mode, the kernels support the CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE attribute. Dimensions m and k must be multiples of 4. Using the IMMA-specific data ordering on Ampere or Turing (but not Hopper) architecture - CUBLASLT_ORDER_COL32` for matrices A, C, D, and CUBLASLT_ORDER_COL4_4R2_8C (on Turing or Ampere architecture) or CUBLASLT_ORDER_COL32_2R_4R4 (on Ampere architecture) for matrix B: Leading dimensions of matrices A, B, C must fulfill conditions specific to the memory ordering (see cublasLtOrder_t ). Matmul descriptor must specify CUBLAS_OP_T on matrix B and CUBLAS_OP_N (default) on matrix A and C. If scaleType CUDA_R_32I is used, the only supported values for alpha and beta are 0 or 1 . Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE , CUBLASLT_POINTER_MODE_DEVICE_VECTOR or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO . These kernels do not support CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE . Only the “NT” format is supported - A must be transposed and B non-transposed. Table 2. When A, B, C, and D Use Layouts for IMMA  computeType scaleType Atype/Btype Ctype Bias Type CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported. CUDA_R_32F CUDA_R_8I CUDA_R_8I CUDA_R_32F To use FP8 kernels, the following set of requirements must be satisfied: All matrix dimensions must meet the optimal requirements listed in Tensor Core Usage (i.e. pointers and matrix dimension must support 16-byte alignment). A must be transposed and B non-transposed (The “TN” format). The compute type must be CUBLAS_COMPUTE_32F . The scale type must be CUDA_R_32F . See the table below when using FP8 kernels: Table 3. When A, B, C, and D Use Layouts for FP8  AType BType CType DType Bias Type CUDA_R_8F_E4M3 CUDA_R_8F_E4M3 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_8F_E5M2 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_8F_E4M3 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_8F_E5M2 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 And finally, see below table when A,B,C,D are planar-complex matrices ( CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0 , see cublasLtMatrixLayoutAttribute_t ) to make use of mixed precision tensor core acceleration. Table 4. When A, B, C, and D are Planar-Complex Matrices  computeType scaleType Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_C_32F CUDA_C_16F 6 CUDA_C_16F 6 CUDA_C_32F 6 CUDA_C_16BF 6 CUDA_C_16BF 6 CUDA_C_32F 6 NOTES: 5 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 ) ReLU, dReLu, GELU, dGELU and Bias epilogue modes (see CUBLASLT_MATMUL_DESC_EPILOGUE in cublasLtMatmulDescAttributes_t ) are not supported when D matrix memory order is defined as CUBLASLT_ORDER_ROW . For best performance when using the bias vector, specify zero beta and set pointer mode to CUBLASLT_POINTER_MODE_HOST . 6 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ) Use of CUBLAS_ORDER_ROW together with CUBLAS_OP_C (Hermitian operator) is not supported unless all of A, B, C, and D matrices use the CUBLAS_ORDER_ROW ordering. Parameters: Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . alpha, beta Device or host Input Pointers to the scalars used in the multiplication. A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc, Bdesc and Cdesc. Adesc, Bdesc and Cdesc. Input Handles to the previous created descriptors of the type cublasLtMatrixLayout_t . D Device Output Pointer to the GPU memory associated with the descriptor Ddesc. Ddesc Input Handle to the previous created descriptor of the type cublasLtMatrixLayout_t . algo Input Handle for matrix multiplication algorithm to be used. See cublasLtMatmulAlgo_t . When NULL, an implicit heuritics query with default search preferences will be performed to determine actual algorithm to use. workspace Device Pointer to the workspace buffer allocated in the GPU memory. Must be 256B aligned (i.e. lowest 8 bits of address must be 0). workspaceSizeInBytes Input Size of the workspace. stream Host Input The CUDA stream where all the GPU work will be submitted. Returns: Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized. CUBLAS_STATUS_INVALID_VALUE If the parameters are unexpectedly NULL, in conflict or in an impossible configuration. For example, when workspaceSizeInBytes is less than workspace required by the configured algo. CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device doesn’t support the configured operation. CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device. CUBLAS_STATUS_EXECUTION_FAILED If CUDA reported an execution error from the device. CUBLAS_STATUS_SUCCESS If the operation completed successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.18. cublasLtMatmulAlgoCapGetAttribute()  cublasStatus_t cublasLtMatmulAlgoCapGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoCapAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried capability attribute for an initialized cublasLtMatmulAlgo_t descriptor structure. The capability attribute value is retrieved from the enumerated type cublasLtMatmulAlgoCapAttributes_t . For example, to get list of supported Tile IDs: cublasLtMatmulTile_t tiles [ CUBLASLT_MATMUL_TILE_END ]; size_t num_tiles , size_written ; if ( cublasLtMatmulAlgoCapGetAttribute ( algo , CUBLASLT_ALGO_CAP_TILE_IDS , tiles , sizeof ( tiles ), & size_written ) == CUBLAS_STATUS_SUCCESS ) { num_tiles = size_written / sizeof ( tiles [ 0 ]);} Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. See cublasLtMatmulAlgo_t . attr Input The capability attribute whose value will be retrieved by this function. See cublasLtMatmulAlgoCapAttributes_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.19. cublasLtMatmulAlgoCheck()  cublasStatus_t cublasLtMatmulAlgoCheck ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , cublasLtMatmulHeuristicResult_t * result ); This function performs the correctness check on the matrix multiply algorithm descriptor for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D. It checks whether the descriptor is supported on the current device, and returns the result containing the required workspace and the calculated wave count. Note CUBLAS_STATUS_SUCCESS doesn’t fully guarantee that the algo will run. The algo will fail if, for example, the buffers are not correctly aligned. However, if cublasLtMatmulAlgoCheck() fails, the algo will not run. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t . algo Input Descriptor which specifies which matrix multiplication algorithm should be used. See cublasLtMatmulAlgo_t . May point to result->algo . result Output Pointer to the structure holding the results returned by this function. The results comprise of the required workspace and the calculated wave count. The algo field is never updated. See cublasLtMatmulHeuristicResult_t . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If matrix layout descriptors or the operation descriptor do not match the algo descriptor. CUBLAS_STATUS_NOT_SUPPORTED If the algo configuration or data type combination is not currently supported on the given device. CUBLAS_STATUS_ARCH_MISMATCH If the algo configuration cannot be run using the selected device. CUBLAS_STATUS_SUCCESS If the check was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.20. cublasLtMatmulAlgoConfigGetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor. The configuration attribute value is retrieved from the enumerated type cublasLtMatmulAlgoConfigAttributes_t . Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. See cublasLtMatmulAlgo_t . attr Input The configuration attribute whose value will be retrieved by this function. See cublasLtMatmulAlgoConfigAttributes_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.21. cublasLtMatmulAlgoConfigSetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigSetAttribute ( cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor. The configuration attribute is an enumerant of the type cublasLtMatmulAlgoConfigAttributes_t . Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. See cublasLtMatmulAlgo_t . attr Input The configuration attribute whose value will be set by this function. See cublasLtMatmulAlgoConfigAttributes_t . buf Input The value to which the configuration attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.22. cublasLtMatmulAlgoGetHeuristic()  cublasStatus_t cublasLtMatmulAlgoGetHeuristic ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , cublasLtMatmulPreference_t preference , int requestedAlgoCount , cublasLtMatmulHeuristicResult_t heuristicResultsArray [] int * returnAlgoCount ); This function retrieves the possible algorithms for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D. The output is placed in heuristicResultsArray[] in the order of increasing estimated compute time. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t . preference Input Pointer to the structure holding the heuristic search preferences descriptor. See cublasLtMatmulPreference_t . requestedAlgoCount Input Size of the heuristicResultsArray (in elements). This is the requested maximum number of algorithms to return. heuristicResultsArray[] Output Array containing the algorithm heuristics and associated runtime characteristics, returned by this function, in the order of increasing estimated compute time. returnAlgoCount Output Number of algorithms returned by this function. This is the number of heuristicResultsArray elements written. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero. CUBLAS_STATUS_NOT_SUPPORTED If no heuristic function available for current configuration. CUBLAS_STATUS_SUCCESS If query was successful. Inspect heuristicResultsArray[0 to (returnAlgoCount -1)].state for the status of the results. See cublasStatus_t for a complete list of valid return codes. Note This function may load some kernels using CUDA Driver API which may fail when there is no available GPU memory. Do not allocate the entire VRAM before running cublasLtMatmulAlgoGetHeuristic() . 3.4.23. cublasLtMatmulAlgoGetIds()  cublasStatus_t cublasLtMatmulAlgoGetIds ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int requestedAlgoCount , int algoIdsArray [], int * returnAlgoCount ); This function retrieves the IDs of all the matrix multiply algorithms that are valid, and can potentially be run by the cublasLtMatmul() function, for given types of the input matrices A, B and C, and of the output matrix D. Note The IDs are returned in no particular order. To make sure the best possible algo is contained in the list, make requestedAlgoCount large enough to receive the full list. The list is guaranteed to be full if returnAlgoCount < requestedAlgoCount . Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeType, scaleType, Atype, Btype, Ctype, and Dtype Inputs Data types of the computation type, scaling factors and of the operand matrices. See cudaDataType_t . requestedAlgoCount Input Number of algorithms requested. Must be > 0. algoIdsArray[] Output Array containing the algorithm IDs returned by this function. returnAlgoCount Output Number of algorithms actually returned by this function. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero. CUBLAS_STATUS_SUCCESS If query was successful. Inspect returnAlgoCount to get actual number of IDs available. See cublasStatus_t for a complete list of valid return codes. 3.4.24. cublasLtMatmulAlgoInit()  cublasStatus_t cublasLtMatmulAlgoInit ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int algoId , cublasLtMatmulAlgo_t * algo ); This function initializes the matrix multiply algorithm structure for the cublasLtMatmul() , for a specified matrix multiply algorithm and input matrices A, B and C, and the output matrix D. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeType Input Compute type. See CUBLASLT_MATMUL_DESC_COMPUTE_TYPE of cublasLtMatmulDescAttributes_t . scaleType Input Scale type. See CUBLASLT_MATMUL_DESC_SCALE_TYPE of cublasLtMatmulDescAttributes_t . Usually same as computeType. Atype, Btype, Ctype, and Dtype Input Datatype precision for the input and output matrices. See cudaDataType_t . algoId Input Specifies the algorithm being initialized. Should be a valid algoId returned by the cublasLtMatmulAlgoGetIds() function. algo Input Pointer to the opaque structure to be initialized. See cublasLtMatmulAlgo_t . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If algo is NULL or algoId is outside the recognized range. CUBLAS_STATUS_NOT_SUPPORTED If algoId is not supported for given combination of data types. CUBLAS_STATUS_SUCCESS If the structure was successfully initialized. See cublasStatus_t for a complete list of valid return codes. 3.4.25. cublasLtMatmulDescCreate()  cublasStatus_t cublasLtMatmulDescCreate ( cublasLtMatmulDesc_t * matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function creates a matrix multiply descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor created by this function. See cublasLtMatmulDesc_t . computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function creates. See cublasComputeType_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.26. cublasLtMatmulDescInit()  cublasStatus_t cublasLtMatmulDescInit ( cublasLtMatmulDesc_t matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function initializes a matrix multiply descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor initialized by this function. See cublasLtMatmulDesc_t . computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function initializes. See cublasComputeType_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.27. cublasLtMatmulDescDestroy()  cublasStatus_t cublasLtMatmulDescDestroy ( cublasLtMatmulDesc_t matmulDesc ); This function destroys a previously created matrix multiply descriptor object. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the structure holding the matrix multiply descriptor that should be destroyed by this function. See cublasLtMatmulDesc_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.28. cublasLtMatmulDescGetAttribute()  cublasStatus_t cublasLtMatmulDescGetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply descriptor. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function. See cublasLtMatmulDesc_t . attr Input The attribute that will be retrieved by this function. See cublasLtMatmulDescAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.29. cublasLtMatmulDescSetAttribute()  cublasStatus_t cublasLtMatmulDescSetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply descriptor. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function. See cublasLtMatmulDesc_t . attr Input The attribute that will be set by this function. See cublasLtMatmulDescAttributes_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.30. cublasLtMatmulPreferenceCreate()  cublasStatus_t cublasLtMatmulPreferenceCreate ( cublasLtMatmulPreference_t * pref ); This function creates a matrix multiply heuristic search preferences descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences descriptor created by this function. See cublasLtMatrixLayout_t . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.31. cublasLtMatmulPreferenceInit()  cublasStatus_t cublasLtMatmulPreferenceInit ( cublasLtMatmulPreference_t pref ); This function initializes a matrix multiply heuristic search preferences descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences descriptor created by this function. See cublasLtMatrixLayout_t . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.32. cublasLtMatmulPreferenceDestroy()  cublasStatus_t cublasLtMatmulPreferenceDestroy ( cublasLtMatmulPreference_t pref ); This function destroys a previously created matrix multiply preferences descriptor object. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the structure holding the matrix multiply preferences descriptor that should be destroyed by this function. See cublasLtMatmulPreference_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.33. cublasLtMatmulPreferenceGetAttribute()  cublasStatus_t cublasLtMatmulPreferenceGetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply heuristic search preferences descriptor. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply heuristic search preferences descriptor queried by this function. See cublasLtMatmulPreference_t . attr Input The attribute that will be queried by this function. See cublasLtMatmulPreferenceAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.34. cublasLtMatmulPreferenceSetAttribute()  cublasStatus_t cublasLtMatmulPreferenceSetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply preferences descriptor. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply preferences descriptor queried by this function. See cublasLtMatmulPreference_t . attr Input The attribute that will be set by this function. See cublasLtMatmulPreferenceAttributes_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.35. cublasLtMatrixLayoutCreate()  cublasStatus_t cublasLtMatrixLayoutCreate ( cublasLtMatrixLayout_t * matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function creates a matrix layout descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor created by this function. See cublasLtMatrixLayout_t . type Input Enumerant that specifies the data precision for the matrix layout descriptor this function creates. See cudaDataType . rows, cols Input Number of rows and columns of the matrix. ld Input The leading dimension of the matrix. In column major layout, this is the number of elements to jump to reach the next column. Thus ld >= m (number of rows). Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If the memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.36. cublasLtMatrixLayoutInit()  cublasStatus_t cublasLtMatrixLayoutInit ( cublasLtMatrixLayout_t matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function initializes a matrix layout descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor initialized by this function. See cublasLtMatrixLayout_t . type Input Enumerant that specifies the data precision for the matrix layout descriptor this function initializes. See cudaDataType . rows, cols Input Number of rows and columns of the matrix. ld Input The leading dimension of the matrix. In column major layout, this is the number of elements to jump to reach the next column. Thus ld >= m (number of rows). Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If the memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.37. cublasLtMatrixLayoutDestroy()  cublasStatus_t cublasLtMatrixLayoutDestroy ( cublasLtMatrixLayout_t matLayout ); This function destroys a previously created matrix layout descriptor object. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the structure holding the matrix layout descriptor that should be destroyed by this function. See cublasLtMatrixLayout_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.38. cublasLtMatrixLayoutGetAttribute()  cublasStatus_t cublasLtMatrixLayoutGetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to the specified matrix layout descriptor. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function. See cublasLtMatrixLayout_t . attr Input The attribute being queried for. See cublasLtMatrixLayoutAttribute_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.39. cublasLtMatrixLayoutSetAttribute()  cublasStatus_t cublasLtMatrixLayoutSetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix layout descriptor. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function. See cublasLtMatrixLayout_t . attr Input The attribute that will be set by this function. See cublasLtMatrixLayoutAttribute_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf , the attribute buffer. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match size of internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.40. cublasLtMatrixTransform()  cublasStatus_t cublasLtMatrixTransform ( cublasLtHandle_t lightHandle , cublasLtMatrixTransformDesc_t transformDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * beta , const void * B , cublasLtMatrixLayout_t Bdesc , void * C , cublasLtMatrixLayout_t Cdesc , cudaStream_t stream ); This function computes the matrix transformation operation on the input matrices A and B, to produce the output matrix C, according to the below operation: C = alpha*transformation(A) + beta*transformation(B), where A , B are input matrices, and alpha and beta are input scalars. The transformation operation is defined by the transformDesc pointer. This function can be used to change the memory order of data or to scale and shift the values. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . transformDesc Input Pointer to the opaque descriptor holding the matrix transformation operation. See cublasLtMatrixTransformDesc_t . alpha, beta Device or host Input Pointers to the scalars used in the multiplication. A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc , Bdesc and Cdesc . Adesc, Bdesc and Cdesc. Input Handles to the previous created descriptors of the type cublasLtMatrixLayout_t . Adesc or Bdesc can be NULL if corresponding pointer is NULL and corresponding scalar is zero. stream Host Input The CUDA stream where all the GPU work will be submitted. Returns : Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized. CUBLAS_STATUS_INVALID_VALUE If the parameters are in conflict or in an impossible configuration. For example, when A is not NULL, but Adesc is NULL. CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device does not support the configured operation. CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device. CUBLAS_STATUS_EXECUTION_FAILED If CUDA reported an execution error from the device. CUBLAS_STATUS_SUCCESS If the operation completed successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.41. cublasLtMatrixTransformDescCreate()  cublasStatus_t cublasLtMatrixTransformDescCreate ( cublasLtMatrixTransformDesc_t * transformDesc , cudaDataType scaleType ); This function creates a matrix transform descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor created by this function. See cublasLtMatrixTransformDesc_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.42. cublasLtMatrixTransformDescInit()  cublasStatus_t cublasLtMatrixTransformDescInit ( cublasLtMatrixTransformDesc_t transformDesc , cudaDataType scaleType ); This function initializes a matrix transform descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor initialized by this function. See cublasLtMatrixTransformDesc_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.43. cublasLtMatrixTransformDescDestroy()  cublasStatus_t cublasLtMatrixTransformDescDestroy ( cublasLtMatrixTransformDesc_t transformDesc ); This function destroys a previously created matrix transform descriptor object. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the structure holding the matrix transform descriptor that should be destroyed by this function. See cublasLtMatrixTransformDesc_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.44. cublasLtMatrixTransformDescGetAttribute()  cublasStatus_t cublasLtMatrixTransformDescGetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix transform descriptor. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function. See cublasLtMatrixTransformDesc_t . attr Input The attribute that will be retrieved by this function. See cublasLtMatrixTransformDescAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.45. cublasLtMatrixTransformDescSetAttribute()  cublasStatus_t cublasLtMatrixTransformDescSetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix transform descriptor. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function. See cublasLtMatrixTransformDesc_t . attr Input The attribute that will be set by this function. See cublasLtMatrixTransformDescAttributes_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes does not match size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 4. Using the cuBLASXt API  4.1. General description  The cuBLASXt API of cuBLAS exposes a multi-GPU capable host interface: when using this API the application only needs to allocate the required matrices on the host memory space. Additionally, the current implementation supports managed memory on Linux with GPU devices that have compute capability 6.x or greater but treats it as host memory. Managed memory is not supported on Windows. There are no restriction on the sizes of the matrices as long as they can fit into the host memory. The cuBLASXt API takes care of allocating the memory across the designated GPUs and dispatched the workload between them and finally retrieves the results back to the host. The cuBLASXt API supports only the compute-intensive BLAS3 routines (e.g matrix-matrix operations) where the PCI transfers back and forth from the GPU can be amortized. The cuBLASXt API has its own header file cublasXt.h . Starting with release 8.0, cuBLASXt API allows any of the matrices to be located on a GPU device. Note : The cuBLASXt API is only supported on 64-bit platforms. 4.1.1. Tiling design approach  To be able to share the workload between multiples GPUs, the cuBLASXt API uses a tiling strategy : every matrix is divided in square tiles of user-controllable dimension BlockDim x BlockDim. The resulting matrix tiling defines the static scheduling policy : each resulting tile is affected to a GPU in a round robin fashion One CPU thread is created per GPU and is responsible to do the proper memory transfers and cuBLAS operations to compute all the tiles that it is responsible for. From a performance point of view, due to this static scheduling strategy, it is better that compute capabilites and PCI bandwidth are the same for every GPU. The figure below illustrates the tiles distribution between 3 GPUs. To compute the first tile G0 from C, the CPU thread 0 responsible of GPU0, have to load 3 tiles from the first row of A and tiles from the first columun of B in a pipeline fashion in order to overlap memory transfer and computations and sum the results into the first tile G0 of C before to move on to the next tile G0. Example of cublasXt<t>gemm tiling for 3 Gpus  When the tile dimension is not an exact multiple of the dimensions of C, some tiles are partially filled on the right border or/and the bottom border. The current implementation does not pad the incomplete tiles but simply keep track of those incomplete tiles by doing the right reduced cuBLAS opearations : this way, no extra computation is done. However it still can lead to some load unbalance when all GPUS do not have the same number of incomplete tiles to work on. When one or more matrices are located on some GPU devices, the same tiling approach and workload sharing is applied. The memory transfers are in this case done between devices. However, when the computation of a tile and some data are located on the same GPU device, the memory transfer to/from the local data into tiles is bypassed and the GPU operates directly on the local data. This can lead to a significant performance increase, especially when only one GPU is used for the computation. The matrices can be located on any GPU device, and do not have to be located on the same GPU device. Furthermore, the matrices can even be located on a GPU device that do not participate to the computation. On the contrary of the cuBLAS API, even if all matrices are located on the same device, the cuBLASXt API is still a blocking API from the host point of view : the data results wherever located will be valid on the call return and no device synchronization is required. 4.1.2. Hybrid CPU-GPU computation  In the case of very large problems, the cuBLASXt API offers the possibility to offload some of the computation to the host CPU. This feature can be setup with the routines cublasXtSetCpuRoutine() and cublasXtSetCpuRatio() The workload affected to the CPU is put aside : it is simply a percentage of the resulting matrix taken from the bottom and the right side whichever dimension is bigger. The GPU tiling is done after that on the reduced resulting matrix. If any of the matrices is located on a GPU device, the feature is ignored and all computation will be done only on the GPUs This feature should be used with caution because it could interfere with the CPU threads responsible of feeding the GPUs. Currently, only the routine cublasXt<t>gemm supports this feature. 4.1.3. Results reproducibility  Currently all cuBLASXt API routines from a given toolkit version, generate the same bit-wise results when the following conditions are respected : all GPUs particating to the computation have the same compute capabilities and the same number of SMs. the tiles size is kept the same between run. either the CPU hybrid computation is not used or the CPU Blas provided is also guaranteed to produce reproducible results. 4.2. cuBLASXt API Datatypes Reference  4.2.1. cublasXtHandle_t  The cublasXtHandle_t type is a pointer type to an opaque structure holding the cuBLASXt API context. The cuBLASXt API context must be initialized using cublasXtCreate() and the returned handle must be passed to all subsequent cuBLASXt API function calls. The context should be destroyed at the end using cublasXtDestroy() . 4.2.2. cublasXtOpType_t  The cublasOpType_t enumerates the four possible types supported by BLAS routines. This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration. Value Meaning CUBLASXT_FLOAT float or single precision type CUBLASXT_DOUBLE double precision type CUBLASXT_COMPLEX single precision complex CUBLASXT_DOUBLECOMPLEX double precision complex 4.2.3. cublasXtBlasOp_t  The cublasXtBlasOp_t type enumerates the BLAS3 or BLAS-like routine supported by cuBLASXt API. This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration. Value Meaning CUBLASXT_GEMM GEMM routine CUBLASXT_SYRK SYRK routine CUBLASXT_HERK HERK routine CUBLASXT_SYMM SYMM routine CUBLASXT_HEMM HEMM routine CUBLASXT_TRSM TRSM routine CUBLASXT_SYR2K SYR2K routine CUBLASXT_HER2K HER2K routine CUBLASXT_SPMM SPMM routine CUBLASXT_SYRKX SYRKX routine CUBLASXT_HERKX HERKX routine 4.2.4. cublasXtPinningMemMode_t  The type is used to enable or disable the Pinning Memory mode through the routine cubasMgSetPinningMemMode Value Meaning CUBLASXT_PINNING_DISABLED the Pinning Memory mode is disabled CUBLASXT_PINNING_ENABLED the Pinning Memory mode is enabled 4.3. cuBLASXt API Helper Function Reference  4.3.1. cublasXtCreate()  cublasStatus_t cublasXtCreate ( cublasXtHandle_t * handle ) This function initializes the cuBLASXt API and creates a handle to an opaque structure holding the cuBLASXt API context. It allocates hardware resources on the host and device and must be called prior to making any other cuBLASXt API calls. Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_NOT_SUPPORTED cuBLASXt API is only supported on 64-bit platform 4.3.2. cublasXtDestroy()  cublasStatus_t cublasXtDestroy ( cublasXtHandle_t handle ) This function releases hardware resources used by the cuBLASXt API context. The release of GPU resources may be deferred until the application exits. This function is usually the last call with a particular handle to the cuBLASXt API. Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 4.3.3. cublasXtDeviceSelect()  cublasXtDeviceSelect ( cublasXtHandle_t handle , int nbDevices , int deviceId []) This function allows the user to provide the number of GPU devices and their respective Ids that will participate to the subsequent cuBLASXt API Math function calls. This function will create a cuBLAS context for every GPU provided in that list. Currently the device configuration is static and cannot be changed between Math function calls. In that regard, this function should be called only once after cublasXtCreate . To be able to run multiple configurations, multiple cuBLASXt API contexts should be created. Return Value Meaning CUBLAS_STATUS_SUCCESS User call was sucessful CUBLAS_STATUS_INVALID_VALUE Access to at least one of the device could not be done or a cuBLAS context could not be created on at least one of the device CUBLAS_STATUS_ALLOC_FAILED Some resources could not be allocated. 4.3.4. cublasXtSetBlockDim()  cublasXtSetBlockDim ( cublasXtHandle_t handle , int blockDim ) This function allows the user to set the block dimension used for the tiling of the matrices for the subsequent Math function calls. Matrices are split in square tiles of blockDim x blockDim dimension. This function can be called anytime and will take effect for the following Math function calls. The block dimension should be chosen in a way to optimize the math operation and to make sure that the PCI transfers are well overlapped with the computation. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blockDim <= 0 4.3.5. cublasXtGetBlockDim()  cublasXtGetBlockDim ( cublasXtHandle_t handle , int * blockDim ) This function allows the user to query the block dimension used for the tiling of the matrices. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful 4.3.6. cublasXtSetCpuRoutine()  cublasXtSetCpuRoutine ( cublasXtHandle_t handle , cublasXtBlasOp_t blasOp , cublasXtOpType_t type , void * blasFunctor ) This function allows the user to provide a CPU implementation of the corresponding BLAS routine. This function can be used with the function cublasXtSetCpuRatio() to define an hybrid computation between the CPU and the GPUs. Currently the hybrid feature is only supported for the xGEMM routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blasOp or type define an invalid combination CUBLAS_STATUS_NOT_SUPPORTED CPU-GPU Hybridization for that routine is not supported 4.3.7. cublasXtSetCpuRatio()  cublasXtSetCpuRatio ( cublasXtHandle_t handle , cublasXtBlasOp_t blasOp , cublasXtOpType_t type , float ratio ) This function allows the user to define the percentage of workload that should be done on a CPU in the context of an hybrid computation. This function can be used with the function cublasXtSetCpuRoutine() to define an hybrid computation between the CPU and the GPUs. Currently the hybrid feature is only supported for the xGEMM routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blasOp or type define an invalid combination CUBLAS_STATUS_NOT_SUPPORTED CPU-GPU Hybridization for that routine is not supported 4.3.8. cublasXtSetPinningMemMode()  cublasXtSetPinningMemMode ( cublasXtHandle_t handle , cublasXtPinningMemMode_t mode ) This function allows the user to enable or disable the Pinning Memory mode. When enabled, the matrices passed in subsequent cuBLASXt API calls will be pinned/unpinned using the CUDART routine cudaHostRegister() and cudaHostUnregister() respectively if the matrices are not already pinned. If a matrix happened to be pinned partially, it will also not be pinned. Pinning the memory improve PCI transfer performace and allows to overlap PCI memory transfer with computation. However pinning/unpinning the memory take some time which might not be amortized. It is advised that the user pins the memory on its own using cudaMallocHost() or cudaHostRegister() and unpin it when the computation sequence is completed. By default, the Pinning Memory mode is disabled. Note The Pinning Memory mode should not enabled when matrices used for different calls to cuBLASXt API overlap. cuBLASXt determines that a matrix is pinned or not if the first address of that matrix is pinned using cudaHostGetFlags() , thus cannot know if the matrix is already partially pinned or not. This is especially true in multi-threaded application where memory could be partially or totally pinned or unpinned while another thread is accessing that memory. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE the mode value is different from CUBLASXT_PINNING_DISABLED and CUBLASXT_PINNING_ENABLED 4.3.9. cublasXtGetPinningMemMode()  cublasXtGetPinningMemMode ( cublasXtHandle_t handle , cublasXtPinningMemMode_t * mode ) This function allows the user to query the Pinning Memory mode. By default, the Pinning Memory mode is disabled. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful 4.4. cuBLASXt API Math Functions Reference  In this chapter we describe the actual Linear Agebra routines that cuBLASXt API supports. We will use abbreviations < type > for type and < t > for the corresponding short type to make a more concise and clear presentation of the implemented functions. Unless otherwise specified < type > and < t > have the following meanings: <type> <t> Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision The abbreviation \\(\\mathbf{Re}(\\cdot)\\) and \\(\\mathbf{Im}(\\cdot)\\) will stand for the real and imaginary part of a number, respectively. Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. Also, the \\(\\bar{\\alpha}\\) will denote the complex conjugate of \\(\\alpha\\) . In general throughout the documentation, the lower case Greek symbols \\(\\alpha\\) and \\(\\beta\\) will denote scalars, lower case English letters in bold type \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) will denote vectors and capital English letters \\(A\\) , \\(B\\) and \\(C\\) will denote matrices. 4.4.1. cublasXt<t>gemm()  cublasStatus_t cublasXtSgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , size_t m , size_t n , size_t k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasXtDgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasXtCgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs the matrix-matrix multiplication \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemm , dgemm , cgemm , zgemm 4.4.2. cublasXt<t>hemm()  cublasStatus_t cublasXtChemm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZhemm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the Hermitian matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a Hermitian matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chemm , zhemm 4.4.3. cublasXt<t>symm()  cublasStatus_t cublasXtSsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the symmetric matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a symmetric matrix stored in lower or upper mode, \\(A\\) and \\(A\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. m input number of rows of matrix A and B , with matrix A sized accordingly. n input number of columns of matrix C and A , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymm , dsymm , csymm , zsymm 4.4.4. cublasXt<t>syrk()  cublasStatus_t cublasXtSsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * beta , float * C , int ldc ) cublasStatus_t cublasXtDsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * beta , double * C , int ldc ) cublasStatus_t cublasXtCsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A. beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 4.4.5. cublasXt<t>syr2k()  cublasStatus_t cublasXtSsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the symmetric rank- \\(2k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\text{op}(B)\\text{op}(A)^{T}) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr2k , dsyr2k , csyr2k , zsyr2k 4.4.6. cublasXt<t>syrkx()  cublasStatus_t cublasXtSsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs a variation of the symmetric rank- \\(k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric. An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk and ssyr2k , dsyr2k , csyr2k , zsyr2k 4.4.7. cublasXt<t>herk()  cublasStatus_t cublasXtCherk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZherk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk 4.4.8. cublasXt<t>her2k()  cublasStatus_t cublasXtCher2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const float * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZher2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const double * beta , cuDoubleComplex * C , size_t ldc ) This function performs the Hermitian rank- \\(2k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\overset{ˉ}{\\alpha}\\text{op}(B)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher2k , zher2k 4.4.9. cublasXt<t>herkx()  cublasStatus_t cublasXtCherkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const float * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZherkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const double * beta , cuDoubleComplex * C , size_t ldc ) This function performs a variation of the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian. An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasXt<t>dgmm . Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input real scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk and cher2k , zher2k 4.4.10. cublasXt<t>trsm()  cublasStatus_t cublasXtStrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const float * alpha , const float * A , size_t lda , float * B , size_t ldb ) cublasStatus_t cublasXtDtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const double * alpha , const double * A , size_t lda , double * B , size_t ldb ) cublasStatus_t cublasXtCtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , cuComplex * B , size_t ldb ) cublasStatus_t cublasXtZtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , cuDoubleComplex * B , size_t ldb ) This function solves the triangular linear system with multiple right-hand-sides \\(\\left\\{ \\begin{matrix}\n{\\text{op}(A)X = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{X\\text{op}(A) = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\) and \\(B\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(X\\) overwrites the right-hand-sides \\(B\\) on exit. No test for singularity or near-singularity is included in this function. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of X . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A is sized accordingly. alpha host input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device in/out <type> array. It has dimensions ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsm , dtrsm , ctrsm , ztrsm 4.4.11. cublasXt<t>trmm()  cublasStatus_t cublasXtStrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , float * C , size_t ldc ) cublasStatus_t cublasXtDtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , double * C , size_t ldc ) cublasStatus_t cublasXtCtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , cuDoubleComplex * C , size_t ldc ) This function performs the triangular matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha\\text{op}(A)B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha B\\text{op}(A)} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrix, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Notice that in order to achieve better parallelism, similarly to the cublas API, cuBLASXt API differs from the BLAS API for this routine. The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLASXt API assumes an out-of-place implementation (with results written into C). The application can still obtain the in-place functionality of BLAS in the cuBLASXt API by passing the address of the matrix B in place of the matrix C. No other overlapping in the input parameters is supported. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . C host or device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strmm , dtrmm , ctrmm , ztrmm 4.4.12. cublasXt<t>spmm()  cublasStatus_t cublasXtSspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const float * alpha , const float * AP , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ); cublasStatus_t cublasXtDspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const double * alpha , const double * AP , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ); cublasStatus_t cublasXtCspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ); cublasStatus_t cublasXtZspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ); This function performs the symmetric packed matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Note The packed matrix AP must be located on the host or managed memory whereas the other matrices can be located on the host or any GPU device Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. m input number of rows of matrix A and B , with matrix A sized accordingly. n input number of columns of matrix C and A , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication. AP host input <type> array with \\(A\\) stored in packed format. B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_NOT_SUPPORTED the matrix AP is located on a GPU device CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymm , dsymm , csymm , zsymm 5. Using the cuBLASDx API  The cuBLASDx library (preview) is a device side API extension for performing BLAS calculations inside CUDA kernels.\nBy fusing numerical operations you can decrease latency and further improve performance of your applications. You can access cuBLASDx documentation here . cuBLASDx is not a part of the CUDA Toolkit. You can download cuBLASDx separately from here . 6. Using the cuBLAS Legacy API  This section does not provide a full reference of each Legacy API datatype and entry point. Instead, it describes how to use the API, especially where this is different from the regular cuBLAS API. Note that in this section, all references to the “cuBLAS Library” refer to the Legacy cuBLAS API only. Warning The legacy cuBLAS API is deprecated and will be removed in future release. 6.1. Error Status  The cublasStatus type is used for function status returns. The cuBLAS Library helper functions return status directly, while the status of core functions can be retrieved using cublasGetError() . Notice that reading the error status via cublasGetError() , resets the internal error state to CUBLAS_STATUS_SUCCESS . Currently, the following values for are defined: Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the resource allocation failed CUBLAS_STATUS_INVALID_VALUE an invalid numerical value was used as an argument CUBLAS_STATUS_ARCH_MISMATCH an absent device architectural feature is required CUBLAS_STATUS_MAPPING_ERROR an access to GPU memory space failed CUBLAS_STATUS_EXECUTION_FAILED the GPU program failed to execute CUBLAS_STATUS_INTERNAL_ERROR an internal operation failed CUBLAS_STATUS_NOT_SUPPORTED the feature required is not supported This legacy type corresponds to type cublasStatus_t in the cuBLAS library API. 6.2. Initialization and Shutdown  The functions cublasInit() and cublasShutdown() are used to initialize and shutdown the cuBLAS library. It is recommended for cublasInit() to be called before any other function is invoked. It allocates hardware resources on the GPU device that is currently bound to the host thread from which it was invoked. The legacy initialization and shutdown functions are similar to the cuBLAS library API routines cublasCreate() and cublasDestroy() . 6.3. Thread Safety  The legacy API is not thread safe when used with multiple host threads and devices. It is recommended to be used only when utmost compatibility with Fortran is required and when a single host thread is used to setup the library and make all the functions calls. 6.4. Memory Management  The memory used by the legacy cuBLAS library API is allocated and released using functions cublasAlloc() and cublasFree() , respectively. These functions create and destroy an object in the GPU memory space capable of holding an array of n elements, where each element requires elemSize bytes of storage. Please see the legacy cuBLAS API header file “cublas.h” for the prototypes of these functions. The function cublasAlloc() is a wrapper around the function cudaMalloc() , therefore device pointers returned by cublasAlloc() can be passed to any CUDA™ device kernel functions. However, these device pointers can not be dereferenced in the host code. The function cublasFree() is a wrapper around the function cudaFree() . 6.5. Scalar Parameters  In the legacy cuBLAS API, scalar parameters are passed by value from the host. Also, the few functions that do return a scalar result, such as dot() and nrm2(), return the resulting value on the host, and hence these routines will wait for kernel execution on the device to complete before returning, which makes parallelism with streams impractical. However, the majority of functions do not return any value, in order to be more compatible with Fortran and the existing BLAS libraries. 6.6. Helper Functions  In this section we list the helper functions provided by the legacy cuBLAS API and their functionality. For the exact prototypes of these functions please refer to the legacy cuBLAS API header file “cublas.h”. Helper function Meaning cublasInit() initialize the library cublasShutdown() shuts down the library cublasGetError() retrieves the error status of the library cublasSetKernelStream() sets the stream to be used by the library cublasAlloc() allocates the device memory for the library cublasFree() releases the device memory allocated for the library cublasSetVector() copies a vector x on the host to a vector on the GPU cublasGetVector() copies a vector x on the GPU to a vector on the host cublasSetMatrix() copies a \\(m \\times n\\) tile from a matrix on the host to the GPU cublasGetMatrix() copies a \\(m \\times n\\) tile from a matrix on the GPU to the host cublasSetVectorAsync() similar to cublasSetVector() , but the copy is asynchronous cublasGetVectorAsync() similar to cublasGetVector() , but the copy is asynchronous cublasSetMatrixAsync() similar to cublasSetMatrix() , but the copy is asynchronous cublasGetMatrixAsync() similar to cublasGetMatrix() , but the copy is asynchronous 6.7. Level-1,2,3 Functions  The Level-1,2,3 cuBLAS functions (also called core functions) have the same name and behavior as the ones listed in the chapters 3, 4 and 5 in this document. Please refer to the legacy cuBLAS API header file “cublas.h” for their exact prototype. Also, the next section talks a bit more about the differences between the legacy and the cuBLAS API prototypes, more specifically how to convert the function calls from one API to another. 6.8. Converting Legacy to the cuBLAS API  There are a few general rules that can be used to convert from legacy to the cuBLAS API: Exchange the header file “cublas.h” for “cublas_v2.h”. Exchange the type cublasStatus for cublasStatus_t . Exchange the function cublasSetKernelStream() for cublasSetStream() . Exchange the function cublasAlloc() and cublasFree() for cudaMalloc() and cudaFree() , respectively. Notice that cudaMalloc() expects the size of the allocated memory to be provided in bytes (usually simply provide n x elemSize to allocate n elements, each of size elemSize bytes). Declare the cublasHandle_t cuBLAS library handle. Initialize the handle using cublasCreate() . Also, release the handle once finished using cublasDestroy() . Add the handle as the first parameter to all the cuBLAS library function calls. Change the scalar parameters to be passed by reference, instead of by value (usually simply adding “&” symbol in C/C++ is enough, because the parameters are passed by reference on the host by default ). However, note that if the routine is running asynchronously, then the variable holding the scalar parameter cannot be changed until the kernels that the routine dispatches are completed. See the CUDA C++ Programming Guide for a detailed discussion of how to use streams. Change the parameter characters N or n (non-transpose operation), T or t (transpose operation) and C or c (conjugate transpose operation) to CUBLAS_OP_N , CUBLAS_OP_T and CUBLAS_OP_C , respectively. Change the parameter characters L or l (lower part filled) and U or u (upper part filled) to CUBLAS_FILL_MODE_LOWER and CUBLAS_FILL_MODE_UPPER , respectively. Change the parameter characters N or n (non-unit diagonal) and U or u (unit diagonal) to CUBLAS_DIAG_NON_UNIT and CUBLAS_DIAG_UNIT , respectively. Change the parameter characters L or l (left side) and R or r (right side) to CUBLAS_SIDE_LEFT and CUBLAS_SIDE_RIGHT , respectively. If the legacy API function returns a scalar value, add an extra scalar parameter of the same type passed by reference, as the last parameter to the same function. Instead of using cublasGetError() , use the return value of the function itself to check for errors. Finally, please use the function prototypes in the header files cublas.h and cublas_v2.h to check the code for correctness. 6.9. Examples  For sample code references that use the legacy cuBLAS API please see the two examples below. They show an application written in C using the legacy cuBLAS library API with two indexing styles (Example A.1. “Application Using C and cuBLAS: 1-based indexing” and Example A.2. “Application Using C and cuBLAS: 0-based Indexing”). This application is analogous to the one using the cuBLAS library API that is shown in the Introduction chapter. Example A.1. Application Using C and cuBLAS: 1-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include \"cublas.h\" #define M 6 #define N 5 #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) static __inline__ void modify ( float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( n - q + 1 , alpha , & m [ IDX2F ( p , q , ldm )], ldm ); cublasSscal ( ldm - p + 1 , beta , & m [ IDX2F ( p , q , ldm )], 1 ); } int main ( void ){ int i , j ; cublasStatus stat ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { a [ IDX2F ( i , j , M )] = ( float )(( i -1 ) * M + j ); } } cublasInit (); stat = cublasAlloc ( M * N , sizeof ( * a ), ( void ** ) & devPtrA ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"device memory allocation failed\" ); cublasShutdown (); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } modify ( devPtrA , M , N , 2 , 3 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } cublasFree ( devPtrA ); cublasShutdown (); for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2F ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } Example A.2. Application Using C and cuBLAS: 0-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include \"cublas.h\" #define M 6 #define N 5 #define IDX2C(i,j,ld) (((j)*(ld))+(i)) static __inline__ void modify ( float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( n - q , alpha , & m [ IDX2C ( p , q , ldm )], ldm ); cublasSscal ( ldm - p , beta , & m [ IDX2C ( p , q , ldm )], 1 ); } int main ( void ){ int i , j ; cublasStatus stat ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { a [ IDX2C ( i , j , M )] = ( float )( i * M + j + 1 ); } } cublasInit (); stat = cublasAlloc ( M * N , sizeof ( * a ), ( void ** ) & devPtrA ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"device memory allocation failed\" ); cublasShutdown (); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } modify ( devPtrA , M , N , 1 , 2 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } cublasFree ( devPtrA ); cublasShutdown (); for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2C ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } 7. cuBLAS Fortran Bindings  The cuBLAS library is implemented using the C-based CUDA toolchain. Thus, it provides a C-style API. This makes interfacing to applications written in C and C++ trivial, but the library can also be used by applications written in Fortran. In particular, the cuBLAS library uses 1-based indexing and Fortran-style column-major storage for multidimensional data to simplify interfacing to Fortran applications. Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform and toolchain. In particular, differences may exist in the following areas: symbol names (capitalization, name decoration) argument passing (by value or reference) passing of string arguments (length information) passing of pointer arguments (size of the pointer) returning floating-point or compound data types (for example single-precision or complex data types) To provide maximum flexibility in addressing those differences, the cuBLAS Fortran interface is provided in the form of wrapper functions and is part of the Toolkit delivery. The C source code of those wrapper functions is located in the src directory and provided in two different forms: the thunking wrapper interface located in the file fortran_thunking.c the direct wrapper interface located in the file fortran.c The code of one of those two files needs to be compiled into an application for it to call the cuBLAS API functions. Providing source code allows users to make any changes necessary for a particular platform and toolchain. The code in those two C files has been used to demonstrate interoperability with the compilers g77 3.2.3 and g95 0.91 on 32-bit Linux, g77 3.4.5 and g95 0.91 on 64-bit Linux, Intel Fortran 9.0 and Intel Fortran 10.0 on 32-bit and 64-bit Microsoft Windows XP, and g77 3.4.0 and g95 0.92 on Mac OS X. Note that for g77, use of the compiler flag -fno-second-underscore is required to use these wrappers as provided. Also, the use of the default calling conventions with regard to argument and return value passing is expected. Using the flag -fno-f2c changes the default calling convention with respect to these two items. The thunking wrappers allow interfacing to existing Fortran applications without any changes to the application. During each call, the wrappers allocate GPU memory, copy source data from CPU memory space to GPU memory space, call cuBLAS, and finally copy back the results to CPU memory space and deallocate the GPU memory. As this process causes very significant call overhead, these wrappers are intended for light testing, not for production code. To use the thunking wrappers, the application needs to be compiled with the file fortran_thunking.c . The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all BLAS functions. To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using cuBLAS_ALLOC and cuBLAS_FREE ) and to copy data between GPU and CPU memory spaces (using cuBLAS_SET_VECTOR , cuBLAS_GET_VECTOR , cuBLAS_SET_MATRIX , and cuBLAS_GET_MATRIX ). The sample wrappers provided in fortran.c map device pointers to the OS-dependent type size_t , which is 32-bit wide on 32-bit platforms and 64-bit wide on a 64-bit platforms. One approach to deal with index arithmetic on device pointers in Fortran code is to use C-style macros, and use the C preprocessor to expand these, as shown in the example below. On Linux and Mac OS X, one way of pre-processing is to use the option -E -x f77-cpp-input when using g77 compiler, or simply the option -cpp when using g95 or gfortran. On Windows platforms with Microsoft Visual C/C++, using ’cl -EP’ achieves similar results. ! Example B.1. Fortran 77 Application Executing on the Host ! ---------------------------------------------------------- subroutine modify ( m , ldm , n , p , q , alpha , beta ) implicit none integer ldm , n , p , q real * 4 m ( ldm , * ) , alpha , beta external cublas_sscal call cublas_sscal ( n - p + 1 , alpha , m ( p , q ), ldm ) call cublas_sscal ( ldm - p + 1 , beta , m ( p , q ), 1 ) return end program matrixmod implicit none integer M , N parameter ( M = 6 , N = 5 ) real * 4 a ( M , N ) integer i , j external cublas_init external cublas_shutdown do j = 1 , N do i = 1 , M a ( i , j ) = ( i - 1 ) * M + j enddo enddo call cublas_init call modify ( a , M , N , 2 , 3 , 1 6.0 , 1 2.0 ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7.0$)\" ) a ( i , j ) enddo write ( * , * ) \"\" enddo stop end When traditional fixed-form Fortran 77 code is ported to use the cuBLAS library, line length often increases when the BLAS calls are exchanged for cuBLAS calls. Longer function names and possible macro expansion are contributing factors. Inadvertently exceeding the maximum line length can lead to run-time errors that are difficult to find, so care should be taken not to exceed the 72-column limit if fixed form is retained. The examples in this chapter show a small application implemented in Fortran 77 on the host and the same application with the non-thunking wrappers after it has been ported to use the cuBLAS library. The second example should be compiled with ARCH_64 defined as 1 on 64-bit OS system and as 0 on 32-bit OS system. For example for g95 or gfortran, this can be done directly on the command line by using the option -cpp -DARCH_64=1 . ! Example B.2. Same Application Using Non-thunking cuBLAS Calls !------------------------------------------------------------- #define IDX2F (i,j,ld) ((((j)-1)*(ld))+((i)-1)) subroutine modify ( devPtrM , ldm , n , p , q , alpha , beta ) implicit none integer sizeof_real parameter ( sizeof_real = 4 ) integer ldm , n , p , q #if ARCH_64 integer * 8 devPtrM #else integer * 4 devPtrM #endif real * 4 alpha , beta call cublas_sscal ( n - p + 1 , alpha , 1 devPtrM + IDX2F ( p , q , ldm ) * sizeof_real , 2 ldm ) call cublas_sscal ( ldm - p + 1 , beta , 1 devPtrM + IDX2F ( p , q , ldm ) * sizeof_real , 2 1 ) return end program matrixmod implicit none integer M , N , sizeof_real #if ARCH_64 integer * 8 devPtrA #else integer * 4 devPtrA #endif parameter ( M = 6 , N = 5 , sizeof_real = 4 ) real * 4 a ( M , N ) integer i , j , stat external cublas_init , cublas_set_matrix , cublas_get_matrix external cublas_shutdown , cublas_alloc integer cublas_alloc , cublas_set_matrix , cublas_get_matrix do j = 1 , N do i = 1 , M a ( i , j ) = ( i - 1 ) * M + j enddo enddo call cublas_init stat = cublas_alloc ( M * N , sizeof_real , devPtrA ) if ( stat . NE . 0 ) then write ( * , * ) \"device memory allocation failed\" call cublas_shutdown stop endif stat = cublas_set_matrix ( M , N , sizeof_real , a , M , devPtrA , M ) if ( stat . NE . 0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data download failed\" call cublas_shutdown stop endif — — Code block continues below. Space added for formatting purposes. — — call modify ( devPtrA , M , N , 2 , 3 , 16.0 , 12.0 ) stat = cublas_get_matrix ( M , N , sizeof_real , devPtrA , M , a , M ) if ( stat . NE .0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data upload failed\" call cublas_shutdown stop endif call cublas_free ( devPtrA ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7.0$)\" ) a ( i , j ) enddo write ( * , * ) \"\" enddo stop end 8. Interaction with Other Libraries and Tools  This section describes important requirements and recommendations that ensure correct use of cuBLAS with other libraries and utilities. 8.1. nvprune  nvprune enables pruning relocatable host objects and static libraries to only contain device code for the specific target architectures. In case of cuBLAS, particular care must be taken if using nvprune with compute capabilities, whose minor revision number is different than 0. To reduce binary size, cuBLAS may only store major revision equivalents of CUDA binary files for kernels reused between different minor revision versions. Therefore, to ensure that a pruned library does not fail for arbitrary problems, the user must keep binaries for a selected architecture and all prior minor architectures in its major architecture. For example, the following call prunes libcublas_static.a to contain only sm_75 (Turing) and sm_70 (Volta) cubins: nvprune -- generate - code code = sm_70 -- generate - code code = sm_75 libcublasLt_static . a - o libcublasLt_static_sm70_sm75 . a which should be used instead of: nvprune - arch = sm_75 libcublasLt_static . a - o libcublasLt_static_sm75 . a 9. Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: Portions of the SGEMM, DGEMM, CGEMM and ZGEMM library routines were written by Vasily Volkov of the University of California. Portions of the SGEMM, DGEMM and ZGEMM library routines were written by Davide Barbieri of the University of Rome Tor Vergata. Portions of the DGEMM and SGEMM library routines optimized for Fermi architecture were developed by the University of Tennessee. Subsequently, several other routines that are optimized for the Fermi architecture have been derived from these initial DGEMM and SGEMM implementations. The substantial optimizations of the STRSV, DTRSV, CTRSV and ZTRSV library routines were developed by Jonathan Hogg of The Science and Technology Facilities Council (STFC). Subsequently, some optimizations of the STRSM, DTRSM, CTRSM and ZTRSM have been derived from these TRSV implementations. Substantial optimizations of the SYMV and HEMV library routines were developed by Ahmad Abdelfattah, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST). Substantial optimizations of the TRMM and TRSM library routines were developed by Ali Charara, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST). This product includes {fmt} - A modern formatting library https://fmt.dev Copyright (c) 2012 - present, Victor Zverovich. This product includes spdlog - Fast C++ logging library. https://github.com/gabime/spdlog The MIT License (MIT). This product includes SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT https://sleef.org Boost Software License - Version 1.0 - August 17th, 2003. This product includes Frozen - a header-only, constexpr alternative to gperf for C++14 users. https://github.com/serge-sans-paille/frozen Apache License - Version 2.0, January 2004. This product includes Boost C++ Libraries - free peer-reviewed portable C++ source libraries https://www.boost.org/ Boost Software License - Version 1.0 - August 17th, 2003. This product includes Zstandard - a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios. https://github.com/facebook/zstd The BSD License. 10. Notices  10.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/index.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback Libdevice User's Guide User's guide to libdevice Table of Contents 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html", "content_type": "text/html", "text": "NVVM IR Specification 1. Introduction 2. Identifiers 3. High Level Structure 3.1. Linkage Types 3.2. Calling Conventions 3.2.1. Rules and Restrictions 3.3. Visibility Styles 3.4. DLL Storage Classes 3.5. Thread Local Storage Models 3.6. Runtime Preemption Specifiers 3.7. Structure Types 3.8. Non-Integral Pointer Type 3.9. Comdats 3.10. source_filename 3.11. Global Variables 3.12. Functions 3.13. Aliases 3.14. Ifuncs 3.15. Named Metadata 3.16. Parameter Attributes 3.17. Garbage Collector Strategy Names 3.18. Prefix Data 3.19. Prologue Data 3.20. Attribute Groups 3.21. Function Attributes 3.22. Global Attributes 3.23. Operand Bundles 3.24. Module-Level Inline Assembly 3.25. Data Layout 3.26. Target Triple 3.27. Pointer Aliasing Rules 3.28. Volatile Memory Access 3.29. Memory Model for Concurrent Operations 3.30. Atomic Memory Ordering Constraints 3.31. Fast-Math Flags 3.32. Use-list Order Directives 4. Type System 5. Constants 6. Other Values 6.1. Inline Assembler Expressions 7. Metadata 7.1. Metadata Nodes and Metadata Strings 8. ThinLTO Summary 9. Intrinsic Global Variables 10. Instructions 10.1. Terminator Instructions 10.2. Binary Operations 10.3. Bitwise Binary Operations 10.4. Vector Operations 10.5. Aggregate Operations 10.6. Memory Access and Addressing Operations 10.6.1. alloca Instruction 10.6.2. load Instruction 10.6.3. store Instruction 10.6.4. fence Instruction 10.6.5. cmpxchg Instruction 10.6.6. atomicrmw Instruction 10.6.7. getelementptr Instruction 10.7. Conversion Operations 10.8. Other Operations 11. Intrinsic Functions 11.1. Variable Argument Handling Intrinsics 11.2. Accurate Garbage Collection Intrinsics 11.3. Code Generator Intrinsics 11.4. Standard C Library Intrinsics 11.5. Bit Manipulations Intrinsics 11.6. Specialised Arithmetic Intrinsics 11.7. Arithmetic with Overflow Intrinsics 11.8. Half Precision Floating Point Intrinsics 11.9. Debugger Intrinsics 11.10. Exception Handling Intrinsics 11.11. Trampoline Intrinsics 11.12. Masked Vector Load and Store Intrinsics 11.13. Masked Vector Expanding Load and Compressing Store Intrinsics 11.14. Experimental Vector Reduction Intrinsics 11.15. Constrained Floating Point Intrinsics 11.16. Constrained libm-equivalent Intrinsics 11.17. Masked Vector Gather and Scatter Intrinsics 11.18. Memory Use Markers 11.19. General Intrinsics 11.20. Element Wise Atomic Memory Intrinsics 11.21. Stack Map Intrinsics 12. Address Space 12.1. Address Spaces 12.2. Generic Pointers and Non-Generic Pointers 12.2.1. Generic Pointers vs. Non-generic Pointers 12.2.2. Conversion 12.2.3. No Aliasing between Two Different Specific Address Spaces 12.3. The alloca Instruction 13. Global Property Annotation 13.1. Overview 13.2. Representation of Properties 13.3. Supported Properties 14. Texture and Surface 14.1. Texture Variable and Surface Variable 14.2. Accessing Texture Memory or Surface Memory 15. NVVM Specific Intrinsic Functions 15.1. Atomic 15.2. Barrier and Memory Fence 15.3. Address space conversion 15.4. Special Registers 15.5. Texture/Surface Access 15.5.1. Texture Reads 15.5.2. Surface Loads 15.5.3. Surface Stores 15.6. Warp-level Operations 15.6.1. Barrier Synchronization 15.6.2. Data Movement 15.6.3. Vote 15.6.4. Match 15.6.5. Matrix Operation 15.6.5.1. Load Fragments 15.6.5.2. Store Fragments 15.6.5.3. Matrix Multiply-and-Accumulate 16. Source Level Debugging Support 17. NVVM ABI for PTX 17.1. Linkage Types 17.2. Parameter Passing and Return 18. Revision History 19. Notices 19.1. Notice 19.2. OpenCL 19.3. Trademarks NVVM IR Specification » 1. Introduction v12.5 | PDF | Archive NVVM IR Specification Reference guide to the NVVM compiler (intermediate representation) based on the LLVM IR. 1. Introduction  NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR. The NVVM compiler (which is based on LLVM) generates PTX code from NVVM IR. NVVM IR and NVVM compilers are mostly agnostic about the source language being used. The PTX codegen part of a NVVM compiler needs to know the source language because of the difference in DCI (driver/compiler interface). NVVM IR is a binary format and is based on a subset of LLVM IR bitcode format. This document uses only human-readable form to describe NVVM IR. Technically speaking, NVVM IR is LLVM IR with a set of rules, restrictions, and conventions, plus a set of supported intrinsic functions. A program specified in NVVM IR is always a legal LLVM program. A legal LLVM program may not be a legal NVVM program. There are three levels of support for NVVM IR. Supported: The feature is fully supported. Most IR features should fall into this category. Accepted and ignored: The NVVM compiler will accept this IR feature, but will ignore the required semantics. This applies to some IR features that do not have meaningful semantics on GPUs and that can be ignored. Calling convention markings are an example. Illegal, not supported: The specified semantics is not supported, such as a fence instruction. Future versions of NVVM may either support or accept and ignore IRs that are illegal in the current version. This document describes version 2.0 of the NVVM IR and version 3.1 of the NVVM debug metadata (see Source Level Debugging Support ).  The 2.0 version of NVVM IR is incompatible with the previous version 1.11.  Linking of NVVM IR Version 1.11 with 2.0 will result in compiler error. The current NVVM IR is based on LLVM 7.0.1. For the complete semantics of the IR, readers of this document should check the official LLVM Language Reference Manual ( https://releases.llvm.org/7.0.1/docs/LangRef.html ). 2. Identifiers  The name of a named global identifier must have the form: @[a-zA-Z$_][a-zA-Z$_0-9]* Note that it cannot contain the . character. [@%]llvm.nvvm.* and [@%]nvvm.* are reserved words. 3. High Level Structure  3.1. Linkage Types  Supported: private internal available_externally linkonce weak common linkonce_odr weak_odr external Not supported: appending extern_weak See NVVM ABI for PTX for details on how linkage types are translated to PTX. 3.2. Calling Conventions  All LLVM calling convention markings are accepted and ignored. Functions and calls are generated according to the PTX calling convention. 3.2.1. Rules and Restrictions  When an argument with width less than 32-bit is passed, the zeroext/signext parameter attribute should be set. zeroext will be assumed if not set. When a value with width less than 32-bit is returned, the zeroext/signext parameter attribute should be set. zeroext will be assumed if not set. Arguments of aggregate or vector types that are passed by value can be passed by pointer with the byval attribute set (referred to as the by-pointer-byval case below). The align attribute must be set if the type requires a non-natural alignment (natural alignment is the alignment inferred for the aggregate type according to the Data Layout section). If a function has an argument of aggregate or vector type that is passed by value directly and the type has a non-natural alignment requirement, the alignment must be annotated by the global property annotation < align , alignment>, where alignment is a 32-bit integer whose upper 16 bits represent the argument position (starting from 1) and the lower 16 bits represent the alignment. If the return type of a function is an aggregate or a vector that has a non-natural alignment, then the alignment requirement must be annotated by the global property annotation < align , alignment>, where the upper 16 bits is 0, and the lower 16 bits represent the alignment. It is not required to annotate a function with < align , alignment> otherwise. If annotated, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case. For an indirect call instruction of a function that has a non-natural alignment for its return value or one of its arguments that is not expressed in alignment in the by-pointer-byval case, the call instruction must have an attached metadata of kind callalign . The metadata contains a sequence of i32 fields each of which represents a non-natural alignment requirement. The upper 16 bits of an i32 field represent the argument position (0 for return value, 1 for the first argument, and so on) and the lower 16 bits represent the alignment. The i32 fields must be sorted in the increasing order. For example, % call = call % struct . S % fp1 ( % struct . S * byval align 8 % arg1p , % struct . S % arg2 ), ! callalign ! 10 ! 10 = ! { i32 8 , i32 520 }; It is not required to have an i32 metadata field for the other arguments or the return value otherwise. If presented, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case . It is not required to have a callalign metadata attached to a direct call instruction. If attached, the alignment must match the natural alignment or the alignment in the by-pointer-byval case. The absence of the metadata in an indirect call instruction means using natural alignment or the align attribute in the by-pointer-byval case. 3.3. Visibility Styles  All styles—default, hidden, and protected—are accepted and ignored. 3.4. DLL Storage Classes  Not supported. 3.5. Thread Local Storage Models  Not supported. 3.6. Runtime Preemption Specifiers  Not supported. 3.7. Structure Types  Fully supported. 3.8. Non-Integral Pointer Type  Not supported. 3.9. Comdats  Not supported. 3.10. source_filename  Accepted and ignored. 3.11. Global Variables  A global variable, that is not an intrinsic global variable, may be optionally declared to reside in one of the following address spaces: global shared constant If no address space is explicitly specified, the global variable is assumed to reside in the global address space with a generic address value. See Address Space for details. thread_local variables are not supported. No explicit section (except for the metadata section) is allowed. Initializations of shared variables are not supported. Use undef initialization. 3.12. Functions  The following are not supported on functions: Alignment Explicit section Garbage collector name Prefix data Prologue Personality 3.13. Aliases  Supported only as aliases of non-kernel functions. 3.14. Ifuncs  Not supported. 3.15. Named Metadata  Accepted and ignored, except for the following: !nvvm.annotations : see Global Property Annotation !nvvmir.version !llvm.dbg.cu !llvm.module.flags The NVVM IR version is specified using a named metadata called !nvvmir.version . The !nvvmir.version named metadata may have one metadata node that contains the NVVM IR version for that module. If multiple such modules are linked together, the named metadata in the linked module may have more than one metadata node with each node containing a version. A metadata node with NVVM IR version takes either of the following forms: It may consist of two i32 values—the first denotes the NVVM IR major version number and the second denotes the minor version number. If absent, the version number is assumed to be 1.0, which can be specified as: !nvvmir.version = !{!0}\n!0 = !{i32 1, i32 0} It may consist of four i32 values—the first two denote the NVVM IR major and minor versions respectively. The third value denotes the NVVM IR debug metadata major version number, and the fourth value denotes the corresponding minor version number. If absent, the version number is assumed to be 1.0, which can be specified as: !nvvmir.version = !{!0}\n!0 = !{i32 1, i32 0, i32 1, i32 0} The version of NVVM IR described in this document is 2.0. The version of NVVM IR debug metadata described in this document is 3.1. 3.16. Parameter Attributes  Fully supported, except the following: Accepted and ignored: inreg nest Not supported: inalloca swiftself swifterror See Calling Conventions for the use of the attributes. 3.17. Garbage Collector Strategy Names  Not supported. 3.18. Prefix Data  Not supported. 3.19. Prologue Data  Not supported. 3.20. Attribute Groups  Fully supported. The set of supported attributes is equal to the set of attributes accepted where the attribute group is used. 3.21. Function Attributes  Supported: allocsize alwaysinline cold convergent inaccessiblememonly inaccessiblemem_or_argmemonly inlinehint minsize no-jump-tables noduplicate noinline noreturn norecurse nounwind \"null-pointer-is-valid\" optforfuzzing optnone optsize readnone readonly writeonly argmemonly speculatable strictfp Not Supported: alignstack builtin nonlazybind naked nobuiltin noimplicitfloat noredzone \"patchable-function\" probe-stack returns_twice sanitize_address sanitize_memory sanitize_thread sanitize_hwaddress ssp sspreq sspstrong \"stack-probe-size\" \"no-stack-arg-probe\" uwtable jumptable safestack \"thunk\" nocf_check shadowcallstack 3.22. Global Attributes  Not supported. 3.23. Operand Bundles  Not supported. 3.24. Module-Level Inline Assembly  Supported. 3.25. Data Layout  Only the following data layout is supported: 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 The following data layouts are deprecated and will be removed in a future release. 32-bit e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 3.26. Target Triple  Only the following target triple is supported, where * can be any name: 64-bit: nvptx64-*-cuda The following target triple is deprecated, and will be removed in future release: 32-bit: nvptx-*-cuda 3.27. Pointer Aliasing Rules  Fully supported. 3.28. Volatile Memory Access  Fully supported. Note that for code generation: ld.volatile and st.volatile will be generated. 3.29. Memory Model for Concurrent Operations  Not applicable. Threads in an NVVM IR program must use atomic operations or barrier synchronization to communicate. 3.30. Atomic Memory Ordering Constraints  Atomic loads and stores are not supported. Other atomic operations on other than 32-bit or 64-bit operands are not supported. 3.31. Fast-Math Flags  Supported. 3.32. Use-list Order Directives  Not supported. 4. Type System  Fully supported, except for the following: Floating point types half , fp128 , x86_fp80 and ppc_fp128 are not supported. The x86_mmx type is not supported. The token type is not supported. The non-integral pointer type is not supported. 5. Constants  Fully supported, except for the following: Token constants is not supported. blockaddress(@function, %block) is not supported. For a constant expression that is used as the initializer of a global variable @g1 , if the constant expression contains a global identifier @g2 , then the constant expression is supported if it can be reduced to the form of bitcast+offset , where offset is an integer number (including 0 ) 6. Other Values  6.1. Inline Assembler Expressions  Inline assembler of PTX instructions is supported, with the following supported constraints: Constraint Type c i8 h i16 r i32 l i64 f f32 d f64 The inline asm metadata !srcloc is accepted and ignored. The inline asm dialect inteldialect is not supported. 7. Metadata  7.1. Metadata Nodes and Metadata Strings  Fully supported. The following metadata are understood by the NVVM compiler: Specialized Metadata Nodes llvm.loop.unroll.count llvm.loop.unroll.disable llvm.loop.unroll.full callalign (see Rules and Restrictions for Calling Conventions) Module flags metadata ( llvm.module.flags ) is supported and verified, but the metadata values will be ignored. All other metadata is accepted and ignored. 8. ThinLTO Summary  Not supported. 9. Intrinsic Global Variables  The llvm.used global variable is supported. The llvm.compiler.used global variable is supported The llvm.global_ctors global variable is not supported The llvm.global_dtors global variable is not supported 10. Instructions  10.1. Terminator Instructions  Supported: ret br switch unreachable Unsupported: indirectbr invoke resume catchswitch catchret cleanupret 10.2. Binary Operations  Supported: add fadd sub fsub mul fmul udiv sdiv fdiv urem srem frem 10.3. Bitwise Binary Operations  Supported: shl lshr ashr and or xor 10.4. Vector Operations  Supported: extractelement insertelement shufflevector 10.5. Aggregate Operations  Supported: extractvalue insertvalue 10.6. Memory Access and Addressing Operations  10.6.1. alloca Instruction  The alloca instruction returns a generic pointer to the local address space. The inalloca attribute is not supported. Maximum alignment supported is 2^23. The addrspace(<num>) specifier is supported only if num is 0. 10.6.2. load Instruction  load atomic is not supported. 10.6.3. store Instruction  store atomic is not supported. 10.6.4. fence Instruction  Not supported. Use NVVM intrinsic functions instead. 10.6.5. cmpxchg Instruction  Supported for i32 , i64 , and i128 types, with the following restrictions: The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space. The weak marker and the failure ordering are accepted and ignored. The i128 type is only supported on compute_90 and above. 10.6.6. atomicrmw Instruction  nand is not supported. The other keywords are supported for i32 , i64 , and i128 types, with the following restrictions. The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space. For i128 , only xchg is supported, and only on compute_90 and above. 10.6.7. getelementptr Instruction  Fully supported. 10.7. Conversion Operations  Supported: trunc .. to zext .. to sext .. to fptrunc .. to fpext .. to fptoui .. to fptosi .. to uitofp .. to sitofp .. to ptrtoint .. to inttoptr .. to addrspacecast .. to bitcast .. to See Conversion for a special use case of bitcast . 10.8. Other Operations  Supported: icmp fcmp phi select va_arg call (See Calling Conventions for other rules and restrictions.) Unsupported: landingpad catchpad cleanuppad 11. Intrinsic Functions  11.1. Variable Argument Handling Intrinsics  llvm.va_start Supported. llvm.va_end Supported. llvm.va_copy Supported. 11.2. Accurate Garbage Collection Intrinsics  Not supported. 11.3. Code Generator Intrinsics  Not supported. 11.4. Standard C Library Intrinsics  llvm.memcpy Supported. Note that the constant address space cannot be used as the destination since it is read-only. llvm.memmove Supported. Note that the constant address space cannot be used since it is read-only. llvm.memset Supported. Note that the constant address space cannot be used since it is read-only. llvm.sqrt Supported for float/double and vector of float/double. Mapped to PTX sqrt.rn.f32 and sqrt.rn.f64 . llvm.powi Not supported. llvm.sin Not supported. llvm.cos Not supported. llvm.pow Not supported. llvm.exp Not supported. llvm.exp2 Not supported. llvm.log Not supported. llvm.log10 Not supported. llvm.log2 Not supported. llvm.fma Supported for float/double and vector of float/double. Mapped to PTX fma.rn.f32 and fma.rn.f64 llvm.fabs Not supported. llvm.copysign Not supported. llvm.floor Not supported. llvm.ceil Not supported. llvm.trunc Not supported. llvm.rint Not supported. llvm.nearbyint Not supported. llvm.round Not supported. llvm.minnum Not supported. llvm.maxnum Not supported. 11.5. Bit Manipulations Intrinsics  llvm.bitreverse Supported for i8 , i16 , i32 , and i64 . llvm.bswap Supported for i16 , i32 , and i64 . llvm.ctpop Supported for i8 , i16 , i32 , i64 , and vectors of these types. llvm.ctlz Supported for i8 , i16 , i32 , i64 , and vectors of these types. llvm.cttz Supported for i8 , i16 , i32 , i64 , and vectors of these types. llvm.fshl Supported for i8 , i16 , i32 , and i64 . llvm.fshr Supported for i8 , i16 , i32 , and i64 . 11.6. Specialised Arithmetic Intrinsics  llvm.fmuladd Supported. llvm.canonicalize Not supported. 11.7. Arithmetic with Overflow Intrinsics  Supported for i16 , i32 , and i64 . 11.8. Half Precision Floating Point Intrinsics  Supported: llvm.convert.to.fp16 , llvm.convert.from.fp16 11.9. Debugger Intrinsics  llvm.dbg.addr Supported. llvm.dbg.declare Supported. llvm.dbg.value Supported. 11.10. Exception Handling Intrinsics  Not supported. 11.11. Trampoline Intrinsics  Not supported. 11.12. Masked Vector Load and Store Intrinsics  Not supported. 11.13. Masked Vector Expanding Load and Compressing Store Intrinsics  Not supported. 11.14. Experimental Vector Reduction Intrinsics  Not supported. 11.15. Constrained Floating Point Intrinsics  Not supported. 11.16. Constrained libm-equivalent Intrinsics  Not supported. 11.17. Masked Vector Gather and Scatter Intrinsics  Not supported. 11.18. Memory Use Markers  Supported: llvm.lifetime.start , llvm.lifetime.end , llvm.invariant.start , and llvm.invariant.end . Not supported: llvm.launder.invariant.group , llvm.strip.invariant.group . 11.19. General Intrinsics  llvm.var.annotation Accepted and ignored. llvm.ptr.annotation Accepted and ignored. llvm.annotation Accepted and ignored. llvm.codeview.annotation Not supported. llvm.trap Supported. llvm.debugtrap Not supported. llvm.stackguard Not supported. llvm.stackprotector Not supported. llvm.objectsize Not supported. llvm.expect Supported. llvm.assume Supported. llvm.ssa_copy Not supported. llvm.type.test Not supported. llvm.type.checked.load Not supported. llvm.donothing Supported. llvm.experimental.deoptimize Not supported. llvm.experimental.guard Not supported. llvm.load.relative Not supported. llvm.sideeffect Supported. 11.20. Element Wise Atomic Memory Intrinsics  Not supported. 11.21. Stack Map Intrinsics  Not supported. 12. Address Space  12.1. Address Spaces  NVVM IR has a set of predefined memory address spaces, whose semantics are similar to those defined in CUDA C/C++, OpenCL C and PTX. Any address space not listed below is not supported . Name Address Space Number Semantics/Example code 0 functions, code CUDA C/C++ function OpenCL C function generic 0 Can only be used to qualify the pointee of a pointer Pointers in CUDA C/C++ global 1 CUDA C/C++ __device__ OpenCL C global shared 3 CUDA C/C++ __shared__ OpenCL C local constant 4 CUDA C/C++ __constant__ OpenCL C constant local 5 CUDA C/C++ local OpenCL C private <reserved> 2, 101 and above Each global variable, that is not an intrinsic global variable, can be declared to reside in a specific non-zero address space, which can only be one of the following: global , shared or constant . If a non-intrinsic global variable is declared without any address space number or with the address space number 0, then this global variable resides in address space global and the pointer of this global variable holds a generic pointer value. The predefined NVVM memory spaces are needed for the language front-ends to model the memory spaces in the source languages. For example, // CUDA C/C++\n__constant__ int c;\n__device__ int g;\n\n; NVVM IR\n@c = addrspace(4) global i32 0, align 4\n@g = addrspace(1) global [2 x i32] zeroinitializer, align 4 Address space numbers 2 and 101 or higher are reserved for NVVM compiler internal use only. No language front-end should generate code that uses these address spaces directly. 12.2. Generic Pointers and Non-Generic Pointers  12.2.1. Generic Pointers vs. Non-generic Pointers  There are generic pointers and non-generic pointers in NVVM IR. A generic pointer is a pointer that may point to memory in any address space. A non-generic pointer points to memory in a specific address space. In NVVM IR, a generic pointer has a pointer type with the address space generic , while a non-generic pointer has a pointer type with a non-generic address space. Note that the address space number for the generic address space is 0—the default in both NVVM IR and LLVM IR. The address space number for the code address space is also 0. Function pointers are qualified by address space code ( addrspace(0) ). Loads/stores via generic pointers are supported, as well as loads/stores via non-generic pointers. Loads/stores via function pointers are not supported @a = addrspace(1) global i32 0, align 4 ; 'global' addrspace, @a holds a specific value\n@b = global i32 0, align 4              ; 'global' addrspace, @b holds a generic value\n@c = addrspace(4) global i32 0, align 4 ; 'constant' addrspace, @c holds a specific value\n\n... = load i32 addrspace(1)* @a, align 4 ; Correct\n... = load i32* @a, align 4              ; Wrong\n... = load i32* @b, align 4              ; Correct\n... = load i32 addrspace(1)* @b, align 4 ; Wrong\n... = load i32 addrspace(4)* @c, align4  ; Correct\n... = load i32* @c, align 4              ; Wrong 12.2.2. Conversion  The bit value of a generic pointer that points to a specific object may be different from the bit value of a specific pointer that points to the same object. The addrspacecast IR instruction should be used to perform pointer casts across address spaces (generic to non-generic or non-generic to generic). Casting a non-generic pointer to a different non-generic pointer is not supported. Casting from a generic to a non-generic pointer is undefined if the generic pointer does not point to an object in the target non-generic address space. inttoptr and ptrtoint are supported. inttoptr and ptrtoint are value preserving instructions when the two operands are of the same size. In general, using ptrtoint and inttoptr to implement an address space cast is undefined. The following intrinsic can be used to query if the argument pointer was derived from the address of a kernel function parameter that has the grid_constant property: i1 @llvm.nvvm.isspacep.grid_const(i8*) The following intrinsic can be used to query if the input generic pointer was derived from the address of a variable allocated in the shared address space, in a CTA that is part of the same cluster as the parent CTA of the invoking thread. This intrinsic is only supported for Hopper+. i1 @llvm.nvvm.isspacep.cluster_shared(i8*) The following intrinsics can be used to query if a generic pointer can be safely cast to a specific non-generic address space: i1 @llvm.nvvm.isspacep.const(i8*) i1 @llvm.nvvm.isspacep.global(i8*) i1 @llvm.nvvm.isspacep.local(i8*) i1 @llvm.nvvm.isspacep.shared(i8*) bitcast on pointers is supported, though LLVM IR forbids bitcast from being used to change the address space of a pointer. 12.2.3. No Aliasing between Two Different Specific Address Spaces  Two different specific address spaces do not overlap. NVVM compiler assumes two memory accesses via non-generic pointers that point to different address spaces are not aliased. 12.3. The alloca Instruction  The alloca instruction returns a generic pointer that only points to address space local . 13. Global Property Annotation  13.1. Overview  NVVM uses Named Metadata to annotate IR objects with properties that are otherwise not representable in the IR. The NVVM IR producers can use the Named Metadata to annotate the IR with properties, which the NVVM compiler can process. 13.2. Representation of Properties  For each translation unit (that is, per bitcode file), there is a named metadata called nvvm.annotations . This named metadata contains a list of MDNodes. The first operand of each MDNode is an entity that the node is annotating using the remaining operands. Multiple MDNodes may provide annotations for the same entity, in which case their first operands will be same. The remaining operands of the MDNode are organized in order as <property-name, value>. The property-name operand is MDString, while the value is i32 . Starting with the operand after the annotated entity, every alternate operand specifies a property. The operand after a property is its value. The following is an example. !nvvm.annotations = !{!12, !13}\n  !12 = !{void (i32, i32)* @_Z6kernelii, !\"kernel\", i32 1}\n  !13 = !{void ()* @_Z7kernel2v, !\"kernel\", i32 1, !\"maxntidx\", i32 16} If two bitcode files are being linked and both have a named metadata nvvm.annotations , the linked file will have a single merged named metadata. If both files define properties for the same entity foo , the linked file will have two MDNodes defining properties for foo . It is illegal for the files to have conflicting properties for the same entity. 13.3. Supported Properties  Property Name Annotated On Description maxntid{x, y, z} kernel function Maximum expected CTA size from any launch. reqntid{x, y, z} kernel function Minimum expected CTA size from any launch. cluster_dim_{x,y,z} kernel function Support for cluster dimensions for Hopper+. If any dimension is specified as 0, then all dimensions must be specified as 0. cluster_max_blocks kernel function Maximum number of blocks per cluster. Must be non-zero. Only supported for Hopper+. minctasm kernel function Hint/directive to the compiler/driver, asking it to put at least these many CTAs on an SM. grid_constant kernel function The argument is a metadata node, which contains a list of integers, where each integer n denotes that the nth parameter has the grid_constant annotation (numbering from 1). The parameter’s type must be of pointer type with byval attribute set. It is undefined behavior to write to memory pointed to by the parameter. This property is only supported for Volta+. maxnreg function Maximum number of registers for function. kernel function Signifies that this function is a kernel function. align function Signifies that the value in low 16-bits of the 32-bit value contains alignment of n th parameter type if its alignment is not the natural alignment. n is specified by high 16-bits of the value. For return type, n is 0. texture global variable Signifies that variable is a texture. surface global variable Signifies that variable is a surface. managed global variable Signifies that variable is a UVM managed variable. 14. Texture and Surface  14.1. Texture Variable and Surface Variable  A texture or a surface variable can be declared/defined as a global variable of i64 type with annotation texture or surface in the global address space. A texture or surface variable must have a name, which must follow identifier naming conventions. It is illegal to store to or load from the address of a texture or surface variable. A texture or a surface variable may only have the following uses: In a metadata node As an intrinsic function argument as shown below In llvm.used Global Variable 14.2. Accessing Texture Memory or Surface Memory  Texture memory and surface memory can be accessed using texture or surface handles. NVVM provides the following intrinsic function to get a texture or surface handle from a texture or surface variable. delcare i64 % llvm . nvvm . texsurf . handle . p1i64 ( metadata , i64 addrspace ( 1 ) * ) The first argument to the intrinsic is a metadata holding the texture or surface variable. Such a metadata may hold only one texture or one surface variable. The second argument to the intrinsic is the texture or surface variable itself. The intrinsic returns a handle of i64 type. The returned handle value from the intrinsic call can be used as an operand (with a constraint of l) in a PTX inline asm to access the texture or surface memory. 15. NVVM Specific Intrinsic Functions  15.1. Atomic  Besides the atomic instructions, the following extra atomic intrinsic functions are supported. declare float @llvm.nvvm.atomic.load.add.f32.p0f32(float* address, float val)\ndeclare float @llvm.nvvm.atomic.load.add.f32.p1f32(float addrspace(1)* address, float val)\ndeclare float @llvm.nvvm.atomic.load.add.f32.p3f32(float addrspace(3)* address, float val)\ndeclare double @llvm.nvvm.atomic.load.add.f64.p0f64(double* address, double val)\ndeclare double @llvm.nvvm.atomic.load.add.f64.p1f64(double addrspace(1)* address, double val)\ndeclare double @llvm.nvvm.atomic.load.add.f64.p3f64(double addrspace(3)* address, double val) reads the single/double precision floating point value old located at the address address , computes old+val , and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns old . declare i32 @llvm.nvvm.atomic.load.inc.32.p0i32(i32* address, i32 val)\ndeclare i32 @llvm.nvvm.atomic.load.inc.32.p1i32(i32 addrspace(1)* address, i32 val)\ndeclare i32 @llvm.nvvm.atomic.load.inc.32.p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes ((old >= val) ? 0 : (old+1)) , and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old . declare i32 @llvm.nvvm.atomic.load.dec.32.p0i32(i32* address, i32 val)\ndeclare i32 @llvm.nvvm.atomic.load.dec.32.p1i32(i32 addrspace(1)* address, i32 val)\ndeclare i32 @llvm.nvvm.atomic.load.dec.32.p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes (((old == 0) | (old > val)) ? val : (old-1) ) , and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old . 15.2. Barrier and Memory Fence  declare void @llvm.nvvm.barrier0() waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to llvm.nvvm.barrier0() are visible to all threads in the block. declare i32 @llvm.nvvm.barrier0.popc(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero. declare i32 @llvm.nvvm.barrier0.and(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them. declare i32 @llvm.nvvm.barrier0.or(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them. declare void @llvm.nvvm.cluster.barrier(i32 %flags) Synchronize and communicate among threads in the same cluster. This intrinsic is only supported for Hopper+. The %flags is encoded according to the following table: %flags bits Meaning 31-8 Reserved 7-4 Memory ordering (See Cluster Barrier Memory Ordering Encoding below) 3-0 Operation mode (See Cluster Barrier Operation Mode Encoding below) Cluster Barrier Operation Mode Encoding Encoding Mode Description 0 Arrive Arrive at cluster barrier 1 Wait Wait at cluster barrier 2-15 RESERVED RESERVED Cluster Barrier Memory Ordering Encoding Encoding Mode Description 0 Default All synchronous memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait. 1 Relaxed All previously fenced memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait. This ordering is only supported when the operation mode is Arrive. 2-15 RESERVED RESERVED declare void @llvm.nvvm.membar.cta() is a memory fence at the thread block level. This intrinsic is deprecated. Please use nvvm.membar with flags as argument instead. declare void @llvm.nvvm.membar.gl() is a memory fence at the device level. This intrinsic is deprecated. Please use nvvm.membar with flags as argument instead. declare void @llvm.nvvm.membar.sys() is a memory fence at the system level. This intrinsic is deprecated. Please use nvvm.membar with flags as argument instead. declare void @llvm.nvvm.membar(i32 %flags) Wait for all prior memory accesses requested by this thread to be performed at a membar level defined by the membar mode below. The memory barrier enforces vertical ordering only. It makes no guarantees as to execution synchronization with other threads. For horizontal synchronization, a barrier should be used instead, or in addition to membar. The %flags is encoded according to the following table: %flags bits Meaning 31-4 Reserved 3-0 Membar modes (See Membar Mode Encoding.) Membar Mode Encoding Encoding Mode Description 0 GLOBAL Membar at the global level 1 CTA Membar at the CTA level 2 SYSTEM Membar at the system level 3 RESERVED RESERVED 4 CLUSTER Membar at the cluster level, only on Hopper+ 5-15 RESERVED RESERVED 15.3. Address space conversion  Note Attention: Please use the addrspacecast IR instruction for address space conversion. 15.4. Special Registers  The following intrinsic functions are provided to support reading special PTX registers: declare i32 @llvm.nvvm.read.ptx.sreg.tid.x()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.tid.y()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.tid.z()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.ntid.x()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.ntid.y()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.ntid.z()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.ctaid.y()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.ctaid.z()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.nctaid.x()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.nctaid.y()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.nctaid.z()\ndeclare i32 @llvm.nvvm.read.ptx.sreg.warpsize() 15.5. Texture/Surface Access  The following intrinsic function is provided to convert a global texture/surface variable into a texture/surface handle. declare i64 %llvm.nvvm.texsurf.handle.p1i64(metadata, i64 addrspace(1)*) See Accessing Texture Memory or Surface Memory for details. The following IR definitions apply to all intrinsics in this section: type %float4 = { float, float, float, float }\ntype %long2 = { i64, i64 }\ntype %int4 = { i32, i32, i32, i32 }\ntype %int2 = { i32, i32 }\ntype %short4 = { i16, i16, i16, i16 }\ntype %short2 = { i16, i16 } 15.5.1. Texture Reads  Sampling a 1D texture: %float4 @llvm.nvvm.tex.unified.1d.v4f32.s32(i64 %tex, i32 %x)\n%float4 @llvm.nvvm.tex.unified.1d.v4f32.f32(i64 %tex, float %x)\n%float4 @llvm.nvvm.tex.unified.1d.level.v4f32.f32(i64 %tex, float %x,\n                                                  float %level)\n%float4 @llvm.nvvm.tex.unified.1d.grad.v4f32.f32(i64 %tex, float %x,\n                                                 float %dPdx,\n                                                 float %dPdy)\n\n%int4 @llvm.nvvm.tex.unified.1d.v4s32.s32(i64 %tex, i32 %x)\n%int4 @llvm.nvvm.tex.unified.1d.v4s32.f32(i64 %tex, float %x)\n%int4 @llvm.nvvm.tex.unified.1d.level.v4s32.f32(i64 %tex, float %x,\n                                                float %level)\n%int4 @llvm.nvvm.tex.unified.1d.grad.v4s32.f32(i64 %tex, float %x,\n                                               float %dPdx,\n                                               float %dPdy)\n\n%int4 @llvm.nvvm.tex.unified.1d.v4u32.s32(i64 %tex, i32 %x)\n%int4 @llvm.nvvm.tex.unified.1d.v4u32.f32(i64 %tex, float %x)\n%int4 @llvm.nvvm.tex.unified.1d.level.v4u32.f32(i64 %tex, float %x,\n                                                float %level)\n%int4 @llvm.nvvm.tex.unified.1d.grad.v4u32.f32(i64 %tex, float %x,\n                                               float %dPdx,\n                                               float %dPdy) Sampling a 1D texture array: %float4 @llvm.nvvm.tex.unified.1d.array.v4f32.s32(i64 %tex, i32 %idx, i32 %x)\n%float4 @llvm.nvvm.tex.unified.1d.array.v4f32.f32(i64 %tex, i32 %idx, float %x)\n%float4 @llvm.nvvm.tex.unified.1d.array.level.v4f32.f32(i64 %tex, i32 %idx,\n                                                        float %x,\n                                                        float %level)\n%float4 @llvm.nvvm.tex.unified.1d.array.grad.v4f32.f32(i64 %tex, i32 %idx,\n                                                       float %x,\n                                                       float %dPdx,\n                                                       float %dPdy)\n\n%int4 @llvm.nvvm.tex.unified.1d.array.v4s32.s32(i64 %tex, i32 %idx, i32 %x)\n%int4 @llvm.nvvm.tex.unified.1d.array.v4s32.f32(i64 %tex, i32 %idx, float %x)\n%int4 @llvm.nvvm.tex.unified.1d.array.level.v4s32.f32(i64 %tex, i32 %idx,\n                                                      float %x,\n                                                      float %level)\n%int4 @llvm.nvvm.tex.unified.1d.array.grad.v4s32.f32(i64 %tex, i32 %idx,\n                                                     float %x,\n                                                     float %dPdx,\n                                                     float %dPdy)\n\n%int4 @llvm.nvvm.tex.unified.1d.array.v4u32.s32(i64 %tex, i32 %idx, i32 %x)\n%int4 @llvm.nvvm.tex.unified.1d.array.v4u32.f32(i64 %tex, i32 %idx, float %x)\n%int4 @llvm.nvvm.tex.unified.1d.array.level.v4u32.f32(i64 %tex, i32 %idx,\n                                                      float %x,\n                                                      float %level)\n%int4 @llvm.nvvm.tex.unified.1d.array.grad.v4u32.f32(i64 %tex, i32 %idx,\n                                                     float %x,\n                                                     float %dPdx,\n                                                     float %dPdy) Sampling a 2D texture: %float4 @llvm.nvvm.tex.unified.2d.v4f32.s32(i64 %tex, i32 %x, i32 %y)\n%float4 @llvm.nvvm.tex.unified.2d.v4f32.f32(i64 %tex, float %x, float %y)\n%float4 @llvm.nvvm.tex.unified.2d.level.v4f32.f32(i64 %tex, float %x, float %y,\n                                                  float %level)\n%float4 @llvm.nvvm.tex.unified.2d.grad.v4f32.f32(i64 %tex, float %x, float %y,\n                                                 float %dPdx_x, float %dPdx_y,\n                                                 float %dPdy_x, float %dPdy_y)\n\n%int4 @llvm.nvvm.tex.unified.2d.v4s32.s32(i64 %tex, i32 %x, i32 %y)\n%int4 @llvm.nvvm.tex.unified.2d.v4s32.f32(i64 %tex, float %x, float %y,)\n%int4 @llvm.nvvm.tex.unified.2d.level.v4s32.f32(i64 %tex, float %x, float %y,\n                                                float %level)\n%int4 @llvm.nvvm.tex.unified.2d.grad.v4s32.f32(i64 %tex, float %x, float %y,\n                                               float %dPdx_x, float %dPdx_y,\n                                               float %dPdy_x, float %dPdy_y)\n\n%int4 @llvm.nvvm.tex.unified.2d.v4u32.s32(i64 %tex, i32 %x i32 %y)\n%int4 @llvm.nvvm.tex.unified.2d.v4u32.f32(i64 %tex, float %x float %y)\n%int4 @llvm.nvvm.tex.unified.2d.level.v4u32.f32(i64 %tex, float %x, float %y,\n                                                float %level)\n%int4 @llvm.nvvm.tex.unified.2d.grad.v4u32.f32(i64 %tex, float %x, float %y,\n                                               float %dPdx_x, float %dPdx_y,\n                                               float %dPdy_x, float %dPdy_y) Sampling a 2D texture array: %float4 @llvm.nvvm.tex.unified.2d.array.v4f32.s32(i64 %tex, i32 %idx,\n                                                  i32 %x, i32 %y)\n%float4 @llvm.nvvm.tex.unified.2d.array.v4f32.f32(i64 %tex, i32 %idx,\n                                                  float %x, float %y)\n%float4 @llvm.nvvm.tex.unified.2d.array.level.v4f32.f32(i64 %tex, i32 %idx,\n                                                        float %x, float %y,\n                                                        float %level)\n%float4 @llvm.nvvm.tex.unified.2d.array.grad.v4f32.f32(i64 %tex, i32 %idx,\n                                                       float %x, float %y,\n                                                       float %dPdx_x,\n                                                       float %dPdx_y,\n                                                       float %dPdy_x,\n                                                       float %dPdy_y)\n\n%int4 @llvm.nvvm.tex.unified.2d.array.v4s32.s32(i64 %tex, i32 %idx,\n                                                i32 %x, i32 %y)\n%int4 @llvm.nvvm.tex.unified.2d.array.v4s32.f32(i64 %tex, i32 %idx,\n                                                float %x, float %y)\n%int4 @llvm.nvvm.tex.unified.2d.array.level.v4s32.f32(i64 %tex, i32 %idx,\n                                                      float %x, float %y,\n                                                      float %level)\n%int4 @llvm.nvvm.tex.unified.2d.array.grad.v4s32.f32(i64 %tex, i32 %idx,\n                                                     float %x, float %y,\n                                                     float %dPdx_x,\n                                                     float %dPdx_y,\n                                                     float %dPdy_x,\n                                                     float %dPdy_y)\n\n%int4 @llvm.nvvm.tex.unified.2d.array.v4u32.s32(i64 %tex, i32 %idx,\n                                                i32 %x i32 %y)\n%int4 @llvm.nvvm.tex.unified.2d.array.v4u32.f32(i64 %tex, i32 %idx,\n                                                float %x float %y)\n%int4 @llvm.nvvm.tex.unified.2d.array.level.v4u32.f32(i64 %tex, i32 %idx,\n                                                      float %x, float %y,\n                                                      float %level)\n%int4 @llvm.nvvm.tex.unified.2d.array.grad.v4u32.f32(i64 %tex, i32 %idx,\n                                                     float %x, float %y,\n                                                     float %dPdx_x,\n                                                     float %dPdx_y,\n                                                     float %dPdy_x,\n                                                     float %dPdy_y) Sampling a 3D texture: %float4 @llvm.nvvm.tex.unified.3d.v4f32.s32(i64 %tex, i32 %x, i32 %y, i32 %z)\n%float4 @llvm.nvvm.tex.unified.3d.v4f32.f32(i64 %tex, float %x, float %y,\n                                            float %z)\n%float4 @llvm.nvvm.tex.unified.3d.level.v4f32.f32(i64 %tex,float %x, float %y,\n                                                  float %z, float %level)\n%float4 @llvm.nvvm.tex.unified.3d.grad.v4f32.f32(i64 %tex, float %x, float %y,\n                                                 float %z, float %dPdx_x,\n                                                 float %dPdx_y, float %dPdx_z,\n                                                 float %dPdy_x, float %dPdy_y,\n                                                 float %dPdy_z)\n\n%int4 @llvm.nvvm.tex.unified.3d.v4s32.s32(i64 %tex, i32 %x, i32 %y, i32 %z)\n%int4 @llvm.nvvm.tex.unified.3d.v4s32.f32(i64 %tex, float %x, float %y,\n                                          float %z)\n%int4 @llvm.nvvm.tex.unified.3d.level.v4s32.f32(i64 %tex, float %x, float %y,\n                                                float %z, float %level)\n%int4 @llvm.nvvm.tex.unified.3d.grad.v4s32.f32(i64 %tex, float %x, float %y,\n                                               float %z, float %dPdx_x,\n                                               float %dPdx_y, float %dPdx_z,\n                                               float %dPdy_x, float %dPdy_y,\n                                               float %dPdy_z)\n\n%int4 @llvm.nvvm.tex.unified.3d.v4u32.s32(i64 %tex, i32 %x i32 %y, i32 %z)\n%int4 @llvm.nvvm.tex.unified.3d.v4u32.f32(i64 %tex, float %x, float %y,\n                                          float %z)\n%int4 @llvm.nvvm.tex.unified.3d.level.v4u32.f32(i64 %tex, float %x, float %y,\n                                                float %z, float %level)\n%int4 @llvm.nvvm.tex.unified.3d.grad.v4u32.f32(i64 %tex, float %x, float %y,\n                                               float %z, float %dPdx_x,\n                                               float %dPdx_y, float %dPdx_z,\n                                               float %dPdy_x, float %dPdy_y,\n                                               float %dPdy_z) Sampling a cube texture: %float4 @llvm.nvvm.tex.unified.cube.v4f32.f32(i64 %tex, float %x, float %y,\n                                              float %z)\n%float4 @llvm.nvvm.tex.unified.cube.level.v4f32.f32(i64 %tex,float %x, float %y,\n                                                    float %z, float %level)\n\n%int4 @llvm.nvvm.tex.unified.cube.v4s32.f32(i64 %tex, float %x, float %y,\n                                            float %z)\n%int4 @llvm.nvvm.tex.unified.cube.level.v4s32.f32(i64 %tex, float %x, float %y,\n                                                  float %z, float %level)\n\n%int4 @llvm.nvvm.tex.unified.cube.v4u32.f32(i64 %tex, float %x, float %y,\n                                            float %z)\n%int4 @llvm.nvvm.tex.unified.cube.level.v4u32.f32(i64 %tex, float %x, float %y,\n                                                  float %z, float %level) Sampling a cube texture array: %float4 @llvm.nvvm.tex.unified.cube.array.v4f32.f32(i64 %tex, i32 %idx,\n                                                    float %x, float %y,\n                                                    float %z)\n%float4 @llvm.nvvm.tex.unified.cube.array.level.v4f32.f32(i64 %tex, i32 %idx,\n                                                          float %x, float %y,\n                                                          float %z,\n                                                          float %level)\n\n%int4 @llvm.nvvm.tex.unified.cube.array.v4s32.f32(i64 %tex, i32 %idx, float %x,\n                                                  float %y, float %z)\n%int4 @llvm.nvvm.tex.unified.cube.array.level.v4s32.f32(i64 %tex, i32 %idx,\n                                                        float %x, float %y,\n                                                        float %z, float %level)\n\n%int4 @llvm.nvvm.tex.unified.cube.array.v4u32.f32(i64 %tex, i32 %idx, float %x,\n                                                  float %y, float %z)\n%int4 @llvm.nvvm.tex.unified.cube.array.level.v4u32.f32(i64 %tex, i32 %idx,\n                                                        float %x, float %y,\n                                                        float %z, float %level) Fetching a four-texel bilerp footprint: %float4 @llvm.nvvm.tld4.unified.r.2d.v4f32.f32(i64 %tex, float %x, float %y)\n%float4 @llvm.nvvm.tld4.unified.g.2d.v4f32.f32(i64 %tex, float %x, float %y)\n%float4 @llvm.nvvm.tld4.unified.b.2d.v4f32.f32(i64 %tex, float %x, float %y)\n%float4 @llvm.nvvm.tld4.unified.a.2d.v4f32.f32(i64 %tex, float %x, float %y)\n\n%int4 @llvm.nvvm.tld4.unified.r.2d.v4s32.f32(i64 %tex, float %x, float %y)\n%int4 @llvm.nvvm.tld4.unified.g.2d.v4s32.f32(i64 %tex, float %x, float %y)\n%int4 @llvm.nvvm.tld4.unified.b.2d.v4s32.f32(i64 %tex, float %x, float %y)\n%int4 @llvm.nvvm.tld4.unified.a.2d.v4s32.f32(i64 %tex, float %x, float %y)\n\n%int4 @llvm.nvvm.tld4.unified.r.2d.v4u32.f32(i64 %tex, float %x, float %y)\n%int4 @llvm.nvvm.tld4.unified.g.2d.v4u32.f32(i64 %tex, float %x, float %y)\n%int4 @llvm.nvvm.tld4.unified.b.2d.v4u32.f32(i64 %tex, float %x, float %y)\n%int4 @llvm.nvvm.tld4.unified.a.2d.v4u32.f32(i64 %tex, float %x, float %y) 15.5.2. Surface Loads  In the following intrinsics, <clamp> represents the surface clamp mode and can be one of the following: clamp , trap , or zero . For surface load instructions that operate on 8-bit data channels, the output operands are of type i16 . The high-order eight bits are undefined. Reading a 1D surface: i16 @llvm.nvvm.suld.1d.i8.<clamp>(i64 %tex, i32 %x)\ni16 @llvm.nvvm.suld.1d.i16.<clamp>(i64 %tex, i32 %x)\ni32 @llvm.nvvm.suld.1d.i32.<clamp>(i64 %tex, i32 %x)\ni64 @llvm.nvvm.suld.1d.i64.<clamp>(i64 %tex, i32 %x)\n\n%short2 @llvm.nvvm.suld.1d.v2i8.<clamp>(i64 %tex, i32 %x)\n%short2 @llvm.nvvm.suld.1d.v2i16.<clamp>(i64 %tex, i32 %x)\n%int2 @llvm.nvvm.suld.1d.v2i32.<clamp>(i64 %tex, i32 %x)\n%long2 @llvm.nvvm.suld.1d.v2i64.<clamp>(i64 %tex, i32 %x)\n\n%short4 @llvm.nvvm.suld.1d.v4i8.<clamp>(i64 %tex, i32 %x)\n%short4 @llvm.nvvm.suld.1d.v4i16.<clamp>(i64 %tex, i32 %x)\n%int4 @llvm.nvvm.suld.1d.v4i32.<clamp>(i64 %tex, i32 %x) Reading a 1D surface array: i16 @llvm.nvvm.suld.1d.array.i8.<clamp>(i64 %tex, i32 %idx, i32 %x)\ni16 @llvm.nvvm.suld.1d.array.i16.<clamp>(i64 %tex, i32 %idx, i32 %x)\ni32 @llvm.nvvm.suld.1d.array.i32.<clamp>(i64 %tex, i32 %idx, i32 %x)\ni64 @llvm.nvvm.suld.1d.array.i64.<clamp>(i64 %tex, i32 %idx, i32 %x)\n\n%short2 @llvm.nvvm.suld.1d.array.v2i8.<clamp>(i64 %tex, i32 %idx, i32 %x)\n%short2 @llvm.nvvm.suld.1d.array.v2i16.<clamp>(i64 %tex, i32 %idx, i32 %x)\n%int2 @llvm.nvvm.suld.1d.array.v2i32.<clamp>(i64 %tex, i32 %idx, i32 %x)\n%long2 @llvm.nvvm.suld.1d.array.v2i64.<clamp>(i64 %tex, i32 %idx, i32 %x)\n\n%short4 @llvm.nvvm.suld.1d.array.v4i8.<clamp>(i64 %tex, i32 %idx, i32 %x)\n%short4 @llvm.nvvm.suld.1d.array.v4i16.<clamp>(i64 %tex, i32 %idx, i32 %x)\n%int4 @llvm.nvvm.suld.1d.array.v4i32.<clamp>(i64 %tex, i32 %idx, i32 %x) Reading a 2D surface: i16 @llvm.nvvm.suld.2d.i8.<clamp>(i64 %tex, i32 %x, i32 %y)\ni16 @llvm.nvvm.suld.2d.i16.<clamp>(i64 %tex, i32 %x, i32 %y)\ni32 @llvm.nvvm.suld.2d.i32.<clamp>(i64 %tex, i32 %x, i32 %y)\ni64 @llvm.nvvm.suld.2d.i64.<clamp>(i64 %tex, i32 %x, i32 %y)\n\n%short2 @llvm.nvvm.suld.2d.v2i8.<clamp>(i64 %tex, i32 %x, i32 %y)\n%short2 @llvm.nvvm.suld.2d.v2i16.<clamp>(i64 %tex, i32 %x, i32 %y)\n%int2 @llvm.nvvm.suld.2d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %y)\n%long2 @llvm.nvvm.suld.2d.v2i64.<clamp>(i64 %tex, i32 %x, i32 %y)\n\n%short4 @llvm.nvvm.suld.2d.v4i8.<clamp>(i64 %tex, i32 %x, i32 %y)\n%short4 @llvm.nvvm.suld.2d.v4i16.<clamp>(i64 %tex, i32 %x, i32 %y)\n%int4 @llvm.nvvm.suld.2d.v4i32.<clamp>(i64 %tex, i32 %x, i32 %y) Reading a 2D surface array: i16 @llvm.nvvm.suld.2d.array.i8.<clamp>(i64 %tex, i32 %idx, i32 %x, i32 %y)\ni16 @llvm.nvvm.suld.2d.array.i16.<clamp>(i64 %tex, i32 %idx, i32 %x, i32 %y)\ni32 @llvm.nvvm.suld.2d.array.i32.<clamp>(i64 %tex, i32 %idx, i32 %x, i32 %y)\ni64 @llvm.nvvm.suld.2d.array.i64.<clamp>(i64 %tex, i32 %idx, i32 %x, i32 %y)\n\n%short2 @llvm.nvvm.suld.2d.array.v2i8.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y)\n%short2 @llvm.nvvm.suld.2d.array.v2i16.<clamp>(i64 %tex, i32 %idx,\n                                               i32 %x, i32 %y)\n%int2 @llvm.nvvm.suld.2d.array.v2i32.<clamp>(i64 %tex, i32 %idx,\n                                             i32 %x, i32 %y)\n%long2 @llvm.nvvm.suld.2d.array.v2i64.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y)\n\n%short4 @llvm.nvvm.suld.2d.array.v4i8.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y)\n%short4 @llvm.nvvm.suld.2d.array.v4i16.<clamp>(i64 %tex, i32 %idx,\n                                               i32 %x, i32 %y)\n%int4 @llvm.nvvm.suld.2d.array.v4i32.<clamp>(i64 %tex, i32 %idx,\n                                             i32 %x, i32 %y) Reading a 3D surface: i16 @llvm.nvvm.suld.3d.i8.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\ni16 @llvm.nvvm.suld.3d.i16.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\ni32 @llvm.nvvm.suld.3d.i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\ni64 @llvm.nvvm.suld.3d.i64.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\n\n%short2 @llvm.nvvm.suld.3d.v2i8.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\n%short2 @llvm.nvvm.suld.3d.v2i16.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\n%int2 @llvm.nvvm.suld.3d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\n%long2 @llvm.nvvm.suld.3d.v2i64.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z)\n\n%short4 @llvm.nvvm.suld.3d.v4i8.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i32 %z)\n%short4 @llvm.nvvm.suld.3d.v4i16.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                         i32 %z)\n%int4 @llvm.nvvm.suld.3d.v4i32.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                       i32 %z) 15.5.3. Surface Stores  In the following intrinsics, <clamp> represents the surface clamp mode. It is trap for the formatted stores, and can be one of the following for unformatted stores: clamp , trap , or zero . For surface store instructions that operate on 8-bit data channels, the input operands are of type i16 . The high-order eight bits are ignored. Writing a 1D surface: ;; Unformatted\nvoid @llvm.nvvm.sust.b.1d.i8.<clamp>(i64 %tex, i32 %x, i16 %r)\nvoid @llvm.nvvm.sust.b.1d.i16.<clamp>(i64 %tex, i32 %x, i16 %r)\nvoid @llvm.nvvm.sust.b.1d.i32.<clamp>(i64 %tex, i32 %x, i32 %r)\nvoid @llvm.nvvm.sust.b.1d.i64.<clamp>(i64 %tex, i32 %x, i64 %r)\n\nvoid @llvm.nvvm.sust.b.1d.v2i8.<clamp>(i64 %tex, i32 %x, i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.1d.v2i16.<clamp>(i64 %tex, i32 %x, i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.1d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %r, i32 %g)\nvoid @llvm.nvvm.sust.b.1d.v2i64.<clamp>(i64 %tex, i32 %x, i64 %r, i64 %g)\n\nvoid @llvm.nvvm.sust.b.1d.v4i8.<clamp>(i64 %tex, i32 %x,\n                                       i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.1d.v4i16.<clamp>(i64 %tex, i32 %x,\n                                        i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.1d.v4i32.<clamp>(i64 %tex, i32 %x,\n                                        i32 %r, i32 %g, i32 %b, i32 %a)\n\n;; Formatted\nvoid @llvm.nvvm.sust.p.1d.i32.<clamp>(i64 %tex, i32 %x, i32 %r)\n\nvoid @llvm.nvvm.sust.p.1d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %r, i32 %g)\n\nvoid @llvm.nvvm.sust.p.1d.v4i32.<clamp>(i64 %tex, i32 %x,\n                                        i32 %r, i32 %g, i32 %b, i32 %a) Writing a 1D surface array: ;; Unformatted\nvoid @llvm.nvvm.sust.b.1d.array.i8.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                                 i16 %r)\nvoid @llvm.nvvm.sust.b.1d.array.i16.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                                  i16 %r)\nvoid @llvm.nvvm.sust.b.1d.array.i32.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                                  i32 %r)\nvoid @llvm.nvvm.sust.b.1d.array.i64.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                                  i64 %r)\n\nvoid @llvm.nvvm.sust.b.1d.array.v2i8.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                             i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.1d.array.v2i16.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.1d.array.v2i32.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i32 %r, i32 %g)\nvoid @llvm.nvvm.sust.b.1d.array.v2i64.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i64 %r, i64 %g)\n\nvoid @llvm.nvvm.sust.b.1d.array.v4i8.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                             i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.1d.array.v4i16.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.1d.array.v4i32.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i32 %r, i32 %g, i32 %b, i32 %a)\n\n;; Formatted\nvoid @llvm.nvvm.sust.p.1d.array.i32.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                                  i32 %r)\n\nvoid @llvm.nvvm.sust.p.1d.array.v2i32.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i32 %r, i32 %g)\n\nvoid @llvm.nvvm.sust.p.1d.array.v4i32.<clamp>(i64 %tex, i32 %idx, i32 %x,\n                                              i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface: ;; Unformatted\nvoid @llvm.nvvm.sust.b.2d.i8.<clamp>(i64 %tex, i32 %x, i32 %y, i16 %r)\nvoid @llvm.nvvm.sust.b.2d.i16.<clamp>(i64 %tex, i32 %x, i32 %y, i16 %r)\nvoid @llvm.nvvm.sust.b.2d.i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %r)\nvoid @llvm.nvvm.sust.b.2d.i64.<clamp>(i64 %tex, i32 %x, i32 %y, i64 %r)\n\nvoid @llvm.nvvm.sust.b.2d.v2i8.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                       i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.2d.v2i16.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.2d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i32 %r, i32 %g)\nvoid @llvm.nvvm.sust.b.2d.v2i64.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i64 %r, i64 %g)\n\nvoid @llvm.nvvm.sust.b.2d.v4i8.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                       i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.2d.v4i16.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.2d.v4i32.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i32 %r, i32 %g, i32 %b, i32 %a)\n\n;; Formatted\nvoid @llvm.nvvm.sust.p.2d.i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %r)\n\nvoid @llvm.nvvm.sust.p.2d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i32 %r, i32 %g)\n\nvoid @llvm.nvvm.sust.p.2d.v4i32.<clamp>(i64 %tex, i32 %x, i32 %y,\n                                        i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface array: ;; Unformatted\nvoid @llvm.nvvm.sust.b.2d.array.i8.<clamp>(i64 %tex, i32 %idx,\n                                           i32 %x, i32 %y, i16 %r)\nvoid @llvm.nvvm.sust.b.2d.array.i16.<clamp>(i64 %tex, i32 %idx,\n                                            i32 %x, i32 %y, i16 %r)\nvoid @llvm.nvvm.sust.b.2d.array.i32.<clamp>(i64 %tex, i32 %idx,\n                                            i32 %x, i32 %y, i32 %r)\nvoid @llvm.nvvm.sust.b.2d.array.i64.<clamp>(i64 %tex, i32 %idx,\n                                            i32 %x, i32 %y, i64 %r)\n\nvoid @llvm.nvvm.sust.b.2d.array.v2i8.<clamp>(i64 %tex, i32 %idx,\n                                             i32 %x, i32 %y,\n                                             i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.2d.array.v2i16.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.2d.array.v2i32.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i32 %r, i32 %g)\nvoid @llvm.nvvm.sust.b.2d.array.v2i64.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i64 %r, i64 %g)\n\nvoid @llvm.nvvm.sust.b.2d.array.v4i8.<clamp>(i64 %tex, i32 %idx,\n                                             i32 %x, i32 %y,\n                                             i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.2d.array.v4i16.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.2d.array.v4i32.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i32 %r, i32 %g, i32 %b, i32 %a)\n\n;; Formatted\nvoid @llvm.nvvm.sust.p.2d.array.i32.<clamp>(i64 %tex, i32 %idx,\n                                            i32 %x, i32 %y, i32 %r)\n\nvoid @llvm.nvvm.sust.p.2d.array.v2i32.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i32 %r, i32 %g)\n\nvoid @llvm.nvvm.sust.p.2d.array.v4i32.<clamp>(i64 %tex, i32 %idx,\n                                              i32 %x, i32 %y,\n                                              i32 %r, i32 %g, i32 %b, i32 %a) Writing a 3D surface: ;; Unformatted\nvoid @llvm.nvvm.sust.b.3d.i8.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r)\nvoid @llvm.nvvm.sust.b.3d.i16.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r)\nvoid @llvm.nvvm.sust.b.3d.i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r)\nvoid @llvm.nvvm.sust.b.3d.i64.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z, i64 %r)\n\nvoid @llvm.nvvm.sust.b.3d.v2i8.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                       i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.3d.v2i16.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i16 %r, i16 %g)\nvoid @llvm.nvvm.sust.b.3d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i32 %r, i32 %g)\nvoid @llvm.nvvm.sust.b.3d.v2i64.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i64 %r, i64 %g)\n\nvoid @llvm.nvvm.sust.b.3d.v4i8.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                       i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.3d.v4i16.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i16 %r, i16 %g, i16 %b, i16 %a)\nvoid @llvm.nvvm.sust.b.3d.v4i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i32 %r, i32 %g, i32 %b, i32 %a)\n\n;; Formatted\nvoid @llvm.nvvm.sust.p.3d.i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r)\n\nvoid @llvm.nvvm.sust.p.3d.v2i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i32 %r, i32 %g)\n\nvoid @llvm.nvvm.sust.p.3d.v4i32.<clamp>(i64 %tex, i32 %x, i32 %y, i32 %z,\n                                        i32 %r, i32 %g, i32 %b, i32 %a) 15.6. Warp-level Operations  15.6.1. Barrier Synchronization  The following intrinsic performs a barrier synchronization among a subset of threads in a warp. declare void @llvm.nvvm.bar.warp.sync(i32 %membermask) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before resuming execution. The argument %membership is a 32bit mask, with each bit corresponding to a lane in the warp. 1 means the thread is in the subset. The behavior of this intrinsic is undefined if the executing thread is not in the %membermask . For compute_62 or below, all threads in %membermask must call the same @llvm.nvvm.bar.warp.sync() in convergence, and only threads belonging to the %membermask can be active when the intrinsic is called. Otherwise, the behavior is undefined. 15.6.2. Data Movement  The following intrinsic synchronizes a subset of threads in a warp and then performs data movement among these threads. declare {i32, i1} @llvm.nvvm.shfl.sync.i32(i32 %membermask, i32 %mode, i32 %a, i32 %b, i32 %c) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before reading data from other threads in the same warp. The argument %membership is a 32bit mask, with each bit corresponding to a lane in the warp. 1 means the thread is in the subset. Each thread in the currently executing warp will compute a source lane index j based on input arguments %b , %c , and %mode . If the computed source lane index j is in range, the returned i32 value will be the value of %a from lane j; otherwise, it will be the the value of %a from the current thread. If the thread corresponding to lane j is inactive, then the returned i32 value is undefined. The returned i1 value is set to 1 if the source lane j is in range, and otherwise set to 0. The argument %mode must be a constant and its encoding is specified in the following table. Encoding Meaning 0 IDX 1 UP 2 DOWN 3 BFLY Argument %b specifies a source lane or source lane offset, depending on %mode . Argument %c contains two packed values specifying a mask for logically splitting warps into sub-segments and an upper bound for clamping the source lane index. The following pseudo code illustrates the semantics of this intrinsic. wait until all threads in %membermask have arrived;\n\n%lane[4:0] = current_lane_id; // position of thread in warp\n%bval[4:0] = %b[4:0]; // source lane or lane offset (0..31)\n%cval[4:0] = %c[4:0]; // clamp value\n%mask[4:0] = %c[12:8];\n\n%maxLane = (%lane[4:0] & %mask[4:0]) | (%cval[4:0] & ~%mask[4:0]);\n%minLane = (%lane[4:0] & %mask[4:0]);\nswitch (%mode) {\ncase UP: %j = %lane - %bval; %pval = (%j >= %maxLane); break;\ncase DOWN: %j = %lane + %bval; %pval = (%j <= %maxLane); break;\ncase BFLY: %j = %lane ^ %bval; %pval = (%j <= %maxLane); break;\ncase IDX: %j = %minLane | (%bval[4:0] & ~%mask[4:0]); %pval = (%j <= %maxLane); break;\n}\nif (!%pval) %j = %lane; // copy from own lane\nif (thread at lane %j is active)\n   %d = %a from lane %j\nelse\n   %d = undef\nreturn {%d, %pval} Note that the return values are undefined if the thread at the source lane is not in %membermask . The behavior of this intrinsic is undefined if the executing thread is not in the %membermask . For compute_62 or below, all threads in %membermask must call the same @llvm.nvvm.shfl.sync.i32() in convergence, and only threads belonging to the %membermask can be active when the intrinsic is called. Otherwise, the behavior is undefined. 15.6.3. Vote  The following intrinsic synchronizes a subset of threads in a warp and then performs a reduce-and-broadcast of a predicate over all threads in the subset. declare {i32, i1} @llvm.nvvm.vote.sync(i32 %membermask, i32 %mode, i1 %predicate) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before performing a reduce-and-broadcast of a predicate over all threads in the subset. The argument %membermask is a 32-bit mask, with each bit corresponding to a lane in the warp. 1 means the thread is in the subset. @llvm.nvvm.vote.sync() performs a reduction of the source %predicate across all threads in %membermask after the synchronization. The return value is the same across all threads in the %membermask . The element in the returned aggregate that holds the return value depends on %mode . The argument %mode must be a constant and its encoding is specified in the following table. Encoding Meaning return value 0 ALL i1: 1 if the source predicates is 1 for all thread in %membermask , 0 otherwise 1 ANY i1: 1 if the source predicate is 1 for any thread in %membermask , 0 otherwise 2 EQ i1: 1 if the source predicates are the same for all thread in %membermask , 0 otherwise 3 BALLOT i32: ballot data, containing the %predicate value from each thread in %membermask For the BALLOT mode, the i32 value represents the ballot data, which contains the %predicate value from each thread in %membermask in the bit position corresponding to the thread’s land id. The bit value corresponding to a thread not in %membermask is 0. Note that the return values are undefined if the thread at the source lane is not in %membermask . The behavior of this intrinsic is undefined if the executing thread is not in the %membermask . For compute_62 or below, all threads in %membermask must call the same @llvm.nvvm.vote.sync() in convergence, and only threads belonging to the %membermask can be active when the intrinsic is called. Otherwise, the behavior is undefined. 15.6.4. Match  The following intrinsics synchronize a subset of threads in a warp and then broadcast and compare a value across threads in the subset. declare i32 @llvm.nvvm.match.any.sync.i32(i32 %membermask, i32 %value)\ndeclare i32 @llvm.nvvm.match.any.sync.i64(i32 %membermask, i64 %value)\ndeclare {i32, i1} @llvm.nvvm.match.all.sync.i32(i32 %membermask, i32 %value)\ndeclare {i32, i1} @llvm.nvvm.match.all.sync.i64(i32 %membermask, i64 %value) These intrinsics cause executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before performing broadcast and compare of operand %value across all threads in the subset. The argument %membership is a 32bit mask, with each bit corresponding to a lane in the warp. 1 means the thread is in the subset. The i32 return value is a 32-bit mask where bit position in mask corresponds to thread’s laneid. In the any version, the i32 return value is set to the mask of active threads in %membermask that have same value as operand %value . In the all version, if all active threads in %membermask have same value as operand %value , the i32 return value is set to %membermask , and the i1 value is set to 1. Otherwise, the i32 return value is set to 0 and the i1 return value is also set to 0. The behavior of this intrinsic is undefined if the executing thread is not in the %membermask . These intrinsics are only available on compute_70 or higher. 15.6.5. Matrix Operation  THIS IS PREVIEW FEATURE. SUPPORT MAY BE REMOVED IN FUTURE RELEASES. NVVM provides warp-level intrinsics for matrix multiply operations. The core operation is a matrix multiply and accumulate of the form: D = A*B + C, or\nC = A*B + C where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices. C and D are also called accumulators. The element type of the A and B matrices is 16-bit floating point. The element type of the accumulators can be either 32-bit floating point or 16-bit floating point. All threads in a warp will collectively hold the contents of each matrix A , B , C and D . Each thread will hold only a fragment of matrix A , a fragment of matrix B , a fragment of matrix C , and a fragment of the result matrix D . How the elements of a matrix are distributed among the fragments is opaque to the user and is different for matrix A , B and the accumulator. A fragment is represented by a sequence of element values. For fp32 matrices, the element type is float . For fp16 matrices, the element type is i32 (each i32 value holds two fp16 values). The number of elements varies with the shape of the matrix. 15.6.5.1. Load Fragments  The following intrinsics synchronize all threads in a warp and then load a fragment of a matrix for each thread. ; load fragment A\ndeclare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.ld.a.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.ld.a.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.ld.a.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\n\n; load fragment B\ndeclare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.ld.b.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.ld.b.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.ld.b.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\n\n; load fragment C\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m16n16k16.ld.c.f32.p<n>f32(float addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m32n8k16.ld.c.f32.p<n>f32(float addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m8n32k16.ld.c.f32.p<n>f32(float addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\n\n; load fragment C\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.ld.c.f16.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.ld.c.f16.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol);\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.ld.c.f16.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol); These intrinsics load and return a matrix fragment from memory at location %ptr . The matrix in memory must be in a canonical matrix layout with leading dimension %ldm . %rowcol specifies which the matrix in memory is row-major (0) or column-major (1). %rowcol must be a constant value. The returned sequence of values represent the fragment held by the calling thread. How the elements of a matrix are distributed among the fragments is opaque to the user and is different for matrix A , B and the accumulator. Therefore, three variants (i.e. ld.a , ld.b , and ld.c ) are provided. These intrinsics are overloaded based on the address spaces. The address space number <n> must be either 0 (generic), 1 (global) or 3 (shared). The behavior of this intrinsic is undefined if any thread in the warp has exited. These intrinsics are only available on compute_70 or higher. 15.6.5.2. Store Fragments  The following intrinsics synchronize all threads in a warp and then store a fragment of a matrix for each thread. ; The last 8 arguments are the elements of the C fragment\ndeclare void @llvm.nvvm.hmma.m16n16k16.st.c.f32.p<n>float(float addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol, float, float, float, float, float, float, float, float);\ndeclare void @llvm.nvvm.hmma.m32n8k16.st.c.f32.p<n>float(float addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol, float, float, float, float, float, float, float, float);\ndeclare void @llvm.nvvm.hmma.m8n32k16.st.c.f32.p<n>float(float addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol, float, float, float, float, float, float, float, float);\n\n; The last 4 arguments are the elements of the C fragment\ndeclare void @llvm.nvvm.hmma.m16n16k16.st.c.f16.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol, i32, i32, i32, i32);\ndeclare void @llvm.nvvm.hmma.m32n8k16.st.c.f16.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol, i32, i32, i32, i32);\ndeclare void @llvm.nvvm.hmma.m8n32k16.st.c.f16.p<n>i32(i32 addrspace(<n>)* %ptr, i32 %ldm, i32 %rowcol, i32, i32, i32, i32); These intrinsics store an accumulator fragment to memory at location %ptr . The matrix in memory must be in a canonical matrix layout with leading dimension %ldm . %rowcol specifies which the matrix in memory is row-major (0) or column-major (1). %rowcol must be a constant value. These intrinsics are overloaded based on the address spaces. The address space number <n> must be either 0 (generic), 1 (global) or 3 (shared). The behavior of this intrinsic is undefined if any thread in the warp has exited. These intrinsics are only available on compute_70 or higher. 15.6.5.3. Matrix Multiply-and-Accumulate  The following intrinsics synchronize all threads in a warp and then perform a matrix multiply-and-accumulate operation. declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.mma.f16.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3);\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.mma.f16.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3);\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.mma.f16.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3);\n\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m16n16k16.mma.f32.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3);\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m32n8k16.mma.f32.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3);\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m8n32k16.mma.f32.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3);\n\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m16n16k16.mma.f32.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7);\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m32n8k16.mma.f32.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7);\ndeclare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m8n32k16.mma.f32.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7);\n\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.mma.f16.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7);\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.mma.f16.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7);\ndeclare {i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.mma.f16.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); These intrinsics perform a matrix multiply-and-accumulate operation. %rowcol specifies the layout of A and B fragments. It must be a constant value, which can have the following values and semantics. Encoding Meaning 0 A fragment is row-major, B fragment is row-major 1 A fragment is row-major, B fragment is column-major 2 A fragment is column-major, B fragment is row-major 3 A fragment is column-major, B fragment is column-major Support for %satf has been removed and this operand must be a constant zero. The behavior of these intrinsics are undefined if any thread in the warp has exited. These intrinsics are only available on compute_70 or higher. 16. Source Level Debugging Support  To enable source level debugging of an IR module, NVVM IR supports debug intrinsics and debug information descriptors to express the debugging information. Debug information descriptors are represented using specialized metadata nodes. The current NVVM IR debug metadata version is 3.1. The current NVVM IR debugging support is based on that in LLVM 7.0.1. For the complete semantics of the IR, readers of this chapter should check the official LLVM IR specialized metadata nodes documentation ( https://releases.llvm.org/7.0.1/docs/LangRef.html#specialized-metadata-nodes ) and the Source Level Debugging with LLVM Manual ( https://releases.llvm.org/7.0.1/docs/SourceLevelDebugging.html ). The following metadata nodes need to be present in the module when debugging support is requested: Named metadata node !llvm.dbg.cu Module flags metadata for \"Debug Info Version\" flag: The behavior flag should be Error . The value of the flag should be DEBUG_METADATA_VERSION in LLVM 7.0.1, which is 3. Named metadata !nvvmir.version containing a metadata node with the NVVM IR major and minor version values followed by the NVVM IR debug metadata major and minor version values. The current NVVM IR debug metadata version is 3.1. The debug resolution (e.g., full, line info only) is controlled by the DICompileUnit’s emissionKind field: FullDebug (value: 1) : Generate symbolic debug and line information. This requires the libNVVM -g option to be specified at compile time. DebugDirectivesOnly (value: 3) : Generate line information. Source level debugging is supported only for a single debug compile unit. If there are multiple input NVVM IR modules, at most one module may have a single debug compile unit. 17. NVVM ABI for PTX  17.1. Linkage Types  The following table provides the mapping of NVVM IR linkage types associated with functions and global variables to PTX linker directives . LLVM Linkage Type PTX Linker Directive private , internal This is the default linkage type and does not require a linker directive. external Function with definition .visible Global variable with initialization Function without definition .extern Global variable without initialization common .common for the global address space, otherwise .weak available_externally , linkonce , linkonce_odr , weak , weak_odr .weak All other linkage types Not supported. 17.2. Parameter Passing and Return  The following table shows the mapping of function argument and return types in NVVM IR to PTX types. Source Type Size in Bits PTX Type Integer types <= 32 .u32 or .b32 (zero-extended if unsigned) .s32 or .b32 (sign-extended if signed) 64 .u64 or .b64 (if unsigned) .s64 or .b64 (if signed) Pointer types (without byval attribute) 32 .u32 or .b32 64 .u64 or .b64 Floating-point types 32 .f32 or .b32 64 .f64 or .b64 Aggregate types Any size .align align .b8 name [ size ] Where align is overall aggregate or vector alignment in bytes, name is variable name associated with aggregate or vector, and size is the aggregate or vector size in bytes. Pointer types to aggregate with byval attribute 32 or 64 Vector type Any size 18. Revision History  Version 1.0 Initial Release. Version 1.1 Added support for UVM managed variables in global property annotation. See Supported Properties . Version 1.2 Update to LLVM 3.4 for CUDA 7.0. Remove address space intrinsics in favor of addrspacecast . Add information about source level debugging support. Version 1.3 Add support for LLVM 3.8 for CUDA 8.0. Version 1.4 Add support for warp-level intrinsics. Version 1.5 Add support for LLVM 5.0 for CUDA 9.2. Version 1.6 Update to LLVM 7.0.1 for CUDA 11.2. Version 1.7 Add support for alloca with dynamic size. Version 1.8 Add support for i128 in data layout. Version 1.9 Modified text about ignoring shared variable initializations. Version 1.10 Added support for grid_constant kernel parameters for CUDA 11.7. Version 1.11 Added support for Hopper+ cluster intrinsics and max_blocks_per_cluster kernel property for CUDA 11.8. Deprecated support for 32-bit compilation. Version 2.0 Updated the NVVM IR to version 2.0 which is incompatible with NVVM IR version 1.x Removed address space conversion intrinsics. The IR verifier on 2.0 IR will give an error when these intrinsics are present. Clients of libNVVM are advised to use addrspacecast instruction instead. Stricter error checking on the supported datalayouts. Older style loop unroll pragma metadata on loop backedges is no longer supported. Clients are advised to use the newer loop pragma metadata defined by the LLVM framework. Shared variable initialization with non-undef values is no longer supported. In 1.x versions these initializers were ignored silently. This feature makes the 2.0 version incompatible with 1.x versions. 19. Notices  19.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 19.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 19.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html", "content_type": "text/html", "text": "NVIDIA Hopper Tuning Guide 1. NVIDIA Hopper Tuning Guide 1.1. NVIDIA Hopper GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Hopper Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Tensor Memory Accelerator 1.4.1.3. Thread Block Clusters 1.4.1.4. Improved FP32 Throughput 1.4.1.5. Dynamic Programming Instructions 1.4.2. Memory System 1.4.2.1. High-Bandwidth Memory HBM3 Subsystem 1.4.2.2. Increased L2 Capacity 1.4.2.3. Inline Compression 1.4.2.4. Unified Shared Memory/L1/Texture Cache 1.4.3. Fourth-Generation NVLink 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Hopper Tuning Guide » 1. NVIDIA Hopper Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Hopper GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the Hopper GPU Architecture. 1. NVIDIA Hopper Tuning Guide  1.1. NVIDIA Hopper GPU Architecture  The NVIDIA® Hopper GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Hopper GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere GPU architecture and NVIDIA Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA H100 GPU without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Hopper GPU architecture’s features. 1 For further details on the programming features discussed in this guide, refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure that global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Hopper Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with NVIDIA Hopper. 1.4. NVIDIA Hopper Tuning  1.4.1. Streaming Multiprocessor  The NVIDIA Hopper Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM remains the same as in NVIDIA Ampere GPU architecture (that is, 64), and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 9.0 (that is, H100 GPUs). For devices of compute capability 9.0 (H100 GPUs), shared memory capacity per SM is 228 KB, a 39% increase compared to A100’s capacity of 164 KB. For devices of compute capability 9.0 (H100 GPUs), the maximum shared memory per thread block is 227 KB. For applications using Thread Block Clusters, it is always recommended to compute the occupancy using cudaOccupancyMaxActiveClusters and launch cluster-based kernels accordingly. Overall, developers can expect similar occupancy as on NVIDIA Ampere GPU architecture GPUs without changes to their application. 1.4.1.2. Tensor Memory Accelerator  The Hopper architecture builds on top of the asynchronous copies introduced by NVIDIA Ampere GPU architecture and provides a more sophisticated asynchronous copy engine: the Tensor Memory Accelerator (TMA). TMA allows applications to transfer 1D and up to 5D tensors between global memory and shared memory, in both directions, as well as between the shared memory regions of different SMs in the same cluster (refer to Thread Block Clusters ). Additionally, for writes from shared memory to global memory, it allows specifying element wise reduction operations such as add/min/max as well as bitwise and/or for most common data types. This has several advantages: Avoids using registers for moving data between the different memory spaces. Avoids using SM instructions for moving data: a single thread can issue large data movement instructions to the TMA unit. The whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually necessary. Enables users to write warp specialized codes, where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the SM. This feature will be exposed through cuda::memcpy_async along with the cuda::barrier and cuda::pipeline for synchronizing data movement. 1.4.1.3. Thread Block Clusters  NVIDIA Hopper Architecture adds a new optional level of hierarchy, Thread Block Clusters, that allows for further possibilities when parallelizing applications. A thread block can read from, write to, and perform atomics in shared memory of other thread blocks within its cluster. This is known as Distributed Shared Memory. As demonstrated in the CUDA C++ Programming Guide , there are applications that cannot fit required data within shared memory and must use global memory instead. Distributed shared memory can act as an intermediate step between these two options. Distributed Shared Memory can be used by an SM simultaneously with L2 cache accesses. This can benefit applications that need to communicate data between SMs by utilizing the combined bandwidth of both distributed shared memory and L2. In order to achieve best performance for accesses to Distributed Shared Memory,\naccess patterns to those described in the CUDA C++ Best Practices Guide for Global Memory should be used.\nSpecifically, accesses to Distributed Shared Memory should be coalesced\nand aligned to 32-byte segments, if possible.\nAccess patterns with non-unit stride should be avoided if possible,\nwhich can be achieved by using local shared memory,\nsimilar to what is shown in the CUDA C++ Best Practices Guide for Shared Memory . The maximum portable cluster size supported is 8; however, NVIDIA Hopper H100 GPU allows for a nonportable cluster size of 16 by opting in. Launching a kernel with a nonportable cluster size requires setting the cudaFuncAttributeNonPortableClusterSizeAllowed function attribute. Using larger cluster sizes may reduce the maximum number of active blocks across the GPU (refer to Occupancy ). 1.4.1.4. Improved FP32 Throughput  Devices of compute capability 9.0 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. 1.4.1.5. Dynamic Programming Instructions  The NVIDIA Hopper architecture adds support for new instructions to accelerate dynamic programming algorithms, such as the Smith-Waterman algorithm for sequence alignment in bioinformatics, and algorithms in graph theory, game theory, ML, and finance problems. The new instructions permit computation of max and min values among three operands, max and min operations yielding predicates, combined add operation with max or min, operating on signed and unsigned 32-bit int and 16-bit short2 types, and half2. All DPX instructions with 16-bit short types DPX instructions enable 128 operations per cycle per SM. 1.4.2. Memory System  1.4.2.1. High-Bandwidth Memory HBM3 Subsystem  The NVIDIA H100 GPU has support for HBM3 and HBM2e memory, with capacity up to 80 GB. GPUs HBM3 memory system supports up to 3 TB/s memory bandwidth, a 93% increase over the 1.55 TB/s on A100-40GB. 1.4.2.2. Increased L2 Capacity  The NVIDIA Hopper architecture increases the L2 cache capacity from 40 MB in the A100 GPU to 50 MB in the H100 GPU. Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased. The NVIDIA Hopper architecture allows CUDA users to control the persistence of data in L2 cache similar to the NVIDIA Ampere GPU Architecture. For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Inline Compression  The NVIDIA Hopper architecture allows CUDA compute kernels to benefit from the new inline compression (ILC). This feature can be applied to individual memory allocation, and the compressor automatically chooses between several possible compression algorithms, or none if there is no suitable pattern. In case compression can be used, this feature allows accessing global memory at significantly higher bandwidth than global memory bandwidth, since only compressed data needs to be transferred between global memory and SMs. However, the feature does not allow for reducing memory footprint: since compression is automatic, even if compression is active, the memory region will use the same footprint as if there was no compression. This is because underlying data may be changed by the user application and may not be compressible during the entire duration of the application. The feature is available through the CUDA driver API. See the CUDA C++ Programming Guide section on compressible memory : CUmemGenericAllocationHandle allocationHandle ; CUmemAllocationProp prop = {}; memset ( prop , 0 , sizeof ( CUmemAllocationProp )); prop -> type = CU_MEM_ALLOCATION_TYPE_PINNED ; prop -> location . type = CU_MEM_LOCATION_TYPE_DEVICE ; prop -> location . id = currentDevice ; prop -> allocFlags . compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; cuMemCreate ( & allocationHandle , size , & prop , 0 ); One can check whether compressible memory is available on the given device with: cuDeviceGetAttribute ( & compressionAvailable , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , currentDevice ) Note that this example code does not handle errors and compiling this code requires linking against the CUDA library ( libcuda.so ). 1.4.2.4. Unified Shared Memory/L1/Texture Cache  The NVIDIA H100 GPU based on compute capability 9.0 increases the maximum capacity of the combined L1 cache, texture cache, and shared memory to 256 KB, from 192 KB in NVIDIA Ampere Architecture, an increase of 33%. In the NVIDIA Hopper GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout) can be selected at runtime as in previous architectures such as NVIDIA Ampere Architecture and NVIDIA Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA H100 GPU supports shared memory capacities of 0, 8, 16, 32, 64, 100, 132, 164, 196 and 228 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, the H100 GPU enables a single thread block to address up to 227 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like the NVIDIA Ampere Architecture and NVIDIA Volta GPU architectures, the NVIDIA Hopper GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp before delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 1.4.3. Fourth-Generation NVLink  The fourth generation of NVIDIA’s high-speed NVLink interconnect is implemented in H100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features. The fourth-generation NVLink has the same bidirectional data rate of 50 GB/s per link. The total number of links available is increased to 18 in H100, compared to 12 in A100, yielding 900 GB/s bidirectional bandwidth compared to 600 GB/s for A100. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History  Version 1.0 Initial Public Release Added support for compute capability 9.0 1 Throughout this guide, NVIDIA Volta refers to devices of compute capability 7.0, NVIDIA Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x, and NVIDIA Hopper refers to devices of compute capability 9.0. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/turing-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/turing-tuning-guide/index.html", "content_type": "text/html", "text": "Turing Tuning Guide 1. Turing Tuning Guide 1.1. NVIDIA Turing Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Turing Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Independent Thread Scheduling 1.4.1.3. Occupancy 1.4.1.4. Integer Arithmetic 1.4.2. Tensor Core Operations 1.4.3. Memory Throughput 1.4.3.1. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Turing Tuning Guide » 1. Turing Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Turing The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Turing Architecture. 1. Turing Tuning Guide  1.1. NVIDIA Turing Compute Architecture  Turing is NVIDIA’s latest architecture for CUDA compute applications. Turing retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Pascal and Volta, and applications that follow the best practices for those architectures should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Turing architectural features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Turing Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Turing. 1.4. Turing Tuning  1.4.1. Streaming Multiprocessor  The Turing Streaming Multiprocessor (SM) is based on the same major architecture (7.x) as Volta, and provides similar improvements over Pascal. 1.4.1.1. Instruction Scheduling  Each Turing SM includes 4 warp-scheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp, or by 16 warps per SM without any instuction-level parallelism. Like Volta, the Turing SM provides 64 FP32 cores, 64 INT32 cores and 8 improved mixed-precision Tensor Cores. Turing has a lower double precision throughput than Volta with only 2 FP64 cores. 1.4.1.2. Independent Thread Scheduling  The Turing architecture features the same Independent Thread Scheduling introduced with Volta. This enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures. When porting existing codes to Volta or Turing, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide . To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier. The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy  The maximum number of concurrent warps per SM is 32 on Turing (versus 64 on Volta). Other factors influencing warp occupancy remain otherwise similar: The register file size is 64k 32-bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 16. Shared memory capacity per SM is 64KB. Overall, developers can expect similar occupancy as on Pascal or Volta without changes to their application. 1.4.1.4. Integer Arithmetic  Similar to Volta, the Turing SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can interleave pointer arithmetic with floating-point computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations  Volta introduced Tensor Cores to accelerate matrix multiply operations on mixed precision floating point data. Turing adds acceleration for integer matrix multiply operations. The tensor cores are exposed as Warp-Level Matrix Operations in the CUDA 10 C++ API. The API provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use Tensor Cores from a CUDA-C++ program. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller matrix fragments. Each Tensor Core performs the matrix multiply-accumulate: D = A x B + C. The Tensor Cores support half precision matrix multiplication, where the matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be either FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition. CUDA 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the Tensor Cores on Volta or Turing with FP16 inputs. Any binary compiled for Volta will run on Turing, but Volta binaries using Tensor Cores will only be able to reach half of Turing’s Tensor Core peak performance. Recompiling the binary specifically for Turing would allow it to reach the peak performance. See the Turing Compatibility Guide for more information. Turing’s Tensor Core supports integer matrix multiply operations, which can operate on 8-bit, 4-bit and 1-bit integer inputs, with 32-bit integer accumulation. When operating on 8-bit inputs, CUDA exposes fragment sizes of 16x16x16, 32x8x16, and 8x32x16. For sub-byte operations the fragment sizes available are 8x8x32 for 4-bit inputs, or 8x8x128 for 1-bit inputs. See the CUDA C++ Programming Guide for more information. 1.4.3. Memory Throughput  1.4.3.1. Unified Shared Memory/L1/Texture Cache  Turing features a unified L1 / Shared Memory cache similar to the one introduced in Volta, but with a smaller size. The total size of the unified L1 / Shared Memory cache in Turing is 96 KB. The portion of the cache dedicated to shared memory or L1 (known as the carveout ) can be changed at runtime, either automatically by the driver, or manually using the cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . Turing supports two carveout configurations, either with 64 KB of shared memory and 32 KB of L1, or with 32 KB of shared memory and 64 KB of L1. Turing allows a single thread block to address the full 64 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Pascal and Volta, Turing combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. The state-of-the-art L1 cache in Volta and Turing offers lower latency, higher bandwidth, and higher capacity compared to the earlier architectures. Like Volta, Turing’s L1 can cache write operations (write-through). The result is that for many applications Volta and Turing narrow the performance gap between explicitly managed shared memory and direct access to device memory. Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to devices of compute capability 6.x, Volta refers to devices of compute capability 7.0, and Turing refers to devices of compute capability 7.5. 2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/volta-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/volta-tuning-guide/index.html", "content_type": "text/html", "text": "Volta Tuning Guide 1. Volta Tuning Guide 1.1. NVIDIA Volta Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Volta Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Independent Thread Scheduling 1.4.1.3. Occupancy 1.4.1.4. Integer Arithmetic 1.4.2. Tensor Core Operations 1.4.3. Memory Throughput 1.4.3.1. High Bandwidth Memory 1.4.3.2. Unified Shared Memory/L1/Texture Cache 1.4.4. Cooperative Groups 1.4.5. Multi-Process Service 1.4.6. NVLink Interconnect 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Volta Tuning Guide » 1. Volta Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Volta The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Volta Architecture. 1. Volta Tuning Guide  1.1. NVIDIA Volta Compute Architecture  Volta is NVIDIA’s latest architecture for CUDA compute applications. Volta retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell and Pascal, and applications that follow the best practices for those architectures should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Volta architectural features. 1 Volta architecture comprises a single variant: GV100. A detailed overview of the major improvements in GV100 over earlier NVIDIA architectures is provided in a white paper entitled NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU . For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Volta Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Volta. 1.4. Volta Tuning  1.4.1. Streaming Multiprocessor  The Volta Streaming Multiprocessor (SM) provides the following improvements over Pascal. 1.4.1.1. Instruction Scheduling  Each Volta SM includes 4 warp-scheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations are reduced to four clock cycles, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp. Many more warps are, of course, recommended to cover the much greater latency of memory transactions and control-flow operations. Similar to GP100, the GV100 SM provides 64 FP32 cores and 32 FP64 cores. The GV100 SM additionally includes 64 INT32 cores and 8 mixed-precision Tensor Cores. GV100 provides up to 84 SMs. 1.4.1.2. Independent Thread Scheduling  The Volta architecture introduces Independent Thread Scheduling among threads in a warp. This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures. When porting existing codes to Volta, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide . To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier. The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy  The maximum number of concurrent warps per SM remains the same as in Pascal (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size is 64k 32-bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 32. Shared memory capacity per SM is 96KB, similar to GP104, and a 50% increase compared to GP100. Overall, developers can expect similar occupancy as on Pascal without changes to their application. 1.4.1.4. Integer Arithmetic  Unlike Pascal GPUs, the GV100 SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can now interleave pointer arithmetic with floating-point computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations  Each Tensor Core performs the following operation: D = AxB + C, where A, B, C, and D are 4x4 matrices. The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition with the other intermediate products for a 4x4x4 matrix multiply. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller elements. The Volta tensor cores are exposed as Warp-Level Matrix Operations in the CUDA 9 C++ API. The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program. At the CUDA level, the warp-level interface assumes 16x16 size matrices spanning all 32 threads of the warp. See the CUDA C++ Programming Guide for more information. 1.4.3. Memory Throughput  1.4.3.1. High Bandwidth Memory  GV100 uses up to eight memory dies per HBM2 stack and four stacks, with a maximum of 32 GB of GPU memory. A faster and more efficient HBM2 implementation delivers up to 900 GB/s of peak memory bandwidth, compared to 732 GB/s for GP100. This combination of a new generation HBM2 memory, and a new generation memory controller, in Volta provides 1.5x delivered memory bandwidth, compared to Pascal GP100—and a greater than 95% memory bandwidth efficiency running many workloads. In order to hide the DRAM latencies at full HBM2 bandwidth more memory accesses must be kept in flight, compared to GPUs equipped with traditional GDDR5. This is accomplished by the large complement of SMs in GV100, which typically boost the number of concurrent threads, and thus the reads-in-flight, compared to previous architectures. Resource-constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread. 1.4.3.2. Unified Shared Memory/L1/Texture Cache  In Volta the L1 cache, texture cache, and shared memory are backed by a combined 128 KB data cache. As in previous architectures, the portion of the cache dedicated to shared memory (known as the carveout ) can be selected at runtime using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . Volta supports shared memory capacities of 0, 8, 16, 32, 64, or 96 KB per SM. A new feature, Volta enables a single thread block to address the full 96 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Pascal, Volta combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Volta increases the maximum capacity of the L1 cache to 128 KB, more than 7x larger than the GP100 L1. Another benefit of its union with shared memory, the Volta L1 improves in terms of both latency and bandwidth compared to Pascal. The result is that for many applications Volta narrows the performance gap between explicitly managed shared memory and direct access to device memory. Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 1.4.4. Cooperative Groups  The Volta architecture introduced Independent Thread Scheduling, which enables intra-warp synchronization patterns that were previously not possible. To efficiently express these new patterns, CUDA 9 introduces Cooperative Groups. This is an extension to the CUDA programming model for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions. See the CUDA C++ Programming Guide for more information. 1.4.5. Multi-Process Service  The Volta Multi-Process Service is significantly improved compared to previous architecutres, both in terms of performance and robustness. Intermediary software schedulers, used for MPS with previous architectures, have been replaced by hardware accelerated units within the GPU. MPS clients now submit tasks directly to the GPU work queues, significantly decreasing submission latency and increasing aggregate throughput. The limit on the number of MPS clients has also been increased by 3x to 48. Volta MPS also provides each client with an isolated address space, 3 and extends Unified Memory support for MPS applications. Volta MPS also provides control for clients to restrict each client to a fraction of the GPU execution resources. Developers can use this feature to reduce or eliminate head-of-line blocking where work from one MPS client overwhelms GPU execution resources and prevents other clients from making progress, and thus improve average latency and jitter accross the system. 1.4.6. NVLink Interconnect  NVLink is NVIDIA’s high-speed data interconnect. NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory. GV100 supports up to six NVLink connections with each connection carrying up to 50 GB/s of bi-directional bandwidth. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Added Cooperative Groups section. Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, and Volta refers to devices of compute capability 7.x. 2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction. 3 As with previous architectures, MPS does not provide fatal fault isolation between clients. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html", "content_type": "text/html", "text": "NVIDIA Ampere GPU Architecture Tuning Guide 1. NVIDIA Ampere GPU Architecture Tuning Guide 1.1. NVIDIA Ampere GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ampere GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier 1.4.1.4. Warp level support for Reduction Operations 1.4.1.5. Improved Tensor Core Operations 1.4.1.6. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased Memory Capacity and High Bandwidth Memory 1.4.2.2. Increased L2 capacity and L2 Residency Controls 1.4.2.3. Unified Shared Memory/L1/Texture Cache 1.4.3. Third Generation NVLink 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Ampere Tuning Guide » 1. NVIDIA Ampere GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ampere GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ampere GPU Architecture. 1. NVIDIA Ampere GPU Architecture Tuning Guide  1.1. NVIDIA Ampere GPU Architecture  The NVIDIA Ampere GPU architecture is NVIDIA’s latest architecture for CUDA compute applications. The NVIDIA Ampere GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as Turing and Volta, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA A100 GPU without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ampere GPU architecture’s features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ampere GPU Architecture. 1.4. NVIDIA Ampere GPU Architecture Tuning  1.4.1. Streaming Multiprocessor  The NVIDIA Ampere GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Volta and Turing. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM remains the same as in Volta (i.e., 64) for compute capability 8.0, while for compute capability 8.6 it is 48. Other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 8.0 (i.e., A100 GPUs) and 16 for GPUs with compute capability 8.6. For devices of compute capability 8.0 (i.e., A100 GPUs) shared memory capacity per SM is 164 KB, a 71% increase compared to V100’s capacity of 96 KB. For GPUs with compute capability 8.6, shared memory capacity per SM is 100 KB. For devices of compute capability 8.0 (i.e., A100 GPUs) the maximum shared memory per thread block is 163 KB. For GPUs with compute capability 8.6 maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on Volta without changes to their application. 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory  The NVIDIA Ampere GPU architecture adds hardware acceleration for copying data from global memory to shared memory. These copy instructions are asynchronous, with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the SM. These instructions also avoid using extra registers for memory copies and can also bypass the L1 cache. This new feature is exposed via the pipeline API in CUDA. For more information please refer to the section on Async Copy in the CUDA C++ Programming Guide . 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier  The NVIDIA Ampere GPU architecture adds hardware acceleration for a split arrive/wait barrier in shared memory. These barriers can be used to implement fine grained thread controls, producer-consumer computation pipeline and divergence code patterns in CUDA. These barriers can also be used alongside the asynchronous copy. For more information on the Arrive/Wait Barriers refer to the Arrive/Wait Barrier section in the CUDA C++ Programming Guide . 1.4.1.4. Warp level support for Reduction Operations  The NVIDIA Ampere GPU architecture adds native support for warp wide reduction operations for 32-bit signed and unsigned integer operands. The warp wide reduction operations support arithmetic add , min , and max operations on 32-bit signed and unsigned integers and bitwise and , or and xor operations on 32-bit unsigned integers. For more details on the new warp wide reduction operations refer to Warp Reduce Functions in the CUDA C++ Programming Guide . 1.4.1.5. Improved Tensor Core Operations  The NVIDIA Ampere GPU architecture includes new Third Generation Tensor Cores that are more powerful than the Tensor Cores used in Volta and Turing SMs. The new Tensor Cores use a larger base matrix size and add powerful new math modes including: Support for FP64 Tensor Core, using new DMMA instructions. Support for Bfloat16 Tensor Core, through HMMA instructions. BFloat16 format is especially effective for DL training scenarios. Bfloat16 provides 8-bit exponent i.e., same range as FP32, 7-bit mantissa and 1 sign-bit. Support for TF32 Tensor Core, through HMMA instructions. TF32 is a new 19-bit Tensor Core format that can be easily integrated into programs for more accurate DL training than 16-bit HMMA formats. TF32 provides 8-bit exponent, 10-bit mantissa and 1 sign-bit. Support for bitwise AND along with bitwise XOR which was introduced in Turing, through BMMA instructions. The following table presents the evolution of matrix instruction sizes and supported data types for Tensor Cores across different GPU architecture generations. Instruction GPU Architecture Input Matrix format Output Accumulator format Matrix Instruction Size (MxNxK) HMMA (16-bit precision) NVIDIA Volta Architecture FP16 FP16 / FP32 8x8x4 NVIDIA Turing Architecture FP16 FP16 / FP32 8x8x4 / 16x8x8 / 16x8x16 NVIDIA Ampere Architecture FP16 / BFloat16 FP16 / FP32\n(BFloat16 only supports FP32 as accumulator) 16x8x8 / 16x8x16 HMMA (19-bit precision) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture N/A N/A N/A NVIDIA Ampere Architecture TF32 (19-bits) FP32 16x8x4 IMMA (Integer MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture unsigned char/signed char (8-bit precision) int32 8x8x16 NVIDIA Ampere Architecture unsigned char/signed char (8-bit precision) int32 8x8x16 / 16x8x16 / 16x8x32 IMMA (Integer sub-byte MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture unsigned u4/signed u4 (4-bit precision) int32 8x8x32 NVIDIA Ampere Architecture unsigned u4/signed u4 (4-bit precision) int32 8x8x32 / 16x8x32 / 16x8x64 BMMA (Binary MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture single bit int32 8x8x128 NVIDIA Ampere Architecture single bit int32 8x8x128 / 16x8x128 / 16x8x256 DMMA (64-bit precision) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture N/A N/A N/A NVIDIA Ampere Architecture FP64 FP64 8x8x4 For more details on the new Tensor Core operations refer to the Warp Matrix Multiply section in the CUDA C++ Programming Guide . 1.4.1.6. Improved FP32 throughput  Devices of compute capability 8.6 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run as is on 8.6, it is recommended to compile explicitly for 8.6 to benefit from the increased FP32 throughput. 1.4.2. Memory System  1.4.2.1. Increased Memory Capacity and High Bandwidth Memory  The NVIDIA A100 GPU increases the HBM2 memory capacity from 32 GB in V100 GPU to 40 GB in A100 GPU. Along with the increased memory capacity, the bandwidth is increased by 72%, from 900 GB/s on Volta V100 to 1550 GB/s on A100. 1.4.2.2. Increased L2 capacity and L2 Residency Controls  The NVIDIA Ampere GPU architecture increases the capacity of the L2 cache to 40 MB in Tesla A100, which is 7x larger than Tesla V100. Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased. The NVIDIA Ampere GPU architecture allows CUDA users to control the persistence of data in L2 cache. For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Unified Shared Memory/L1/Texture Cache  The NVIDIA A100 GPU based on compute capability 8.0 increases the maximum capacity of the combined L1 cache, texture cache and shared memory to 192 KB, 50% larger than the L1 cache in NVIDIA V100 GPU. The combined L1 cache capacity for GPUs with compute capability 8.6 is 128 KB. In the NVIDIA Ampere GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures such as Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA A100 GPU supports shared memory capacity of 0, 8, 16, 32, 64, 100, 132 or 164 KB per SM. GPUs with compute capability 8.6 support shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, the A100 GPU enables a single thread block to address up to 163 KB of shared memory and GPUs with compute capability 8.6 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Volta, the NVIDIA Ampere GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to Volta L1 is improvement in terms of both latency and bandwidth. 1.4.3. Third Generation NVLink  The third generation of NVIDIA’s high-speed NVLink interconnect is implemented in A100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features. The third generation NVLink has the same bi-directional data rate of 50 GB/s per link, but uses half the number of signal pairs to achieve this bandwidth. Therefore, the total number of links available is increased to twelve in A100, versus six in V100, yielding 600 GB/s bidirectional bandwidth versus 300 GB/s for V100. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. In the NVIDIA Ampere GPU architecture remote NVLINK accesses go through a Link TLB on the remote GPU. This Link TLB has a reach of 64 GB to the remote GPU’s memory. Applications with remote random accesses may want to constrain the remotely accessed region to 64 GB for each peer GPU. 2. Revision History  Version 1.1 Initial Public Release Added support for compute capability 8.6 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, and NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/libnvvm-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/libnvvm-api/index.html", "content_type": "text/html", "text": "libNVVM API 1. libNVVM API 1.1. Introduction 1.2. Thread Safety 1.3. Module 2. Error Handling 2.1. Enumerations 2.2. Functions 3. General Information Query 3.1. Functions 4. Compilation 4.1. Functions 4.2. Typedefs 5. Notices 5.1. Notice 5.2. OpenCL 5.3. Trademarks libNVVM API v4.0 » 1. libNVVM API v12.5 | Archive libNVVM API libNVVM API v4.0 Reference Manual 1. libNVVM API  1.1. Introduction  libNVVM API provides an interface for generating PTX code from both binary\nand text NVVM IR inputs. Compatible input can be generated by tools and\nlibraries that produce LLVM 7.0 IR and bitcode. Support for reading the text\nNVVM IR representation is deprecated and may be removed in a later release. 1.2. Thread Safety  libNVVM API provides a thread-safe interface to libNVVM. Clients can take\nadvantage of improved compilation speeds by spawning multiple compilation\nthreads concurrently. 1.3. Module  This chapter presents the API of the libNVVM library.\nHere is a list of all modules: Error Handling General Information Query Compilation 2. Error Handling  Enumerations nvvmResult NVVM API call result code. Functions const char * nvvmGetErrorString (nvvmResult result) Get the message string for the given nvvmResult code. 2.1. Enumerations  enum nvvmResult  NVVM API call result code. Values: enumerator NVVM_SUCCESS  enumerator NVVM_ERROR_OUT_OF_MEMORY  enumerator NVVM_ERROR_PROGRAM_CREATION_FAILURE  enumerator NVVM_ERROR_IR_VERSION_MISMATCH  enumerator NVVM_ERROR_INVALID_INPUT  enumerator NVVM_ERROR_INVALID_PROGRAM  enumerator NVVM_ERROR_INVALID_IR  enumerator NVVM_ERROR_INVALID_OPTION  enumerator NVVM_ERROR_NO_MODULE_IN_PROGRAM  enumerator NVVM_ERROR_COMPILATION  2.2. Functions  const char * nvvmGetErrorString ( nvvmResult result )  Get the message string for the given nvvmResult code. Parameters result – [in] NVVM API result code. Returns Message string for the given nvvmResult code. 3. General Information Query  Functions nvvmResult nvvmIRVersion (int *majorIR, int *minorIR, int *majorDbg, int *minorDbg) Get the NVVM IR version. nvvmResult nvvmVersion (int *major, int *minor) Get the NVVM version. 3.1. Functions  nvvmResult nvvmIRVersion ( int * majorIR , int * minorIR , int * majorDbg , int * minorDbg )  Get the NVVM IR version. Parameters majorIR – [out] NVVM IR major version number. minorIR – [out] NVVM IR minor version number. majorDbg – [out] NVVM IR debug metadata major version number. minorDbg – [out] NVVM IR debug metadata minor version number. Returns NVVM_SUCCESS nvvmResult nvvmVersion ( int * major , int * minor )  Get the NVVM version. Parameters major – [out] NVVM major version number. minor – [out] NVVM minor version number. Returns NVVM_SUCCESS 4. Compilation  Functions nvvmResult nvvmAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program. nvvmResult nvvmCompileProgram (nvvmProgram prog, int numOptions, const char **options) Compile the NVVM program. nvvmResult nvvmCreateProgram (nvvmProgram *prog) Create a program, and set the value of its handle to *prog . nvvmResult nvvmDestroyProgram (nvvmProgram *prog) Destroy a program. nvvmResult nvvmGetCompiledResult (nvvmProgram prog, char *buffer) Get the compiled result. nvvmResult nvvmGetCompiledResultSize (nvvmProgram prog, size_t *bufferSizeRet) Get the size of the compiled result. nvvmResult nvvmGetProgramLog (nvvmProgram prog, char *buffer) Get the Compiler/Verifier Message. nvvmResult nvvmGetProgramLogSize (nvvmProgram prog, size_t *bufferSizeRet) Get the Size of Compiler/Verifier Message. nvvmResult nvvmLazyAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program. nvvmResult nvvmVerifyProgram (nvvmProgram prog, int numOptions, const char **options) Verify the NVVM program. Typedefs nvvmProgram NVVM Program. 4.1. Functions  nvvmResult nvvmAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name )  Add a module level NVVM IR to a program. The buffer should contain an NVVM IR module. The module should have NVVM IR either in the LLVM 7.0.1 bitcode representation or in the LLVM 7.0.1 text representation. Support for reading the text representation of NVVM IR is deprecated and may be removed in a later version. Parameters prog – [in] NVVM program. buffer – [in] NVVM IR module in the bitcode or text representation. size – [in] Size of the NVVM IR module. name – [in] Name of the NVVM IR module. If NULL, “<unnamed>” is used as the name. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmCompileProgram ( nvvmProgram prog , int numOptions , const char * * options )  Compile the NVVM program. The NVVM IR modules in the program will be linked at the IR level. The linked IR program is compiled to PTX. The target datalayout in the linked IR program is used to determine the address size (32bit vs 64bit). The valid compiler options are: -g (enable generation of full debugging information). Full debug support is only valid with ‘-opt=0’. Debug support requires the input module to utilize NVVM IR Debug Metadata. Line number (line info) only generation is also enabled via NVVM IR Debug Metadata, there is no specific libNVVM API flag for that case. -opt= 0 (disable optimizations) 3 (default, enable optimizations) -arch= compute_50 compute_52 (default) compute_53 compute_60 compute_61 compute_62 compute_70 compute_72 compute_75 compute_80 compute_87 compute_89 compute_90 -ftz= 0 (default, preserve denormal values, when performing single-precision floating-point operations) 1 (flush denormal values to zero, when performing single-precision floating-point operations) -prec-sqrt= 0 (use a faster approximation for single-precision floating-point square root) 1 (default, use IEEE round-to-nearest mode for single-precision floating-point square root) -prec-div= 0 (use a faster approximation for single-precision floating-point division and reciprocals) 1 (default, use IEEE round-to-nearest mode for single-precision floating-point division and reciprocals) -fma= 0 (disable FMA contraction) 1 (default, enable FMA contraction) -jump-table-density=[0-101] Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement. Default value is 101. The percentage ranges from 0 to 101 inclusively. -gen-lto (Generate LTO IR instead of PTX). Parameters prog – [in] NVVM program. numOptions – [in] Number of compiler options passed. options – [in] Compiler options in the form of C string array. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM NVVM_ERROR_COMPILATION nvvmResult nvvmCreateProgram ( nvvmProgram * prog )  Create a program, and set the value of its handle to *prog . See also nvvmDestroyProgram() Parameters prog – [in] NVVM program. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmDestroyProgram ( nvvmProgram * prog )  Destroy a program. See also nvvmCreateProgram() Parameters prog – [in] NVVM program. Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResult ( nvvmProgram prog , char * buffer )  Get the compiled result. The result is stored in the memory pointed to by buffer . Parameters prog – [in] NVVM program. buffer – [out] Compiled result. Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResultSize ( nvvmProgram prog , size_t * bufferSizeRet )  Get the size of the compiled result. Parameters prog – [in] NVVM program. bufferSizeRet – [out] Size of the compiled result (including the trailing NULL). Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLog ( nvvmProgram prog , char * buffer )  Get the Compiler/Verifier Message. The NULL terminated message string is stored in the memory pointed to by buffer when the return value is NVVM_SUCCESS. Parameters prog – [in] NVVM program. buffer – [out] Compilation/Verification log. Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLogSize ( nvvmProgram prog , size_t * bufferSizeRet )  Get the Size of Compiler/Verifier Message. The size of the message string (including the trailing NULL) is stored into bufferSizeRet when the return value is NVVM_SUCCESS. Parameters prog – [in] NVVM program. bufferSizeRet – [out] Size of the compilation/verification log (including the trailing NULL). Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmLazyAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name )  Add a module level NVVM IR to a program. The buffer should contain an NVVM IR module. The module should have NVVM IR in the LLVM 7.0.1 bitcode representation. A module added using this API is lazily loaded - the only symbols loaded are those that are required by module(s) loaded using nvvmAddModuleToProgram. It is an error for a program to have all modules loaded using this API. Compiler may also optimize entities in this module by making them internal to the linked NVVM IR module, making them eligible for other optimizations. Due to these optimizations, this API to load a module is more efficient and should be used where possible. Parameters prog – [in] NVVM program. buffer – [in] NVVM IR module in the bitcode representation. size – [in] Size of the NVVM IR module. name – [in] Name of the NVVM IR module. If NULL, “<unnamed>” is used as the name. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmVerifyProgram ( nvvmProgram prog , int numOptions , const char * * options )  Verify the NVVM program. The valid compiler options are: Same as for nvvmCompileProgram() . See also nvvmCompileProgram() Parameters prog – [in] NVVM program. numOptions – [in] Number of compiler options passed. options – [in] Compiler options in the form of C string array. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_IR NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM 4.2. Typedefs  typedef struct _nvvmProgram * nvvmProgram  NVVM Program. An opaque handle for a program. 5. Notices  5.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 5.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 5.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html", "parent_url": "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html", "content_type": "text/html", "text": "PTX ISA 8.5 1. Introduction 1.1. Scalable Data-Parallel Computing using GPUs 1.2. Goals of PTX 1.3. PTX ISA Version 8.5 1.4. Document Structure 2. Programming Model 2.1. A Highly Multithreaded Coprocessor 2.2. Thread Hierarchy 2.2.1. Cooperative Thread Arrays 2.2.2. Cluster of Cooperative Thread Arrays 2.2.3. Grid of Clusters 2.3. Memory Hierarchy 3. PTX Machine Model 3.1. A Set of SIMT Multiprocessors 3.2. Independent Thread Scheduling 3.3. On-chip Shared Memory 4. Syntax 4.1. Source Format 4.2. Comments 4.3. Statements 4.3.1. Directive Statements 4.3.2. Instruction Statements 4.4. Identifiers 4.5. Constants 4.5.1. Integer Constants 4.5.2. Floating-Point Constants 4.5.3. Predicate Constants 4.5.4. Constant Expressions 4.5.5. Integer Constant Expression Evaluation 4.5.6. Summary of Constant Expression Evaluation Rules 5. State Spaces, Types, and Variables 5.1. State Spaces 5.1.1. Register State Space 5.1.2. Special Register State Space 5.1.3. Constant State Space 5.1.3.1. Banked Constant State Space (deprecated) 5.1.4. Global State Space 5.1.5. Local State Space 5.1.6. Parameter State Space 5.1.6.1. Kernel Function Parameters 5.1.6.2. Kernel Function Parameter Attributes 5.1.6.3. Kernel Parameter Attribute: .ptr 5.1.6.4. Device Function Parameters 5.1.7. Shared State Space 5.1.8. Texture State Space (deprecated) 5.2. Types 5.2.1. Fundamental Types 5.2.2. Restricted Use of Sub-Word Sizes 5.2.3. Alternate Floating-Point Data Formats 5.2.4. Packed Data Types 5.2.4.1. Packed Floating Point Data Types 5.2.4.2. Packed Integer Data Types 5.3. Texture Sampler and Surface Types 5.3.1. Texture and Surface Properties 5.3.2. Sampler Properties 5.3.3. Channel Data Type and Channel Order Fields 5.4. Variables 5.4.1. Variable Declarations 5.4.2. Vectors 5.4.3. Array Declarations 5.4.4. Initializers 5.4.5. Alignment 5.4.6. Parameterized Variable Names 5.4.7. Variable Attributes 5.4.8. Variable and Function Attribute Directive: .attribute 5.5. Tensors 5.5.1. Tensor Dimension, size and format 5.5.2. Tensor Access Modes 5.5.3. Tiled Mode 5.5.3.1. Bounding Box 5.5.3.2. Traversal-Stride 5.5.3.3. Out of Boundary Access 5.5.4. Im2col mode 5.5.4.1. Bounding Box 5.5.4.2. Traversal Stride 5.5.4.3. Out of Boundary Access 5.5.5. Interleave layout 5.5.6. Swizzling Modes 5.5.7. Tensor-map 6. Instruction Operands 6.1. Operand Type Information 6.2. Source Operands 6.3. Destination Operands 6.4. Using Addresses, Arrays, and Vectors 6.4.1. Addresses as Operands 6.4.1.1. Generic Addressing 6.4.2. Arrays as Operands 6.4.3. Vectors as Operands 6.4.4. Labels and Function Names as Operands 6.5. Type Conversion 6.5.1. Scalar Conversions 6.5.2. Rounding Modifiers 6.6. Operand Costs 7. Abstracting the ABI 7.1. Function Declarations and Definitions 7.1.1. Changes from PTX ISA Version 1.x 7.2. Variadic Functions 7.3. Alloca 8. Memory Consistency Model 8.1. Scope and applicability of the model 8.1.1. Limitations on atomicity at system scope 8.2. Memory operations 8.2.1. Overlap 8.2.2. Aliases 8.2.3. Multimem Addresses 8.2.4. Memory Operations on Vector Data Types 8.2.5. Memory Operations on Packed Data Types 8.2.6. Initialization 8.3. State spaces 8.4. Operation types 8.4.1. mmio Operation 8.5. Scope 8.6. Proxies 8.7. Morally strong operations 8.7.1. Conflict and Data-races 8.7.2. Limitations on Mixed-size Data-races 8.8. Release and Acquire Patterns 8.9. Ordering of memory operations 8.9.1. Program Order 8.9.1.1. Asynchronous Operations 8.9.2. Observation Order 8.9.3. Fence-SC Order 8.9.4. Memory synchronization 8.9.5. Causality Order 8.9.6. Coherence Order 8.9.7. Communication Order 8.10. Axioms 8.10.1. Coherence 8.10.2. Fence-SC 8.10.3. Atomicity 8.10.4. No Thin Air 8.10.5. Sequential Consistency Per Location 8.10.6. Causality 9. Instruction Set 9.1. Format and Semantics of Instruction Descriptions 9.2. PTX Instructions 9.3. Predicated Execution 9.3.1. Comparisons 9.3.1.1. Integer and Bit-Size Comparisons 9.3.1.2. Floating Point Comparisons 9.3.2. Manipulating Predicates 9.4. Type Information for Instructions and Operands 9.4.1. Operand Size Exceeding Instruction-Type Size 9.5. Divergence of Threads in Control Constructs 9.6. Semantics 9.6.1. Machine-Specific Semantics of 16-bit Code 9.7. Instructions 9.7.1. Integer Arithmetic Instructions 9.7.1.1. Integer Arithmetic Instructions: add 9.7.1.2. Integer Arithmetic Instructions: sub 9.7.1.3. Integer Arithmetic Instructions: mul 9.7.1.4. Integer Arithmetic Instructions: mad 9.7.1.5. Integer Arithmetic Instructions: mul24 9.7.1.6. Integer Arithmetic Instructions: mad24 9.7.1.7. Integer Arithmetic Instructions: sad 9.7.1.8. Integer Arithmetic Instructions: div 9.7.1.9. Integer Arithmetic Instructions: rem 9.7.1.10. Integer Arithmetic Instructions: abs 9.7.1.11. Integer Arithmetic Instructions: neg 9.7.1.12. Integer Arithmetic Instructions: min 9.7.1.13. Integer Arithmetic Instructions: max 9.7.1.14. Integer Arithmetic Instructions: popc 9.7.1.15. Integer Arithmetic Instructions: clz 9.7.1.16. Integer Arithmetic Instructions: bfind 9.7.1.17. Integer Arithmetic Instructions: fns 9.7.1.18. Integer Arithmetic Instructions: brev 9.7.1.19. Integer Arithmetic Instructions: bfe 9.7.1.20. Integer Arithmetic Instructions: bfi 9.7.1.21. Integer Arithmetic Instructions: szext 9.7.1.22. Integer Arithmetic Instructions: bmsk 9.7.1.23. Integer Arithmetic Instructions: dp4a 9.7.1.24. Integer Arithmetic Instructions: dp2a 9.7.2. Extended-Precision Integer Arithmetic Instructions 9.7.2.1. Extended-Precision Arithmetic Instructions: add.cc 9.7.2.2. Extended-Precision Arithmetic Instructions: addc 9.7.2.3. Extended-Precision Arithmetic Instructions: sub.cc 9.7.2.4. Extended-Precision Arithmetic Instructions: subc 9.7.2.5. Extended-Precision Arithmetic Instructions: mad.cc 9.7.2.6. Extended-Precision Arithmetic Instructions: madc 9.7.3. Floating-Point Instructions 9.7.3.1. Floating Point Instructions: testp 9.7.3.2. Floating Point Instructions: copysign 9.7.3.3. Floating Point Instructions: add 9.7.3.4. Floating Point Instructions: sub 9.7.3.5. Floating Point Instructions: mul 9.7.3.6. Floating Point Instructions: fma 9.7.3.7. Floating Point Instructions: mad 9.7.3.8. Floating Point Instructions: div 9.7.3.9. Floating Point Instructions: abs 9.7.3.10. Floating Point Instructions: neg 9.7.3.11. Floating Point Instructions: min 9.7.3.12. Floating Point Instructions: max 9.7.3.13. Floating Point Instructions: rcp 9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64 9.7.3.15. Floating Point Instructions: sqrt 9.7.3.16. Floating Point Instructions: rsqrt 9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64 9.7.3.18. Floating Point Instructions: sin 9.7.3.19. Floating Point Instructions: cos 9.7.3.20. Floating Point Instructions: lg2 9.7.3.21. Floating Point Instructions: ex2 9.7.3.22. Floating Point Instructions: tanh 9.7.4. Half Precision Floating-Point Instructions 9.7.4.1. Half Precision Floating Point Instructions: add 9.7.4.2. Half Precision Floating Point Instructions: sub 9.7.4.3. Half Precision Floating Point Instructions: mul 9.7.4.4. Half Precision Floating Point Instructions: fma 9.7.4.5. Half Precision Floating Point Instructions: neg 9.7.4.6. Half Precision Floating Point Instructions: abs 9.7.4.7. Half Precision Floating Point Instructions: min 9.7.4.8. Half Precision Floating Point Instructions: max 9.7.4.9. Half Precision Floating Point Instructions: tanh 9.7.4.10. Half Precision Floating Point Instructions: ex2 9.7.5. Comparison and Selection Instructions 9.7.5.1. Comparison and Selection Instructions: set 9.7.5.2. Comparison and Selection Instructions: setp 9.7.5.3. Comparison and Selection Instructions: selp 9.7.5.4. Comparison and Selection Instructions: slct 9.7.6. Half Precision Comparison Instructions 9.7.6.1. Half Precision Comparison Instructions: set 9.7.6.2. Half Precision Comparison Instructions: setp 9.7.7. Logic and Shift Instructions 9.7.7.1. Logic and Shift Instructions: and 9.7.7.2. Logic and Shift Instructions: or 9.7.7.3. Logic and Shift Instructions: xor 9.7.7.4. Logic and Shift Instructions: not 9.7.7.5. Logic and Shift Instructions: cnot 9.7.7.6. Logic and Shift Instructions: lop3 9.7.7.7. Logic and Shift Instructions: shf 9.7.7.8. Logic and Shift Instructions: shl 9.7.7.9. Logic and Shift Instructions: shr 9.7.8. Data Movement and Conversion Instructions 9.7.8.1. Cache Operators 9.7.8.2. Cache Eviction Priority Hints 9.7.8.3. Data Movement and Conversion Instructions: mov 9.7.8.4. Data Movement and Conversion Instructions: mov 9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated) 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync 9.7.8.7. Data Movement and Conversion Instructions: prmt 9.7.8.8. Data Movement and Conversion Instructions: ld 9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc 9.7.8.10. Data Movement and Conversion Instructions: ldu 9.7.8.11. Data Movement and Conversion Instructions: st 9.7.8.12. Data Movement and Conversion Instructions: st.async 9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red 9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu 9.7.8.15. Data Movement and Conversion Instructions: applypriority 9.7.8.16. Data Movement and Conversion Instructions: discard 9.7.8.17. Data Movement and Conversion Instructions: createpolicy 9.7.8.18. Data Movement and Conversion Instructions: isspacep 9.7.8.19. Data Movement and Conversion Instructions: cvta 9.7.8.20. Data Movement and Conversion Instructions: cvt 9.7.8.21. Data Movement and Conversion Instructions: cvt.pack 9.7.8.22. Data Movement and Conversion Instructions: mapa 9.7.8.23. Data Movement and Conversion Instructions: getctarank 9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copy 9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operations 9.7.8.24.2. Async Proxy 9.7.8.24.3. Data Movement and Conversion Instructions: cp.async 9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_group 9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all 9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulk 9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk 9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch 9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor 9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor 9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor 9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group 9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group 9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace 9.7.9. Texture Instructions 9.7.9.1. Texturing Modes 9.7.9.2. Mipmaps 9.7.9.3. Texture Instructions: tex 9.7.9.4. Texture Instructions: tld4 9.7.9.5. Texture Instructions: txq 9.7.9.6. Texture Instructions: istypep 9.7.10. Surface Instructions 9.7.10.1. Surface Instructions: suld 9.7.10.2. Surface Instructions: sust 9.7.10.3. Surface Instructions: sured 9.7.10.4. Surface Instructions: suq 9.7.11. Control Flow Instructions 9.7.11.1. Control Flow Instructions: {} 9.7.11.2. Control Flow Instructions: @ 9.7.11.3. Control Flow Instructions: bra 9.7.11.4. Control Flow Instructions: brx.idx 9.7.11.5. Control Flow Instructions: call 9.7.11.6. Control Flow Instructions: ret 9.7.11.7. Control Flow Instructions: exit 9.7.12. Parallel Synchronization and Communication Instructions 9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrier 9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.sync 9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.cluster 9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fence 9.7.12.5. Parallel Synchronization and Communication Instructions: atom 9.7.12.6. Parallel Synchronization and Communication Instructions: red 9.7.12.7. Parallel Synchronization and Communication Instructions: red.async 9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated) 9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync 9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync 9.7.12.11. Parallel Synchronization and Communication Instructions: activemask 9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync 9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol 9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync 9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier 9.7.12.15.1. Size and alignment of mbarrier object 9.7.12.15.2. Contents of the mbarrier object 9.7.12.15.3. Lifecycle of the mbarrier object 9.7.12.15.4. Phase of the mbarrier object 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object 9.7.12.15.5.1. expect-tx operation 9.7.12.15.5.2. complete-tx operation 9.7.12.15.6. Phase Completion of the mbarrier object 9.7.12.15.7. Arrive-on operation on mbarrier object 9.7.12.15.8. mbarrier support with shared memory 9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init 9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval 9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx 9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx 9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive 9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop 9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive 9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait 9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count 9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy 9.7.13. Warp Level Matrix Multiply-Accumulate Instructions 9.7.13.1. Matrix Shape 9.7.13.2. Matrix Data-types 9.7.13.3. Matrix multiply-accumulate operation using wmma instructions 9.7.13.3.1. Matrix Fragments for WMMA 9.7.13.3.2. Matrix Storage for WMMA 9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load 9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store 9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma 9.7.13.4. Matrix multiply-accumulate operation using mma instruction 9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type 9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point type 9.7.13.4.3. Matrix Fragments for mma.m8n8k16 9.7.13.4.4. Matrix Fragments for mma.m8n8k32 9.7.13.4.5. Matrix Fragments for mma.m8n8k128 9.7.13.4.6. Matrix Fragments for mma.m16n8k4 9.7.13.4.7. Matrix Fragments for mma.m16n8k8 9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type 9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type 9.7.13.4.10. Matrix Fragments for mma.m16n8k32 9.7.13.4.11. Matrix Fragments for mma.m16n8k64 9.7.13.4.12. Matrix Fragments for mma.m16n8k128 9.7.13.4.13. Matrix Fragments for mma.m16n8k256 9.7.13.4.14. Multiply-and-Accumulate Instruction: mma 9.7.13.4.15. Warp-level matrix load instruction: ldmatrix 9.7.13.4.16. Warp-level matrix store instruction: stmatrix 9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix 9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A 9.7.13.5.1. Sparse matrix storage 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types 9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types 9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type 9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type 9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type 9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 type 9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type 9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer type 9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata 9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions 9.7.14.1. Warpgroup 9.7.14.2. Matrix Shape 9.7.14.3. Matrix Data-types 9.7.14.4. Async Proxy 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction 9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts 9.7.14.5.1.1. Register Fragments 9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16 9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8 9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32 9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256 9.7.14.5.1.2. Shared Memory Matrix Layout 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16 9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8 9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32 9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256 9.7.14.5.1.2.5. Strides 9.7.14.5.1.2.6. Swizzling Modes 9.7.14.5.1.2.7. Matrix Descriptor Format 9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async 9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction 9.7.14.6.1. Sparse matrix storage 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32 9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16 9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64 9.7.14.6.3. Shared Memory Matrix Layout 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32 9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16 9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64 9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp 9.7.14.7. Asynchronous wgmma Proxy Operations 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence 9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group 9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group 9.7.15. Stack Manipulation Instructions 9.7.15.1. Stack Manipulation Instructions: stacksave 9.7.15.2. Stack Manipulation Instructions: stackrestore 9.7.15.3. Stack Manipulation Instructions: alloca 9.7.16. Video Instructions 9.7.16.1. Scalar Video Instructions 9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax 9.7.16.1.2. Scalar Video Instructions: vshl, vshr 9.7.16.1.3. Scalar Video Instructions: vmad 9.7.16.1.4. Scalar Video Instructions: vset 9.7.16.2. SIMD Video Instructions 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 9.7.16.2.2. SIMD Video Instructions: vset2 9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 9.7.16.2.4. SIMD Video Instructions: vset4 9.7.17. Miscellaneous Instructions 9.7.17.1. Miscellaneous Instructions: brkpt 9.7.17.2. Miscellaneous Instructions: nanosleep 9.7.17.3. Miscellaneous Instructions: pmevent 9.7.17.4. Miscellaneous Instructions: trap 9.7.17.5. Miscellaneous Instructions: setmaxnreg 10. Special Registers 10.1. Special Registers: %tid 10.2. Special Registers: %ntid 10.3. Special Registers: %laneid 10.4. Special Registers: %warpid 10.5. Special Registers: %nwarpid 10.6. Special Registers: %ctaid 10.7. Special Registers: %nctaid 10.8. Special Registers: %smid 10.9. Special Registers: %nsmid 10.10. Special Registers: %gridid 10.11. Special Registers: %is_explicit_cluster 10.12. Special Registers: %clusterid 10.13. Special Registers: %nclusterid 10.14. Special Registers: %cluster_ctaid 10.15. Special Registers: %cluster_nctaid 10.16. Special Registers: %cluster_ctarank 10.17. Special Registers: %cluster_nctarank 10.18. Special Registers: %lanemask_eq 10.19. Special Registers: %lanemask_le 10.20. Special Registers: %lanemask_lt 10.21. Special Registers: %lanemask_ge 10.22. Special Registers: %lanemask_gt 10.23. Special Registers: %clock, %clock_hi 10.24. Special Registers: %clock64 10.25. Special Registers: %pm0..%pm7 10.26. Special Registers: %pm0_64..%pm7_64 10.27. Special Registers: %envreg<32> 10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi 10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2> 10.30. Special Registers: %total_smem_size 10.31. Special Registers: %aggr_smem_size 10.32. Special Registers: %dynamic_smem_size 10.33. Special Registers: %current_graph_exec 11. Directives 11.1. PTX Module Directives 11.1.1. PTX Module Directives: .version 11.1.2. PTX Module Directives: .target 11.1.3. PTX Module Directives: .address_size 11.2. Specifying Kernel Entry Points and Functions 11.2.1. Kernel and Function Directives: .entry 11.2.2. Kernel and Function Directives: .func 11.2.3. Kernel and Function Directives: .alias 11.3. Control Flow Directives 11.3.1. Control Flow Directives: .branchtargets 11.3.2. Control Flow Directives: .calltargets 11.3.3. Control Flow Directives: .callprototype 11.4. Performance-Tuning Directives 11.4.1. Performance-Tuning Directives: .maxnreg 11.4.2. Performance-Tuning Directives: .maxntid 11.4.3. Performance-Tuning Directives: .reqntid 11.4.4. Performance-Tuning Directives: .minnctapersm 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated) 11.4.6. Performance-Tuning Directives: .noreturn 11.4.7. Performance-Tuning Directives: .pragma 11.5. Debugging Directives 11.5.1. Debugging Directives: @@dwarf 11.5.2. Debugging Directives: .section 11.5.3. Debugging Directives: .file 11.5.4. Debugging Directives: .loc 11.6. Linking Directives 11.6.1. Linking Directives: .extern 11.6.2. Linking Directives: .visible 11.6.3. Linking Directives: .weak 11.6.4. Linking Directives: .common 11.7. Cluster Dimension Directives 11.7.1. Cluster Dimension Directives: .reqnctapercluster 11.7.2. Cluster Dimension Directives: .explicitcluster 11.7.3. Cluster Dimension Directives: .maxclusterrank 12. Release Notes 12.1. Changes in PTX ISA Version 8.5 12.2. Changes in PTX ISA Version 8.4 12.3. Changes in PTX ISA Version 8.3 12.4. Changes in PTX ISA Version 8.2 12.5. Changes in PTX ISA Version 8.1 12.6. Changes in PTX ISA Version 8.0 12.7. Changes in PTX ISA Version 7.8 12.8. Changes in PTX ISA Version 7.7 12.9. Changes in PTX ISA Version 7.6 12.10. Changes in PTX ISA Version 7.5 12.11. Changes in PTX ISA Version 7.4 12.12. Changes in PTX ISA Version 7.3 12.13. Changes in PTX ISA Version 7.2 12.14. Changes in PTX ISA Version 7.1 12.15. Changes in PTX ISA Version 7.0 12.16. Changes in PTX ISA Version 6.5 12.17. Changes in PTX ISA Version 6.4 12.18. Changes in PTX ISA Version 6.3 12.19. Changes in PTX ISA Version 6.2 12.20. Changes in PTX ISA Version 6.1 12.21. Changes in PTX ISA Version 6.0 12.22. Changes in PTX ISA Version 5.0 12.23. Changes in PTX ISA Version 4.3 12.24. Changes in PTX ISA Version 4.2 12.25. Changes in PTX ISA Version 4.1 12.26. Changes in PTX ISA Version 4.0 12.27. Changes in PTX ISA Version 3.2 12.28. Changes in PTX ISA Version 3.1 12.29. Changes in PTX ISA Version 3.0 12.30. Changes in PTX ISA Version 2.3 12.31. Changes in PTX ISA Version 2.2 12.32. Changes in PTX ISA Version 2.1 12.33. Changes in PTX ISA Version 2.0 14. Descriptions of .pragma Strings 14.1. Pragma Strings: “nounroll” 14.2. Pragma Strings: “used_bytes_mask” 15. Notices 15.1. Notice 15.2. OpenCL 15.3. Trademarks PTX ISA » 1. Introduction v8.5 | PDF | Archive Parallel Thread Execution ISA Version 8.5 The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture). 1. Introduction  This document describes PTX, a low-level parallel thread execution virtual machine and instruction\nset architecture (ISA). PTX exposes the GPU as a data-parallel computing device . 1.1. Scalable Data-Parallel Computing using GPUs  Driven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable\nGPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous\ncomputational horsepower and very high memory bandwidth. The GPU is especially well-suited to\naddress problems that can be expressed as data-parallel computations - the same program is executed\non many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic\noperations to memory operations. Because the same program is executed for each data element, there\nis a lower requirement for sophisticated flow control; and because it is executed on many data\nelements and has high arithmetic intensity, the memory access latency can be hidden with\ncalculations instead of big data caches. Data-parallel processing maps data elements to parallel processing threads. Many applications that\nprocess large data sets can use a data-parallel programming model to speed up the computations. In\n3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and\nmedia processing applications such as post-processing of rendered images, video encoding and\ndecoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to\nparallel processing threads. In fact, many algorithms outside the field of image rendering and\nprocessing are accelerated by data-parallel processing, from general signal processing or physics\nsimulation to computational finance or computational biology. PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs\nare translated at install time to the target hardware instruction set. The PTX-to-GPU translator\nand driver enable NVIDIA GPUs to be used as programmable parallel computers. 1.2. Goals of PTX  PTX provides a stable programming model and instruction set for general purpose parallel\nprogramming. It is designed to be efficient on NVIDIA GPUs supporting the computation features\ndefined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA\nand C/C++ generate PTX instructions, which are optimized for and translated to native\ntarget-architecture instructions. The goals for PTX include the following: Provide a stable ISA that spans multiple GPU generations. Achieve performance in compiled applications comparable to native GPU performance. Provide a machine-independent ISA for C/C++ and other compilers to target. Provide a code distribution ISA for application and middleware developers. Provide a common source-level ISA for optimizing code generators and translators, which map PTX to\nspecific target machines. Facilitate hand-coding of libraries, performance kernels, and architecture tests. Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units. 1.3. PTX ISA Version 8.5  PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction. 1.4. Document Structure  The information in this document is organized into the following Chapters: Programming Model outlines the programming model. PTX Machine Model gives an overview of the PTX virtual machine model. Syntax describes the basic syntax of the PTX language. State Spaces, Types, and Variables describes\nstate spaces, types, and variable declarations. Instruction Operands describes instruction operands. Abstracting the ABI describes the function and call syntax,\ncalling convention, and PTX support for abstracting the Application Binary Interface (ABI) . Instruction Set describes the instruction set. Special Registers lists special registers. Directives lists the assembly directives supported in PTX. Release Notes provides release notes for PTX ISA versions 2.x and\nbeyond. References 754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008. http://ieeexplore.ieee.org/servlet/opac?punumber=4610933 The OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011. http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf CUDA Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA Dynamic Parallelism Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism CUDA Atomicity Requirements. https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity PTX Writers Guide to Interoperability. https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html 2. Programming Model  2.1. A Highly Multithreaded Coprocessor  The GPU is a compute device capable of executing a very large number of threads in parallel. It\noperates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive\nportions of applications running on the host are off-loaded onto the device. More precisely, a portion of an application that is executed many times, but independently on\ndifferent data, can be isolated into a kernel function that is executed on the GPU as many different\nthreads. To that effect, such a function is compiled to the PTX instruction set and the resulting\nkernel is translated at install time to the target GPU instruction set. 2.2. Thread Hierarchy  The batch of threads that executes a kernel is organized as a grid. A grid consists of either\ncooperative thread arrays or clusters of cooperative thread arrays as described in this section and\nillustrated in Figure 1 and Figure 2 . Cooperative thread arrays (CTAs) implement CUDA\nthread blocks and clusters implement CUDA thread block clusters. 2.2.1. Cooperative Thread Arrays  The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program\nspecifies the execution of a given thread of a parallel thread array. A cooperative thread array ,\nor CTA, is an array of threads that execute a kernel concurrently or in parallel. Threads within a CTA can communicate with each other. To coordinate the communication of the threads\nwithin the CTA, one can specify synchronization points where threads wait until all threads in the\nCTA have arrived. Each thread has a unique thread identifier within the CTA. Programs use a data parallel\ndecomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread\nuses its thread identifier to determine its assigned role, assign specific input and output\npositions, compute addresses, and select work to perform. The thread identifier is a three-element\nvector tid , (with elements tid.x , tid.y , and tid.z ) that specifies the thread’s\nposition within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the\nnumber of thread ids in that CTA dimension. Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements ntid.x , ntid.y , and ntid.z ). The vector ntid specifies the number of threads in each\nCTA dimension. Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps . A warp is a maximal subset of threads from a single CTA, such that the threads execute\nthe same instructions at the same time. Threads within a warp are sequentially numbered. The warp\nsize is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be\nable to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate\nconstant, WARP_SZ , which may be used in any instruction where an immediate operand is allowed. 2.2.2. Cluster of Cooperative Thread Arrays  Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate\nwith each other via shared memory. The executing CTA has to make sure that the shared memory of the\npeer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before\ncompleting the shared memory operation. Threads within the different CTAs in a cluster can synchronize and communicate with each other via\nshared memory. Cluster-wide barriers can be used to synchronize all the threads within the\ncluster. Each CTA in a cluster has a unique CTA identifier within its cluster\n( cluster_ctaid ). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter cluster_nctaid . Each CTA in the cluster also has a unique CTA identifier ( cluster_ctarank )\nacross all dimensions. The total number of CTAs across all the dimensions in the cluster is\nspecified by cluster_nctarank . Threads may read and use these values through predefined, read-only\nspecial registers %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank . Cluster level is applicable only on target architecture sm_90 or higher. Specifying cluster\nlevel during launch time is optional. If the user specifies the cluster dimensions at launch time\nthen it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster\nlaunch with default dimension 1x1x1. PTX provides read-only special register %is_explicit_cluster to differentiate between explicit and implicit cluster launch. 2.2.3. Grid of Clusters  There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a\ncluster can contain. However, clusters with CTAs that execute the same kernel can be batched\ntogether into a grid of clusters, so that the total number of threads that can be launched in a\nsingle kernel invocation is very large. This comes at the expense of reduced thread communication\nand synchronization, because threads in different clusters cannot communicate and synchronize with\neach other. Each cluster has a unique cluster identifier ( clusterid ) within a grid of clusters. Each grid of\nclusters has a 1D, 2D , or 3D shape specified by the parameter nclusterid . Each grid also has a\nunique temporal grid identifier ( gridid ). Threads may read and use these values through\npredefined, read-only special registers %tid , %ntid , %clusterid , %nclusterid , and %gridid . Each CTA has a unique identifier ( ctaid ) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape\nspecified by the parameter nctaid . Thread may use and read these values through predefined,\nread-only special registers %ctaid and %nctaid . Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs\nwhere cluster is optional level and is applicable only for target architectures sm_90 and\nhigher. Figure 1 shows a grid consisting of CTAs and Figure 2 shows a grid consisting of clusters. Grids may be launched with dependencies between one another - a grid may be a dependent grid and/or\na prerequisite grid. To understand how grid dependencies may be defined, refer to the section on CUDA Graphs in the Cuda Programming Guide . Figure 1 Grid with CTAs  Figure 2 Grid with clusters  A cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads\nthat execute the same kernel program. A grid is a set of clusters consisting of CTAs that\nexecute independently. 2.3. Memory Hierarchy  PTX threads may access data from multiple state spaces during their execution as illustrated by Figure 3 where cluster level is introduced from\ntarget architecture sm_90 onwards. Each thread has a private local memory. Each thread block\n(CTA) has a shared memory visible to all threads of the block and to all active blocks in the\ncluster and with the same lifetime as the block. Finally, all threads have access to the same global\nmemory. There are additional state spaces accessible by all threads: the constant, param, texture, and\nsurface state spaces.  Constant and texture memory are read-only; surface memory is readable and\nwritable. The global, constant, param, texture, and surface state spaces are optimized for different\nmemory usages. For example, texture memory offers different addressing modes as well as data\nfiltering for specific data formats. Note that texture and surface memory is cached, and within the\nsame kernel call, the cache is not kept coherent with respect to global memory writes and surface\nmemory writes, so any texture fetch or surface read to an address that has been written to via a\nglobal or a surface write in the same kernel call returns undefined data. In other words, a thread\ncan safely read some texture or surface memory location only if this memory location has been\nupdated by a previous kernel call or memory copy, but not if it has been previously updated by the\nsame thread or another thread from the same kernel call. The global, constant, and texture state spaces are persistent across kernel launches by the same\napplication. Both the host and the device maintain their own local memory, referred to as host memory and device memory , respectively. The device memory may be mapped and read or written by the host, or,\nfor more efficient transfer, copied from the host memory through optimized API calls that utilize\nthe device’s high-performance Direct Memory Access (DMA) engine. Figure 3 Memory Hierarchy  3. PTX Machine Model  3.1. A Set of SIMT Multiprocessors  The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming\nMultiprocessors (SMs) . When a host program invokes a kernel grid, the blocks of the grid are\nenumerated and distributed to multiprocessors with available execution capacity. The threads of a\nthread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are\nlaunched on the vacated multiprocessors. A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction\nunit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent\nthreads in hardware with zero scheduling overhead. It implements a single-instruction barrier\nsynchronization. Fast barrier synchronization together with lightweight thread creation and\nzero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for\nexample, a low granularity decomposition of problems by assigning one thread to each data element\n(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation). To manage hundreds of threads running several different programs, the multiprocessor employs an\narchitecture we call SIMT (single-instruction, multiple-thread) . The multiprocessor maps each\nthread to one scalar processor core, and each scalar thread executes independently with its own\ninstruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,\nand executes threads in groups of parallel threads called warps . (This term originates from\nweaving, the first parallel thread technology.) Individual threads composing a SIMT warp start\ntogether at the same program address but are otherwise free to branch and execute independently. When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that\nget scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp\ncontains threads of consecutive, increasing thread IDs with the first warp containing thread 0. At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues\nthe next instruction to the active threads of the warp. A warp executes one common instruction at a\ntime, so full efficiency is realized when all threads of a warp agree on their execution path. If\nthreads of a warp diverge via a data-dependent conditional branch, the warp serially executes each\nbranch path taken, disabling threads that are not on that path, and when all paths complete, the\nthreads converge back to the same execution path. Branch divergence occurs only within a warp;\ndifferent warps execute independently regardless of whether they are executing common or disjointed\ncode paths. SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a\nsingle instruction controls multiple processing elements. A key difference is that SIMD vector\norganizations expose the SIMD width to the software, whereas SIMT instructions specify the execution\nand branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables\nprogrammers to write thread-level parallel code for independent, scalar threads, as well as\ndata-parallel code for coordinated threads. For the purposes of correctness, the programmer can\nessentially ignore the SIMT behavior; however, substantial performance improvements can be realized\nby taking care that the code seldom requires threads in a warp to diverge. In practice, this is\nanalogous to the role of cache lines in traditional code: Cache line size can be safely ignored when\ndesigning for correctness but must be considered in the code structure when designing for peak\nperformance. Vector architectures, on the other hand, require the software to coalesce loads into\nvectors and manage divergence manually. How many blocks a multiprocessor can process at once depends on how many registers per thread and\nhow much shared memory per block are required for a given kernel since the multiprocessor’s\nregisters and shared memory are split among all the threads of the batch of blocks. If there are not\nenough registers or shared memory available per multiprocessor to process at least one block, the\nkernel will fail to launch. Figure 4 Hardware Model  A set of SIMT multiprocessors with on-chip shared memory. 3.2. Independent Thread Scheduling  On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads\nin the warp together with an active mask specifying the active threads of the warp. As a result,\nthreads from the same warp in divergent regions or different states of execution cannot signal each\nother or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or\nmutexes can easily lead to deadlock, depending on which warp the contending threads come from. Starting with the Volta architecture, Independent Thread Scheduling allows full concurrency\nbetween threads, regardless of warp. With Independent Thread Scheduling , the GPU maintains\nexecution state per thread, including a program counter and call stack, and can yield execution at a\nper-thread granularity, either to make better use of execution resources or to allow one thread to\nwait for data to be produced by another. A schedule optimizer determines how to group active threads\nfrom the same warp together into SIMT units. This retains the high throughput of SIMT execution as\nin prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at\nsub-warp granularity. Independent Thread Scheduling can lead to a rather different set of threads participating in the\nexecuted code than intended if the developer made assumptions about warp-synchronicity of previous\nhardware architectures. In particular, any warp-synchronous code (such as synchronization-free,\nintra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the\nsection on Compute Capability 7.x in the Cuda Programming Guide for further details. 3.3. On-chip Shared Memory  As illustrated by Figure 4 , each multiprocessor has\non-chip memory of the four following types: One set of local 32-bit registers per processor, A parallel data cache or shared memory that is shared by all scalar processor cores and is where\nthe shared memory space resides, A read-only constant cache that is shared by all scalar processor cores and speeds up reads from\nthe constant memory space, which is a read-only region of device memory, A read-only texture cache that is shared by all scalar processor cores and speeds up reads from\nthe texture memory space, which is a read-only region of device memory; each multiprocessor\naccesses the texture cache via a texture unit that implements the various addressing modes and\ndata filtering. The local and global memory spaces are read-write regions of device memory. 4. Syntax  PTX programs are a collection of text source modules (files). PTX source modules have an\nassembly-language style syntax with instruction operation codes and operands. Pseudo-operations\nspecify symbol and addressing management. The ptxas optimizing backend compiler optimizes and\nassembles PTX source modules to produce corresponding binary object files. 4.1. Source Format  Source modules are ASCII text. Lines are separated by the newline character ( \\n ). All whitespace characters are equivalent; whitespace is ignored except for its use in separating\ntokens in the language. The C preprocessor cpp may be used to process PTX source modules. Lines beginning with # are\npreprocessor directives. The following are common preprocessor directives: #include , #define , #if , #ifdef , #else , #endif , #line , #file C: A Reference Manual by Harbison and Steele provides a good description of the C preprocessor. PTX is case sensitive and uses lowercase for keywords. Each PTX module must begin with a .version directive specifying the PTX language version,\nfollowed by a .target directive specifying the target architecture assumed. See PTX Module\nDirectives for a more information on these directives. 4.2. Comments  Comments in PTX follow C/C++ syntax, using non-nested /* and */ for comments that may span\nmultiple lines, and using // to begin a comment that extends up to the next newline character,\nwhich terminates the current line. Comments cannot occur within character constants, string\nliterals, or within other comments. Comments in PTX are treated as whitespace. 4.3. Statements  A PTX statement is either a directive or an instruction. Statements begin with an optional label and\nend with a semicolon. Examples .reg     .b32 r1, r2;\n        .global  .f32  array[N];\n\nstart:  mov.b32   r1, %tid.x;\n        shl.b32   r1, r1, 2;          // shift thread id by 2 bits\n        ld.global.b32 r2, array[r1];  // thread[tid] gets array[tid]\n        add.f32   r2, r2, 0.5;        // add 1/2 4.3.1. Directive Statements  Directive keywords begin with a dot, so no conflict is possible with user-defined identifiers. The\ndirectives in PTX are listed in Table 1 and\ndescribed in State Spaces, Types, and Variables and Directives . Table 1 PTX Directives  .address_size .explicitcluster .maxnreg .section .alias .extern .maxntid .shared .align .file .minnctapersm .sreg .branchtargets .func .noreturn .target .callprototype .global .param .tex .calltargets .loc .pragma .version .common .local .reg .visible .const .maxclusterrank .reqnctapercluster .weak .entry .maxnctapersm .reqntid 4.3.2. Instruction Statements  Instructions are formed from an instruction opcode followed by a comma-separated list of zero or\nmore operands, and terminated with a semicolon. Operands may be register variables, constant\nexpressions, address expressions, or label names. Instructions have an optional guard predicate\nwhich controls conditional execution. The guard predicate follows the optional label and precedes\nthe opcode, and is written as @p , where p is a predicate register. The guard predicate may\nbe optionally negated, written as @!p . The destination operand is first, followed by source operands. Instruction keywords are listed in Table 2 . All instruction keywords are\nreserved tokens in PTX. Table 2 Reserved Instruction Keywords  abs discard min shf vadd activemask div mma shfl vadd2 add dp2a mov shl vadd4 addc dp4a movmatrix shr vavrg2 alloca elect mul sin vavrg4 and ex2 mul24 slct vmad applypriority exit multimem sqrt vmax atom fence nanosleep st vmax2 bar fma neg stackrestore vmax4 barrier fns not stacksave vmin bfe getctarank or stmatrix vmin2 bfi griddepcontrol pmevent sub vmin4 bfind isspacep popc subc vote bmsk istypep prefetch suld vset bra ld prefetchu suq vset2 brev ldmatrix prmt sured vset4 brkpt ldu rcp sust vshl brx lg2 red szext vshr call lop3 redux tanh vsub clz mad rem testp vsub2 cnot mad24 ret tex vsub4 copysign madc rsqrt tld4 wgmma cos mapa sad trap wmma cp match selp txq xor createpolicy max set vabsdiff cvt mbarrier setmaxnreg vabsdiff2 cvta membar setp vabsdiff4 4.4. Identifiers  User-defined identifiers follow extended C++ rules: they either start with a letter followed by zero\nor more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,\nor percentage character followed by one or more letters, digits, underscore, or dollar characters: followsym:   [a-zA-Z0-9_$]\nidentifier:  [a-zA-Z]{followsym}* | {[_$%]{followsym}+ PTX does not specify a maximum length for identifiers and suggests that all implementations support\na minimum length of at least 1024 characters. Many high-level languages such as C and C++ follow similar rules for identifier names, except that\nthe percentage sign is not allowed. PTX allows the percentage sign as the first character of an\nidentifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined\nvariable names and compiler-generated names. PTX predefines one constant and a small number of special registers that begin with the percentage\nsign, listed in Table 3 . Table 3 Predefined Identifiers  %clock %laneid %lanemask_gt %pm0, ..., %pm7 %clock64 %lanemask_eq %nctaid %smid %ctaid %lanemask_le %ntid %tid %envreg<32> %lanemask_lt %nsmid %warpid %gridid %lanemask_ge %nwarpid WARP_SZ 4.5. Constants  PTX supports integer and floating-point constants and constant expressions. These constants may be\nused in data initialization and as operands to instructions. Type checking rules remain the same for\ninteger, floating-point, and bit-size types. For predicate-type data and instructions, integer\nconstants are allowed and are interpreted as in C, i.e., zero values are False and non-zero\nvalues are True . 4.5.1. Integer Constants  Integer constants are 64-bits in size and are either signed or unsigned, i.e., every integer\nconstant has type .s64 or .u64 . The signed/unsigned nature of an integer constant is needed\nto correctly evaluate constant expressions containing operations such as division and ordered\ncomparisons, where the behavior of the operation depends on the operand types. When used in an\ninstruction or data initialization, each integer constant is converted to the appropriate size based\non the data or instruction type at its use. Integer literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax\nfollows that of C. Integer literals may be followed immediately by the letter U to indicate that\nthe literal is unsigned. hexadecimal literal:  0[xX]{hexdigit}+U?\noctal literal:        0{octal digit}+U?\nbinary literal:       0[bB]{bit}+U?\ndecimal literal       {nonzero-digit}{digit}*U? Integer literals are non-negative and have a type determined by their magnitude and optional type\nsuffix as follows: literals are signed ( .s64 ) unless the value cannot be fully represented in .s64 or the unsigned suffix is specified, in which case the literal is unsigned ( .u64 ). The predefined integer constant WARP_SZ specifies the number of threads per warp for the target\nplatform; to date, all target architectures have a WARP_SZ value of 32. 4.5.2. Floating-Point Constants  Floating-point constants are represented as 64-bit double-precision values, and all floating-point\nconstant expressions are evaluated using 64-bit double precision arithmetic. The only exception is\nthe 32-bit hex notation for expressing an exact single-precision floating-point value; such values\nretain their exact 32-bit single-precision value and may not be used in constant expressions. Each\n64-bit floating-point constant is converted to the appropriate floating-point size based on the data\nor instruction type at its use. Floating-point literals may be written with an optional decimal point and an optional signed\nexponent. Unlike C and C++, there is no suffix letter to specify size; literals are always\nrepresented in 64-bit double-precision format. PTX includes a second representation of floating-point constants for specifying the exact machine\nrepresentation using a hexadecimal constant. To specify IEEE 754 double-precision floating point\nvalues, the constant begins with 0d or 0D followed by 16 hex digits. To specify IEEE 754\nsingle-precision floating point values, the constant begins with 0f or 0F followed by 8 hex\ndigits. 0[fF]{hexdigit}{8}      // single-precision floating point\n0[dD]{hexdigit}{16}     // double-precision floating point Example mov.f32  $f3, 0F3f800000;       //  1.0 4.5.3. Predicate Constants  In PTX, integer constants may be used as predicates. For predicate-type data initializers and\ninstruction operands, integer constants are interpreted as in C, i.e., zero values are False and\nnon-zero values are True . 4.5.4. Constant Expressions  In PTX, constant expressions are formed using operators as in C and are evaluated using rules\nsimilar to those in C, but simplified by restricting types and sizes, removing most casts, and\ndefining full semantics to eliminate cases where expression evaluation in C is implementation\ndependent. Constant expressions are formed from constant literals, unary plus and minus, basic arithmetic\noperators (addition, subtraction, multiplication, division), comparison operators, the conditional\nternary operator ( ?: ), and parentheses. Integer constant expressions also allow unary logical\nnegation ( ! ), bitwise complement ( ~ ), remainder ( % ), shift operators ( << and >> ), bit-type operators ( & , | , and ^ ), and logical operators ( && , || ). Constant expressions in PTX do not support casts between integer and floating-point. Constant expressions are evaluated using the same operator precedence as\nin C. Table 4 gives operator precedence and\nassociativity. Operator precedence is highest for unary operators and decreases with each line in\nthe chart. Operators on the same line have the same precedence and are evaluated right-to-left for\nunary operators and left-to-right for binary operators. Table 4 Operator Precedence  Kind Operator Symbols Operator Names Associates Primary () parenthesis n/a Unary +- ! ~ plus, minus, negation, complement right (.s64) (.u64) casts right Binary */ % multiplication, division, remainder left +- addition, subtraction >> << shifts < > <= >= ordered comparisons == != equal, not equal & bitwise AND ^ bitwise XOR | bitwise OR && logical AND || logical OR Ternary ?: conditional right 4.5.5. Integer Constant Expression Evaluation  Integer constant expressions are evaluated at compile time according to a set of rules that\ndetermine the type (signed .s64 versus unsigned .u64 ) of each sub-expression. These rules\nare based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and\nbehavior is fully defined in all cases (specifically, for remainder and shift operators). Literals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a U suffix. For example: 42 , 0x1234 , 0123 are signed. 0xfabc123400000000 , 42U , 0x1234U are unsigned. Unary plus and minus preserve the type of the input operand. For example: +123 , -1 , -(-42) are signed. -1U , -0xfabc123400000000 are unsigned. Unary logical negation ( ! ) produces a signed result with value 0 or 1 . Unary bitwise complement ( ~ ) interprets the source operand as unsigned and produces an\nunsigned result. Some binary operators require normalization of source operands. This normalization is known as the usual arithmetic conversions and simply converts both operands to unsigned type if either\noperand is unsigned. Addition, subtraction, multiplication, and division perform the usual arithmetic conversions and\nproduce a result with the same type as the converted operands. That is, the operands and result\nare unsigned if either source operand is unsigned, and is otherwise signed. Remainder ( % ) interprets the operands as unsigned. Note that this differs from C, which allows\na negative divisor but defines the behavior to be implementation dependent. Left and right shift interpret the second operand as unsigned and produce a result with the same\ntype as the first operand. Note that the behavior of right-shift is determined by the type of the\nfirst operand: right shift of a signed value is arithmetic and preserves the sign, and right shift\nof an unsigned value is logical and shifts in a zero bit. AND ( & ), OR ( | ), and XOR ( ^ ) perform the usual arithmetic conversions and produce a\nresult with the same type as the converted operands. AND_OP ( && ), OR_OP ( || ), Equal ( == ), and Not_Equal ( != ) produce a signed\nresult. The result value is 0 or 1. Ordered comparisons ( < , <= , > , >= ) perform the usual arithmetic conversions on\nsource operands and produce a signed result. The result value is 0 or 1 . Casting of expressions to signed or unsigned is supported using ( .s64 ) and ( .u64 ) casts. For the conditional operator ( ? : ) , the first operand must be an integer, and the second\nand third operands are either both integers or both floating-point. The usual arithmetic\nconversions are performed on the second and third operands, and the result type is the same as the\nconverted type. 4.5.6. Summary of Constant Expression Evaluation Rules  Table 5 contains a summary of the constant expression evaluation rules. Table 5 Constant Expression Evaluation Rules  Kind Operator Operand Types Operand Interpretation Result Type Primary () any type same as source same as source constant literal n/a n/a .u64 , .s64 , or .f64 Unary +- any type same as source same as source ! integer zero or non-zero .s64 ~ integer .u64 .u64 Cast (.u64) integer .u64 .u64 (.s64) integer .s64 .s64 Binary +- * / .f64 .f64 .f64 integer use usual conversions converted type < > <= >= .f64 .f64 .s64 integer use usual conversions .s64 == != .f64 .f64 .s64 integer use usual conversions .s64 % integer .u64 .s64 >> << integer 1st unchanged, 2nd is .u64 same as 1st operand & | ^ integer .u64 .u64 && || integer zero or non-zero .s64 Ternary ?: int ? .f64 : .f64 same as sources .f64 int ? int : int use usual conversions converted type 5. State Spaces, Types, and Variables  While the specific resources available in a given target GPU will vary, the kinds of resources will\nbe common across platforms, and these resources are abstracted in PTX through state spaces and data\ntypes. 5.1. State Spaces  A state space is a storage area with particular characteristics. All variables reside in some state\nspace. The characteristics of a state space include its size, addressability, access speed, access\nrights, and level of sharing between threads. The state spaces defined in PTX are a byproduct of parallel programming and graphics\nprogramming. The list of state spaces is shown in Table 6 ,and\nproperties of state spaces are shown in Table 7 . Table 6 State Spaces  Name Description .reg Registers, fast. .sreg Special registers. Read-only; pre-defined; platform-specific. .const Shared, read-only memory. .global Global memory, shared by all threads. .local Local memory, private to each thread. .param Kernel parameters, defined per-grid; or Function or local parameters, defined per-thread. .shared Addressable memory, defined per CTA, accessible to all threads in the cluster\nthroughout the lifetime of the CTA that defines it. .tex Global texture memory (deprecated). Table 7 Properties of State Spaces  Name Addressable Initializable Access Sharing .reg No No R/W per-thread .sreg No No RO per-CTA .const Yes Yes 1 RO per-grid .global Yes Yes 1 R/W Context .local Yes No R/W per-thread .param (as input to kernel) Yes 2 No RO per-grid .param (used in functions) Restricted 3 No R/W per-thread .shared Yes No R/W per-cluster 5 .tex No 4 Yes, via driver RO Context Notes: 1 Variables in .const and .global state spaces are initialized to zero by default. 2 Accessible only via the ld.param{::entry} instruction. Address may be taken via mov instruction. 3 Accessible via ld.param{::func} and st.param{::func} instructions. Device function\ninput and return parameters may have their address taken via mov ; the parameter is then located\non the stack frame and its address is in the .local state space. 4 Accessible only via the tex instruction. 5 Visible to the owning CTA and other active CTAs in the cluster. 5.1.1. Register State Space  Registers ( .reg state space) are fast storage locations. The number of registers is limited, and\nwill vary from platform to platform. When the limit is exceeded, register variables will be spilled\nto memory, causing changes in performance. For each architecture, there is a recommended maximum\nnumber of registers to use (see the CUDA Programming Guide for details). Registers may be typed (signed integer, unsigned integer, floating point, predicate) or\nuntyped. Register size is restricted; aside from predicate registers which are 1-bit, scalar\nregisters have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of\n16-, 32-, 64-, or 128-bits. The most common use of 8-bit registers is with ld , st , and cvt instructions, or as elements of vector tuples. Registers differ from the other state spaces in that they are not fully addressable, i.e., it is not\npossible to refer to the address of a register. When compiling to use the Application Binary\nInterface (ABI), register variables are restricted to function scope and may not be declared at\nmodule scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg variables, the compiler silently disables use of the ABI. Registers may have alignment\nboundaries required by multi-word loads and stores. 5.1.2. Special Register State Space  The special register ( .sreg ) state space holds predefined, platform-specific registers, such as\ngrid, cluster, CTA, and thread parameters, clock counters, and performance monitoring registers. All\nspecial registers are predefined. 5.1.3. Constant State Space  The constant ( .const ) state space is a read-only memory initialized by the host. Constant memory\nis accessed with a ld.const instruction. Constant memory is restricted in size, currently\nlimited to 64 KB which can be used to hold statically-sized constant variables. There is an\nadditional 640 KB of constant memory, organized as ten independent 64 KB regions. The driver may\nallocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel\nfunction parameters. Since the ten regions are not contiguous, the driver must ensure that constant\nbuffers are allocated so that each buffer fits entirely within a 64 KB region and does not span a\nregion boundary. Statically-sized constant variables have an optional variable initializer; constant variables with\nno explicit initializer are initialized to zero by default. Constant buffers allocated by the driver\nare initialized by the host, and pointers to such buffers are passed to the kernel as\nparameters. See the description of kernel parameter attributes in Kernel Function Parameter\nAttributes for more details on passing pointers\nto constant buffers as kernel parameters. 5.1.3.1. Banked Constant State Space (deprecated)  Previous versions of PTX exposed constant memory as a set of eleven 64 KB banks, with explicit bank\nnumbers required for variable declaration and during access. Prior to PTX ISA version 2.2, the constant memory was organized into fixed size banks. There were\neleven 64 KB banks, and banks were specified using the .const[bank] modifier, where bank ranged from 0 to 10. If no bank number was given, bank zero was assumed. By convention, bank zero was used for all statically-sized constant variables. The remaining banks\nwere used to declare incomplete constant arrays (as in C, for example), where the size is not\nknown at compile time. For example, the declaration .extern .const[2] .b32 const_buffer[]; resulted in const_buffer pointing to the start of constant bank two. This pointer could then be\nused to access the entire 64 KB constant bank. Multiple incomplete array variables declared in the\nsame bank were aliased, with each pointing to the start address of the specified constant bank. To access data in contant banks 1 through 10, the bank number was required in the state space of the\nload instruction. For example, an incomplete array in bank 2 was accessed as follows: .extern .const[2] .b32 const_buffer[];\nld.const[2].b32  %r1, [const_buffer+4]; // load second word In PTX ISA version 2.2, we eliminated explicit banks and replaced the incomplete array\nrepresentation of driver-allocated constant buffers with kernel parameter attributes that allow\npointers to constant buffers to be passed as kernel parameters. 5.1.4. Global State Space  The global ( .global ) state space is memory that is accessible by all threads in a context. It is\nthe mechanism by which threads in different CTAs, clusters, and grids can communicate. Use ld.global , st.global , and atom.global to access global variables. Global variables have an optional variable initializer; global variables with no explicit\ninitializer are initialized to zero by default. 5.1.5. Local State Space  The local state space ( .local ) is private memory for each thread to keep its own data. It is\ntypically standard memory with cache. The size is limited, as it must be allocated on a per-thread\nbasis. Use ld.local and st.local to access local variables. When compiling to use the Application Binary Interface (ABI) , .local state-space variables\nmust be declared within function scope and are allocated on the stack. In implementations that do\nnot support a stack, all local memory variables are stored at fixed addresses, recursive function\ncalls are not supported, and .local variables may be declared at module scope. When compiling\nlegacy PTX code (ISA versions prior to 3.0) containing module-scoped .local variables, the\ncompiler silently disables use of the ABI. 5.1.6. Parameter State Space  The parameter ( .param ) state space is used (1) to pass input arguments from the host to the\nkernel, (2a) to declare formal input and return parameters for device functions called from within\nkernel execution, and (2b) to declare locally-scoped byte array variables that serve as function\ncall arguments, typically for passing large structures by value to a function. Kernel function\nparameters differ from device function parameters in terms of access and sharing (read-only versus\nread-write, per-kernel versus per-thread). Note that PTX ISA versions 1.x supports only kernel\nfunction parameters in .param space; device function parameters were previously restricted to the\nregister state space. The use of parameter state space for device function parameters was introduced\nin PTX ISA version 2.0 and requires target architecture sm_20 or higher. Additional sub-qualifiers ::entry or ::func can be specified on instructions with .param state space to indicate\nwhether the address refers to kernel function parameter or device function parameter. If no\nsub-qualifier is specified with the .param state space, then the default sub-qualifier is specific\nto and dependent on the exact instruction. For example, st.param is equivalent to st.param::func whereas isspacep.param is equivalent to isspacep.param::entry . Refer to the instruction\ndescription for more details on default sub-qualifier assumption. Note The location of parameter space is implementation specific. For example, in some implementations\nkernel parameters reside in global memory. No access protection is provided between parameter and\nglobal space in this case. Though the exact location of the kernel parameter space is\nimplementation specific, the kernel parameter space window is always contained within the global\nspace window. Similarly, function parameters are mapped to parameter passing registers and/or\nstack locations based on the function calling conventions of the Application Binary Interface\n(ABI) . Therefore, PTX code should make no assumptions about the relative locations or ordering\nof .param space variables. 5.1.6.1. Kernel Function Parameters  Each kernel function definition includes an optional list of parameters. These parameters are\naddressable, read-only variables declared in the .param state space. Values passed from the host\nto the kernel are accessed through these parameter variables using ld.param instructions. The\nkernel parameter variables are shared across all CTAs from all clusters within a grid. The address of a kernel parameter may be moved into a register using the mov instruction. The\nresulting address is in the .param state space and is accessed using ld.param instructions. Example .entry foo ( .param .b32 N, .param .align 8 .b8 buffer[64] )\n{\n    .reg .u32 %n;\n    .reg .f64 %d;\n\n    ld.param.u32 %n, [N];\n    ld.param.f64 %d, [buffer];\n    ... Example .entry bar ( .param .b32 len )\n{\n    .reg .u32 %ptr, %n;\n\n    mov.u32      %ptr, len;\n    ld.param.u32 %n, [%ptr];\n    ... Kernel function parameters may represent normal data values, or they may hold addresses to objects\nin constant, global, local, or shared state spaces. In the case of pointers, the compiler and\nruntime system need information about which parameters are pointers, and to which state space they\npoint. Kernel parameter attribute directives are used to provide this information at the PTX\nlevel. See Kernel Function Parameter Attributes for a description of kernel parameter attribute\ndirectives. Note The current implementation does not allow creation of generic pointers to constant variables\n( cvta.const ) in programs that have pointers to constant buffers passed as kernel parameters. 5.1.6.2. Kernel Function Parameter Attributes  Kernel function parameters may be declared with an optional .ptr attribute to indicate that a\nparameter is a pointer to memory, and also indicate the state space and alignment of the memory\nbeing pointed to. Kernel Parameter Attribute: .ptr describes the .ptr kernel parameter attribute. 5.1.6.3. Kernel Parameter Attribute: .ptr  .ptr Kernel parameter alignment attribute. Syntax .param .type .ptr .space .align N  varname\n.param .type .ptr        .align N  varname\n\n.space = { .const, .global, .local, .shared }; Description Used to specify the state space and, optionally, the alignment of memory pointed to by a pointer\ntype kernel parameter. The alignment value N , if present, must be a power of two. If no state\nspace is specified, the pointer is assumed to be a generic address pointing to one of const, global,\nlocal, or shared memory. If no alignment is specified, the memory pointed to is assumed to be\naligned to a 4 byte boundary. Spaces between .ptr , .space , and .align may be eliminated to improve readability. PTX ISA Notes Introduced in PTX ISA version 2.2. Support for generic addressing of .const space added in PTX ISA version 3.1. Target ISA Notes Supported on all target architectures. Examples .entry foo ( .param .u32 param1,\n             .param .u32 .ptr.global.align 16 param2,\n             .param .u32 .ptr.const.align 8 param3,\n             .param .u32 .ptr.align 16 param4  // generic address\n                                               // pointer\n) { .. } 5.1.6.4. Device Function Parameters  PTX ISA version 2.0 extended the use of parameter space to device function parameters. The most\ncommon use is for passing objects by value that do not fit within a PTX register, such as C\nstructures larger than 8 bytes. In this case, a byte array in parameter space is used. Typically,\nthe caller will declare a locally-scoped .param byte array variable that represents a flattened\nC structure or union. This will be passed by value to a callee, which declares a .param formal\nparameter having the same size and alignment as the passed argument. Example // pass object of type struct { double d; int y; };\n.func foo ( .reg .b32 N, .param .align 8 .b8 buffer[12] )\n{\n    .reg .f64 %d;\n    .reg .s32 %y;\n\n    ld.param.f64 %d, [buffer];\n    ld.param.s32 %y, [buffer+8];\n    ...\n}\n\n// code snippet from the caller\n// struct { double d; int y; } mystruct; is flattened, passed to foo\n    ...\n    .reg .f64 dbl;\n    .reg .s32 x;\n    .param .align 8 .b8 mystruct;\n    ...\n    st.param.f64 [mystruct+0], dbl;\n    st.param.s32 [mystruct+8], x;\n    call foo, (4, mystruct);\n    ... See the section on function call syntax for more details. Function input parameters may be read via ld.param and function return parameters may be written\nusing st.param ; it is illegal to write to an input parameter or read from a return parameter. Aside from passing structures by value, .param space is also required whenever a formal\nparameter has its address taken within the called function. In PTX, the address of a function input\nparameter may be moved into a register using the mov instruction. Note that the parameter will\nbe copied to the stack if necessary, and so the address will be in the .local state space and is\naccessed via ld.local and st.local instructions. It is not possible to use mov to get\nthe address of or a locally-scoped .param space variable. Starting PTX ISA version 6.0, it is\npossible to use mov instruction to get address of return parameter of device function. Example // pass array of up to eight floating-point values in buffer\n.func foo ( .param .b32 N, .param .b32 buffer[32] )\n{\n    .reg .u32  %n, %r;\n    .reg .f32  %f;\n    .reg .pred %p;\n\n    ld.param.u32 %n, [N];\n    mov.u32      %r, buffer;  // forces buffer to .local state space\nLoop:\n    setp.eq.u32  %p, %n, 0;\n@%p: bra         Done;\n    ld.local.f32 %f, [%r];\n    ...\n    add.u32      %r, %r, 4;\n    sub.u32      %n, %n, 1;\n    bra          Loop;\nDone:\n    ...\n} 5.1.7. Shared State Space  The shared ( .shared ) state space is a memory that is owned by an executing CTA and is accessible\nto the threads of all the CTAs within a cluster. An address in shared memory can be read and written\nby any thread in a CTA cluster. Additional sub-qualifiers ::cta or ::cluster can be specified on instructions with .shared state space to indicate whether the address belongs to the shared memory window of the\nexecuting CTA or of any CTA in the cluster respectively. The addresses in the .shared::cta window also fall within the .shared::cluster window. If no sub-qualifier is specified with the .shared state space, then it defaults to ::cta . For example, ld.shared is equivalent to ld.shared::cta . Variables declared in .shared state space refer to the memory addresses in the current\nCTA. Instruction mapa gives the .shared::cluster address of the corresponding variable in\nanother CTA in the cluster. Shared memory typically has some optimizations to support the sharing. One example is broadcast;\nwhere all threads read from the same address. Another is sequential access from sequential threads. 5.1.8. Texture State Space (deprecated)  The texture ( .tex ) state space is global memory accessed via the texture instruction. It is\nshared by all threads in a context. Texture memory is read-only and cached, so accesses to texture\nmemory are not coherent with global memory stores to the texture image. The GPU hardware has a fixed number of texture bindings that can be accessed within a single kernel\n(typically 128). The .tex directive will bind the named texture memory variable to a hardware\ntexture identifier, where texture identifiers are allocated sequentially beginning with\nzero. Multiple names may be bound to the same physical texture identifier. An error is generated if\nthe maximum number of physical resources is exceeded. The texture name must be of type .u32 or .u64 . Physical texture resources are allocated on a per-kernel granularity, and .tex variables are\nrequired to be defined in the global scope. Texture memory is read-only. A texture’s base address is assumed to be aligned to a 16 byte\nboundary. Example .tex .u32 tex_a;         // bound to physical texture 0\n.tex .u32 tex_c, tex_d;  // both bound to physical texture 1\n.tex .u32 tex_d;         // bound to physical texture 2\n.tex .u32 tex_f;         // bound to physical texture 3 Note Explicit declarations of variables in the texture state space is deprecated, and programs should\ninstead reference texture memory through variables of type .texref . The .tex directive is\nretained for backward compatibility, and variables declared in the .tex state space are\nequivalent to module-scoped .texref variables in the .global state space. For example, a legacy PTX definitions such as .tex .u32 tex_a; is equivalent to: .global .texref tex_a; See Texture Sampler and Surface Types for the\ndescription of the .texref type and Texture Instructions for its use in texture instructions. 5.2. Types  5.2.1. Fundamental Types  In PTX, the fundamental types reflect the native data types supported by the target architectures. A\nfundamental type specifies both a basic type and a size. Register variables are always of a\nfundamental type, and instructions operate on these types. The same type-size specifiers are used\nfor both variable definitions and for typing instructions, so their names are intentionally short. Table 8 lists the fundamental type specifiers for\neach basic type: Table 8 Fundamental Type Specifiers  Basic Type Fundamental Type Specifiers Signed integer .s8 , .s16 , .s32 , .s64 Unsigned integer .u8 , .u16 , .u32 , .u64 Floating-point .f16 , .f16x2 , .f32 , .f64 Bits (untyped) .b8 , .b16 , .b32 , .b64 , .b128 Predicate .pred Most instructions have one or more type specifiers, needed to fully specify instruction\nbehavior. Operand types and sizes are checked against instruction types for compatibility. Two fundamental types are compatible if they have the same basic type and are the same size. Signed\nand unsigned integer types are compatible if they have the same size. The bit-size type is\ncompatible with any fundamental type having the same size. In principle, all variables (aside from predicates) could be declared using only bit-size types, but\ntyped variables enhance program readability and allow for better operand type checking. 5.2.2. Restricted Use of Sub-Word Sizes  The .u8 , .s8 , and .b8 instruction types are restricted to ld , st , and cvt instructions. The .f16 floating-point type is allowed only in conversions to and from .f32 , .f64 types, in half precision floating point instructions and texture fetch instructions. The .f16x2 floating point type is allowed only in half precision floating point arithmetic\ninstructions and texture fetch instructions. For convenience, ld , st , and cvt instructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. 5.2.3. Alternate Floating-Point Data Formats  The fundamental floating-point types supported in PTX have implicit bit representations that\nindicate the number of bits used to store exponent and mantissa. For example, the .f16 type\nindicates 5 bits reserved for exponent and 10 bits reserved for mantissa. In addition to the\nfloating-point representations assumed by the fundamental types, PTX allows the following alternate\nfloating-point data formats: bf16 data format: This data format is a 16-bit floating point format with 8 bits for exponent and 7 bits for\nmantissa. A register variable containing bf16 data must be declared with .b16 type. e4m3 data format: This data format is an 8-bit floating point format with 4 bits for exponent and 3 bits for\nmantissa. The e4m3 encoding does not support infinity and NaN values are limited to 0x7f and 0xff . A register variable containing e4m3 value must be declared using\nbit-size type. e5m2 data format: This data format is an 8-bit floating point format with 5 bits for exponent and 2 bits for\nmantissa. A register variable containing e5m2 value must be declared using bit-size type. tf32 data format: This data format is a special 32-bit floating point format supported by the matrix\nmultiply-and-accumulate instructions, with the same range as .f32 and reduced precision (>=10\nbits). The internal layout of tf32 format is implementation defined. PTX facilitates\nconversion from single precision .f32 type to tf32 format. A register variable containing tf32 data must be declared with .b32 type. Alternate data formats cannot be used as fundamental types. They are supported as source or\ndestination formats by certain instructions. 5.2.4. Packed Data Types  Certain PTX instructions operate on two sets of inputs in parallel, and produce two outputs. Such\ninstructions can use the data stored in a packed format. PTX supports packing two values of the same\nscalar data type into a single, larger value. The packed value is considered as a value of a packed\ndata type . In this section we describe the packed data types supported in PTX. 5.2.4.1. Packed Floating Point Data Types  PTX supports the following four variants of packed floating point data types: .f16x2 packed type containing two .f16 floating point values. .bf16x2 packed type containing two .bf16 alternate floating point values. .e4m3x2 packed type containing two .e4m3 alternate floating point values. .e5m2x2 packed type containing two .e5m2 alternate floating point values. .f16x2 is supported as a fundamental type. .bf16x2 , .e4m3x2 and .e5m2x2 cannot be\nused as fundamental types - they are supported as instruction types on certain instructions. A\nregister variable containing .bf16x2 data must be declared with .b32 type. A register\nvariable containing .e4m3x2 or .e5m2x2 data must be declared with .b16 type. 5.2.4.2. Packed Integer Data Types  PTX supports two variants of packed integer data types: .u16x2 and .s16x2 . The packed data\ntype consists of two .u16 or .s16 values. A register variable containing .u16x2 or .s16x2 data must be declared with .b32 type. Packed integer data types cannot be used as\nfundamental types. They are supported as instruction types on certain instructions. 5.3. Texture Sampler and Surface Types  PTX includes built-in opaque types for defining texture, sampler, and surface descriptor\nvariables. These types have named fields similar to structures, but all information about layout,\nfield ordering, base address, and overall size is hidden to a PTX program, hence the term opaque . The use of these opaque types is limited to: Variable definition within global (module) scope and in kernel entry parameter lists. Static initialization of module-scope variables using comma-delimited static assignment\nexpressions for the named members of the type. Referencing textures, samplers, or surfaces via texture and surface load/store instructions\n( tex , suld , sust , sured ). Retrieving the value of a named member via query instructions ( txq , suq ). Creating pointers to opaque variables using mov , e.g., mov.u64 reg, opaque_var; . The\nresulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and\nde-referenced by texture and surface load, store, and query instructions, but the pointer cannot\notherwise be treated as an address, i.e., accessing the pointer with ld and st instructions, or performing pointer arithmetic will result in undefined results. Opaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque\nvariable. Note Indirect access to textures and surfaces using pointers to opaque variables is supported\nbeginning with PTX ISA version 3.1 and requires target sm_20 or later. Indirect access to textures is supported only in unified texture mode (see below). The three built-in types are .texref , .samplerref , and .surfref . For working with\ntextures and samplers, PTX has two modes of operation. In the unified mode, texture and sampler\ninformation is accessed through a single .texref handle. In the independent mode , texture and\nsampler information each have their own handle, allowing them to be defined separately and combined\nat the site of usage in the program. In independent mode, the fields of the .texref type that\ndescribe sampler properties are ignored, since these properties are defined by .samplerref variables. Table 9 and Table 10 list the named members\nof each type for unified and independent texture modes. These members and their values have\nprecise mappings to methods and values defined in the texture HW class as well as\nexposed values via the API. Table 9 Opaque Type Fields in Unified Texture Mode  Member .texref values .surfref values width in elements height in elements depth in elements channel_data_type enum type corresponding to source language API channel_order enum type corresponding to source language API normalized_coords 0 , 1 N/A filter_mode nearest , linear N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A array_size as number of textures in a texture\narray as number of surfaces in a surface array num_mipmap_levels as number of levels in a mipmapped\ntexture N/A num_samples as number of samples in a multi-sample\ntexture N/A memory_layout N/A 1 for linear memory layout; 0 otherwise 5.3.1. Texture and Surface Properties  Fields width , height , and depth specify the size of the texture or surface in number of\nelements in each dimension. The channel_data_type and channel_order fields specify these properties of the texture or\nsurface using enumeration types corresponding to the source language API. For example, see Channel\nData Type and Channel Order Fields for\nthe OpenCL enumeration types currently supported in PTX. 5.3.2. Sampler Properties  The normalized_coords field indicates whether the texture or surface uses normalized coordinates\nin the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is\nspecified, the default is set by the runtime system based on the source language. The filter_mode field specifies how the values returned by texture reads are computed based on\nthe input texture coordinates. The addr_mode_{0,1,2} fields define the addressing mode in each dimension, which determine how\nout-of-range coordinates are handled. See the CUDA C++ Programming Guide for more details of these properties. Table 10 Opaque Type Fields in Independent Texture Mode  Member .samplerref values .texref values .surfref values width N/A in elements height N/A in elements depth N/A in elements channel_data_type N/A enum type corresponding to source\nlanguage API channel_order N/A enum type corresponding to source\nlanguage AP normalized_coords N/A 0 , 1 N/A force_unnormalized_coords 0 , 1 N/A N/A filter_mode nearest , linear ignored N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A N/A array_size N/A as number of textures\nin a texture array as number of surfaces in\na surface array num_mipmap_levels N/A as number of levels\nin a mipmapped\ntexture N/A num_samples N/A as number of samples\nin a multi-sample\ntexture N/A memory_layout N/A N/A 1 for linear memory\nlayout; 0 otherwise In independent texture mode, the sampler properties are carried in an independent .samplerref variable, and these fields are disabled in the .texref variables. One additional sampler\nproperty, force_unnormalized_coords , is available in independent texture mode. The force_unnormalized_coords field is a property of .samplerref variables that allows the\nsampler to override the texture header normalized_coords property. This field is defined only in\nindependent texture mode. When True , the texture header setting is overridden and unnormalized\ncoordinates are used; when False , the texture header setting is used. The force_unnormalized_coords property is used in compiling OpenCL; in OpenCL, the property of\nnormalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are\nalways initialized with normalized_coords set to True, and the OpenCL sampler-based normalized_coords flag maps (negated) to the PTX-level force_unnormalized_coords flag. Variables using these types may be declared at module scope or within kernel entry parameter\nlists. At module scope, these variables must be in the .global state space. As kernel\nparameters, these variables are declared in the .param state space. Example .global .texref     my_texture_name;\n.global .samplerref my_sampler_name;\n.global .surfref    my_surface_name; When declared at module scope, the types may be initialized using a list of static expressions\nassigning values to the named members. Example .global .texref tex1;\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             }; 5.3.3. Channel Data Type and Channel Order Fields  The channel_data_type and channel_order fields have enumeration types corresponding to the\nsource language API. Currently, OpenCL is the only source language that defines these\nfields. Table 12 and Table 11 show the\nenumeration values defined in OpenCL version 1.0 for channel data type and channel order. Table 11 OpenCL 1.0 Channel Data Type Definition  CL_SNORM_INT8 0x10D0 CL_SNORM_INT16 0x10D1 CL_UNORM_INT8 0x10D2 CL_UNORM_INT16 0x10D3 CL_UNORM_SHORT_565 0x10D4 CL_UNORM_SHORT_555 0x10D5 CL_UNORM_INT_101010 0x10D6 CL_SIGNED_INT8 0x10D7 CL_SIGNED_INT16 0x10D8 CL_SIGNED_INT32 0x10D9 CL_UNSIGNED_INT8 0x10DA CL_UNSIGNED_INT16 0x10DB CL_UNSIGNED_INT32 0x10DC CL_HALF_FLOAT 0x10DD CL_FLOAT 0x10DE Table 12 OpenCL 1.0 Channel Order Definition  CL_R 0x10B0 CL_A 0x10B1 CL_RG 0x10B2 CL_RA 0x10B3 CL_RGB 0x10B4 CL_RGBA 0x10B5 CL_BGRA 0x10B6 CL_ARGB 0x10B7 CL_INTENSITY 0x10B8 CL_LUMINANCE 0x10B9 5.4. Variables  In PTX, a variable declaration describes both the variable’s type and its state space. In addition\nto fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays. 5.4.1. Variable Declarations  All storage for data is specified with variable declarations. Every variable must reside in one of\nthe state spaces enumerated in the previous section. A variable declaration names the space in which the variable resides, its type and size, its name,\nan optional array size, an optional initializer, and an optional fixed address for the variable. Predicate variables may only be declared in the register state space. Examples .global .u32 loc;\n.reg    .s32 i;\n.const  .f32 bias[] = {-1.0, 1.0};\n.global .u8  bg[4] = {0, 0, 0, 0};\n.reg    .v4 .f32 accel;\n.reg    .pred p, q, r; 5.4.2. Vectors  Limited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate\nfundamental type can be declared by prefixing the type with .v2 or .v4 . Vectors must be\nbased on a fundamental type, and they may reside in the register space. Vectors cannot exceed\n128-bits in length; for example, .v4 .f64 is not allowed. Three-element vectors may be\nhandled by using a .v4 vector, where the fourth element provides padding. This is a common case\nfor three-dimensional grids, textures, etc. Examples .global .v4 .f32 V;   // a length-4 vector of floats\n.shared .v2 .u16 uv;  // a length-2 vector of unsigned ints\n.global .v4 .b8  v;   // a length-4 vector of bytes By default, vector variables are aligned to a multiple of their overall size (vector length times\nbase-type size), to enable vector load and store instructions which require addresses aligned to a\nmultiple of the access size. 5.4.3. Array Declarations  Array declarations are provided to allow the programmer to reserve space. To declare an array, the\nvariable name is followed with dimensional declarations similar to fixed-size array declarations\nin C. The size of each dimension is a constant expression. Examples .local  .u16 kernel[19][19];\n.shared .u8  mailbox[128]; The size of the array specifies how many elements should be reserved. For the declaration of array kernel above, 19*19 = 361 halfwords are reserved, for a total of 722 bytes. When declared with an initializer, the first dimension of the array may be omitted. The size of the\nfirst array dimension is determined by the number of elements in the array initializer. Examples .global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n.global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} }; Array index has eight elements, and array offset is a 4x2 array. 5.4.4. Initializers  Declared variables may specify an initial value using a syntax similar to C/C++, where the variable\nname is followed by an equals sign and the initial value or values for the variable. A scalar takes\na single value, while vectors and arrays take nested lists of values inside of curly braces (the\nnesting matches the dimensionality of the declaration). As in C, array initializers may be incomplete, i.e., the number of initializer elements may be less\nthan the extent of the corresponding array dimension, with remaining array locations initialized to\nthe default value for the specified array type. Examples .const  .f32 vals[8] = { 0.33, 0.25, 0.125 };\n.global .s32 x[3][2] = { {1,2}, {3} }; is equivalent to .const  .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 };\n.global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} }; Currently, variable initialization is supported only for constant and global state spaces. Variables\nin constant and global state spaces with no explicit initializer are initialized to zero by\ndefault. Initializers are not allowed in external variable declarations. Variable names appearing in initializers represent the address of the variable; this can be used to\nstatically initialize a pointer to a variable. Initializers may also contain var+offset expressions, where offset is a byte offset added to the address of var . Only variables in .global or .const state spaces may be used in initializers. By default, the resulting\naddress is the offset in the variable’s state space (as is the case when taking the address of a\nvariable with a mov instruction). An operator, generic() , is provided to create a generic\naddress for variables used in initializers. Starting PTX ISA version 7.1, an operator mask() is provided, where mask is an integer\nimmediate. The only allowed expressions in the mask() operator are integer constant expression\nand symbol expression representing address of variable. The mask() operator extracts n consecutive bits from the expression used in initializers and inserts these bits at the lowest\nposition of the initialized variable. The number n and the starting position of the bits to be\nextracted is specified by the integer immediate mask . PTX ISA version 7.1 only supports\nextracting a single byte starting at byte boundary from the address of the variable. PTX ISA version\n7.3 supports Integer constant expression as an operand in the mask() operator. Supported values for mask are: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000,\n0xFF000000000000, 0xFF00000000000000. Examples .const  .u32 foo = 42;\n.global .u32 bar[] = { 2, 3, 5 };\n.global .u32 p1 = foo;          // offset of foo in .const space\n.global .u32 p2 = generic(foo); // generic address of foo\n\n// array of generic-address pointers to elements of bar\n.global .u32 parr[] = { generic(bar), generic(bar)+4,\ngeneric(bar)+8 };\n\n// examples using mask() operator are pruned for brevity\n.global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...};\n\n.global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...}\n\n.global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...}\n\n.global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...}\n\n// mask() operator with integer const expression\n.global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...}; Note PTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer. PTX 3.1 code should\neither include explicit generic() operators in initializers, use cvta.global to form\ngeneric addresses at runtime, or load from the non-generic address using ld.global . Device function names appearing in initializers represent the address of the first instruction in\nthe function; this can be used to initialize a table of function pointers to be used with indirect\ncalls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to\ninitialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch\nkernels from GPU. See the CUDA Dynamic Parallelism Programming Guide for details. Labels cannot be used in initializers. Variables that hold addresses of variables or functions should be of type .u8 or .u32 or .u64 . Type .u8 is allowed only if the mask() operator is used. Initializers are allowed for all types except .f16 , .f16x2 and .pred . Examples .global .s32 n = 10;\n.global .f32 blur_kernel[][3]\n               = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}};\n\n.global .u32 foo[] = { 2, 3, 5, 7, 9, 11 };\n.global .u64 ptr = generic(foo);   // generic address of foo[0]\n.global .u64 ptr = generic(foo)+8; // generic address of foo[2] 5.4.5. Alignment  Byte alignment of storage for all addressable variables can be specified in the variable\ndeclaration. Alignment is specified using an optional .align byte-count specifier immediately\nfollowing the state-space specifier. The variable will be aligned to an address which is an integer\nmultiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment\nspecifies the address alignment for the starting address of the entire array, not for individual\nelements. The default alignment for scalar and array variables is to a multiple of the base-type size. The\ndefault alignment for vector variables is to a multiple of the overall vector size. Examples // allocate array at 4-byte aligned address.  Elements are bytes.\n.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0}; Note that all PTX instructions that access memory require that the address be aligned to a multiple\nof the access size. The access size of a memory instruction is the total number of bytes accessed in\nmemory. For example, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4 bytes. 5.4.6. Parameterized Variable Names  Since PTX supports virtual registers, it is quite common for a compiler frontend to generate a large\nnumber of register names. Rather than require explicit declaration of every name, PTX supports a\nsyntax for creating a set of variables having a common prefix string appended with integer suffixes. For example, suppose a program uses a large number, say one hundred, of .b32 variables, named %r0 , %r1 , …, %r99 . These 100 register variables can be declared as follows: .reg .b32 %r<100>;    // declare %r0, %r1, ..., %r99 This shorthand syntax may be used with any of the fundamental types and with any state space, and\nmay be preceded by an alignment specifier. Array variables cannot be declared this way, nor are\ninitializers permitted. 5.4.7. Variable Attributes  Variables may be declared with an optional .attribute directive which allows specifying special\nattributes of variables. Keyword .attribute is followed by attribute specification inside\nparenthesis. Multiple attributes are separated by comma. Variable and Function Attribute Directive: .attribute describes the .attribute directive. 5.4.8. Variable and Function Attribute Directive: .attribute  .attribute Variable and function attributes Description Used to specify special attributes of a variable or a function. The following attributes are supported. .managed .managed attribute specifies that variable will be allocated at a location in unified virtual\nmemory environment where host and other devices in the system can reference the variable\ndirectly. This attribute can only be used with variables in .global state space. See the CUDA\nUVM-Lite Programming Guide for details. .unified .unified attribute specifies that function has the same memory address on the host and on\nother devices in the system. Integer constants uuid1 and uuid2 respectively specify upper\nand lower 64 bits of the unique identifier associated with the function or the variable. This\nattribute can only be used on device functions or on variables in the .global state\nspace. Variables with .unified attribute are read-only and must be loaded by specifying .unified qualifier on the address operand of ld instruction, otherwise the behavior is\nundefined. PTX ISA Notes Introduced in PTX ISA version 4.0. Support for function attributes introduced in PTX ISA version 8.0. Target ISA Notes .managed attribute requires sm_30 or higher. .unified attribute requires sm_90 or higher. Examples .global .attribute(.managed) .s32 g;\n.global .attribute(.managed) .u64 x;\n\n.global .attribute(.unified(19,95)) .f32 f;\n\n.func .attribute(.unified(0xAB, 0xCD)) bar() { ... } 5.5. Tensors  A tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following\nproperties: Dimensionality Dimension sizes across each dimension Individual element types Tensor stride across each dimension PTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: Copying data between global and shared memories Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma , mma and wgmma.mma_async instructions. PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure\nand treat the data in the shared memory as a linear data. 5.5.1. Tensor Dimension, size and format  Tensors can have dimensions: 1D, 2D, 3D, 4D or 5D. Each dimension has a size which represents the number of elements along the dimension. The elements\ncan have one the following types: Bit-sized type: .b32 , .b64 Integer: .u8 , .u16 , .u32 , .s32 , .u64 , .s64 Floating point and alternate floating point: .f16 , .bf16 , .tf32 , .f32 , .f64 (rounded to nearest even). Tensor can have padding at the end in each of the dimensions to provide alignment for the data in\nthe subsequent dimensions. Tensor stride can be used to specify the amount of padding in each\ndimension. 5.5.2. Tensor Access Modes  Tensor data can be accessed in two modes: Tiled mode: In tiled mode, the source multi-dimensional tensor layout is preserved at the destination. Im2col mode: In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns\nat the destination. Refer here for more details. 5.5.3. Tiled Mode  This section talks about how Tensor and Tensor access work in tiled mode. 5.5.3.1. Bounding Box  A tensor can be accessed in chunks known as Bounding Box . The Bounding Box has the same\ndimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple\nof 16 bytes. The address of the bounding Box must also be aligned to 16 bytes. Bounding Box has the following access properties: Bounding Box dimension sizes Out of boundary access mode Traversal strides The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the\nbounding box. Starting offset of the bounding box along with the rest of the bounding box\ninformation together are used to determine the elements which are to be accessed. 5.5.3.2. Traversal-Stride  While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies\nthe exact number of elements to be skipped. If no jump over is required, default value of 1 must be\nspecified. The traversal stride in dimension 0 can be used for the Interleave layout . For non-interleaved layout, the traversal stride in\ndimension 0 must always be 1. Figure 5 illustrates tensor, tensor size, tensor stride,\nBounding Box size and traversal stride. Figure 5 Tiled mode bounding box, tensor size and traversal stride  5.5.3.3. Out of Boundary Access  PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor\nboundary in any dimension. There are 2 modes: Zero fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to 0. OOB-NaN fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN\ncalled OOB-NaN . Figure 6 shows an example of the out of boundary access. Figure 6 Out of boundary access  5.5.4. Im2col mode  Im2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data\nis treated as a batch of images with the following properties: N : number of images in the batch D, H, W : size of a 3D image (depth, height and width) C: channels per image element The above properties are associated with 3D, 4D and 5D tensors as follows: Dimension N/D/H/W/C applicability 3D NWC 4D NHWC 5D NDHWC 5.5.4.1. Bounding Box  In im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are\nspecified by Pixels-per-Column and Channels-per-Pixel parameters as described below. The dimensionality of the Bounding Box is two less than the tensor dimensionality. The following properties describe how to access of the elements in im2col mode: Bounding-Box Lower-Corner Bounding-Box Upper-Corner Pixels-per-Column Channels-per-Pixel Bounding-box Lower-Corner and Bounding-box Upper-Corner specify the two opposite corners of the\nBounding Box in the DHW space. Bounding-box Lower-Corner specifies the corner with the smallest\ncoordinate and Bounding-box Upper-Corner specifies the corner with the largest coordinate. Bounding-box Upper- and Lower-Corners are 16-bit signed values whose limits varies across the\ndimensions and are as shown below: 3D 4D 5D Upper- / Lower- Corner sizes [-2 15 , 2 15 -1] [-2 7 , 2 7 -1] [-2 4 , 2 4 -1] Figure 7 and Figure 8 show the Upper-Corners and Lower-Corners. Figure 7 im2col mode bounding box example 1  Figure 8 im2col mode bounding box example 2  The Bounding-box Upper- and Lower- Corners specify only the boundaries and not the number of\nelements to be accessed. Pixels-per-Column specifies the number of elements to be accessed in the\nNDHW space. Channels-per-Pixel specifies the number of elements to access across the C dimension. The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different\ndimensions: Across N and C dimensions: specify the starting offsets along the dimension, similar to the tiled\nmode. Across DHW dimensions: specify the location of the convolution filter base in the tensor\nspace. The filter corner location must be within the bounding box. The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter\nbase coordinates to determine the starting location in the tensor space from where the elements are\naccessed. The size of the im2col offsets varies across the dimensions and their valid ranges are as shown\nbelow: 3D 4D 5D im2col offsets range [0, 2 16 -1] [0, 2 8 -1] [0, 2 5 -1] Following are some examples of the im2col mode accesses: Example 1 ( Figure 9 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1. tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 0 , 0 ) Figure 9 im2col mode example 1  Example 2 ( Figure 10 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = 0 Bounding - Box Lower - Corner H = 0 Bounding - Box Upper - Corner W = -2 Bounding - Box Upper - Corner H = -2 tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 2 , 2 ) Figure 10 im2col mode example 2  5.5.4.2. Traversal Stride  The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being\naccessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being\naccessed, in im2col mode. The number of elements traversed along the D, H and W dimensions is strided by the traversal stride\nfor that dimension. The following example with Figure 11 illustrates accesse with traversal-strides: Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 8 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Traversal Stride = 2 Pixels - per - Column = 32 channels - per - pixel = 16 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1. Tensor coordinates in the instruction = ( 7 , 7 , 5 , 0 ) Im2col offsets in the instruction : ( 1 , 1 ) Figure 11 im2col mode traversal stride example  5.5.4.3. Out of Boundary Access  In im2col mode, when the number of requested pixels in NDHW space specified by Pixels-per-Column exceeds the number of available pixels in the image batch then out-of-bounds access is performed. Similar to tiled mode, zero fill or OOB-NaN fill can be performed based on the Fill-Mode\nspecified. 5.5.5. Interleave layout  Tensor can be interleaved and the following interleave layouts are supported: No interleave (NDHWC) 8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel. 16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel. The C information is organized in slices where sequential C elements are grouped in 16 byte or 32\nbyte quantities. If the total number of channels is not a multiple of the number of channels per slice, then the last\nslice must be padded with zeros to make it complete 16B or 32B slice. Interleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D. 5.5.6. Swizzling Modes  The layout of the data in the shared memory can be different to that of global memory, for access\nperformance reasons. The following describes various swizzling modes: No swizzle mode: There is no swizzling in this mode and the destination data layout is exactly similar to the\nsource data layout. 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 … Pattern repeats … 32 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is\n256 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 … Pattern repeats … An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,\nwith the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in Figure 12 . Figure 12 32-byte swizzle mode example  Figure 13 shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1. Figure 13 32-byte swizzle mode fragments  Figure 14 shows the destination data layout with 32 byte swizzling. Figure 14 32-byte swizzle mode destination data layout  64 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is\n512 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 … Pattern repeats … An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 32 channels, is shown in Figure 15 . Figure 15 64-byte swizzle mode example  Each colored cell represents 8 channels. Figure 16 shows the source data layout. Figure 16 64-byte swizzle mode source data layout  Figure 17 shows the destination data layout with 64 byte swizzling. Figure 17 64-byte swizzle mode destination data layout  128 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is\n1024 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 4 5 6 7 0 1 2 3 5 4 7 6 1 0 3 2 6 7 4 5 2 3 0 1 … Pattern repeats … An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 64 channels, is shown in Figure 18 . Figure 18 128-byte swizzle mode example  Each colored cell represents 8 channels. Figure 19 shows the source data layout. Figure 19 128-byte swizzle mode source data layout  Figure 20 shows the destination data layout with 128 byte swizzling. Figure 20 128-byte swizzle mode destination data layout  5.5.7. Tensor-map  The tensor-map is a 128-byte opaque object either in .const space or .param (kernel function\nparameter) space or .global space which describes the tensor properties and the access properties\nof the tensor data described in previous sections. Tensor-Map can be created using CUDA APIs. Refer to CUDA programming guide for more details. 6. Instruction Operands  6.1. Operand Type Information  All operands in instructions have a known type from their declarations. Each operand type must be\ncompatible with the type determined by the instruction template and instruction type. There is no\nautomatic conversion between types. The bit-size type is compatible with every type having the same size. Integer types of a common size\nare compatible with each other. Operands having type different from but compatible with the\ninstruction type are silently cast to the instruction type. 6.2. Source Operands  The source operands are denoted in the instruction descriptions by the names a , b , and c . PTX describes a load-store machine, so operands for ALU instructions must all be in variables\ndeclared in the .reg register state space. For most operations, the sizes of the operands must\nbe consistent. The cvt (convert) instruction takes a variety of operand types and sizes, as its job is to\nconvert from nearly any data type to any other data type (and size). The ld , st , mov , and cvt instructions copy data from one location to\nanother. Instructions ld and st move data from/to addressable state spaces to/from\nregisters. The mov instruction copies data between registers. Most instructions have an optional predicate guard that controls conditional execution, and a few\ninstructions have additional predicate source operands. Predicate operands are denoted by the names p , q , r , s . 6.3. Destination Operands  PTX instructions that produce a single result store the result in the field denoted by d (for\ndestination) in the instruction descriptions. The result operand is a scalar or vector variable in\nthe register state space. 6.4. Using Addresses, Arrays, and Vectors  Using scalar variables as operands is straightforward. The interesting capabilities begin with\naddresses, arrays, and vectors. 6.4.1. Addresses as Operands  All the memory instructions take an address operand that specifies the memory location being\naccessed. This addressable operand is one of: [var] the name of an addressable variable var . [reg] an integer or bit-size type register reg containing a byte address. [reg+immOff] a sum of register reg containing a byte address plus a constant integer byte offset (signed, 32-bit). [var+immOff] a sum of address of addressable variable var containing a byte address plus a constant integer\nbyte offset (signed, 32-bit). [immAddr] an immediate absolute byte address (unsigned, 32-bit). var[immOff] an array element as described in Arrays as Operands . The register containing an address may be declared as a bit-size type or integer type. The access size of a memory instruction is the total number of bytes accessed in memory. For\nexample, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4\nbytes. The address must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined. For example, among other things, the access\nmay proceed by silently masking off low-order address bits to achieve proper rounding, or the\ninstruction may fault. The address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are\nzero-extended to the specified width as needed, and truncated if the register width exceeds the\nstate space address width for the target architecture. Address arithmetic is performed using integer arithmetic and logical instructions. Examples include\npointer arithmetic and pointer comparisons. All addresses and address computations are byte-based;\nthere is no support for C-style pointer arithmetic. The mov instruction can be used to move the address of a variable into a pointer. The address is\nan offset in the state space in which the variable is declared. Load and store operations move data\nbetween registers and locations in addressable state spaces. The syntax is similar to that used in\nmany assembly languages, where scalar variables are simply named and addresses are de-referenced by\nenclosing the address expression in square brackets. Address expressions include variable names,\naddress registers, address register plus byte offset, and immediate address expressions which\nevaluate at compile-time to a constant address. Here are a few examples: .shared .u16 x;\n.reg    .u16 r0;\n.global .v4 .f32 V;\n.reg    .v4 .f32 W;\n.const  .s32 tbl[256];\n.reg    .b32 p;\n.reg    .s32 q;\n\nld.shared.u16   r0,[x];\nld.global.v4.f32 W, [V];\nld.const.s32    q, [tbl+12];\nmov.u32         p, tbl; 6.4.1.1. Generic Addressing  If a memory instruction does not specify a state space, the operation is performed using generic\naddressing. The state spaces .const , Kernel Function Parameters ( .param ), .local and .shared are modeled as\nwindows within the generic address space. Each window is defined by a window base and a window size\nthat is equal to the size of the corresponding state space. A generic address maps to global memory unless it falls within the window for const , local , or shared memory. The Kernel\nFunction Parameters ( .param ) window is contained\nwithin the .global window. Within each window, a generic address maps to an address in the\nunderlying state space by subtracting the window base from the generic address. 6.4.2. Arrays as Operands  Arrays of all types can be declared, and the identifier becomes an address constant in the space\nwhere the array is declared. The size of the array is a constant in the program. Array elements can be accessed using an explicitly calculated byte address, or by indexing into the\narray using square-bracket notation. The expression within square brackets is either a constant\ninteger, a register variable, or a simple register with constant offset expression, where the\noffset is a constant expression that is either added or subtracted from a register variable. If more\ncomplicated indexing is desired, it must be written as an address calculation prior to use. Examples\nare: ld.global.u32  s, a[0];\nld.global.u32  s, a[N-1];\nmov.u32        s, a[1];  // move address of a[1] into s 6.4.3. Vectors as Operands  Vector operands are supported by a limited subset of instructions, which include mov , ld , st , atom , red and tex . Vectors may also be passed as arguments to called functions. Vector elements can be extracted from the vector with the suffixes .x , .y , .z and .w , as well as the typical color fields .r , .g , .b and .a . A brace-enclosed list is used for pattern matching to pull apart vectors. .reg .v4 .f32 V;\n.reg .f32     a, b, c, d;\n\nmov.v4.f32 {a,b,c,d}, V; Vector loads and stores can be used to implement wide loads and stores, which may improve memory\nperformance. The registers in the load/store operations can be a vector, or a brace-enclosed list of\nsimilarly typed scalars. Here are examples: ld.global.v4.f32  {a,b,c,d}, [addr+16];\nld.global.v2.u32  V2, [addr+8]; Elements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows: Ra = V.x = V.r\nRb = V.y = V.g\nRc = V.z = V.b\nRd = V.w = V.a 6.4.4. Labels and Function Names as Operands  Labels and function names can be used only in bra / brx.idx and call instructions\nrespectively. Function names can be used in mov instruction to get the address of the function\ninto a register, for use in an indirect call. Beginning in PTX ISA version 3.1, the mov instruction may be used to take the address of kernel\nfunctions, to be passed to a system call that initiates a kernel launch from the GPU. This feature\nis part of the support for CUDA Dynamic Parallelism. See the CUDA Dynamic Parallelism Programming\nGuide for details. 6.5. Type Conversion  All operands to all arithmetic, logic, and data movement instruction must be of the same type and\nsize, except for operations where changing the size and/or type is part of the definition of the\ninstruction. Operands of different sizes or types must be converted prior to the operation. 6.5.1. Scalar Conversions  Table 13 shows what\nprecision and format the cvt instruction uses given operands of differing types. For example, if a cvt.s32.u16 instruction is given a u16 source operand and s32 as a destination operand,\nthe u16 is zero-extended to s32 . Conversions to floating-point that are beyond the range of floating-point numbers are represented\nwith the maximum floating-point value (IEEE 754 Inf for f32 and f64 , and ~131,000 for f16 ). Table 13 Convert Instruction Precision and Format  Destination Format s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Source\nFormat s8 – sext sext sext – sext sext sext s2f s2f s2f s16 chop 1 – sext sext chop 1 – sext sext s2f s2f s2f s32 chop 1 chop 1 – sext chop 1 chop 1 – sext s2f s2f s2f s64 chop 1 chop 1 chop – chop 1 chop 1 chop – s2f s2f s2f u8 – zext zext zext – zext zext zext u2f u2f u2f u16 chop 1 – zext zext chop 1 – zext zext u2f u2f u2f u32 chop 1 chop 1 – zext chop 1 chop 1 – zext u2f u2f u2f u64 chop 1 chop 1 chop – chop 1 chop 1 chop – u2f u2f u2f f16 f2s f2s f2s f2s f2u f2u f2u f2u – f2f f2f f32 f2s f2s f2s f2s f2u f2u f2u f2u f2f – f2f f64 f2s f2s f2s f2s f2u f2u f2u f2u f2f f2f – Notes sext = sign-extend; zext = zero-extend; chop = keep only low bits that fit; s2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float; f2u = float-to-unsigned; f2f = float-to-float. 1 If the destination register is wider than the destination format, the result is extended to the\ndestination register width after chopping. The type of extension (sign or zero) is based on the\ndestination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then\nsign-extends to 32-bit. 6.5.2. Rounding Modifiers  Conversion instructions may specify a rounding modifier. In PTX, there are four integer rounding\nmodifiers and four floating-point rounding\nmodifiers. Table 14 and Table 15 summarize the rounding modifiers. Table 14 Floating-Point Rounding Modifiers  Modifier Description .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Table 15 Integer Rounding Modifiers  Modifier Description .rni round to nearest integer, choosing even integer if source is equidistant between two integers. .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity 6.6. Operand Costs  Operands from different state spaces affect the speed of an operation. Registers are fastest, while\nglobal memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first\nis to have multiple threads of execution so that the hardware can issue a memory operation and then\nswitch to other execution. Another way to hide latency is to issue the load instructions as early as\npossible, as execution is not blocked until the desired result is used in a subsequent (in time)\ninstruction. The register in a store operation is available much more\nquickly. Table 16 gives estimates of the\ncosts of using different kinds of memory. Table 16 Cost Estimates for Accessing State-Spaces  Space Time Notes Register 0 Shared 0 Constant 0 Amortized cost is low, first access is high Local > 100 clocks Parameter 0 Immediate 0 Global > 100 clocks Texture > 100 clocks Surface > 100 clocks 7. Abstracting the ABI  Rather than expose details of a particular calling convention, stack layout, and Application Binary\nInterface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI\nimplementations. In this section, we describe the features of PTX needed to achieve this hiding of\nthe ABI. These include syntax for function definitions, function calls, parameter passing, support\nfor variadic functions ( varargs ), and memory allocated on the stack ( alloca ). Refer to PTX Writers Guide to Interoperability for details on generating PTX compliant with\nApplication Binary Interface (ABI) for the CUDA ® architecture. 7.1. Function Declarations and Definitions  In PTX, functions are declared and defined using the .func directive. A function declaration specifies an optional list of return parameters, the function name, and an optional list of input\nparameters; together these specify the function’s interface, or prototype. A function definition specifies both the interface and the body of the function. A function must be declared or defined\nprior to being called. The simplest function has no parameters or return values, and is represented in PTX as follows: .func foo\n{\n    ...\n    ret;\n}\n\n    ...\n    call foo;\n    ... Here, execution of the call instruction transfers control to foo , implicitly saving the\nreturn address. Execution of the ret instruction within foo transfers control to the\ninstruction following the call. Scalar and vector base-type input and return parameters may be represented simply as register\nvariables. At the call, arguments may be register variables or constants, and return values may be\nplaced directly into register variables. The arguments and return variables at the call must have\ntype and size that match the callee’s corresponding formal parameters. Example .func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc )\n{\n    add.u32 %res, %ptr, %inc;\n    ret;\n}\n\n    ...\n    call (%r1), inc_ptr, (%r1,4);\n    ... When using the ABI, .reg state space parameters must be at least 32-bits in size. Subword scalar\nobjects in the source language should be promoted to 32-bit registers in PTX, or use .param state space byte arrays described next. Objects such as C structures and unions are flattened into registers or byte arrays in PTX and are\nrepresented using .param space memory. For example, consider the following C structure, passed\nby value to a function: struct {\n    double dbl;\n    char   c[4];\n}; In PTX, this structure will be flattened into a byte array. Since memory accesses are required to be\naligned to a multiple of the access size, the structure in this example will be a 12 byte array with\n8 byte alignment so that accesses to the .f64 field are aligned. The .param state space is\nused to pass the structure by value: Example .func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12])\n{\n    .reg .f64 f1;\n    .reg .b32 c1, c2, c3, c4;\n    ...\n    ld.param.f64 f1, [y+0];\n    ld.param.b8  c1, [y+8];\n    ld.param.b8  c2, [y+9];\n    ld.param.b8  c3, [y+10];\n    ld.param.b8  c4, [y+11];\n    ...\n    ... // computation using x,f1,c1,c2,c3,c4;\n}\n\n{\n     .param .b8 .align 8 py[12];\n     ...\n     st.param.b64 [py+ 0], %rd;\n     st.param.b8  [py+ 8], %rc1;\n     st.param.b8  [py+ 9], %rc2;\n     st.param.b8  [py+10], %rc1;\n     st.param.b8  [py+11], %rc2;\n     // scalar args in .reg space, byte array in .param space\n     call (%out), bar, (%x, py);\n     ... In this example, note that .param space variables are used in two ways. First, a .param variable y is used in function definition bar to represent a formal parameter. Second, a .param variable py is declared in the body of the calling function and used to set up the\nstructure being passed to bar. The following is a conceptual way to think about the .param state space use in device functions. For a caller, The .param state space is used to set values that will be passed to a called function and/or\nto receive return values from a called function. Typically, a .param byte array is used to\ncollect together fields of a structure being passed by value. For a callee, The .param state space is used to receive parameter values and/or pass return values back to\nthe caller. The following restrictions apply to parameter passing. For a caller, Arguments may be .param variables, .reg variables, or constants. In the case of .param space formal parameters that are byte arrays, the argument must also be\na .param space byte array with matching type, size, and alignment. A .param argument must\nbe declared within the local scope of the caller. In the case of .param space formal parameters that are base-type scalar or vector variables,\nthe corresponding argument may be either a .param or .reg space variable with matching\ntype and size, or a constant that can be represented in the type of the formal parameter. In the case of .reg space formal parameters, the corresponding argument may be either a .param or .reg space variable of matching type and size, or a constant that can be\nrepresented in the type of the formal parameter. In the case of .reg space formal parameters, the register must be at least 32-bits in size. All st.param instructions used for passing arguments to function call must immediately precede\nthe corresponding call instruction and ld.param instruction used for collecting return\nvalue must immediately follow the call instruction without any control flow\nalteration. st.param and ld.param instructions used for argument passing cannot be\npredicated. This enables compiler optimization and ensures that the .param variable does not\nconsume extra space in the caller’s frame beyond that needed by the ABI. The .param variable\nsimply allows a mapping to be made at the call site between data that may be in multiple\nlocations (e.g., structure being manipulated by caller is located in registers and memory) to\nsomething that can be passed as a parameter or return value to the callee. For a callee, Input and return parameters may be .param variables or .reg variables. Parameters in .param memory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes. Parameters in the .reg state space must be at least 32-bits in size. The .reg state space can be used to receive and return base-type scalar and vector values,\nincluding sub-word size objects when compiling in non-ABI mode. Supporting the .reg state\nspace provides legacy support. Note that the choice of .reg or .param state space for parameter passing has no impact on\nwhether the parameter is ultimately passed in physical registers or on the stack. The mapping of\nparameters to physical registers and stack locations depends on the ABI definition and the order,\nsize, and alignment of parameters. 7.1.1. Changes from PTX ISA Version 1.x  In PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no\nsupport for array parameters. Objects such as C structures were flattened and passed or returned\nusing multiple registers. PTX ISA version 1.x supports multiple return values for this purpose. Beginning with PTX ISA version 2.0, formal parameters may be in either .reg or .param state\nspace, and .param space parameters support arrays. For targets sm_20 or higher, PTX\nrestricts functions to a single return value, and a .param byte array should be used to return\nobjects that do not fit into a register. PTX continues to support multiple return registers for sm_1x targets. Note PTX implements a stack-based ABI only for targets sm_20 or higher. PTX ISA versions prior to 3.0 permitted variables in .reg and .local state spaces to be\ndefined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows\nmodule-scoped .reg and .local variables and restricts their use to within function\nscope. When compiling without use of the ABI, module-scoped .reg and .local variables are\nsupported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing\nmodule-scoped .reg or .local variables, the compiler silently disables use of the ABI. 7.2. Variadic Functions  Note Support for variadic functions which was unimplemented has been removed from the spec. PTX version 6.0 supports passing unsized array parameter to a function which can be used to\nimplement variadic functions. Refer to Kernel and Function Directives: .func for details 7.3. Alloca  PTX provides alloca instruction for allocating storage at runtime on the per-thread local memory\nstack. The allocated stack memory can be accessed with ld.local and st.local instructions\nusing the pointer returned by alloca . In order to facilitate deallocation of memory allocated with alloca , PTX provides two additional\ninstructions: stacksave which allows reading the value of stack pointer in a local variable, and stackrestore which can restore the stack pointer with the saved value. alloca , stacksave , and stackrestore instructions are described in Stack Manipulation\nInstructions . Preview Feature: Stack manipulation instructions alloca , stacksave and stackrestore are preview features\nin PTX ISA version 7.3. All details are subject to change with no guarantees of backward\ncompatibility on future PTX ISA versions or SM architectures. 8. Memory Consistency Model  In multi-threaded executions, the side-effects of memory operations performed by each thread become\nvisible to other threads in a partial and non-identical order. This means that any two operations\nmay appear to happen in no order, or in different orders, to different threads. The axioms\nintroduced by the memory consistency model specify exactly which contradictions are forbidden\nbetween the orders observed by different threads. In the absence of any constraint, each read operation returns the value committed by some write\noperation to the same memory location, including the initial write to that memory location. The\nmemory consistency model effectively constrains the set of such candidate writes from which a read\noperation can return a value. 8.1. Scope and applicability of the model  The constraints specified under this model apply to PTX programs with any PTX ISA version number,\nrunning on sm_70 or later architectures. The memory consistency model does not apply to texture (including ld.global.nc ) and surface\naccesses. 8.1.1. Limitations on atomicity at system scope  When communicating with the host CPU, certain strong operations with system scope may not be\nperformed atomically on some systems. For more details on atomicity guarantees to host memory, see\nthe CUDA Atomicity Requirements . 8.2. Memory operations  The fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state\nspace available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX\nstate space has a unique address relative to all threads that have access to the same state space. Each PTX memory instruction specifies an address operand and a data type. The address operand\ncontains a virtual address that gets converted to a physical address during memory access. The\nphysical address and the size of the data type together define a physical memory location, which is\nthe range of bytes starting from the physical address and extending up to the size of the data type\nin bytes. The memory consistency model specification uses the terms “address” or “memory address” to indicate\na virtual address, and the term “memory location” to indicate a physical memory location. Each PTX memory instruction also specifies the operation — either a read, a write or an atomic\nread-modify-write — to be performed on all the bytes in the corresponding memory location. 8.2.1. Overlap  Two memory locations are said to overlap when the starting address of one location is within the\nrange of bytes constituting the other location. Two memory operations are said to overlap when they\nspecify the same virtual address and the corresponding memory locations overlap. The overlap is said\nto be complete when both memory locations are identical, and it is said to be partial otherwise. 8.2.2. Aliases  Two distinct virtual addresses are said to be aliases if they map to the same memory location. 8.2.3. Multimem Addresses  A multimem address is a virtual address which points to multiple distinct memory locations across\ndevices. Only multimem. * operations are valid on multimem addresses. That is, the behavior of accessing\na multimem address in any other memory operation is undefined. 8.2.4. Memory Operations on Vector Data Types  The memory consistency model relates operations executed on memory locations with scalar data types,\nwhich have a maximum size and alignment of 64 bits. Memory operations with a vector data type are\nmodelled as a set of equivalent memory operations with a scalar data type, executed in an\nunspecified order on the elements in the vector. 8.2.5. Memory Operations on Packed Data Types  A packed data type consists of two values of the same scalar data type, as described in Packed Data\nTypes . These values are accessed in adjacent memory locations. A\nmemory operation on a packed data type is modelled as a pair of equivalent memory operations on the\nscalar data type, executed in an unspecified order on each element of the packed data. 8.2.6. Initialization  Each byte in memory is initialized by a hypothetical write W0 executed before starting any thread\nin the program. If the byte is included in a program variable, and that variable has an initial\nvalue, then W0 writes the corresponding initial value for that byte; else W0 is assumed to have\nwritten an unknown but constant value to the byte. 8.3. State spaces  The relations defined in the memory consistency model are independent of state spaces. In\nparticular, causality order closes over all memory operations across all the state spaces. But the\nside-effect of a memory operation in one state space can be observed directly only by operations\nthat also have access to the same state space. This further constrains the synchronizing effect of a\nmemory operation in addition to scope. For example, the synchronizing effect of the PTX instruction ld.relaxed.shared.sys is identical to that of ld.relaxed.shared.cluster , since no thread\noutside the same cluster can execute an operation that accesses the same memory location. 8.4. Operation types  For simplicity, the rest of the document refers to the following operation types, instead of\nmentioning specific instructions that give rise to them. Table 17 Operation Types  Operation Type Instruction/Operation atomic operation atom or red instruction. read operation All variants of ld instruction and atom instruction (but not red instruction). write operation All variants of st instruction, and atomic operations if they result\nin a write. memory operation A read or write operation. volatile operation An instruction with .volatile qualifier. acquire operation A memory operation with .acquire or .acq_rel qualifier. release operation A memory operation with .release or .acq_rel qualifier. mmio operation An ld or st instruction with .mmio qualifier. memory fence operation A membar , fence.sc or fence.acq_rel instruction. proxy fence operation A fence.proxy or a membar.proxy instruction. strong operation A memory fence operation, or a memory operation with a .relaxed , .acquire , .release , .acq_rel , .volatile , or .mmio qualifier. weak operation An ld or st instruction with a .weak qualifier. synchronizing operation A barrier instruction, fence operation, release operation or acquire operation. 8.4.1. mmio Operation  An mmio operation is a memory operation with .mmio qualifier specified. It is usually performed\non a memory location which is mapped to the control registers of peer I/O devices. It can also be\nused for communication between threads but has poor performance relative to non- mmio operations. The semantic meaning of mmio operations cannot be defined precisely as it is defined by the\nunderlying I/O device. For formal specification of semantics of mmio operation from Memory\nConsistency Model perspective, it is equivalent to the semantics of a strong operation. But it\nfollows a few implementation-specific properties, if it meets the CUDA atomicity requirements at\nthe specified scope: Writes are always performed and are never combined within the scope specified. Reads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any\ncache within the scope specified. As an exception, in some implementations, the surrounding locations may also be loaded. In such\ncases the amount of data loaded is implementation specific and varies between 32 and 128 bytes\nin size. 8.5. Scope  Each strong operation must specify a scope , which is the set of threads that may interact\ndirectly with that operation and establish any of the relations described in the memory consistency\nmodel. There are four scopes: Table 18 Scopes  Scope Description .cta The set of all threads executing in the same CTA as the current thread. .cluster The set of all threads executing in the same cluster as the current thread. .gpu The set of all threads in the current program executing on the same compute\ndevice as the current thread. This also includes other kernel grids invoked by\nthe host program on the same compute device. .sys The set of all threads in the current program, including all kernel grids\ninvoked by the host program on all compute devices, and all threads\nconstituting the host program itself. Note that the warp is not a scope ; the CTA is the smallest collection of threads that qualifies as\na scope in the memory consistency model. 8.6. Proxies  A memory proxy , or a proxy is an abstract label applied to a method of memory access. When two\nmemory operations use distinct methods of memory access, they are said to be different proxies . Memory operations as defined in Operation types use generic method of memory access, i.e. a generic proxy . Other operations such as textures and surfaces all\nuse distinct methods of memory access, also distinct from the generic method. A proxy fence is required to synchronize memory operations across different proxies . Although\nvirtual aliases use the generic method of memory access, since using distinct virtual addresses\nbehaves as if using different proxies , they require a proxy fence to establish memory ordering. 8.7. Morally strong operations  Two operations are said to be morally strong relative to each other if they satisfy all of the\nfollowing conditions: The operations are related in program order (i.e, they are both executed by the same thread),\nor each operation is strong and specifies a scope that includes the thread executing the\nother operation. Both operations are performed via the same proxy . If both are memory operations, then they overlap completely. Most (but not all) of the axioms in the memory consistency model depend on relations between morally strong operations. 8.7.1. Conflict and Data-races  Two overlapping memory operations are said to conflict when at least one of them is a write . Two conflicting memory operations are said to be in a data-race if they are not related in causality order and they are not morally strong . 8.7.2. Limitations on Mixed-size Data-races  A data-race between operations that overlap completely is called a uniform-size data-race ,\nwhile a data-race between operations that overlap partially is called a mixed-size data-race . The axioms in the memory consistency model do not apply if a PTX program contains one or more mixed-size data-races . But these axioms are sufficient to describe the behavior of a PTX program\nwith only uniform-size data-races . Atomicity of mixed-size RMW operations In any program with or without mixed-size data-races , the following property holds for every pair\nof overlapping atomic operations A1 and A2 such that each specifies a scope that includes the\nother: Either the read-modify-write operation specified by A1 is performed completely before A2 is\ninitiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2\noverlap partially or completely. 8.8. Release and Acquire Patterns  Some sequences of instructions give rise to patterns that participate in memory synchronization as\ndescribed later. The release pattern makes prior operations from the current thread 1 visible to some operations from other threads. The acquire pattern makes some operations from\nother threads visible to later operations from the current thread. A release pattern on a location M consists of one of the following: A release operation on M E.g.: st.release [M]; or atom.acq_rel [M]; or mbarrier.arrive.release [M]; Or a release operation on M followed by a strong write on M in program order E.g.: st.release [M] ; st.relaxed [M]; Or a memory fence followed by a strong write on M in program order E.g.: fence; st.relaxed [M]; Any memory synchronization established by a release pattern only affects operations occurring in program order before the first instruction in that pattern. An acquire pattern on a location M consists of one of the following: An acquire operation on M E.g.: ld.acquire [M]; or atom.acq_rel [M]; or mbarrier.test_wait.acquire [M]; Or a strong read on M followed by an acquire operation on M in program order E.g.: ld.relaxed [M]; ld.acquire [M]; Or a strong read on M followed by a memory fence in program order E.g.: ld.relaxed [M]; fence; Any memory synchronization established by an acquire pattern only affects operations occurring\nin program order after the last instruction in that pattern. 1 For both release and acquire patterns, this effect is further extended to operations in\nother threads through the transitive nature of causality order . 8.9. Ordering of memory operations  The sequence of operations performed by each thread is captured as program order while memory\nsynchronization across threads is captured as causality order . The visibility of the side-effects\nof memory operations to other memory operations is captured as communication order . The memory\nconsistency model defines contradictions that are disallowed between communication order on the one\nhand, and causality order and program order on the other. 8.9.1. Program Order  The program order relates all operations performed by a thread to the order in which a sequential\nprocessor will execute instructions in the corresponding PTX source. It is a transitive relation\nthat forms a total order over the operations performed by the thread, but does not relate operations\nfrom different threads. 8.9.1.1. Asynchronous Operations  Some PTX instructions (all variants of cp.async , cp.async.bulk , cp.reduce.async.bulk , wgmma.mma_async ) perform operations that are asynchronous to the thread that executed the\ninstruction. These asynchronous operations are ordered after prior instructions in the same thread\n(except in the case of wgmma.mma_async ), but they are not part of the program order for that\nthread. Instead, they provide weaker ordering guarantees as documented in the instruction\ndescription. For example, the loads and stores performed as part of a cp.async are ordered with respect to\neach other, but not to those of any other cp.async instructions initiated by the same thread,\nnor any other instruction subsequently issued by the thread with the exception of cp.async.commit_group or cp.async.mbarrier.arrive . The asynchronous mbarrier arrive-on operation\nperformed by a cp.async.mbarrier.arrive instruction is ordered with respect to the memory\noperations performed by all prior cp.async operations initiated by the same thread, but not to\nthose of any other instruction issued by the thread. The implicit mbarrier complete-tx operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk instructions is ordered only with respect to the memory operations performed by the same\nasynchronous instruction, and in particular it does not transitively establish ordering with respect\nto prior instructions from the issuing thread. 8.9.2. Observation Order  Observation order relates a write W to a read R through an optional sequence of atomic\nread-modify-write operations. A write W precedes a read R in observation order if: R and W are morally strong and R reads the value written by W, or For some atomic operation Z, W precedes Z and Z precedes R in observation order . 8.9.3. Fence-SC Order  The Fence-SC order is an acyclic partial order, determined at runtime, that relates every pair of morally strong fence.sc operations. 8.9.4. Memory synchronization  Synchronizing operations performed by different threads synchronize with each other at runtime as\ndescribed here. The effect of such synchronization is to establish causality order across threads. A fence.sc operation X synchronizes with a fence.sc operation Y if X precedes Y in the Fence-SC order. A bar{.cta}.sync or bar{.cta}.red or bar{.cta}.arrive operation synchronizes with a bar{.cta}.sync or bar{.cta}.red operation executed on the same barrier. A barrier.cluster.arrive operation synchronizes with a barrier.cluster.wait operation. A release pattern X synchronizes with an acquire pattern Y, if a write operation in X\nprecedes a read operation in Y in observation order , and the first operation in X and the\nlast operation in Y are morally strong . API synchronization A synchronizes relation can also be established by certain CUDA APIs. Completion of a task enqueued in a CUDA stream synchronizes with the start of the following\ntask in the same stream, if any. For purposes of the above, recording or waiting on a CUDA event in a stream, or causing a\ncross-stream barrier to be inserted due to cudaStreamLegacy , enqueues tasks in the associated\nstreams even if there are no direct side effects. An event record task synchronizes with\nmatching event wait tasks, and a barrier arrival task synchronizes with matching barrier wait\ntasks. Start of a CUDA kernel synchronizes with start of all threads in the kernel. End of all threads\nin a kernel synchronize with end of the kernel. Start of a CUDA graph synchronizes with start of all source nodes in the graph. Completion of\nall sink nodes in a CUDA graph synchronizes with completion of the graph. Completion of a graph\nnode synchronizes with start of all nodes with a direct dependency. Start of a CUDA API call to enqueue a task synchronizes with start of the task. Completion of the last task queued to a stream, if any, synchronizes with return from cudaStreamSynchronize . Completion of the most recently queued matching event record task, if\nany, synchronizes with return from cudaEventSynchronize . Synchronizing a CUDA device or\ncontext behaves as if synchronizing all streams in the context, including ones that have been\ndestroyed. Returning cudaSuccess from an API to query a CUDA handle, such as a stream or event, behaves\nthe same as return from the matching synchronization API. In addition to establishing a synchronizes relation, the CUDA API synchronization mechanisms above\nalso participate in proxy-preserved base causality order . 8.9.5. Causality Order  Causality order captures how memory operations become visible across threads through synchronizing\noperations. The axiom “Causality” uses this order to constrain the set of write operations from\nwhich a read operation may read a value. Relations in the causality order primarily consist of relations in Base causality order 1 , which is a transitive order, determined at runtime. Base causality order An operation X precedes an operation Y in base causality order if: X precedes Y in program order , or X synchronizes with Y, or For some operation Z, X precedes Z in program order and Z precedes Y in base causality order , or X precedes Z in base causality order and Z precedes Y in program order , or X precedes Z in base causality order and Z precedes Y in base causality order . Proxy-preserved base causality order A memory operation X precedes a memory operation Y in proxy-preserved base causality order if X\nprecedes Y in base causality order , and: X and Y are performed to the same address, using the generic proxy , or X and Y are performed to the same address, using the same proxy , and by the same thread block,\nor X and Y are aliases and there is an alias proxy fence along the base causality path from X\nto Y. Causality order Causality order combines base causality order with some non-transitive relations as follows: An operation X precedes an operation Y in causality order if: X precedes Y in proxy-preserved base causality order , or For some operation Z, X precedes Z in observation order, and Z precedes Y in proxy-preserved\nbase causality order . 1 The transitivity of base causality order accounts for the “cumulativity” of synchronizing\noperations. 8.9.6. Coherence Order  There exists a partial transitive order that relates overlapping write operations, determined at\nruntime, called the coherence order 1 . Two overlapping write operations are related in coherence order if they are morally strong or if they are related in causality order . Two overlapping writes are unrelated in coherence order if they are in a data-race , which gives\nrise to the partial nature of coherence order . 1 Coherence order cannot be observed directly since it consists entirely of write\noperations. It may be observed indirectly by its use in constraining the set of candidate\nwrites that a read operation may read from. 8.9.7. Communication Order  The communication order is a non-transitive order, determined at runtime, that relates write\noperations to other overlapping memory operations. A write W precedes an overlapping read R in communication order if R returns the value of any\nbyte that was written by W. A write W precedes a write W’ in communication order if W precedes W’ in coherence order . A read R precedes an overlapping write W in communication order if, for any byte accessed by\nboth R and W, R returns the value written by a write W’ that precedes W in coherence order . Communication order captures the visibility of memory operations — when a memory operation X1\nprecedes a memory operation X2 in communication order , X1 is said to be visible to X2. 8.10. Axioms  8.10.1. Coherence  If a write W precedes an overlapping write W’ in causality order , then W must precede W’ in coherence order . 8.10.2. Fence-SC  Fence-SC order cannot contradict causality order . For a pair of morally strong fence.sc operations F1 and F2, if F1 precedes F2 in causality order , then F1 must precede F2 in Fence-SC order. 8.10.3. Atomicity  Single-Copy Atomicity Conflicting morally strong operations are performed with single-copy atomicity . When a read R\nand a write W are morally strong , then the following two communications cannot both exist in the\nsame execution, for the set of bytes accessed by both R and W: R reads any byte from W. R reads any byte from any write W’ which precedes W in coherence order . Atomicity of read-modify-write (RMW) operations When an atomic operation A and a write W overlap and are morally strong , then the following\ntwo communications cannot both exist in the same execution, for the set of bytes accessed by both A\nand W: A reads any byte from a write W’ that precedes W in coherence order . A follows W in coherence order . Litmus Test 1: . global . u32 x = 0 ; T1 T2 A1 : atom . sys . inc . u32 % r0 , [ x ]; A2 : atom . sys . inc . u32 % r0 , [ x ]; FINAL STATE : x == 2 Atomicity is guaranteed when the operations are morally strong . Litmus Test 2: . global . u32 x = 0 ; T1 T2 (In a different CTA) A1 : atom . cta . inc . u32 % r0 , [ x ]; A2 : atom . gpu . inc . u32 % r0 , [ x ]; FINAL STATE : x == 1 OR x == 2 Atomicity is not guaranteed if the operations are not morally strong . 8.10.4. No Thin Air  Values may not appear “out of thin air”: an execution cannot speculatively produce a value in such a\nway that the speculation becomes self-satisfying through chains of instruction dependencies and\ninter-thread communication. This matches both programmer intuition and hardware reality, but is\nnecessary to state explicitly when performing formal analysis. Litmus Test: Load Buffering . global . u32 x = 0 ; . global . u32 y = 0 ; T1 T2 A1 : ld . global . u32 % r0 , [ x ]; B1 : st . global . u32 [ y ], % r0 ; A2 : ld . global . u32 % r1 , [ y ]; B2 : st . global . u32 [ x ], % r1 ; FINAL STATE : x == 0 AND y == 0 The litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of\nthin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a\nsecond variable, with the first and second variable exchanged between the threads. If each variable\nis initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1,\nthen values passing through the memory operations in this example form a cycle:\nA1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of\nthe memory operations in this example were to speculatively associate a different value with the\ncorresponding memory location, then such a speculation would become self-fulfilling, and hence\nforbidden. 8.10.5. Sequential Consistency Per Location  Within any set of overlapping memory operations that are pairwise morally strong , communication\norder cannot contradict program order , i.e., a concatenation of program order between overlapping operations and morally strong relations in communication order cannot result in a\ncycle. This ensures that each program slice of overlapping pairwise morally strong operations is\nstrictly sequentially-consistent . Litmus Test: CoRR . global . u32 x = 0 ; T1 T2 W1 : st . global . relaxed . sys . u32 [ x ], 1 ; R1 : ld . global . relaxed . sys . u32 % r0 , [ x ]; R2 : ld . global . relaxed . sys . u32 % r1 , [ x ]; IF % r0 == 1 THEN % r1 == 1 The litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee. A\nthread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence\nof) reads R1 and R2 on the same location x. No other writes are executed on x, except the one\nmodelling the initial value. The operations W1, R1 and R2 are pairwise morally strong . If R1 reads\nfrom W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial\nvalue of x instead, then this would form a sequence of morally-strong relations R2->W1->R1 in communication order that contradicts the program order R1->R2 in thread T2. Hence R2 cannot read\nthe initial value of x in such an execution. 8.10.6. Causality  Relations in communication order cannot contradict causality order . This constrains the set of\ncandidate write operations that a read operation may read from: If a read R precedes an overlapping write W in causality order , then R cannot read from W. If a write W precedes an overlapping read R in causality order , then for any byte accessed by\nboth R and W, R cannot read from any write W’ that precedes W in coherence order . Litmus Test: Message Passing . global . u32 data = 0 ; . global . u32 flag = 0 ; T1 T2 W1 : st . global . u32 [ data ], 1 ; F1 : fence . sys ; W2 : st . global . relaxed . sys . u32 [ flag ], 1 ; R1 : ld . global . relaxed . sys . u32 % r0 , [ flag ]; F2 : fence . sys ; R2 : ld . global . u32 % r1 , [ data ]; IF % r0 == 1 THEN % r1 == 1 The litmus test known as “MP” (Message Passing) represents the essence of typical synchronization\nalgorithms. A vast majority of useful programs can be reduced to sequenced applications of this\npattern. Thread T1 first writes to a data variable and then to a flag variable while a second thread T2 first\nreads from the flag variable and then from the data variable. The operations on the flag are morally strong and the memory operations in each thread are separated by a fence , and these fences are morally strong . If R1 observes W2, then the release pattern “F1; W2” synchronizes with the acquire pattern “R1;\nF2”. This establishes the causality order W1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom causality guarantees that R2 cannot read from any write that precedes W1 in coherence order . In the absence\nof any other writes in this example, R2 must read from W1. Litmus Test: CoWR // These addresses are aliases . global . u32 data_alias_1 ; . global . u32 data_alias_2 ; T1 W1 : st . global . u32 [ data_alias_1 ], 1 ; F1 : fence . proxy . alias ; R1 : ld . global . u32 % r1 , [ data_alias_2 ]; % r1 == 1 Virtual aliases require an alias proxy fence along the synchronization path. Litmus Test: Store Buffering The litmus test known as “SB” (Store Buffering) demonstrates the sequential consistency enforced\nby the fence.sc . A thread T1 writes to a first variable, and then reads the value of a second\nvariable, while a second thread T2 writes to the second variable and then reads the value of the\nfirst variable. The memory operations in each thread are separated by fence. sc instructions,\nand these fences are morally strong . . global . u32 x = 0 ; . global . u32 y = 0 ; T1 T2 W1 : st . global . u32 [ x ], 1 ; F1 : fence . sc . sys ; R1 : ld . global . u32 % r0 , [ y ]; W2 : st . global . u32 [ y ], 1 ; F2 : fence . sc . sys ; R2 : ld . global . u32 % r1 , [ x ]; % r0 == 1 OR % r1 == 1 In any execution, either F1 precedes F2 in Fence-SC order, or vice versa. If F1 precedes F2 in Fence-SC order, then F1 synchronizes with F2. This establishes the causality order in W1 -> F1\n-> F2 -> R2. Axiom causality ensures that R2 cannot read from any write that precedes W1 in coherence order . In the absence of any other write to that variable, R2 must read from\nW1. Similarly, in the case where F2 precedes F1 in Fence-SC order, R1 must read from W2. If each fence.sc in this example were replaced by a fence.acq_rel instruction, then this outcome is\nnot guaranteed. There may be an execution where the write from each thread remains unobserved from\nthe other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0”\nfor variables y and x respectively. 9. Instruction Set  9.1. Format and Semantics of Instruction Descriptions  This section describes each PTX instruction. In addition to the name and the format of the\ninstruction, the semantics are described, followed by some examples that attempt to show several\npossible instantiations of the instruction. 9.2. PTX Instructions  PTX instructions generally have from zero to four operands, plus an optional guard predicate\nappearing after an @ symbol to the left of the opcode : @p opcode; @p opcode a; @p opcode d, a; @p opcode d, a, b; @p opcode d, a, b, c; For instructions that create a result value, the d operand is the destination operand, while a , b , and c are source operands. The setp instruction writes two destination registers. We use a | symbol to separate\nmultiple destination registers. setp.lt.s32  p|q, a, b;  // p = (a < b); q = !(a < b); For some instructions the destination operand is optional. A bit bucket operand denoted with an\nunderscore ( _ ) may be used in place of a destination register. 9.3. Predicated Execution  In PTX, predicate registers are virtual and have .pred as the type specifier. So, predicate\nregisters can be declared as .reg .pred p, q, r; All instructions have an optional guard predicate which controls conditional execution of the\ninstruction. The syntax to specify conditional execution is to prefix an instruction with @{!}p ,\nwhere p is a predicate variable, optionally negated. Instructions without a guard predicate are\nexecuted unconditionally. Predicates are most commonly set as the result of a comparison performed by the setp instruction. As an example, consider the high-level code if (i < n)\n    j = j + 1; This can be written in PTX as setp.lt.s32  p, i, n;    // p = (i < n)\n@p    add.s32      j, j, 1;    // if i < n, add 1 to j To get a conditional branch or conditional function call, use a predicate to control the execution\nof the branch or call instructions. To implement the above example as a true conditional branch, the\nfollowing PTX instruction sequence might be used: setp.lt.s32  p, i, n;    // compare i to n\n@!p   bra  L1;                 // if False, branch over\n      add.s32      j, j, 1;\nL1:     ... 9.3.1. Comparisons  9.3.1.1. Integer and Bit-Size Comparisons  The signed integer comparisons are the traditional eq (equal), ne (not-equal), lt (less-than), le (less-than-or-equal), gt (greater-than), and ge (greater-than-or-equal). The unsigned comparisons are eq , ne , lo (lower), ls (lower-or-same), hi (higher), and hs (higher-or-same). The bit-size comparisons are eq and ne ; ordering comparisons are not defined for bit-size types. Table 19 shows the operators for signed integer, unsigned integer, and bit-size types. Table 19 Operators for Signed Integer, Unsigned Integer, and Bit-Size Types  Meaning Signed Operator Unsigned Operator Bit-Size Operator a == b eq eq eq a != b ne ne ne a < b lt lo n/a a <= b le ls n/a a > b gt hi n/a a >= b ge hs n/a 9.3.1.2. Floating Point Comparisons  The ordered floating-point comparisons are eq , ne , lt , le , gt , and ge . If\neither operand is NaN , the result is False . Table 20 lists the floating-point\ncomparison operators. Table 20 Floating-Point Comparison Operators  Meaning Floating-Point Operator a == b && !isNaN(a) && !isNaN(b) eq a != b && !isNaN(a) && !isNaN(b) ne a < b && !isNaN(a) && !isNaN(b) lt a <= b && !isNaN(a) && !isNaN(b) le a > b && !isNaN(a) && !isNaN(b) gt a >= b && !isNaN(a) && !isNaN(b) ge To aid comparison operations in the presence of NaN values, unordered floating-point comparisons\nare provided: equ , neu , ltu , leu , gtu , and geu . If both operands are numeric\nvalues (not NaN ), then the comparison has the same result as its ordered counterpart. If either\noperand is NaN , then the result of the comparison is True . Table 21 lists the floating-point\ncomparison operators accepting NaN values. Table 21 Floating-Point Comparison Operators Accepting NaN  Meaning Floating-Point Operator a == b || isNaN(a) || isNaN(b) equ a != b || isNaN(a) || isNaN(b) neu a < b || isNaN(a) || isNaN(b) ltu a <= b || isNaN(a) || isNaN(b) leu a > b || isNaN(a) || isNaN(b) gtu a >= b || isNaN(a) || isNaN(b) geu To test for NaN values, two operators num ( numeric ) and nan ( isNaN ) are\nprovided. num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Table 22 lists the\nfloating-point comparison operators testing for NaN values. Table 22 Floating-Point Comparison Operators Testing for NaN  Meaning Floating-Point Operator !isNaN(a) && !isNaN(b) num isNaN(a) || isNaN(b) nan 9.3.2. Manipulating Predicates  Predicate values may be computed and manipulated using the following instructions: and , or , xor , not , and mov . There is no direct conversion between predicates and integer values, and no direct way to load or\nstore predicate register values. However, setp can be used to generate a predicate from an\ninteger, and the predicate-based select ( selp ) instruction can be used to generate an integer\nvalue based on the value of a predicate; for example: selp.u32 %r1,1,0,%p;    // convert predicate to 32-bit value 9.4. Type Information for Instructions and Operands  Typed instructions must have a type-size modifier. For example, the add instruction requires\ntype and size information to properly perform the addition operation (signed, unsigned, float,\ndifferent sizes), and this information must be specified as a suffix to the opcode. Example .reg .u16 d, a, b;\n\nadd.u16 d, a, b;    // perform a 16-bit unsigned add Some instructions require multiple type-size modifiers, most notably the data conversion instruction cvt . It requires separate type-size modifiers for the result and source, and these are placed in\nthe same order as the operands. For example: .reg .u16 a;\n.reg .f32 d;\n\ncvt.f32.u16 d, a;   // convert 16-bit unsigned to 32-bit float In general, an operand’s type must agree with the corresponding instruction-type modifier. The rules\nfor operand and instruction type conformance are as follows: Bit-size types agree with any type of the same size. Signed and unsigned integer types agree provided they have the same size, and integer operands are\nsilently cast to the instruction type if needed. For example, an unsigned integer operand used in\na signed integer instruction will be treated as a signed integer by the instruction. Floating-point types agree only if they have the same size; i.e., they must match exactly. Table 23 summarizes these type\nchecking rules. Table 23 Type Checking Rules  Operand Type .bX .sX .uX .fX Instruction Type .bX okay okay okay okay .sX okay okay okay invalid .uX okay okay okay invalid .fX okay invalid invalid okay Note Some operands have their type and size defined independently from the instruction type-size. For\nexample, the shift amount operand for left and right shift instructions always has type .u32 ,\nwhile the remaining operands have their type and size determined by the instruction type. Example // 64-bit arithmetic right shift; shift amount 'b' is .u32\n    shr.s64 d,a,b; 9.4.1. Operand Size Exceeding Instruction-Type Size  For convenience, ld , st , and cvt instructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned)\ninstruction types; floating-point instruction types still require that the operand type-size matches\nexactly, unless the operand is of bit-size type. When a source operand has a size that exceeds the instruction-type size, the source data is\ntruncated (chopped) to the appropriate number of bits specified by the instruction type-size. Table 24 summarizes the relaxed type-checking rules for source operands. Note that some combinations may\nstill be invalid for a particular instruction; for example, the cvt instruction does not support .bX instruction types, so those rows are invalid for cvt . Table 24 Relaxed Type-checking Rules for Source Operands  Source Operand Type b8 b16 b32 b64 b128 s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Instruction Type b8 – chop chop chop chop – chop chop chop – chop chop chop chop chop chop b16 inv – chop chop chop inv – chop chop inv – chop chop – chop chop b32 inv inv – chop chop inv inv – chop inv inv – chop inv – chop b64 inv inv inv – chop inv inv inv – inv inv inv – inv inv – b128 inv inv inv inv – inv inv inv inv inv inv inv inv inv inv inv s8 – chop chop chop chop – chop chop chop – chop chop chop inv inv inv s16 inv – chop chop chop inv – chop chop inv – chop chop inv inv inv s32 inv inv – chop chop inv inv – chop inv inv – chop inv inv inv s64 inv inv inv – chop inv inv inv – inv inv inv – inv inv inv u8 – chop chop chop chop – chop chop chop – chop chop chop inv inv inv u16 inv – chop chop chop inv – chop chop inv – chop chop inv inv inv u32 inv inv – chop chop inv inv – chop inv inv – chop inv inv inv u64 inv inv inv – chop inv inv inv – inv inv inv – inv inv inv f16 inv – chop chop chop inv inv inv inv inv inv inv inv – inv inv f32 inv inv – chop chop inv inv inv inv inv inv inv inv inv – inv f64 inv inv inv – chop inv inv inv inv inv inv inv inv inv inv – Notes chop = keep only low bits that fit; “–” = allowed, but no conversion needed; inv = invalid, parse error. Source register size must be of equal or greater size than the instruction-type size. Bit-size source registers may be used with any appropriately-sized instruction type. The data are\ntruncated (“chopped”) to the instruction-type size and interpreted according to the instruction\ntype. Integer source registers may be used with any appropriately-sized bit-size or integer instruction\ntype. The data are truncated to the instruction-type size and interpreted according to the\ninstruction type. Floating-point source registers can only be used with bit-size or floating-point instruction types.\nWhen used with a narrower bit-size instruction type, the data are truncated. When used with a\nfloating-point instruction type, the size must match exactly. When a destination operand has a size that exceeds the instruction-type size, the destination data\nis zero- or sign-extended to the size of the destination register. If the corresponding instruction\ntype is signed integer, the data is sign-extended; otherwise, the data is zero-extended. Table 25 summarizes the relaxed type-checking rules for destination operands. Table 25 Relaxed Type-checking Rules for Destination Operands  Destination Operand Type b8 b16 b32 b64 b128 s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Instruction Type b8 – zext zext zext zext – zext zext zext – zext zext zext zext zext zext b16 inv – zext zext zext inv – zext zext inv – zext zext – zext zext b32 inv inv – zext zext inv inv – zext inv inv – zext inv – zext b64 inv inv inv – zext inv inv inv – inv inv inv – inv inv – b128 inv inv inv inv – inv inv inv inv inv inv inv inv inv inv inv s8 – sext sext sext sext – sext sext sext – sext sext sext inv inv inv s16 inv – sext sext sext inv – sext sext inv – sext sext inv inv inv s32 inv inv – sext sext inv inv – sext inv inv – sext inv inv inv s64 inv inv inv – sext inv inv inv – inv inv inv – inv inv inv u8 – zext zext zext zext – zext zext zext – zext zext zext inv inv inv u16 inv – zext zext zext inv – zext zext inv – zext zext inv inv inv u32 inv inv – zext zext inv inv – zext inv inv – zext inv inv inv u64 inv inv inv – zext inv inv inv – inv inv inv – inv inv inv f16 inv – zext zext zext inv inv inv inv inv inv inv inv – inv inv f32 inv inv – zext zext inv inv inv inv inv inv inv inv inv – inv f64 inv inv inv – zext inv inv inv inv inv inv inv inv inv inv – Notes sext = sign-extend; zext = zero-extend; “–” = allowed, but no conversion needed; inv = invalid, parse error. Destination register size must be of equal or greater size than the instruction-type size. Bit-size destination registers may be used with any appropriately-sized instruction type. The data\nare sign-extended to the destination register width for signed integer instruction types, and are\nzero-extended to the destination register width otherwise. Integer destination registers may be used with any appropriately-sized bit-size or integer\ninstruction type. The data are sign-extended to the destination register width for signed integer\ninstruction types, and are zero-extended to the destination register width for bit-size an d\nunsigned integer instruction types. Floating-point destination registers can only be used with bit-size or floating-point instruction\ntypes. When used with a narrower bit-size instruction type, the data are zero-extended. When used\nwith a floating-point instruction type, the size must match exactly. 9.5. Divergence of Threads in Control Constructs  Threads in a CTA execute together, at least in appearance, until they come to a conditional control\nconstruct such as a conditional branch, conditional function call, or conditional return. If threads\nexecute down different control flow paths, the threads are called divergent . If all of the threads\nact in unison and follow a single control flow path, the threads are called uniform . Both\nsituations occur often in programs. A CTA with divergent threads may have lower performance than a CTA with uniformly executing threads,\nso it is important to have divergent threads re-converge as soon as possible. All control constructs\nare assumed to be divergent points unless the control-flow instruction is marked as uniform, using\nthe .uni suffix. For divergent control flow, the optimizing code generator automatically\ndetermines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore\nthe issue of divergent threads, but has the opportunity to improve performance by marking branch\npoints as uniform when the compiler or author can guarantee that the branch point is non-divergent. 9.6. Semantics  The goal of the semantic description of an instruction is to describe the results in all cases in as\nsimple language as possible. The semantics are described using C, until C is not expressive enough. 9.6.1. Machine-Specific Semantics of 16-bit Code  A PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a\n32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit\ncomputations are promoted to 32-bit computations. This can lead to computational differences\nbetween code run on a 16-bit machine versus the same code run on a 32-bit machine, since the\npromoted computation may have bits in the high-order half-word of registers that are not present in\n16-bit physical registers. These extra precision bits can become visible at the application level,\nfor example, by a right-shift instruction. At the PTX language level, one solution would be to define semantics for 16-bit code that is\nconsistent with execution on a 16-bit data path. This approach introduces a performance penalty for\n16-bit code executing on a 32-bit data path, since the translated code would require many additional\nmasking instructions to suppress extra precision bits in the high-order half-word of 32-bit\nregisters. Rather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of\n16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce\nportable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at\nappropriate points in the program to guarantee portability of the code. However, for many\nperformance-critical applications, this is not desirable, and for many applications the difference\nin execution is preferable to limiting performance. 9.7. Instructions  All PTX instructions may be predicated. In the following descriptions, the optional guard predicate\nis omitted from the syntax. 9.7.1. Integer Arithmetic Instructions  Integer arithmetic instructions operate on the integer types in register and constant immediate\nforms. The integer arithmetic instructions are: add sub mul mad mul24 mad24 sad div rem abs neg min max popc clz bfind fns brev bfe bfi bmsk szext dp4a dp2a 9.7.1.1. Integer Arithmetic Instructions: add  add Add two values. Syntax add.type       d, a, b;\nadd{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64,\n          .u16x2, .s16x2 }; Description Performs addition and writes the resulting value into a destination register. For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce .u16x2 , .s16x2 result in\ndestination. Operands d , a and b have type .type . For instruction types .u16x2 , .s16x2 ,\noperands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = iA[i] + iB[i];\n    }\n} else {\n    d = a + b;\n} Notes Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type. PTX ISA Notes Introduced in PTX ISA version 1.0. add.u16x2 and add.s16x2 introduced in PTX ISA version 8.0. Target ISA Notes Supported on all target architectures. add.u16x2 and add.s16x2 require sm_90 or higher. Examples @p  add.u32     x,y,z;\n    add.sat.s32 c,c,1;\n    add.u16x2   u,v,w; 9.7.1.2. Integer Arithmetic Instructions: sub  sub Subtract one value from another. Syntax sub.type       d, a, b;\nsub{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Performs subtraction and writes the resulting value into a destination register. Semantics d = a - b; Notes Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples sub.s32 c,a,b; 9.7.1.3. Integer Arithmetic Instructions: mul  mul Multiply two values. Syntax mul.mode.type  d, a, b;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Compute the product of two values. Semantics t = a * b;\nn = bitwidth of type;\nd = t;            // for .wide\nd = t<2n-1..n>;   // for .hi variant\nd = t<n-1..0>;    // for .lo variant Notes The type of the operation represents the types of the a and b operands. If .hi or .lo is specified, then d is the same size as a and b , and either the upper or lower\nhalf of the result is written to the destination register. If .wide is specified, then d is\ntwice as wide as a and b to receive the full result of the multiplication. The .wide suffix is supported only for 16- and 32-bit integer types. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mul.wide.s16 fa,fxs,fys;   // 16*16 bits yields 32 bits\nmul.lo.s16 fa,fxs,fys;     // 16*16 bits, save only the low 16 bits\nmul.wide.s32 z,x,y;        // 32*32 bits, creates 64 bit result 9.7.1.4. Integer Arithmetic Instructions: mad  mad Multiply two values, optionally extract the high or low half of the intermediate result, and add a third value. Syntax mad.mode.type  d, a, b, c;\nmad.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Multiplies two values, optionally extracts the high or low half of the intermediate result, and adds\na third value. Writes the result into a destination register. Semantics t = a * b;\nn = bitwidth of type;\nd = t + c;           // for .wide\nd = t<2n-1..n> + c;  // for .hi variant\nd = t<n-1..0> + c;   // for .lo variant Notes The type of the operation represents the types of the a and b operands. If .hi or .lo is\nspecified, then d and c are the same size as a and b , and either the upper or lower\nhalf of the result is written to the destination register. If .wide is specified, then d and c are twice as wide as a and b to receive the result of the multiplication. The .wide suffix is supported only for 16-bit and 32-bit integer types. Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type in .hi mode. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples @p  mad.lo.s32 d,a,b,c;\n    mad.lo.s32 r,p,q,r; 9.7.1.5. Integer Arithmetic Instructions: mul24  mul24 Multiply two 24-bit integer values. Syntax mul24.mode.type  d, a, b;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and return either\nthe high or low 32-bits of the 48-bit result. Semantics t = a * b;\nd = t<47..16>;    // for .hi variant\nd = t<31..0>;     // for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits. mul24.hi performs a 24x24-bit multiply and returns the high 32 bits of the 48-bit result. mul24.lo performs a 24x24-bit multiply and returns the low 32 bits of the 48-bit result. All operands are of the same type and size. mul24.hi may be less efficient on machines without hardware support for 24-bit multiply. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mul24.lo.s32 d,a,b;   // low 32-bits of 24x24-bit signed multiply. 9.7.1.6. Integer Arithmetic Instructions: mad24  mad24 Multiply two 24-bit integer values and add a third value. Syntax mad24.mode.type  d, a, b, c;\nmad24.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and add a third,\n32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low\n32-bits of the 48-bit result. Semantics t = a * b;\nd = t<47..16> + c;   // for .hi variant\nd = t<31..0> + c;    // for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits. mad24.hi performs a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third\nvalue. mad24.lo performs a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third\nvalue. All operands are of the same type and size. Saturation modifier: .sat limits result of 32-bit signed addition to MININT..MAXINT (no overflow). Applies only to .s32 type in .hi mode. mad24.hi may be less efficient on machines without hardware support for 24-bit multiply. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mad24.lo.s32 d,a,b,c;   // low 32-bits of 24x24-bit signed multiply. 9.7.1.7. Integer Arithmetic Instructions: sad  sad Sum of absolute differences. Syntax sad.type  d, a, b, c;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Adds the absolute value of a-b to c and writes the resulting value into d . Semantics d = c + ((a<b) ? b-a : a-b); PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples sad.s32  d,a,b,c;\nsad.u32  d,a,b,d;  // running sum 9.7.1.8. Integer Arithmetic Instructions: div  div Divide one value by another. Syntax div.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Divides a by b , stores result in d . Semantics d = a / b; Notes Division by zero yields an unspecified, machine-specific value. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples div.s32  b,n,i; 9.7.1.9. Integer Arithmetic Instructions: rem  rem The remainder of integer division. Syntax rem.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Divides a by b , store the remainder in d . Semantics d = a % b; Notes The behavior for negative numbers is machine-dependent and depends on whether divide rounds towards\nzero or negative infinity. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples rem.s32  x,x,8;    // x = x%8; 9.7.1.10. Integer Arithmetic Instructions: abs  abs Absolute value. Syntax abs.type  d, a;\n\n.type = { .s16, .s32, .s64 }; Description Take the absolute value of a and store it in d . Semantics d = |a|; Notes Only for signed integers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples abs.s32  r0,a; 9.7.1.11. Integer Arithmetic Instructions: neg  neg Arithmetic negate. Syntax neg.type  d, a;\n\n.type = { .s16, .s32, .s64 }; Description Negate the sign of a and store the result in d . Semantics d = -a; Notes Only for signed integers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples neg.s32  r0,a; 9.7.1.12. Integer Arithmetic Instructions: min  min Find the minimum of two values. Syntax min.atype         d, a, b;\nmin{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 }; Description Store the minimum of a and b in d . For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce .u16x2 , .s16x2 result\nin destination. Operands d , a and b have the same type as the instruction type. For instruction types .u16x2 , .s16x2 , operands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] < iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a < b) ? a : b; // Integer (signed and unsigned)\n} Notes Signed and unsigned differ. Saturation modifier: min.relu.{s16x2, s32} clamps the result to 0 if negative. PTX ISA Notes Introduced in PTX ISA version 1.0. min.u16x2 , min{.relu}.s16x2 and min.relu.s32 introduced in PTX ISA version 8.0. Target ISA Notes Supported on all target architectures. min.u16x2 , min{.relu}.s16x2 and min.relu.s32 require sm_90 or higher. Examples min.s32  r0,a,b;\n@p  min.u16  h,i,j;\n    min.s16x2.relu u,v,w; 9.7.1.13. Integer Arithmetic Instructions: max  max Find the maximum of two values. Syntax max.atype         d, a, b;\nmax{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 }; Description Store the maximum of a and b in d . For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce .u16x2 , .s16x2 result\nin destination. Operands d , a and b have the same type as the instruction type. For instruction types .u16x2 , .s16x2 , operands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] > iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a > b) ? a : b; // Integer (signed and unsigned)\n} Notes Signed and unsigned differ. Saturation modifier: max.relu.{s16x2, s32} clamps the result to 0 if negative. PTX ISA Notes Introduced in PTX ISA version 1.0. max.u16x2 , max{.relu}.s16x2 and max.relu.s32 introduced in PTX ISA version 8.0. Target ISA Notes Supported on all target architectures. max.u16x2 , max{.relu}.s16x2 and max.relu.s32 require sm_90 or higher. Examples max.u32  d,a,b;\nmax.s32  q,q,0;\nmax.relu.s16x2 t,t,u; 9.7.1.14. Integer Arithmetic Instructions: popc  popc Population count. Syntax popc.type  d, a;\n\n.type = { .b32, .b64 }; Description Count the number of one bits in a and place the resulting population count in 32-bit\ndestination register d . Operand a has the instruction type and destination d has type .u32 . Semantics .u32  d = 0;\nwhile (a != 0) {\n   if (a & 0x1)  d++;\n   a = a >> 1;\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes popc requires sm_20 or higher. Examples popc.b32  d, a;\npopc.b64  cnt, X;  // cnt is .u32 9.7.1.15. Integer Arithmetic Instructions: clz  clz Count leading zeros. Syntax clz.type  d, a;\n\n.type = { .b32, .b64 }; Description Count the number of leading zeros in a starting with the most-significant bit and place the\nresult in 32-bit destination register d . Operand a has the instruction type, and destination d has type .u32 . For .b32 type, the number of leading zeros is between 0 and 32,\ninclusively. For .b64 type, the number of leading zeros is between 0 and 64, inclusively. Semantics .u32  d = 0;\nif (.type == .b32)   { max = 32; mask = 0x80000000; }\nelse                 { max = 64; mask = 0x8000000000000000; }\n\nwhile (d < max && (a&mask == 0) ) {\n    d++;\n    a = a << 1;\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes clz requires sm_20 or higher. Examples clz.b32  d, a;\nclz.b64  cnt, X;  // cnt is .u32 9.7.1.16. Integer Arithmetic Instructions: bfind  bfind Find most significant non-sign bit. Syntax bfind.type           d, a;\nbfind.shiftamt.type  d, a;\n\n.type = { .u32, .u64,\n          .s32, .s64 }; Description Find the bit position of the most significant non-sign bit in a and place the result in d . Operand a has the instruction type, and destination d has type .u32 . For unsigned\nintegers, bfind returns the bit position of the most significant 1 . For signed integers, bfind returns the bit position of the most significant 0 for negative inputs and the most\nsignificant 1 for non-negative inputs. If .shiftamt is specified, bfind returns the shift amount needed to left-shift the found bit\ninto the most-significant bit position. bfind returns 0xffffffff if no non-sign bit is found. Semantics msb = (.type==.u32 || .type==.s32) ? 31 : 63;\n// negate negative signed inputs\nif ( (.type==.s32 || .type==.s64) && (a & (1<<msb)) ) {\n    a = ~a;\n}\n.u32  d = 0xffffffff;\nfor (.s32 i=msb; i>=0; i--) {\n    if (a & (1<<i))  { d = i; break; }\n}\nif (.shiftamt && d != 0xffffffff)  { d = msb - d; } PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes bfind requires sm_20 or higher. Examples bfind.u32  d, a;\nbfind.shiftamt.s64  cnt, X;  // cnt is .u32 9.7.1.17. Integer Arithmetic Instructions: fns  fns Find the n-th set bit Syntax fns.b32 d, mask, base, offset; Description Given a 32-bit value mask and an integer value base (between 0 and 31), find the n-th (given\nby offset) set bit in mask from the base bit, and store the bit position in d . If not\nfound, store 0xffffffff in d . Operand mask has a 32-bit type. Operand base has .b32 , .u32 or .s32 type. Operand offset has .s32 type. Destination d has type .b32. Operand base must be <= 31, otherwise behavior is undefined. Semantics d = 0xffffffff;\nif (offset == 0) {\n    if (mask[base] == 1) {\n        d = base;\n    }\n} else {\n    pos = base;\n    count = |offset| - 1;\n    inc = (offset > 0) ? 1 : -1;\n\n    while ((pos >= 0) && (pos < 32)) {\n        if (mask[pos] == 1) {\n            if (count == 0) {\n              d = pos;\n              break;\n           } else {\n               count = count – 1;\n           }\n        }\n        pos = pos + inc;\n    }\n} PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes fns requires sm_30 or higher. Examples fns.b32 d, 0xaaaaaaaa, 3, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 3, -1;  // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, -1;  // d = 1 9.7.1.18. Integer Arithmetic Instructions: brev  brev Bit reverse. Syntax brev.type  d, a;\n\n.type = { .b32, .b64 }; Description Perform bitwise reversal of input. Semantics msb = (.type==.b32) ? 31 : 63;\n\nfor (i=0; i<=msb; i++) {\n    d[i] = a[msb-i];\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes brev requires sm_20 or higher. Examples brev.b32  d, a; 9.7.1.19. Integer Arithmetic Instructions: bfe  bfe Bit Field Extract. Syntax bfe.type  d, a, b, c;\n\n.type = { .u32, .u64,\n          .s32, .s64 }; Description Extract bit field from a and place the zero or sign-extended result in d . Source b gives\nthe bit field starting bit position, and source c gives the bit field length in bits. Operands a and d have the same type as the instruction type. Operands b and c are\ntype .u32 , but are restricted to the 8-bit value range 0..255 . The sign bit of the extracted field is defined as: .u32 , .u64 : zero .s32 , .s64 : msb of input a if the extracted field extends beyond the msb of a msb of extracted\nfield, otherwise If the bit field length is zero, the result is zero. The destination d is padded with the sign bit of the extracted field. If the start position is\nbeyond the msb of the input, the destination d is filled with the replicated sign bit of the\nextracted field. Semantics msb = (.type==.u32 || .type==.s32) ? 31 : 63;\npos = b & 0xff;  // pos restricted to 0..255 range\nlen = c & 0xff;  // len restricted to 0..255 range\n\nif (.type==.u32 || .type==.u64 || len==0)\n    sbit = 0;\nelse\n    sbit = a[min(pos+len-1,msb)];\n\nd = 0;\nfor (i=0; i<=msb; i++) {\n    d[i] = (i<len && pos+i<=msb) ? a[pos+i] : sbit;\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes bfe requires sm_20 or higher. Examples bfe.b32  d,a,start,len; 9.7.1.20. Integer Arithmetic Instructions: bfi  bfi Bit Field Insert. Syntax bfi.type  f, a, b, c, d;\n\n.type = { .b32, .b64 }; Description Align and insert a bit field from a into b , and place the result in f . Source c gives the starting bit position for the insertion, and source d gives the bit field length in\nbits. Operands a , b , and f have the same type as the instruction type. Operands c and d are type .u32 , but are restricted to the 8-bit value range 0..255 . If the bit field length is zero, the result is b . If the start position is beyond the msb of the input, the result is b . Semantics msb = (.type==.b32) ? 31 : 63;\npos = c & 0xff;  // pos restricted to 0..255 range\nlen = d & 0xff;  // len restricted to 0..255 range\n\nf = b;\nfor (i=0; i<len && pos+i<=msb; i++) {\n    f[pos+i] = a[i];\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes bfi requires sm_20 or higher. Examples bfi.b32  d,a,b,start,len; 9.7.1.21. Integer Arithmetic Instructions: szext  szext Sign-extend or Zero-extend. Syntax szext.mode.type  d, a, b;\n\n.mode = { .clamp, .wrap };\n.type = { .u32, .s32 }; Description Sign-extends or zero-extends an N-bit value from operand a where N is specified in operand b . The resulting value is stored in the destination operand d . For the .s32 instruction type, the value in a is treated as an N-bit signed value and the\nmost significant bit of this N-bit value is replicated up to bit 31. For the .u32 instruction\ntype, the value in a is treated as an N-bit unsigned number and is zero-extended to 32\nbits. Operand b is an unsigned 32-bit value. If the value of N is 0, then the result of szext is 0. If the value of N is 32 or higher, then\nthe result of szext depends upon the value of the .mode qualifier as follows: If .mode is .clamp , then the result is the same as the source operand a . If .mode is .wrap , then the result is computed using the wrapped value of N. Semantics b1        = b & 0x1f;\ntoo_large = (b >= 32 && .mode == .clamp) ? true : false;\nmask      = too_large ? 0 : (~0) << b1;\nsign_pos  = (b1 - 1) & 0x1f;\n\nif (b1 == 0 || too_large || .type != .s32) {\n    sign_bit = false;\n} else {\n    sign_bit = (a >> sign_pos) & 1;\n}\nd = (a & ~mask) | (sign_bit ? mask | 0); PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes szext requires sm_70 or higher. Examples szext.clamp.s32 rd, ra, rb;\nszext.wrap.u32  rd, 0xffffffff, 0; // Result is 0. 9.7.1.22. Integer Arithmetic Instructions: bmsk  bmsk Bit Field Mask. Syntax bmsk.mode.b32  d, a, b;\n\n.mode = { .clamp, .wrap }; Description Generates a 32-bit mask starting from the bit position specified in operand a , and of the width\nspecified in operand b . The generated bitmask is stored in the destination operand d . The resulting bitmask is 0 in the following cases: When the value of a is 32 or higher and .mode is .clamp . When either the specified value of b or the wrapped value of b (when .mode is\nspecified as .wrap ) is 0. Semantics a1    = a & 0x1f;\nmask0 = (~0) << a1;\nb1    = b & 0x1f;\nsum   = a1 + b1;\nmask1 = (~0) << sum;\n\nsum-overflow          = sum >= 32 ? true : false;\nbit-position-overflow = false;\nbit-width-overflow    = false;\n\nif (.mode == .clamp) {\n    if (a >= 32) {\n        bit-position-overflow = true;\n        mask0 = 0;\n    }\n    if (b >= 32) {\n        bit-width-overflow = true;\n    }\n}\n\nif (sum-overflow || bit-position-overflow || bit-width-overflow) {\n    mask1 = 0;\n} else if (b1 == 0) {\n    mask1 = ~0;\n}\nd = mask0 & ~mask1; Notes The bitmask width specified by operand b is limited to range 0..32 in .clamp mode and to\nrange 0..31 in .wrap mode. PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes bmsk requires sm_70 or higher. Examples bmsk.clamp.b32  rd, ra, rb;\nbmsk.wrap.b32   rd, 1, 2; // Creates a bitmask of 0x00000006. 9.7.1.23. Integer Arithmetic Instructions: dp4a  dp4a Four-way byte dot product-accumulate. Syntax dp4a.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 }; Description Four-way byte dot product which is accumulated in 32-bit result. Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product. Operand c has type .u32 if both .atype and .btype are .u32 else operand c has type .s32 . Semantics d = c;\n\n// Extract 4 bytes from a 32bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_4(a, .atype);\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nfor (i = 0; i < 4; ++i) {\n    d += Va[i] * Vb[i];\n} PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes Requires sm_61 or higher. Examples dp4a.u32.u32           d0, a0, b0, c0;\ndp4a.u32.s32           d1, a1, b1, c1; 9.7.1.24. Integer Arithmetic Instructions: dp2a  dp2a Two-way dot product-accumulate. Syntax dp2a.mode.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\n.mode = { .lo, .hi }; Description Two-way 16-bit to 8-bit dot product which is accumulated in 32-bit result. Operand a and b are 32-bit inputs. Operand a holds two 16-bits inputs in packed form and\noperand b holds 4 byte inputs in packed form for dot product. Depending on the .mode specified, either lower half or upper half of operand b will be used\nfor dot product. Operand c has type .u32 if both .atype and .btype are .u32 else operand c has type .s32 . Semantics d = c;\n// Extract two 16-bit values from a 32-bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_2(a, .atype);\n\n// Extract four 8-bit values from a 32-bit input and sign or zer extend\n// based on input type.\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nb_select = (.mode == .lo) ? 0 : 2;\n\nfor (i = 0; i < 2; ++i) {\n    d += Va[i] * Vb[b_select + i];\n} PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes Requires sm_61 or higher. Examples dp2a.lo.u32.u32           d0, a0, b0, c0;\ndp2a.hi.u32.s32           d1, a1, b1, c1; 9.7.2. Extended-Precision Integer Arithmetic Instructions  Instructions add.cc , addc , sub.cc , subc , mad.cc and madc reference an\nimplicitly specified condition code register ( CC ) having a single carry flag bit ( CC.CF )\nholding carry-in/carry-out or borrow-in/borrow-out. These instructions support extended-precision\ninteger addition, subtraction, and multiplication. No other instructions access the condition code,\nand there is no support for setting, clearing, or testing the condition code. The condition code\nregister is not preserved across calls and is mainly intended for use in straight-line code\nsequences for computing extended-precision integer addition, subtraction, and multiplication. The extended-precision arithmetic instructions are: add.cc , addc sub.cc , subc mad.cc , madc 9.7.2.1. Extended-Precision Arithmetic Instructions: add.cc  add.cc Add two values with carry-out. Syntax add.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer addition and writes the carry-out value into the condition code register. Semantics d = a + b; carry-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit add.cc introduced in PTX ISA version 1.2. 64-bit add.cc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit add.cc is supported on all target architectures. 64-bit add.cc requires sm_20 or higher. Examples @p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4; 9.7.2.2. Extended-Precision Arithmetic Instructions: addc  addc Add two values with carry-in and optional carry-out. Syntax addc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer addition with carry-in and optionally writes the carry-out value into the condition\ncode register. Semantics d = a + b + CC.CF; if .cc specified, carry-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit addc introduced in PTX ISA version 1.2. 64-bit addc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit addc is supported on all target architectures. 64-bit addc requires sm_20 or higher. Examples @p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4; 9.7.2.3. Extended-Precision Arithmetic Instructions: sub.cc  sub.cc Subtract one value from another, with borrow-out. Syntax sub.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer subtraction and writes the borrow-out value into the condition code register. Semantics d = a - b; borrow-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit sub.cc introduced in PTX ISA version 1.2. 64-bit sub.cc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit sub.cc is supported on all target architectures. 64-bit sub.cc requires sm_20 or higher. Examples @p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4; 9.7.2.4. Extended-Precision Arithmetic Instructions: subc  subc Subtract one value from another, with borrow-in and optional borrow-out. Syntax subc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer subtraction with borrow-in and optionally writes the borrow-out value into the\ncondition code register. Semantics d = a  - (b + CC.CF); if .cc specified, borrow-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit subc introduced in PTX ISA version 1.2. 64-bit subc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit subc is supported on all target architectures. 64-bit subc requires sm_20 or higher. Examples @p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4; 9.7.2.5. Extended-Precision Arithmetic Instructions: mad.cc  mad.cc Multiply two values, extract high or low half of result, and add a third value with carry-out. Syntax mad{.hi,.lo}.cc.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third\nvalue. Writes the result to the destination register and the carry-out from the addition into the\ncondition code register. Semantics t = a * b;\nd = t<63..32> + c;    // for .hi variant\nd = t<31..0> + c;     // for .lo variant carry-out from addition is written to CC.CF Notes Generally used in combination with madc and addc to implement extended-precision multi-word\nmultiplication. See madc for an example. PTX ISA Notes 32-bit mad.cc introduced in PTX ISA version 3.0. 64-bit mad.cc introduced in PTX ISA version 4.3. Target ISA Notes Requires target sm_20 or higher. Examples @p  mad.lo.cc.u32 d,a,b,c;\n    mad.lo.cc.u32 r,p,q,r; 9.7.2.6. Extended-Precision Arithmetic Instructions: madc  madc Multiply two values, extract high or low half of result, and add a third value with carry-in and\noptional carry-out. Syntax madc{.hi,.lo}{.cc}.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third value\nalong with carry-in. Writes the result to the destination register and optionally writes the\ncarry-out from the addition into the condition code register. Semantics t = a * b;\nd = t<63..32> + c + CC.CF;     // for .hi variant\nd = t<31..0> + c + CC.CF;      // for .lo variant if .cc specified, carry-out from addition is written to CC.CF Notes Generally used in combination with mad.cc and addc to implement extended-precision\nmulti-word multiplication. See example below. PTX ISA Notes 32-bit madc introduced in PTX ISA version 3.0. 64-bit madc introduced in PTX ISA version 4.3. Target ISA Notes Requires target sm_20 or higher. Examples // extended-precision multiply:  [r3,r2,r1,r0] = [r5,r4] * [r7,r6]\nmul.lo.u32     r0,r4,r6;      // r0=(r4*r6).[31:0], no carry-out\nmul.hi.u32     r1,r4,r6;      // r1=(r4*r6).[63:32], no carry-out\nmad.lo.cc.u32  r1,r5,r6,r1;   // r1+=(r5*r6).[31:0], may carry-out\nmadc.hi.u32    r2,r5,r6,0;    // r2 =(r5*r6).[63:32]+carry-in,\n                              // no carry-out\nmad.lo.cc.u32   r1,r4,r7,r1;  // r1+=(r4*r7).[31:0], may carry-out\nmadc.hi.cc.u32  r2,r4,r7,r2;  // r2+=(r4*r7).[63:32]+carry-in,\n                              // may carry-out\naddc.u32        r3,0,0;       // r3 = carry-in, no carry-out\nmad.lo.cc.u32   r2,r5,r7,r2;  // r2+=(r5*r7).[31:0], may carry-out\nmadc.hi.u32     r3,r5,r7,r3;  // r3+=(r5*r7).[63:32]+carry-in 9.7.3. Floating-Point Instructions  Floating-point instructions operate on .f32 and .f64 register operands and constant\nimmediate values. The floating-point instructions are: testp copysign add sub mul fma mad div abs neg min max rcp sqrt rsqrt sin cos lg2 ex2 tanh Instructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions\nsupport subnormal inputs and results. Single-precision instructions support subnormal inputs and\nresults by default for sm_20 and subsequent targets, and flush subnormal inputs and results to\nsign-preserving zero for sm_1x targets. The optional .ftz modifier on single-precision\ninstructions provides backward compatibility with sm_1x targets by flushing subnormal inputs and\nresults to sign-preserving zero regardless of the target architecture. Single-precision add , sub , mul , and mad support saturation of results to the range\n[0.0, 1.0], with NaN s being flushed to positive zero. NaN payloads are supported for\ndouble-precision instructions (except for rcp.approx.ftz.f64 and rsqrt.approx.ftz.f64 , which\nmaps input NaN s to a canonical NaN ). Single-precision instructions return an unspecified NaN . Note that future implementations may support NaN payloads for single-precision\ninstructions, so PTX programs should not rely on the specific single-precision NaN s being\ngenerated. Table 26 summarizes\nfloating-point instructions in PTX. Table 26 Summary of Floating-Point Instructions  Instruction .rn .rz .rm .rp .ftz .sat Notes {add,sub,mul}.rnd.f32 x x x x x x If no rounding modifier is specified,\ndefault is .rn and instructions may\nbe folded into a multiply-add. {add,sub,mul}.rnd.f64 x x x x n/a n/a If no rounding modifier is specified,\ndefault is .rn and instructions may\nbe folded into a multiply-add. mad.f32 n/a n/a n/a n/a x x .target sm_1x No rounding modifier. {mad,fma}.rnd.f32 x x x x x x .target sm_20 or higher mad.f32 and fma.f32 are the same. {mad,fma}.rnd.f64 x x x x n/a n/a mad.f64 and fma.f64 are the same. div.full.f32 n/a n/a n/a n/a x n/a No rounding modifier. {div,rcp,sqrt}.approx.f32 n/a n/a n/a n/a x n/a n/a rcp.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f32 x x x x x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f64 x x x x n/a n/a .target sm_20 or higher {abs,neg,min,max}.f32 n/a n/a n/a n/a x n/a {abs,neg,min,max}.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.f32 n/a n/a n/a n/a x n/a rsqrt.approx.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {sin,cos,lg2,ex2}.approx.f32 n/a n/a n/a n/a x n/a tanh.approx.f32 n/a n/a n/a n/a n/a n/a .target sm_75 or higher 9.7.3.1. Floating Point Instructions: testp  testp Test floating-point property. Syntax testp.op.type  p, a;  // result is .pred\n\n.op   = { .finite, .infinite,\n          .number, .notanumber,\n          .normal, .subnormal };\n.type = { .f32, .f64 }; Description testp tests common properties of floating-point numbers and returns a predicate value of 1 if True and 0 if False . testp.finite True if the input is not infinite or NaN testp.infinite True if the input is positive or negative infinity testp.number True if the input is not NaN testp.notanumber True if the input is NaN testp.normal True if the input is a normal number (not NaN , not infinity) testp.subnormal True if the input is a subnormal number (not NaN , not infinity) As a special case, positive and negative zero are considered normal numbers. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Examples testp.notanumber.f32  isnan, f0;\ntestp.infinite.f64    p, X; 9.7.3.2. Floating Point Instructions: copysign  copysign Copy sign of one input to another. Syntax copysign.type  d, a, b;\n\n.type = { .f32, .f64 }; Description Copy sign bit of a into value of b , and return the result as d . PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Examples copysign.f32  x, y, z;\ncopysign.f64  A, B, C; 9.7.3.3. Floating Point Instructions: add  add Add two values. Syntax add{.rnd}{.ftz}{.sat}.f32  d, a, b;\nadd{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Performs addition and writes the resulting value into a destination register. Semantics d = a + b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that an add instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An add instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / add sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. add.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x add.f64 supports subnormal numbers. add.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: add.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes add.f32 supported on all target architectures. add.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for add.f64 , requires sm_13 or higher. for add.f32 , requires sm_20 or higher. Examples @p  add.rz.ftz.f32  f1,f2,f3; 9.7.3.4. Floating Point Instructions: sub  sub Subtract one value from another. Syntax sub{.rnd}{.ftz}{.sat}.f32  d, a, b;\nsub{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Performs subtraction and writes the resulting value into a destination register. Semantics d = a - b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that a sub instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A sub instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / sub sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sub.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x sub.f64 supports subnormal numbers. sub.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: sub.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes sub.f32 supported on all target architectures. sub.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for sub.f64 , requires sm_13 or higher. for sub.f32 , requires sm_20 or higher. Examples sub.f32 c,a,b;\nsub.rn.ftz.f32  f1,f2,f3; 9.7.3.5. Floating Point Instructions: mul  mul Multiply two values. Syntax mul{.rnd}{.ftz}{.sat}.f32  d, a, b;\nmul{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Compute the product of two values. Semantics d = a * b; Notes For floating-point multiplication, all operands must be the same size. Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that a mul instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A mul instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul/add and mul/sub sequences with no rounding modifiers may be\noptimized to use fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. mul.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x mul.f64 supports subnormal numbers. mul.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mul.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes mul.f32 supported on all target architectures. mul.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for mul.f64 , requires sm_13 or higher. for mul.f32 , requires sm_20 or higher. Examples mul.ftz.f32 circumf,radius,pi  // a single-precision multiply 9.7.3.6. Floating Point Instructions: fma  fma Fused multiply-add. Syntax fma.rnd{.ftz}{.sat}.f32  d, a, b, c;\nfma.rnd.f64              d, a, b, c;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition. Semantics d = a*b + c; Notes fma.f32 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by .rnd . fma.f64 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by .rnd . fma.f64 is the same as mad.f64 . Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. fma.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x fma.f64 supports subnormal numbers. fma.f32 is unimplemented for sm_1x targets. Saturation: fma.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes fma.f64 introduced in PTX ISA version 1.4. fma.f32 introduced in PTX ISA version 2.0. Target ISA Notes fma.f32 requires sm_20 or higher. fma.f64 requires sm_13 or higher. Examples fma.rn.ftz.f32  w,x,y,z;\n@p  fma.rn.f64      d,a,b,c; 9.7.3.7. Floating Point Instructions: mad  mad Multiply two values and add a third value. Syntax mad{.ftz}{.sat}.f32      d, a, b, c;    // .target sm_1x\nmad.rnd{.ftz}{.sat}.f32  d, a, b, c;    // .target sm_20\nmad.rnd.f64              d, a, b, c;    // .target sm_13 and higher\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Multiplies two values and adds a third, and then writes the resulting value into a destination\nregister. Semantics d = a*b + c; Notes For .target sm_20 and higher: mad.f32 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by .rnd . mad.f64 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by .rnd . mad.{f32,f64} is the same as fma.{f32,f64} . For .target sm_1x : mad.f32 computes the product of a and b at double precision, and then the mantissa is\ntruncated to 23 bits, but the exponent is preserved. Note that this is different from computing\nthe product with mul , where the mantissa can be rounded and the exponent will be clamped. The\nexception for mad.f32 is when c = +/-0.0 , mad.f32 is identical to the result computed\nusing separate mul and add instructions. When JIT-compiled for SM 2.0 devices, mad.f32 is\nimplemented as a fused multiply-add (i.e., fma.rn.ftz.f32 ). In this case, mad.f32 can\nproduce slightly different numeric results and backward compatibility is not guaranteed in this\ncase. mad.f64 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by .rnd . Unlike mad.f32 , the treatment of subnormal\ninputs and output follows IEEE 754 standard. mad.f64 is the same as fma.f64 . Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. mad.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x mad.f64 supports subnormal numbers. mad.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mad.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. In PTX ISA versions 1.4 and later, a rounding modifier is required for mad.f64 . Legacy mad.f64 instructions having no rounding modifier will map to mad.rn.f64 . In PTX ISA versions 2.0 and later, a rounding modifier is required for mad.f32 for sm_20 and higher targets. Errata mad.f32 requires a rounding modifier for sm_20 and higher targets. However for PTX ISA\nversion 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently defaults\nto mad.rn.f32 . For PTX ISA version 3.1, ptxas generates a warning and defaults to mad.rn.f32 , and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later. Target ISA Notes mad.f32 supported on all target architectures. mad.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz , .rm , .rp for mad.f64 , requires sm_13 or higher. .rn , .rz , .rm , .rp for mad.f32 , requires sm_20 or higher. Examples @p  mad.f32  d,a,b,c; 9.7.3.8. Floating Point Instructions: div  div Divide one value by another. Syntax div.approx{.ftz}.f32  d, a, b;  // fast, approximate divide\ndiv.full{.ftz}.f32    d, a, b;  // full-range approximate divide\ndiv.rnd{.ftz}.f32     d, a, b;  // IEEE 754 compliant rounding\ndiv.rnd.f64           d, a, b;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Divides a by b , stores result in d . Semantics d = a / b; Notes Fast, approximate single-precision divides: div.approx.f32 implements a fast approximation to divide, computed as d = a * (1/b) . For |b| in [2 -126 , 2 126 ], the maximum ulp error is 2. For 2 126 < |b| < 2 128 , if a is infinity, div.approx.f32 returns NaN , otherwise it\nreturns 0. div.full.f32 implements a relatively fast, full-range approximation that scales operands to\nachieve better accuracy, but is not fully IEEE 754 compliant and does not support rounding\nmodifiers. The maximum ulp error is 2 across the full range of inputs. Subnormal inputs and results are flushed to sign-preserving zero. Fast, approximate division by\nzero creates a value of infinity (with same sign as a ). Divide with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. div.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x div.f64 supports subnormal numbers. div.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes div.f32 and div.f64 introduced in PTX ISA version 1.0. Explicit modifiers .approx , .full , .ftz , and rounding introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, one of .approx , .full , or .rnd is required. For PTX ISA versions 1.0 through 1.3, div.f32 defaults to div.approx.ftz.f32 , and div.f64 defaults to div.rn.f64 . Target ISA Notes div.approx.f32 and div.full.f32 supported on all target architectures. div.rnd.f32 requires sm_20 or higher. div.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32 . div.{rz,rm,rp}.f64 requires sm_20 or higher. Examples div.approx.ftz.f32  diam,circum,3.14159;\ndiv.full.ftz.f32    x, y, z;\ndiv.rn.f64          xd, yd, zd; 9.7.3.9. Floating Point Instructions: abs  abs Absolute value. Syntax abs{.ftz}.f32  d, a;\nabs.f64        d, a; Description Take the absolute value of a and store the result in d . Semantics d = |a|; Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. abs.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x abs.f64 supports subnormal numbers. abs.f32 flushes subnormal inputs and results to sign-preserving zero. For abs.f32 , NaN input yields unspecified NaN . For abs.f64 , NaN input is passed\nthrough unchanged. Future implementations may comply with the IEEE 754 standard by preserving\npayload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes abs.f32 supported on all target architectures. abs.f64 requires sm_13 or higher. Examples abs.ftz.f32  x,f0; 9.7.3.10. Floating Point Instructions: neg  neg Arithmetic negate. Syntax neg{.ftz}.f32  d, a;\nneg.f64        d, a; Description Negate the sign of a and store the result in d . Semantics d = -a; Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. neg.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x neg.f64 supports subnormal numbers. neg.f32 flushes subnormal inputs and results to sign-preserving zero. NaN inputs yield an unspecified NaN . Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes neg.f32 supported on all target architectures. neg.f64 requires sm_13 or higher. Examples neg.ftz.f32  x,f0; 9.7.3.11. Floating Point Instructions: min  min Find the minimum of two values. Syntax min{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmin.f64                            d, a, b; Description Store the minimum of a and b in d . If .NaN modifier is specified, then the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the minimum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of min is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (.xorsign) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n    if (.abs) {\n        a = |a|;\n        b = |b|;\n   }\n}\nif (isNaN(a) && isNaN(b))                 d = NaN;\nelse if (.NaN && (isNaN(a) || isNaN(b)))  d = NaN;\nelse if (isNaN(a))                        d = b;\nelse if (isNaN(b))                        d = a;\nelse                                      d = (a < b) ? a : b;\nif (.xorsign && !isNaN(d)) {\n    setSignBit(d, xorsign);\n} Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. min.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x min.f64 supports subnormal numbers. min.f32 flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 1.0. min.NaN introduced in PTX ISA version 7.0. min.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes min.f32 supported on all target architectures. min.f64 requires sm_13 or higher. min.NaN requires sm_80 or higher. min.xorsign.abs requires sm_86 or higher. Examples @p  min.ftz.f32  z,z,x;\n    min.f64      a,b,c;\n    // fp32 min with .NaN\n    min.NaN.f32  f0,f1,f2;\n    // fp32 min with .xorsign.abs\n    min.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.12. Floating Point Instructions: max  max Find the maximum of two values. Syntax max{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmax.f64                            d, a, b; Description Store the maximum of a and b in d . If .NaN modifier is specified, the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the maximum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of max is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (.xorsign) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n    if (.abs) {\n        a = |a|;\n        b = |b|;\n    }\n}\nif (isNaN(a) && isNaN(b))                 d = NaN;\nelse if (.NaN && (isNaN(a) || isNaN(b)))  d = NaN;\nelse if (isNaN(a))                        d = b;\nelse if (isNaN(b))                        d = a;\nelse                                      d = (a > b) ? a : b;\nif (.xorsign && !isNaN(d)) {\n    setSignBit(d, xorsign);\n} Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. max.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x max.f64 supports subnormal numbers. max.f32 flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 1.0. max.NaN introduced in PTX ISA version 7.0. max.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes max.f32 supported on all target architectures. max.f64 requires sm_13 or higher. max.NaN requires sm_80 or higher. max.xorsign.abs requires sm_86 or higher. Examples max.ftz.f32  f0,f1,f2;\nmax.f64      a,b,c;\n// fp32 max with .NaN\nmax.NaN.f32  f0,f1,f2;\n// fp32 max with .xorsign.abs\nmax.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.13. Floating Point Instructions: rcp  rcp Take the reciprocal of a value. Syntax rcp.approx{.ftz}.f32  d, a;  // fast, approximate reciprocal\nrcp.rnd{.ftz}.f32     d, a;  // IEEE 754 compliant rounding\nrcp.rnd.f64           d, a;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Compute 1/a , store result in d . Semantics d = 1 / a; Notes Fast, approximate single-precision reciprocal: rcp.approx.f32 implements a fast approximation to reciprocal. The maximum absolute error is 2 -23.0 over the range 1.0-2.0. Input Result -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Reciprocal with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. rcp.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x rcp.f64 supports subnormal numbers. rcp.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes rcp.f32 and rcp.f64 introduced in PTX ISA version 1.0. rcp.rn.f64 and explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. General rounding modifiers were\nadded in PTX ISA version 2.0. For PTX ISA version 1.4 and later, one of .approx or .rnd is required. For PTX ISA versions 1.0 through 1.3, rcp.f32 defaults to rcp.approx.ftz.f32 , and rcp.f64 defaults to rcp.rn.f64 . Target ISA Notes rcp.approx.f32 supported on all target architectures. rcp.rnd.f32 requires sm_20 or higher. rcp.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32. rcp.{rz,rm,rp}.f64 requires sm_20 or higher. Examples rcp.approx.ftz.f32  ri,r;\nrcp.rn.ftz.f32      xi,x;\nrcp.rn.f64          xi,x; 9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64  rcp.approx.ftz.f64 Compute a fast, gross approximation to the reciprocal of a value. Syntax rcp.approx.ftz.f64  d, a; Description Compute a fast, gross approximation to the reciprocal as follows: extract the most-significant 32 bits of .f64 operand a in 1.11.20 IEEE floating-point\nformat (i.e., ignore the least-significant 32 bits of a ), compute an approximate .f64 reciprocal of this value using the most-significant 20 bits of\nthe mantissa of operand a , place the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits\nof destination d ,and zero the least significant 32 mantissa bits of .f64 destination d . Semantics tmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / tmp;\nd[31:0] = 0x00000000; Notes rcp.approx.ftz.f64 implements a fast, gross approximation to reciprocal. Input a[63:32] Result d[63:32] -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 . Subnormal inputs and results are flushed to sign-preserving zero. PTX ISA Notes rcp.approx.ftz.f64 introduced in PTX ISA version 2.1. Target ISA Notes rcp.approx.ftz.f64 requires sm_20 or higher. Examples rcp.ftz.f64  xi,x; 9.7.3.15. Floating Point Instructions: sqrt  sqrt Take the square root of a value. Syntax sqrt.approx{.ftz}.f32  d, a; // fast, approximate square root\nsqrt.rnd{.ftz}.f32     d, a; // IEEE 754 compliant rounding\nsqrt.rnd.f64           d, a; // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Compute sqrt( a ) and store the result in d . Semantics d = sqrt(a); Notes sqrt.approx.f32 implements a fast approximation to square root. Input Result -Inf NaN -normal NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf +Inf NaN NaN Square root with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x sqrt.f64 supports subnormal numbers. sqrt.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes sqrt.f32 and sqrt.f64 introduced in PTX ISA version 1.0. sqrt.rn.f64 and explicit\nmodifiers .approx and .ftz were introduced in PTX ISA version 1.4. General rounding\nmodifiers were added in PTX ISA version 2.0. For PTX ISA version 1.4 and later, one of .approx or .rnd is required. For PTX ISA versions 1.0 through 1.3, sqrt.f32 defaults to sqrt.approx.ftz.f32 , and sqrt.f64 defaults to sqrt.rn.f64 . Target ISA Notes sqrt.approx.f32 supported on all target architectures. sqrt.rnd.f32 requires sm_20 or higher. sqrt.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32 . sqrt.{rz,rm,rp}.f64 requires sm_20 or higher. Examples sqrt.approx.ftz.f32  r,x;\nsqrt.rn.ftz.f32      r,x;\nsqrt.rn.f64          r,x; 9.7.3.16. Floating Point Instructions: rsqrt  rsqrt Take the reciprocal of the square root of a value. Syntax rsqrt.approx{.ftz}.f32  d, a;\nrsqrt.approx.f64        d, a; Description Compute 1/sqrt(a) and store the result in d . Semantics d = 1/sqrt(a); Notes rsqrt.approx implements an approximation to the reciprocal square root. Input Result -Inf NaN -normal NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN The maximum absolute error for rsqrt.f32 is 2 -22.4 over the range 1.0-4.0. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. rsqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x rsqrt.f64 supports subnormal numbers. rsqrt.f32 flushes subnormal inputs and results to sign-preserving zero. Note that rsqrt.approx.f64 is emulated in software and are relatively slow. PTX ISA Notes rsqrt.f32 and rsqrt.f64 were introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, rsqrt.f32 defaults to rsqrt.approx.ftz.f32 , and rsqrt.f64 defaults to rsqrt.approx.f64 . Target ISA Notes rsqrt.f32 supported on all target architectures. rsqrt.f64 requires sm_13 or higher. Examples rsqrt.approx.ftz.f32  isr, x;\nrsqrt.approx.f64      ISR, X; 9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64  rsqrt.approx.ftz.f64 Compute an approximation of the square root reciprocal of a value. Syntax rsqrt.approx.ftz.f64 d, a; Description Compute a double-precision ( .f64 ) approximation of the square root reciprocal of a value. The\nleast significant 32 bits of the double-precision ( .f64 ) destination d are all zeros. Semantics tmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / sqrt(tmp);\nd[31:0] = 0x00000000; Notes rsqrt.approx.ftz.f64 implements a fast approximation of the square root reciprocal of a value. Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 . Subnormal inputs and results are flushed to sign-preserving zero. PTX ISA Notes rsqrt.approx.ftz.f64 introduced in PTX ISA version 4.0. Target ISA Notes rsqrt.approx.ftz.f64 requires sm_20 or higher. Examples rsqrt.approx.ftz.f64 xi,x; 9.7.3.18. Floating Point Instructions: sin  sin Find the sine of a value. Syntax sin.approx{.ftz}.f32  d, a; Description Find the sine of the angle a (in radians). Semantics d = sin(a); Notes sin.approx.f32 implements a fast approximation to sine. Input Result -Inf NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sin.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes sin.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, sin.f32 defaults to sin.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples sin.approx.ftz.f32  sa, a; 9.7.3.19. Floating Point Instructions: cos  cos Find the cosine of a value. Syntax cos.approx{.ftz}.f32  d, a; Description Find the cosine of the angle a (in radians). Semantics d = cos(a); Notes cos.approx.f32 implements a fast approximation to cosine. Input Result -Inf NaN -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. cos.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes cos.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, cos.f32 defaults to cos.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples cos.approx.ftz.f32  ca, a; 9.7.3.20. Floating Point Instructions: lg2  lg2 Find the base-2 logarithm of a value. Syntax lg2.approx{.ftz}.f32  d, a; Description Determine the log 2 of a . Semantics d = log(a) / log(2); Notes lg2.approx.f32 implements a fast approximation to log 2 (a). Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 -Inf +subnormal -Inf +Inf +Inf NaN NaN The maximum absolute error is 2 -22.6 for mantissa. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. lg2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes lg2.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, lg2.f32 defaults to lg2.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples lg2.approx.ftz.f32  la, a; 9.7.3.21. Floating Point Instructions: ex2  ex2 Find the base-2 exponential of a value. Syntax ex2.approx{.ftz}.f32  d, a; Description Raise 2 to the power a . Semantics d = 2 ^ a; Notes ex2.approx.f32 implements a fast approximation to 2 a . Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN The maximum absolute error is 2 -22.5 for fraction in the primary range. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. ex2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes ex2.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, ex2.f32 defaults to ex2.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples ex2.approx.ftz.f32  xa, a; 9.7.3.22. Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.f32 d, a; Description Take hyperbolic tangent value of a . The operands d and a are of type .f32 . Semantics d = tanh(a); Notes tanh.approx.f32 implements a fast approximation to FP32 hyperbolic-tangent. Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -subnormal Same as input -0.0 -0.0 +0.0 +0.0 +subnormal Same as input +Inf 1.0 NaN NaN The subnormal numbers are supported. Note The subnormal inputs gets passed through to the output since the value of tanh(x) for small\nvalues of x is approximately the same as x . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_75 or higher. Examples tanh.approx.f32 sa, a; 9.7.4. Half Precision Floating-Point Instructions  Half precision floating-point instructions operate on .f16 and .f16x2 register operands. The\nhalf precision floating-point instructions are: add sub mul fma neg abs min max tanh ex2 Half-precision add , sub , mul , and fma support saturation of results to the range\n[0.0, 1.0], with NaN s being flushed to positive zero. Half-precision instructions return an\nunspecified NaN . 9.7.4.1. Half Precision Floating Point Instructions: add  add Add two values. Syntax add{.rnd}{.ftz}{.sat}.f16   d, a, b;\nadd{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nadd{.rnd}.bf16   d, a, b;\nadd{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn }; Description Performs addition and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce .f16x2 or .bf16x2 result\nin destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type,\noperands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a + b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] + fB[i];\n    }\n} Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even The default value of rounding modifier is .rn . Note that an add instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An add instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / add sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: By default, subnormal numbers are supported. add.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: add.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 4.2. add{.rnd}.bf16 and add{.rnd}.bf16x2 introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. add{.rnd}.bf16 and add{.rnd}.bf16x2 requires sm_90 or higher. Examples // scalar f16 additions\nadd.f16        d0, a0, b0;\nadd.rn.f16     d1, a1, b1;\nadd.bf16       bd0, ba0, bb0;\nadd.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 addition\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nadd.f16x2  p3, p1, p2;   // SIMD f16x2 addition\n\n// SIMD bf16 addition\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nadd.bf16x2  p6, p4, p5;       // SIMD bf16x2 addition\n\n// SIMD fp16 addition\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nadd.f16x2       f2, f0, f1;     // SIMD f16x2 addition\n\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nadd.bf16x2      f5, f3, f4;      // SIMD bf16x2 addition 9.7.4.2. Half Precision Floating Point Instructions: sub  sub Subtract two values. Syntax sub{.rnd}{.ftz}{.sat}.f16   d, a, b;\nsub{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nsub{.rnd}.bf16   d, a, b;\nsub{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn }; Description Performs subtraction and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then subtracted in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type,\noperands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a - b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] - fB[i];\n    }\n} Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even The default value of rounding modifier is .rn . Note that a sub instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A sub instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / sub sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: By default, subnormal numbers are supported. sub.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: sub.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 4.2. sub{.rnd}.bf16 and sub{.rnd}.bf16x2 introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. sub{.rnd}.bf16 and sub{.rnd}.bf16x2 requires sm_90 or higher. Examples // scalar f16 subtractions\nsub.f16        d0, a0, b0;\nsub.rn.f16     d1, a1, b1;\nsub.bf16       bd0, ba0, bb0;\nsub.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 subtraction\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nsub.f16x2  p3, p1, p2;   // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nsub.bf16x2  p6, p4, p5;       // SIMD bf16x2 subtraction\n\n// SIMD fp16 subtraction\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nsub.f16x2       f2, f0, f1;     // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nsub.bf16x2      f5, f3, f4;      // SIMD bf16x2 subtraction 9.7.4.3. Half Precision Floating Point Instructions: mul  mul Multiply two values. Syntax mul{.rnd}{.ftz}{.sat}.f16   d, a, b;\nmul{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nmul{.rnd}.bf16   d, a, b;\nmul{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn }; Description Performs multiplication and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then multiplied in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type,\noperands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a * b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i];\n    }\n} Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even The default value of rounding modifier is .rn . Note that a mul instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A mul instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / add and mul/sub sequences with no rounding modifiers may\nbe optimized to use fused-multiply-add instructions on the target device. Subnormal numbers: By default, subnormal numbers are supported. mul.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mul.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 4.2. mul{.rnd}.bf16 and mul{.rnd}.bf16x2 introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. mul{.rnd}.bf16 and mul{.rnd}.bf16x2 requires sm_90 or higher. Examples // scalar f16 multiplications\nmul.f16        d0, a0, b0;\nmul.rn.f16     d1, a1, b1;\nmul.bf16       bd0, ba0, bb0;\nmul.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 multiplication\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nmul.f16x2  p3, p1, p2;   // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nmul.bf16x2  p6, p4, p5;       // SIMD bf16x2 multiplication\n\n// SIMD fp16 multiplication\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nmul.f16x2       f2, f0, f1;     // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nmul.bf16x2      f5, f3, f4;      // SIMD bf16x2 multiplication 9.7.4.4. Half Precision Floating Point Instructions: fma  fma Fused multiply-add Syntax fma.rnd{.ftz}{.sat}.f16     d, a, b, c;\nfma.rnd{.ftz}{.sat}.f16x2   d, a, b, c;\nfma.rnd{.ftz}.relu.f16      d, a, b, c;\nfma.rnd{.ftz}.relu.f16x2    d, a, b, c;\nfma.rnd{.relu}.bf16         d, a, b, c;\nfma.rnd{.relu}.bf16x2       d, a, b, c;\nfma.rnd.oob.{relu}.type     d, a, b, c;\n\n.rnd = { .rn }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then operated in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a , b and c have .f16 or .b16 type. For .f16x2 instruction type, operands d , a , b and c have .b32 type. For .bf16 instruction type, operands d , a , b and c have .b16 type. For .bf16x2 instruction type, operands d , a , b and c have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a * b + c;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    fC[0] = c[0:15];\n    fC[1] = c[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i] + fC[i];\n    }\n} Notes Rounding modifiers (default is .rn ): .rn mantissa LSB rounds to nearest even Subnormal numbers: By default, subnormal numbers are supported. fma.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: fma.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . fma.relu.{f16, f16x2, bf16, bf16x2} clamps the result to 0 if negative. NaN result is\nconverted to canonical NaN . Out Of Bounds modifier: fma.oob.{f16, f16x2, bf16, bf16x2} clamps the result to 0 if either of the operands\nis OOB NaN (defined under Tensors ) value. The test for the special NaN value\nand resultant forcing of the result to +0.0 is performed independently for each of the\ntwo SIMD operations. PTX ISA Notes Introduced in PTX ISA version 4.2. fma.relu.{f16, f16x2} and fma{.relu}.{bf16, bf16x2} introduced in PTX ISA version 7.0. Support for modifier .oob introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_53 or higher. fma.relu.{f16, f16x2} and fma{.relu}.{bf16, bf16x2} require sm_80 or higher. fma{.oob}.{f16, f16x2, bf16, bf16x2} requires sm_90 or higher. Examples // scalar f16 fused multiply-add\nfma.rn.f16         d0, a0, b0, c0;\nfma.rn.f16         d1, a1, b1, c1;\nfma.rn.relu.f16    d1, a1, b1, c1;\nfma.rn.oob.f16      d1, a1, b1, c1;\nfma.rn.oob.relu.f16 d1, a1, b1, c1;\n\n// scalar bf16 fused multiply-add\nfma.rn.bf16        d1, a1, b1, c1;\nfma.rn.relu.bf16   d1, a1, b1, c1;\nfma.rn.oob.bf16       d1, a1, b1, c1;\nfma.rn.oob.relu.bf16  d1, a1, b1, c1;\n\n// SIMD f16 fused multiply-add\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1}; // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3}; // pack two f16 to 32bit f16x2\nfma.rn.f16x2  p3, p1, p2, p2;   // SIMD f16x2 fused multiply-add\nfma.rn.relu.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.f16x2 p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier and relu saturation mode\n\n// SIMD fp16 fused multiply-add\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nfma.rn.f16x2    f2, f0, f1, f1; // SIMD f16x2 fused multiply-add\n\n// SIMD bf16 fused multiply-add\nfma.rn.bf16x2       f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add\nfma.rn.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier and relu saturation mode 9.7.4.5. Half Precision Floating Point Instructions: neg  neg Arithmetic negate. Syntax neg{.ftz}.f16    d, a;\nneg{.ftz}.f16x2  d, a;\nneg.bf16         d, a;\nneg.bf16x2       d, a; Description Negate the sign of a and store the result in d . For .f16x2 and .bf16x2 instruction type, forms input vector by extracting half word values\nfrom the source operand. Half-word operands are then negated in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .b32 type. For .bf16 instruction\ntype, operands d and a have .b16 type. For .bf16x2 instruction type, operands d and a have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = -a;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = -fA[i];\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. neg.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. NaN inputs yield an unspecified NaN . Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 6.0. neg.bf16 and neg.bf16x2 introduced in PTX ISA 7.0. Target ISA Notes Requires sm_53 or higher. neg.bf16 and neg.bf16x2 requires architecture sm_80 or higher. Examples neg.ftz.f16  x,f0;\nneg.bf16     x,b0;\nneg.bf16x2   x1,b1; 9.7.4.6. Half Precision Floating Point Instructions: abs  abs Absolute value Syntax abs{.ftz}.f16    d, a;\nabs{.ftz}.f16x2  d, a;\nabs.bf16         d, a;\nabs.bf16x2       d, a; Description Take absolute value of a and store the result in d . For .f16x2 and .bf16x2 instruction type, forms input vector by extracting half word values\nfrom the source operand. Absolute values of half-word operands are then computed in parallel to\nproduce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction\ntype, operands d and a have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = |a|;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = |fA[i]|;\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. abs.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. NaN inputs yield an unspecified NaN . Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 6.5. abs.bf16 and abs.bf16x2 introduced in PTX ISA 7.0. Target ISA Notes Requires sm_53 or higher. abs.bf16 and abs.bf16x2 requires architecture sm_80 or higher. Examples abs.ftz.f16  x,f0;\nabs.bf16     x,b0;\nabs.bf16x2   x1,b1; 9.7.4.7. Half Precision Floating Point Instructions: min  min Find the minimum of two values. Syntax min{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmin{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16x2         d, a, b; Description Store the minimum of a and b in d . For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction\ntype, operands d and a have .b32 type. If .NaN modifier is specified, then the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the minimum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of min is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a < b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n               fA[i] = |fA[i]|;\n               fB[i] = |fB[i]|;\n           }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] < fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(d[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. min.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 7.0. min.xorsign introduced in PTX ISA version 7.2. Target ISA Notes Requires sm_80 or higher. min.xorsign.abs support requires sm_86 or higher. Examples min.ftz.f16       h0,h1,h2;\nmin.f16x2         b0,b1,b2;\n// SIMD fp16 min with .NaN\nmin.NaN.f16x2     b0,b1,b2;\nmin.bf16          h0, h1, h2;\n// SIMD bf16 min with NaN\nmin.NaN.bf16x2    b0, b1, b2;\n// scalar bf16 min with xorsign.abs\nmin.xorsign.abs.bf16 Rd, Ra, Rb 9.7.4.8. Half Precision Floating Point Instructions: max  max Find the maximum of two values. Syntax max{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmax{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16x2         d, a, b; Description Store the maximum of a and b in d . For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction\ntype, operands d and a have .b32 type. If .NaN modifier is specified, the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the maximum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of max is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a > b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n                fA[i] = |fA[i]|;\n                fB[i] = |fB[i]|;\n            }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] > fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(fA[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. max.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 7.0. max.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes Requires sm_80 or higher. max.xorsign.abs support requires sm_86 or higher. Examples max.ftz.f16       h0,h1,h2;\nmax.f16x2         b0,b1,b2;\n// SIMD fp16 max with NaN\nmax.NaN.f16x2     b0,b1,b2;\n// scalar f16 max with xorsign.abs\nmax.xorsign.abs.f16 Rd, Ra, Rb;\nmax.bf16          h0, h1, h2;\n// scalar bf16 max and NaN\nmax.NaN.bf16x2    b0, b1, b2;\n// SIMD bf16 max with xorsign.abs\nmax.xorsign.abs.bf16x2 Rd, Ra, Rb; 9.7.4.9. Half Precision Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.type d, a;\n\n.type = {.f16, .f16x2, .bf16, .bf16x2} Description Take hyperbolic tangent value of a . The type of operands d and a are as specified by .type . For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a .f16x2 or .bf16x2 . Semantics if (.type == .f16 || .type == .bf16) {\n  d = tanh(a)\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = tanh(fA[0])\n  d[1] = tanh(fA[1])\n} Notes tanh.approx.{f16, f16x2, bf16, bf16x2} implements an approximate hyperbolic tangent in the\ntarget format. Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -0.0 -0.0 +0.0 +0.0 +Inf 1.0 NaN NaN The maximum absolute error for .f16 type is 2-10.987. The maximum absolute error for .bf16 type is 2-8. The subnormal numbers are supported. PTX ISA Notes Introduced in PTX ISA version 7.0. tanh.approx.{bf16/bf16x2} introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. tanh.approx.{bf16/bf16x2} requires sm_90 or higher. Examples tanh.approx.f16    h1, h0;\ntanh.approx.f16x2  hd1, hd0;\ntanh.approx.bf16   b1, b0;\ntanh.approx.bf16x2 hb1, hb0; 9.7.4.10. Half Precision Floating Point Instructions: ex2  ex2 Find the base-2 exponent of input. Syntax ex2.approx.atype     d, a;\nex2.approx.ftz.btype d, a;\n\n.atype = { .f16,  .f16x2}\n.btype = { .bf16, .bf16x2} Description Raise 2 to the power a . The type of operands d and a are as specified by .type . For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a .f16x2 or .bf16x2 . Semantics if (.type == .f16 || .type == .bf16) {\n  d = 2 ^ a\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = 2 ^ fA[0]\n  d[1] = 2 ^ fA[1]\n} Notes ex2.approx.{f16, f16x2, bf16, bf16x2} implement a fast approximation to 2 a . For the .f16 type, subnormal inputs are supported. ex2.approx.ftz.bf16 flushes subnormal\ninputs and results to sign-preserving zero. Results of ex2.approx.ftz.bf16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN Results of ex2.approx.f16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -0.0 +1.0 +0.0 +1.0 +Inf +Inf NaN NaN The maximum relative error for .f16 type is 2-9.9. The maximum relative error for .bf16 type\nis 2-7. PTX ISA Notes Introduced in PTX ISA version 7.0. ex2.approx.ftz.{bf16/bf16x2} introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. ex2.approx.ftz.{bf16/bf16x2} requires sm_90 or higher. Examples ex2.approx.f16         h1, h0;\nex2.approx.f16x2       hd1, hd0;\nex2.approx.ftz.bf16    b1, b2;\nex2.approx.ftz.bf16x2  hb1, hb2; 9.7.5. Comparison and Selection Instructions  The comparison select instructions are: set setp selp slct As with single-precision floating-point instructions, the set , setp , and slct instructions support subnormal numbers for sm_20 and higher targets and flush single-precision\nsubnormal inputs to sign-preserving zero for sm_1x targets. The optional .ftz modifier\nprovides backward compatibility with sm_1x targets by flushing subnormal inputs and results to\nsign-preserving zero regardless of the target architecture. 9.7.5.1. Comparison and Selection Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator. Syntax set.CmpOp{.ftz}.dtype.stype         d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.stype  d, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.dtype  = { .u32, .s32, .f32 };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 }; Description Compares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. If this result is True , 1.0f is written for floating-point\ndestination types, and 0xffffffff is written for integer destination types. Otherwise, 0x00000000 is written. Operand d has type .dtype ; operands a and b have type .stype ; operand c has\ntype .pred . Semantics t = (a CmpOp b) ? 1 : 0;\nif (isFloat(dtype))\n    d = BoolOp(t, c) ? 1.0f : 0x00000000;\nelse\n    d = BoolOp(t, c) ? 0xffffffff : 0x00000000; Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge . For unsigned values, the comparison operators lo , ls , hi , and hs for lower,\nlower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge ,\nrespectively. The untyped, bit-size comparisons are eq and ne . Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: sm_20+ By default, subnormal numbers are supported. set.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero. sm_1x set.dtype.f64 supports subnormal numbers. set.dtype.f32 flushes subnormal inputs to sign-preserving zero. Modifier .ftz applies only to .f32 comparisons. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes set with .f64 source type requires sm_13 or higher. Examples @p  set.lt.and.f32.s32  d,a,b,r;\n    set.eq.u32.u32      d,i,n; 9.7.5.2. Comparison and Selection Instructions: setp  setp Compare two numeric values with a relational operator, and (optionally) combine this result with a\npredicate value by applying a Boolean operator. Syntax setp.CmpOp{.ftz}.type         p[|q], a, b;\nsetp.CmpOp.BoolOp{.ftz}.type  p[|q], a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.type   = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 }; Description Compares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the first destination operand. A related value computed using\nthe complement of the compare result is written to the second destination operand. Applies to all numeric types. Operands a and b have type .type ; operands p , q ,\nand c have type .pred . The sink symbol ‘_’ may be used in place of any one of the\ndestination operands. Semantics t = (a CmpOp b) ? 1 : 0;\np = BoolOp(t, c);\nq = BoolOp(!t, c); Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge . For unsigned values, the comparison operators lo , ls , hi , and hs for lower,\nlower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge ,\nrespectively. The untyped, bit-size comparisons are eq and ne . Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: sm_20+ By default, subnormal numbers are supported. setp.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero. sm_1x setp.dtype.f64 supports subnormal numbers. setp.dtype.f32 flushes subnormal inputs to sign-preserving zero. Modifier .ftz applies only to .f32 comparisons. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes setp with .f64 source type requires sm_13 or higher. Examples setp.lt.and.s32  p|q,a,b,r;\n@q  setp.eq.u32      p,i,n; 9.7.5.3. Comparison and Selection Instructions: selp  selp Select between source operands, based on the value of the predicate source operand. Syntax selp.type d, a, b, c;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 }; Description Conditional selection. If c is True , a is stored in d , b otherwise. Operands d , a , and b must be of the same type. Operand c is a predicate. Semantics d = (c == 1) ? a : b; PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes selp.f64 requires sm_13 or higher. Examples selp.s32  r0,r,g,p;\n@q  selp.f32  f0,t,x,xp; 9.7.5.4. Comparison and Selection Instructions: slct  slct Select one source operand, based on the sign of the third operand. Syntax slct.dtype.s32        d, a, b, c;\nslct{.ftz}.dtype.f32  d, a, b, c;\n\n.dtype = { .b16, .b32, .b64,\n           .u16, .u32, .u64,\n           .s16, .s32, .s64,\n                 .f32, .f64 }; Description Conditional selection. If c ≥ 0, a is stored in d , otherwise b is stored in d . Operands d , a , and b are treated as a bitsize type of the same width as the first\ninstruction type; operand c must match the second instruction type ( .s32 or .f32 ). The\nselected input is copied to the output without modification. Semantics d = (c >= 0) ? a : b; Floating Point Notes For .f32 comparisons, negative zero equals zero. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. slct.ftz.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and\noperand a is selected. sm_1x slct.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand a is selected. Modifier .ftz applies only to .f32 comparisons. If operand c is NaN , the comparison is unordered and operand b is selected. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes slct.f64 requires sm_13 or higher. Examples slct.u32.s32  x, y, z, val;\nslct.ftz.u64.f32  A, B, C, fval; 9.7.6. Half Precision Comparison Instructions  The comparison instructions are: set setp 9.7.6.1. Half Precision Comparison Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator. Syntax set.CmpOp{.ftz}.f16.stype            d, a, b;\nset.CmpOp.BoolOp{.ftz}.f16.stype     d, a, b, {!}c;\n\nset.CmpOp.bf16.stype                 d, a, b;\nset.CmpOp.BoolOp.bf16.stype          d, a, b, {!}c;\n\nset.CmpOp{.ftz}.dtype.f16            d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16     d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp.dtype.bf16                 d, a, b;\nset.CmpOp.BoolOp.dtype.bf16          d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp{.ftz}.dtype.f16x2          d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16x2   d, a, b, {!}c;\n.dtype  = { .f16x2, .u32, .s32}\n\nset.CmpOp.dtype.bf16x2               d, a, b;\nset.CmpOp.BoolOp.dtype.bf16x2        d, a, b, {!}c;\n.dtype  = { .bf16x2, .u32, .s32}\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n            .f16, .f32, .f64}; Description Compares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. Result of this computation is written in destination register in the following way: If result is True , 0xffffffff is written for destination types .u32 / .s32 . 0xffff is written for destination types .u16 / .s16 . 1.0 in target precision floating point format is written for destination type .f16 , .bf16 . If result is False , 0x0 is written for all integer destination types. 0.0 in target precision floating point format is written for destination type .f16 , .bf16 . If the source type is .f16x2 or .bf16x2 then result of individual operations are packed in\nthe 32-bit destination operand. Operand c has type .pred . Semantics if (stype == .f16x2 || stype == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0]   = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1]   = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    if (dtype == .f16x2 || stype == .bf16x2) {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 1.0 : 0.0;\n        }\n    } else {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 0xffff : 0;\n        }\n    }\n} else if (dtype == .f16 || stype == .bf16) {\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? 1.0 : 0.0;\n} else  { // Integer destination type\n    trueVal = (isU16(dtype) || isS16(dtype)) ?  0xffff : 0xffffffff;\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? trueVal : 0;\n} Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: By default, subnormal numbers are supported. When .ftz modifier is specified then subnormal inputs and results are flushed to sign\npreserving zero. PTX ISA Notes Introduced in PTX ISA version 4.2. set.{u16, u32, s16, s32}.f16 and set.{u32, s32}.f16x2 are introduced in PTX ISA version 6.5. set.{u16, u32, s16, s32}.bf16 , set.{u32, s32, bf16x2}.bf16x2 , set.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64} are introduced in PTX ISA version\n7.8. Target ISA Notes Requires sm_53 or higher. set.{u16, u32, s16, s32}.bf16 , set.{u32, s32, bf16x2}.bf16x2 , set.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64} require sm_90 or higher. Examples set.lt.and.f16.f16  d,a,b,r;\nset.eq.f16x2.f16x2  d,i,n;\nset.eq.u32.f16x2    d,i,n;\nset.lt.and.u16.f16  d,a,b,r;\nset.ltu.or.bf16.f16    d,u,v,s;\nset.equ.bf16x2.bf16x2  d,j,m;\nset.geu.s32.bf16x2     d,j,m;\nset.num.xor.s32.bf16   d,u,v,s; 9.7.6.2. Half Precision Comparison Instructions: setp  setp Compare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator. Syntax setp.CmpOp{.ftz}.f16           p, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16    p, a, b, {!}c;\n\nsetp.CmpOp{.ftz}.f16x2         p|q, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16x2  p|q, a, b, {!}c;\n\nsetp.CmpOp.bf16                p, a, b;\nsetp.CmpOp.BoolOp.bf16         p, a, b, {!}c;\n\nsetp.CmpOp.bf16x2              p|q, a, b;\nsetp.CmpOp.BoolOp.bf16x2       p|q, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor }; Description Compares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the destination operand. Operand c , p and q has type .pred . For instruction type .f16 , operands a and b have type .b16 or .f16 . For instruction type .f16x2 , operands a and b have type .b32 . For instruction type .bf16 , operands a and b have type .b16 . For instruction type .bf16x2 , operands a and b have type .b32 . Semantics if (type == .f16 || type == .bf16) {\n     t = (a CmpOp b) ? 1 : 0;\n     p = BoolOp(t, c);\n} else if (type == .f16x2 || type == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    p = BoolOp(t[0], c);\n    q = BoolOp(t[1], c);\n} Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: By default, subnormal numbers are supported. setp.ftz.{f16,f16x2} flushes subnormal inputs to sign-preserving zero. PTX ISA Notes Introduced in PTX ISA version 4.2. setp.{bf16/bf16x2} introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. setp.{bf16/bf16x2} requires sm_90 or higher. Examples setp.lt.and.f16x2  p|q,a,b,r;\n@q  setp.eq.f16    p,i,n;\n\nsetp.gt.or.bf16x2  u|v,c,d,s;\n@q  setp.eq.bf16   u,j,m; 9.7.7. Logic and Shift Instructions  The logic and shift instructions are fundamentally untyped, performing bit-wise operations on\noperands of any type, provided the operands are of the same size. This permits bit-wise operations\non floating point values without having to define a union to access the bits. Instructions and , or , xor , and not also operate on predicates. The logical shift instructions are: and or xor not cnot lop3 shf shl shr 9.7.7.1. Logic and Shift Instructions: and  and Bitwise AND. Syntax and.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Compute the bit-wise and operation for the bits in a and b . Semantics d = a & b; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicate registers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples and.b32  x,q,r;\nand.b32  sign,fpvalue,0x80000000; 9.7.7.2. Logic and Shift Instructions: or  or Biwise OR. Syntax or.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Compute the bit-wise or operation for the bits in a and b . Semantics d = a | b; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicate registers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples or.b32  mask mask,0x00010001\nor.pred  p,q,r; 9.7.7.3. Logic and Shift Instructions: xor  xor Bitwise exclusive-OR (inequality). Syntax xor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Compute the bit-wise exclusive-or operation for the bits in a and b . Semantics d = a ^ b; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicate registers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples xor.b32  d,q,r;\nxor.b16  d,x,0x0001; 9.7.7.4. Logic and Shift Instructions: not  not Bitwise negation; one’s complement. Syntax not.type d, a;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Invert the bits in a . Semantics d = ~a; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicates. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples not.b32  mask,mask;\nnot.pred  p,q; 9.7.7.5. Logic and Shift Instructions: cnot  cnot C/C++ style logical negation. Syntax cnot.type d, a;\n\n.type = { .b16, .b32, .b64 }; Description Compute the logical negation using C/C++ semantics. Semantics d = (a==0) ? 1 : 0; Notes The size of the operands must match, but not necessarily the type. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples cnot.b32 d,a; 9.7.7.6. Logic and Shift Instructions: lop3  lop3 Arbitrary logical operation on 3 inputs. Syntax lop3.b32 d, a, b, c, immLut;\nlop3.BoolOp.b32 d|p, a, b, c, immLut, q;\n\n.BoolOp   = { .or , .and }; Description Compute bitwise logical operation on inputs a , b , c and store the result in destination d . Optionally, .BoolOp can be specified to compute the predicate result p by performing a\nBoolean operation on the destination operand d with the predicate q in the following manner: p = (d != 0) BoolOp q; The sink symbol ‘_’ may be used in place of the destination operand d when .BoolOp qualifier\nis specified. The logical operation is defined by a look-up table which, for 3 inputs, can be represented as an\n8-bit value specified by operand immLut as described below. immLut is an integer constant\nthat can take values from 0 to 255, thereby allowing up to 256 distinct logical operations on inputs a , b , c . For a logical operation F(a, b, c) the value of immLut can be computed by applying the same\noperation to three predefined constant values as follows: ta = 0xF0;\ntb = 0xCC;\ntc = 0xAA;\n\nimmLut = F(ta, tb, tc); Examples: If F = (a & b & c);\nimmLut = 0xF0 & 0xCC & 0xAA = 0x80\n\nIf F = (a | b | c);\nimmLut = 0xF0 | 0xCC | 0xAA = 0xFE\n\nIf F = (a & b & ~c);\nimmLut = 0xF0 & 0xCC & (~0xAA) = 0x40\n\nIf F = ((a & b | c) ^ a);\nimmLut = (0xF0 & 0xCC | 0xAA) ^ 0xF0 = 0x1A The following table illustrates computation of immLut for various logical operations: ta tb tc Oper 0 (False) Oper 1 (ta & tb & tc) Oper 2 (ta & tb & ~tc) … Oper 254 (ta | tb | tc) Oper 255 (True) 0 0 0 0 0 0 … 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 immLut 0x0 0x80 0x40 … 0xFE 0xFF Semantics F = GetFunctionFromTable(immLut); // returns the function corresponding to immLut value\nd = F(a, b, c);\nif (BoolOp specified) {\n    p = (d != 0) BoolOp q;\n} PTX ISA Notes Introduced in PTX ISA version 4.3. Support for .BoolOp qualifier introduced in PTX ISA version 8.2. Target ISA Notes Requires sm_50 or higher. Qualifier .BoolOp requires sm_70 or higher. Examples lop3.b32       d, a, b, c, 0x40;\nlop3.or.b32  d|p, a, b, c, 0x3f, q;\nlop3.and.b32 _|p, a, b, c, 0x3f, q; 9.7.7.7. Logic and Shift Instructions: shf  shf Funnel shift. Syntax shf.l.mode.b32  d, a, b, c;  // left shift\nshf.r.mode.b32  d, a, b, c;  // right shift\n\n.mode = { .clamp, .wrap }; Description Shift the 64-bit value formed by concatenating operands a and b left or right by the amount\nspecified by the unsigned 32-bit value in c . Operand b holds bits 63:32 and operand a\nholds bits 31:0 of the 64-bit source value. The source is shifted left or right by the clamped\nor wrapped value in c . For shf.l , the most-significant 32-bits of the result are written\ninto d ; for shf.r , the least-significant 32-bits of the result are written into d . Semantics u32  n = (.mode == .clamp) ? min(c, 32) : c & 0x1f;\nswitch (shf.dir) {  // shift concatenation of [b, a]\n    case shf.l:     // extract 32 msbs\n           u32  d = (b << n)      | (a >> (32-n));\n    case shf.r:     // extract 32 lsbs\n           u32  d = (b << (32-n)) | (a >> n);\n} Notes Use funnel shift for multi-word shift operations and for rotate operations. The shift amount is\nlimited to the range 0..32 in clamp mode and 0..31 in wrap mode, so shifting multi-word\nvalues by distances greater than 32 requires first moving 32-bit words, then using shf to shift\nthe remaining 0..31 distance. To shift data sizes greater than 64 bits to the right, use repeated shf.r instructions applied\nto adjacent words, operating from least-significant word towards most-significant word. At each\nstep, a single word of the shifted result is computed. The most-significant word of the result is\ncomputed using a shr.{u32,s32} instruction, which zero or sign fills based on the instruction\ntype. To shift data sizes greater than 64 bits to the left, use repeated shf.l instructions applied to\nadjacent words, operating from most-significant word towards least-significant word. At each step, a\nsingle word of the shifted result is computed. The least-significant word of the result is computed\nusing a shl instruction. Use funnel shift to perform 32-bit left or right rotate by supplying the same value for source\narguments a and b . PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Requires sm_32 or higher. Example shf.l.clamp.b32  r3,r1,r0,16;\n\n// 128-bit left shift; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] << n\nshf.l.clamp.b32  r7,r2,r3,n;\nshf.l.clamp.b32  r6,r1,r2,n;\nshf.l.clamp.b32  r5,r0,r1,n;\nshl.b32          r4,r0,n;\n\n// 128-bit right shift, arithmetic; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] >> n\nshf.r.clamp.b32  r4,r0,r1,n;\nshf.r.clamp.b32  r5,r1,r2,n;\nshf.r.clamp.b32  r6,r2,r3,n;\nshr.s32          r7,r3,n;     // result is sign-extended\n\nshf.r.clamp.b32  r1,r0,r0,n;  // rotate right by n; n < 32\nshf.l.clamp.b32  r1,r0,r0,n;  // rotate left by n; n < 32\n\n// extract 32-bits from [r1,r0] starting at position n < 32\nshf.r.clamp.b32  r0,r0,r1,n; 9.7.7.8. Logic and Shift Instructions: shl  shl Shift bits left, zero-fill on right. Syntax shl.type d, a, b;\n\n.type = { .b16, .b32, .b64 }; Description Shift a left by the amount specified by unsigned 32-bit value in b . Semantics d = a << b; Notes Shift amounts greater than the register width N are clamped to N . The sizes of the destination and first source operand must match, but not necessarily the type. The b operand must be a 32-bit value, regardless of the instruction type. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Example shl.b32  q,a,2; 9.7.7.9. Logic and Shift Instructions: shr  shr Shift bits right, sign or zero-fill on left. Syntax shr.type d, a, b;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Shift a right by the amount specified by unsigned 32-bit value in b . Signed shifts fill with\nthe sign bit, unsigned and untyped shifts fill with 0 . Semantics d = a >> b; Notes Shift amounts greater than the register width N are clamped to N . The sizes of the destination and first source operand must match, but not necessarily the type. The b operand must be a 32-bit value, regardless of the instruction type. Bit-size types are included for symmetry with shl . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Example shr.u16  c,a,2;\nshr.s32  i,i,1;\nshr.b16  k,i,j; 9.7.8. Data Movement and Conversion Instructions  These instructions copy data from place to place, and from state space to state space, possibly\nconverting it from one format to another. mov , ld , ldu , and st operate on both\nscalar and vector types. The isspacep instruction is provided to query whether a generic address\nfalls within a particular state space window. The cvta instruction converts addresses between generic and const , global , local , or shared state spaces. Instructions ld , st , suld , and sust support optional cache operations. The Data Movement and Conversion Instructions are: mov shfl.sync prmt ld ldu st st.async multimem.ld_reduce , multimem.st , multimem.red prefetch , prefetchu isspacep cvta cvt cvt.pack cp.async cp.async.commit_group cp.async.wait_group , cp.async.wait_all cp.async.bulk cp.reduce.async.bulk cp.async.bulk.prefetch cp.async.bulk.tensor cp.reduce.async.bulk.tensor cp.async.bulk.prefetch.tensor cp.async.bulk.commit_group cp.async.bulk.wait_group tensormap.replace 9.7.8.1. Cache Operators  PTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache\noperators require a target architecture of sm_20 or higher. Cache operators on load or store instructions are treated as performance hints only. The use of a\ncache operator on an ld or st instruction does not change the memory consistency behavior of\nthe program. For sm_20 and higher, the cache operators have the following definitions and behavior. Table 27 Cache Operators for Memory Load Instructions  Operator Meaning .ca Cache at all levels, likely to be accessed again. The default load instruction cache operation is ld.ca, which allocates cache lines in all\nlevels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but\nmultiple L1 caches are not coherent for global data. If one thread stores to global memory\nvia one L1 cache, and a second thread loads that address via a second L1 cache with ld.ca ,\nthe second thread may get stale L1 cache data, rather than the data stored by the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of parallel threads.\nStores by the first grid program are then correctly fetched by the second grid program issuing\ndefault ld.ca loads cached in L1. .cg Cache at global level (cache in L2 and below, not L1). Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2\ncache. .cs Cache streaming, likely to be accessed once. The ld.cs load cached streaming operation allocates global lines with evict-first policy in\nL1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or\ntwice. When ld.cs is applied to a Local window address, it performs the ld.lu operation. .lu Last use. The compiler/programmer may use ld.lu when restoring spilled registers and popping function\nstack frames to avoid needless write-backs of lines that will not be used again. The ld.lu instruction performs a load cached streaming operation ( ld.cs ) on global addresses. .cv Don’t cache and fetch again (consider cached system memory lines stale, fetch again). The ld.cv load operation applied to a global System Memory address invalidates (discards) a\nmatching L2 line and re-fetches the line on each new load. Table 28 Cache Operators for Memory Store Instructions  Operator Meaning .wb Cache write-back all coherent levels. The default store instruction cache operation is st.wb , which writes back cache lines of\ncoherent cache levels with normal eviction policy. If one thread stores to global memory, bypassing its L1 cache, and a second thread in a\ndifferent SM later loads from that address via a different L1 cache with ld.ca , the second\nthread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored\nby the first thread. The driver must invalidate global L1 cache lines between dependent grids of thread arrays.\nStores by the first grid program are then correctly missed in L1 and fetched by the second grid\nprogram issuing default ld.ca loads. .cg Cache at global level (cache in L2 and below, not L1). Use st.cg to cache global store data only globally, bypassing the L1 cache, and cache only\nin the L2 cache. .cs Cache streaming, likely to be accessed once. The st.cs store cached-streaming operation allocates cache lines with evict-first policy to\nlimit cache pollution by streaming output data. .wt Cache write-through (to system memory). The st.wt store write-through operation applied to a global System Memory address writes\nthrough the L2 cache. 9.7.8.2. Cache Eviction Priority Hints  PTX ISA version 7.4 adds optional cache eviction priority hints on load and store\ninstructions. Cache eviction priority requires target architecture sm_70 or higher. Cache eviction priority on load or store instructions is treated as a performance hint. It is\nsupported for .global state space and generic addresses where the address points to .global state space. Table 29 Cache Eviction Priority Hints for Memory Load and Store Instructions  Cache Eviction Priority Meaning evict_normal Cache data with normal eviction priority. This is the default eviction priority. evict_first Data cached with this priority will be first in the eviction priority order and\nwill likely be evicted when cache eviction is required. This priority is suitable\nfor streaming data. evict_last Data cached with this priority will be last in the eviction priority order and will\nlikely be evicted only after other data with evict_normal or evict_first eviction priotity is already evicted. This priority is suitable for data that\nshould remain persistent in cache. evict_unchanged Do not change eviction priority order as part of this operation. no_allocate Do not allocate data to cache. This priority is suitable for streaming data. 9.7.8.3. Data Movement and Conversion Instructions: mov  mov Set a register variable with the value of a register variable or an immediate value. Take the\nnon-generic address of a variable in global, local, or shared state space. Syntax mov.type  d, a;\nmov.type  d, sreg;\nmov.type  d, avar;       // get address of variable\nmov.type  d, avar+imm;   // get address of variable with offset\nmov.u32   d, fname;      // get address of device function\nmov.u64   d, fname;      // get address of device function\nmov.u32   d, kernel;     // get address of entry function\nmov.u64   d, kernel;     // get address of entry function\n\n.type = { .pred,\n          .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 }; Description Write register d with the value of a . Operand a may be a register, special register, variable with optional offset in an addressable\nmemory space, or function name. For variables declared in .const , .global , .local , and .shared state spaces, mov places the non-generic address of the variable (i.e., the address of the variable in its state\nspace) into the destination register. The generic address of a variable in const , global , local , or shared state space may be generated by first taking the address within the state\nspace with mov and then converting it to a generic address using the cvta instruction;\nalternately, the generic address of a variable declared in const , global , local , or shared state space may be taken directly using the cvta instruction. Note that if the address of a device function parameter is moved to a register, the parameter will\nbe copied onto the stack and the address will be in the local state space. Semantics d = a;\nd = sreg;\nd = &avar;        // address is non-generic; i.e., within the variable's declared state space\nd = &avar+imm; Notes Although only predicate and bit-size types are required, we include the arithmetic types for the\nprogrammer’s convenience: their use enhances program readability and allows additional type\nchecking. When moving address of a kernel or a device function, only .u32 or .u64 instruction types\nare allowed. However, if a signed type is used, it is not treated as a compilation error. The\ncompiler issues a warning in this case. PTX ISA Notes Introduced in PTX ISA version 1.0. Taking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function\naddresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the CUDA\nDynamic Parallelism Programming Guide for details. Target ISA Notes mov.f64 requires sm_13 or higher. Taking the address of kernel entry functions requires sm_35 or higher. Examples mov.f32  d,a;\nmov.u16  u,v;\nmov.f32  k,0.1;\nmov.u32  ptr, A;        // move address of A into ptr\nmov.u32  ptr, A[5];     // move address of A[5] into ptr\nmov.u32  ptr, A+20;     // move address with offset into ptr\nmov.u32  addr, myFunc;  // get address of device function 'myFunc'\nmov.u64  kptr, main;    // get address of entry function 'main' 9.7.8.4. Data Movement and Conversion Instructions: mov  mov Move vector-to-scalar (pack) or scalar-to-vector (unpack). Syntax mov.type  d, a;\n\n.type = { .b16, .b32, .b64, .b128 }; Description Write scalar register d with the packed value of vector register a , or write vector register d with the unpacked values from scalar register a . When destination operand d is a vector register, the sink symbol '_' may be used for one or\nmore elements provided that at least one element is a scalar register. For bit-size types, mov may be used to pack vector elements into a scalar register or unpack\nsub-fields of a scalar register into a vector. Both the overall size of the vector and the size of\nthe scalar must match the size of the instruction type. Semantics // pack two 8-bit elements into .b16\nd = a.x | (a.y << 8)\n// pack four 8-bit elements into .b32\nd = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)\n// pack two 16-bit elements into .b32\nd = a.x | (a.y << 16)\n// pack four 16-bit elements into .b64\nd = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)\n// pack two 32-bit elements into .b64\nd = a.x | (a.y << 32)\n// pack four 32-bit elements into .b128\nd = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)\n// pack two 64-bit elements into .b128\nd = a.x | (a.y << 64)\n\n// unpack 8-bit elements from .b16\n{ d.x, d.y } = { a[0..7], a[8..15] }\n// unpack 8-bit elements from .b32\n{ d.x, d.y, d.z, d.w }\n        { a[0..7], a[8..15], a[16..23], a[24..31] }\n\n// unpack 16-bit elements from .b32\n{ d.x, d.y }  = { a[0..15], a[16..31] }\n// unpack 16-bit elements from .b64\n{ d.x, d.y, d.z, d.w } =\n        { a[0..15], a[16..31], a[32..47], a[48..63] }\n\n// unpack 32-bit elements from .b64\n{ d.x, d.y } = { a[0..31], a[32..63] }\n\n// unpack 32-bit elements from .b128\n{ d.x, d.y, d.z, d.w } =\n        { a[0..31], a[32..63], a[64..95], a[96..127] }\n// unpack 64-bit elements from .b128\n{ d.x, d.y } = { a[0..63], a[64..127] } PTX ISA Notes Introduced in PTX ISA version 1.0. Support for .b128 type introduced in PTX ISA version 8.3. Target ISA Notes Supported on all target architectures. Support for .b128 type requires sm_70 or higher. Examples mov.b32 %r1,{a,b};      // a,b have type .u16\nmov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32\nmov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8\nmov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8\nmov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32\nmov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64\nmov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64 9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated)  shfl (deprecated) Register data shuffle within threads of a warp. Syntax shfl.mode.b32  d[|p], a, b, c;\n\n.mode = { .up, .down, .bfly, .idx }; Deprecation Note The shfl instruction without a .sync qualifier is deprecated in PTX ISA version 6.0. Support for this instruction with .target lower than sm_70 may be removed in a future PTX ISA version. Removal Note Support for shfl instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .target sm_70 or higher. Description Exchange register data between threads of a warp. Each thread in the currently executing warp will compute a source lane index j based on input\noperands b and c and the mode . If the computed source lane index j is in range, the\nthread will copy the input operand a from lane j into its own destination register d ;\notherwise, the thread will simply copy its own input a to destination d . The optional\ndestination predicate p is set to True if the computed source lane is in range, and\notherwise set to False . Note that an out of range value of b may still result in a valid computed source lane index j . In this case, a data transfer occurs and the destination predicate p is True. Note that results are undefined in divergent control flow within a warp, if an active thread sources\na register from an inactive thread. Operand b specifies a source lane or source lane offset, depending on the mode. Operand c contains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index. Semantics lane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);\nminLane = (lane[4:0] & mask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval; PTX ISA Notes Introduced in PTX ISA version 3.0. Deprecated in PTX ISA version 6.0 in favor of shfl.sync . Not supported in PTX ISA version 6.4 for .target sm_70 or higher. Target ISA Notes shfl requires sm_30 or higher. shfl is not supported on sm_70 or higher starting PTX ISA version 6.4. Examples // Warp-level INCLUSIVE PLUS SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;\n@p  add.f32      Rx, Ry, Rx;\n\n\n    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;\n@p  add.f32        Rx, Ry, Rx;\n\n\n    // BUTTERFLY REDUCTION:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    //\n    // All threads now hold sum in Rx 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync  shfl.sync Register data shuffle within threads of a warp. Syntax shfl.sync.mode.b32  d[|p], a, b, c, membermask;\n\n.mode = { .up, .down, .bfly, .idx }; Description Exchange register data between threads of a warp. shfl.sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed shfl.sync with the same qualifiers and same membermask value\nbefore resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s laneid . shfl.sync exchanges register data between threads in membermask . Each thread in the currently executing warp will compute a source lane index j based on input\noperands b and c and the mode . If the computed source lane index j is in range, the\nthread will copy the input operand a from lane j into its own destination register d ;\notherwise, the thread will simply copy its own input a to destination d . The optional\ndestination predicate p is set to True if the computed source lane is in range, and\notherwise set to False . Note that an out of range value of b may still result in a valid computed source lane index j . In this case, a data transfer occurs and the destination predicate p is True. Note that results are undefined if a thread sources a register from an inactive thread or a thread\nthat is not in membermask . Operand b specifies a source lane or source lane offset, depending on the mode. Operand c contains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index. The behavior of shfl.sync is undefined if the executing thread is not in the membermask . Note For .target sm_6x or below, all threads in membermask must execute the same shfl.sync instruction in convergence, and only threads belonging to some membermask can be active when\nthe shfl.sync instruction is executed. Otherwise, the behavior is undefined. Semantics // wait for all threads in membermask to arrive\nwait_for_specified_threads(membermask);\n\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nsegmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);\nminLane = (lane[4:0] & segmask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval; PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples shfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff; 9.7.8.7. Data Movement and Conversion Instructions: prmt  prmt Permute bytes from register pair. Syntax prmt.b32{.mode}  d, a, b, c;\n\n.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 }; Description Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination\nregister. In the generic form (no mode specified), the permute control consists of four 4-bit selection\nvalues. The bytes in the two source registers are numbered from 0 to 7: {b, a} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}} . For each byte in the target register, a 4-bit selection value is defined. The 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the\ntarget position. The msb defines if the byte value should be copied, or if the sign (msb of the\nbyte) should be replicated over all 8 bits of the target position (sign extend of the byte value); msb=0 means copy the literal value; msb=1 means replicate the sign. Note that the sign\nextension is only performed as part of generic form. Thus, the four 4-bit values fully specify an arbitrary byte permute, as a 16b permute code. default mode d.b3 source select d.b2 source select d.b1 source select d.b0 source select index c[15:12] c[11:8] c[7:4] c[3:0] The more specialized form of the permute control uses the two lsb’s of operand c (which is\ntypically an address pointer) to control the byte extraction. mode selector c[1:0] d.b3 source d.b2 source d.b1 source d.b0 source f4e (forward 4 extract) 0 3 2 1 0 1 4 3 2 1 2 5 4 3 2 3 6 5 4 3 b4e (backward 4 extract) 0 5 6 7 0 1 6 7 0 1 2 7 0 1 2 3 0 1 2 3 rc8 (replicate 8) 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 ecl (edge clamp left) 0 3 2 1 0 1 3 2 1 1 2 3 2 2 2 3 3 3 3 3 ecr (edge clamp right) 0 0 0 0 0 1 1 1 1 0 2 2 2 1 0 3 3 2 1 0 rc16 (replicate 16) 0 1 0 1 0 1 3 2 3 2 2 1 0 1 0 3 3 2 3 2 Semantics tmp64 = (b<<32) | a;  // create 8 byte source\n\nif ( ! mode ) {\n   ctl[0] = (c >>  0) & 0xf;\n   ctl[1] = (c >>  4) & 0xf;\n   ctl[2] = (c >>  8) & 0xf;\n   ctl[3] = (c >> 12) & 0xf;\n} else {\n   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;\n}\n\ntmp[07:00] = ReadByte( mode, ctl[0], tmp64 );\ntmp[15:08] = ReadByte( mode, ctl[1], tmp64 );\ntmp[23:16] = ReadByte( mode, ctl[2], tmp64 );\ntmp[31:24] = ReadByte( mode, ctl[3], tmp64 ); PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes prmt requires sm_20 or higher. Examples prmt.b32      r1, r2, r3, r4;\nprmt.b32.f4e  r1, r2, r3, r4; 9.7.8.8. Data Movement and Conversion Instructions: ld  ld Load a register variable from an addressable state space variable. Syntax ld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];\n\nld.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.acquire.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.mmio.relaxed.sys{.global}.type  d, [a];\n\n.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };\n.cop =                      { .ca, .cg, .cs, .lu, .cv };\n.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                              .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 }; Description Load register variable d from the location specified by the source address operand a in\nspecified state space. If no state space is given, perform the load using Generic Addressing . If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands If no sub-qualifier is specified with .param state space, then : ::func is assumed when access is inside a device function. ::entry is assumed when accessing kernel function parameters from entry function. Otherwise, when\naccessing device function parameters or any other .param variables from entry function ::func is assumed by default. For ld.param::entry instruction, operand a must be a kernel parameter address, otherwise behavior\nis undefined. For ld.param::func instruction, operand a must be a device function parameter address,\notherwise behavior is undefined. Instruction ld.param{::func} used for reading value returned from device function call cannot be\npredicated. See Parameter State Space and Function Declarations and Definitions for descriptions\nof the proper use of ld.param . The .relaxed and .acquire qualifiers indicate memory synchronization as described in the Memory Consistency Model . The .scope qualifier\nindicates the set of threads with which an ld.relaxed or ld.acquire instruction can directly\nsynchronize 1 . The .weak qualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means. The semantic details of .mmio qualifier are described in the Memory Consistency Model . Only .sys thread scope is valid for ld.mmio operation. The\nqualifiers .mmio and .relaxed must be specified together. The .weak , .volatile , .relaxed and .acquire qualifiers are mutually exclusive. When\nnone of these is specified, the .weak qualifier is assumed by default. An ld.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location. volatile and non-volatile load operations\nto the same memory location may be reordered. ld.volatile has the same memory synchronization\nsemantics as ld.relaxed.sys . The qualifiers .volatile , .relaxed and .acquire may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio may be used only with .global space and with generic addressing, where the address points to .global space. The optional qualifier .unified must be specified on operand a if a is the address of a\nvariable declared with .unified attribute as described in Variable and Function Attribute\nDirective: .attribute . The qualifier .level::eviction_priority specifies the eviction policy that will be used during\nmemory access. The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively. The qualifier .level::prefetch_size may only be used with .global state space and with\ngeneric addressing where the address points to .global state space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifiers .unified and .level::cache_hint are only supported for .global state\nspace and for generic addressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. 1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model. Semantics d = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See Table 25 for a description of these relaxed type-checking rules. .f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions. .f16x2 data may be loaded using ld.b32 and then used in half precision floating point\ninstructions. PTX ISA Notes ld introduced in PTX ISA version 1.0. ld.volatile introduced in PTX ISA version 1.1. Generic addressing and cache operations introduced in PTX ISA version 2.0. Support for scope qualifier, .relaxed , .acquire , .weak qualifiers introduced in PTX ISA\nversion 6.0. Support for generic addressing of .const space added in PTX ISA version 3.1. Support for .level::eviction_priority , .level::prefetch_size and .level::cache_hint qualifiers introduced in PTX ISA version 7.4. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for .unified qualifier introduced in PTX ISA version 8.0. Support for .mmio qualifier introduced in PTX ISA version 8.2. Support for ::entry and ::func sub-qualifiers on .param space introduced in PTX ISA\nversion 8.3. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes ld.f64 requires sm_13 or higher. Support for scope qualifier, .relaxed , .acquire , .weak qualifiers require sm_70 or\nhigher. Generic addressing requires sm_20 or higher. Cache operations require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::prefetch_size qualifier requires sm_75 or higher. Support for .L2::256B and .L2::cache_hint qualifiers requires sm_80 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .unified qualifier requires sm_90 or higher. Support for .mmio qualifier requires sm_70 or higher. Support for .b128 type requires sm_70 or higher. Examples ld.global.f32    d,[a];\nld.shared.v4.b32 Q,[p];\nld.const.s32     d,[p+4];\nld.local.b32     x,[p+-8]; // negative offset\nld.local.b64     x,[240];  // immediate address\n\nld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg\ncvt.f32.f16      %r,%r;    // up-convert f16 data to f32\n\nld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg\nld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg\nadd.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data\nld.global.relaxed.gpu.u32 %r0, [gbl];\nld.shared.acquire.gpu.u32 %r1, [sh];\nld.global.relaxed.cluster.u32 %r2, [gbl];\nld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];\nld.shared::cluster.u32 %r3, [sh + 8];\nld.global.mmio.relaxed.sys.u32 %r3, [gbl];\n\nld.global.f32    d,[ugbl].unified;\nld.b32           %r0, [%r1].unified;\n\nld.global.L1::evict_last.u32  d, [p];\n\nld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2\nld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2\nld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;\nld.global.L2::cache_hint.b64  x, [p], cache-policy;\nld.param::entry.b32 %rp1, [kparam1];\n\nld.global.b128   %r0, [gbl];   // 128-bit load 9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc  ld.global.nc Load a register variable from global state space via non-coherent cache. Syntax ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};\n\nld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};\nld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};\n\n.cop  =                     { .ca, .cg, .cs };     // cache operation\n.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                              .L1::evict_first, .L1::evict_last, .L1::no_allocate};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.vec  =                     { .v2, .v4 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 }; Description Load register variable d from the location specified by the source address operand a in the\nglobal state space, and optionally cache in non-coherent read-only cache. Note On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than\nthe global memory cache. For applications with sufficient parallelism to cover the longer\nlatency, ld.global.nc should offer better performance than ld.global on such\narchitectures. The address operand a may contain a generic address pointing to the .global state space. Supported addressing modes for operand a and alignment requirements are\ndescribed in Addresses as Operands The qualifier .level::eviction_priority specifies the eviction policy that will be used during\nmemory access. The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. Semantics d = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. .f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt . PTX ISA Notes Introduced in PTX ISA version 3.1. Support for .level::eviction_priority , .level::prefetch_size and .level::cache_hint qualifiers introduced in PTX ISA version 7.4. Support for .b128 type introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_32 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::prefetch_size qualifier requires sm_75 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. Support for .b128 type requires sm_70 or higher. Examples ld.global.nc.f32           d, [a];\nld.gloal.nc.L1::evict_last.u32 d, [a];\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;\nld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;\n\nld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2\nld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2\n\nld.global.nc.b128             d,  [a]; 9.7.8.10. Data Movement and Conversion Instructions: ldu  ldu Load read-only data from an address that is common across threads in the warp. Syntax ldu{.ss}.type      d, [a];       // load from address\nldu{.ss}.vec.type  d, [a];       // vec load from address\n\n.ss   = { .global };             // state space\n.vec  = { .v2, .v4 };\n.type = { .b8, .b16, .b32, .b64, .b128,\n          .u8, .u16, .u32, .u64,\n          .s8, .s16, .s32, .s64,\n                     .f32, .f64 }; Description Load read-only data into register variable d from the location specified by the source address\noperand a in the global state space, where the address is guaranteed to be the same across all\nthreads in the warp. If no state space is given, perform the load using Generic Addressing . Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands Semantics d = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See Table 25 for a description of these relaxed type-checking rules. .f16 data may be loaded using ldu.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions. .f16x2 data may be loaded using ldu.b32 and then used in half precision floating point\ninstructions. PTX ISA Notes Introduced in PTX ISA version 2.0. Support for .b128 type introduced in PTX ISA version 8.3. Target ISA Notes ldu.f64 requires sm_13 or higher. Support for .b128 type requires sm_70 or higher. Examples ldu.global.f32    d,[a];\nldu.global.b32    d,[p+4];\nldu.global.v4.f32 Q,[p];\nldu.global.b128   d,[a]; 9.7.8.11. Data Movement and Conversion Instructions: st  st Store data to an addressable state space variable. Syntax st{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};\nst{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.volatile{.ss}{.vec}.type                           [a], b;\nst.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.release.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.mmio.relaxed.sys{.global}.type         [a], b;\n\n.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };\n.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                              .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level::cache_hint =        { .L2::cache_hint };\n.cop =                      { .wb, .cg, .cs, .wt };\n.sem =                      { .relaxed, .release };\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 }; Description Store the value of operand b in the location specified by the destination address\noperand a in specified state space. If no state space is given, perform the store using Generic\nAddressing . Stores to const memory are illegal. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands If .param is specified without any sub-qualifiers then it defaults to .param::func . Instruction st.param{::func} used for passing arguments to device function cannot be predicated.\nSee Parameter State Space and Function Declarations and\nDefinitions for descriptions of the proper use\nof st.param . The qualifiers .relaxed and .release indicate memory synchronization as described in the Memory Consistency Model . The .scope qualifier\nindicates the set of threads with which an st.relaxed or st.release instruction can directly\nsynchronize 1 . The .weak qualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means. The semantic details of .mmio qualifier are described in the Memory Consistency Model . Only .sys thread scope is valid for st.mmio operation. The\nqualifiers .mmio and .relaxed must be specified together. The .weak , .volatile , .relaxed and .release qualifiers are mutually exclusive. When\nnone of these is specified, the .weak qualifier is assumed by default. An st.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location. st.volatile has the same memory\nsynchronization semantics as st.relaxed.sys . The qualifiers .volatile , .relaxed and .release may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio may be used only with .global space and with generic addressing, where the address points to .global space. The qualifier .level::eviction_priority specifies the eviction policy that will be used during\nmemory access. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. 1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model. Semantics d = a;                // named variable d\n*(&a+immOffset) = b;            // variable-plus-offset\n*a = b;               // register\n*(a+immOffset) = b;   // register-plus-offset\n*(immAddr) = b;       // immediate address Notes Operand b must be in the .reg state space. A source register wider than the specified type may be used. The lower n bits corresponding to\nthe instruction-type width are stored to memory. See Table 24 for a description of these relaxed type-checking rules. .f16 data resulting from a cvt instruction may be stored using st.b16 . .f16x2 data may be stored using st.b32 . PTX ISA Notes st introduced in PTX ISA version 1.0. st.volatile introduced in PTX ISA version 1.1. Generic addressing and cache operations introduced in PTX ISA version 2.0. Support for scope qualifier, .relaxed , .release , .weak qualifiers introduced in PTX ISA\nversion 6.0. Support for .level::eviction_priority and .level::cache_hint qualifiers introduced in PTX\nISA version 7.4. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for .mmio qualifier introduced in PTX ISA version 8.2. Support for ::func sub-qualifier on .param space introduced in PTX ISA version 8.3. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes st.f64 requires sm_13 or higher. Support for scope qualifier, .relaxed , .release , .weak qualifiers require sm_70 or\nhigher. Generic addressing requires sm_20 or higher. Cache operations require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .mmio qualifier requires sm_70 or higher. Support for .b128 type requires sm_70 or higher. Examples st.global.f32    [a],b;\nst.local.b32     [q+4],a;\nst.global.v4.s32 [p],Q;\nst.local.b32     [q+-8],a; // negative offset\nst.local.s32     [100],r7; // immediate address\n\ncvt.f16.f32      %r,%r;    // %r is 32-bit register\nst.b16           [fs],%r;  // store lower\nst.global.relaxed.sys.u32 [gbl], %r0;\nst.shared.release.cta.u32 [sh], %r1;\nst.global.relaxed.cluster.u32 [gbl], %r2;\nst.shared::cta.release.cta.u32 [sh + 4], %r1;\nst.shared::cluster.u32 [sh + 8], %r1;\nst.global.mmio.relaxed.sys.u32 [gbl], %r1;\n\nst.global.L1::no_allocate.f32 [p], a;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nst.global.L2::cache_hint.b32  [a], b, cache-policy;\n\nst.param::func.b64 [param1], %rp1;\n\nst.global.b128  [a], b;  // 128-bit store 9.7.8.12. Data Movement and Conversion Instructions: st.async  st.async Asynchronous store operation on shared memory. Syntax st.async{.weak}{.ss}{.completion_mechanism}{.vec}.type [a], b, [mbar];\n\n.ss   =                 { .shared::cluster };\n.type =                 { .b32, .b64,\n                          .u32, .u64,\n                          .s32, .s64,\n                          .f32, .f64 };\n.vec  =                 { .v2, .v4 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes }; Description st.async is a non-blocking instruction which initiates an asynchronous store operation that\nstores the value specified by source operand b to the destination memory location\nspecified by operand a . The modifier .completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands . The shared memory addresses of destination operand a and the mbarrier object mbar , must\nmeet all of the following conditions: They belong to the same CTA. They are different to the CTA of the executing thread but must be within the same cluster. Otherwise, the behavior is undefined. The state space of the address {.ss} , if specified, is applicable to both operands a and mbar . If not specified, then Generic Addressing is used for\nboth a and mbar . If the generic addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined. The store operation in st.async is treated as a weak memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples st.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr] 9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red  The multimem.* operations operate on multimem addresses and accesses all of the multiple memory\nlocations which the multimem address points to. Multimem addresses can only be accessed only by multimem.* operations. Accessing a multimem address\nwith ld , st or any other memory operations results in undefined behavior. Refer to CUDA programming guide for creation and management of the multimem addresses. multimem.ld_reduce, multimem.st, multimem.red Perform memory operations on the multimem address. Syntax // Integer type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];\nmultimem.st{.stsem}{.scope}{.ss}.type                [a], b;\nmultimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;\n\n.ss =       { .global }\n.ldsem =    { .weak, .relaxed, .acquire }\n.stsem =    { .weak, .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add, .and, .or, .xor }\n.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }\n\n// Floating point type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];\nmultimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;\nmultimem.red{.redsem}{.scope}{.ss}.redop{.vec}.type                 [a], b;\n\n.ss =       { .global }\n.ldsem =    { .weak, .relaxed, .acquire }\n.stsem =    { .weak, .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add }\n.redop  =   { .add }\n.acc_prec = { .acc::f32 }\n.vec =      { .v2, .v4, .v8 }\n.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 } Description Instruction multimem.ld_reduce performs the following operations: load operation on the multimem address a , which involves loading of data from all of the\nmultiple memory locations pointed to by the multimem address a , reduction operation specified by .op on the multiple data loaded from the multimem address a . The result of the reduction operation in returned in register d . Instruction multimem.st performs a store operation of the input operand b to all the memory\nlocations pointed to by the multimem address a . Instruction multimem.red performs a reduction operation on all the memory locations pointed to\nby the multimem address a , with operand b . Instruction multimem.ld_reduce performs reduction on the values loaded from all the memory\nlocations that the multimem address points to. In contrast, the multimem.red perform reduction\non all the memory locations that the multimem address points to. Address operand a must be a multimem address. Otherwise, the behavior is undefined.  Supported\naddressing modes for operand a and alignment requirements are described in Addresses as Operands . If no state space is specified then Generic Addressing is\nused. If the address specified by a does not fall within the address window of .global state\nspace then the behavior is undefined. For floating-point type multi- operations, the size of the specified type along with .vec must\nequal either 32-bits or 64-bits or 128-bits. No other combinations of .vec and type are\nallowed. Type .f64 cannot be used with .vec qualifier. The following table describes the valid combinations of .op and base type: op Base type .add .u32 , .u64 , .s32 .f16 , .f16x2 , .bf16 , .bf16x2 .f32 , .f64 .and , .or , .xor .b32 , .b64 .min , .max .u32 , .s32 , .u64 , .s644 .f16 , .f16x2 , .bf16 , .bf16x2 For multimem.ld_reduce , the default precision of the intermediate accumulation is same as the\nspecified type. Optionally for .f16 , .f16x2 , .bf16 and .bf16x2 types, .acc::f32 can be specified to change the precision of the intermediate accumulation to .f32 . Optional qualifiers .ldsem , .stsem and .redsem specify the memory synchronizing effect\nof the multimem.ld_reduce , multimem.st and multimem.red respectively, as described in Memory Consistency Model . If explicit semantics qualifiers\nare not specified, then multimem.ld_reduce and multimem.st default to .weak and multimem.red defaults to .relaxed . The optional .scope qualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in Memory Consistency Model . If the .scope qualifier is not specified for multimem.red then .sys scope is assumed by default. PTX ISA Notes Introduced in PTX ISA version 8.1. Support for .acc::f32 qualifier introduced in PTX ISA version 8.2. Target ISA Notes Requires sm_90 or higher. Examples multimem.ld_reduce.and.b32                    val1_b32, [addr1];\nmultimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];\n\nmultimem.st.relaxed.gpu.b32                [addr3], val3_b32;\nmultimem.st.release.cta.global.u32         [addr4], val4_u32;\n\nmultimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;\nmultimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};\nmultimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7]; 9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu  prefetch, prefetchu Prefetch line containing a generic address at a specified level of memory hierarchy, in specified\nstate space. Syntax prefetch{.space}.level                    [a];   // prefetch to data cache\nprefetch.global.level::eviction_priority  [a];   // prefetch to data cache\n\nprefetchu.L1  [a];             // prefetch to uniform cache\n\nprefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap\n\n.space =                    { .global, .local };\n.level =                    { .L1, .L2 };\n.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };\n.tensormap_space =          { .const, .param }; Description The prefetch instruction brings the cache line containing the specified address in global or\nlocal memory state space into the specified cache level. If the .tensormap qualifier is specified then the prefetch instruction brings the cache line\ncontaining the specified address in the .const or .param memory state space for subsequent\nuse by the cp.async.bulk.tensor instruction. If no state space is given, the prefetch uses Generic Addressing . Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the\nmodifier .level::eviction_priority . Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands The prefetchu instruction brings the cache line containing the specified generic address into\nthe specified uniform cache level. A prefetch to a shared memory location performs no operation. A prefetch into the uniform cache requires a generic address, and no operation occurs if the\naddress maps to a const , local , or shared memory location. PTX ISA Notes Introduced in PTX ISA version 2.0. Support for .level::eviction_priority qualifier introduced in PTX ISA version 7.4. Support for the .tensormap qualifier is introduced in PTX ISA version 8.0. Target ISA Notes prefetch and prefetchu require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_80 or higher. Support for the .tensormap qualifier requires sm_90 or higher. Examples prefetch.global.L1             [ptr];\nprefetch.global.L2::evict_last [ptr];\nprefetchu.L1  [addr];\nprefetch.global.tensormap      [ptr]; 9.7.8.15. Data Movement and Conversion Instructions: applypriority  applypriority Apply the cache eviction priority to the specified address in the specified cache level. Syntax applypriority{.global}.level::eviction_priority  [a], size;\n\n.level::eviction_priority = { .L2::evict_normal }; Description The applypriority instruction applies the cache eviction priority specified by the .level::eviction_priority qualifier to the address range [a..a+size) in the specified cache\nlevel. If no state space is specified then Generic Addressing is\nused. If the specified address does not fall within the address window of .global state space\nthen the behavior is undefined. The operand size is an integer constant that specifies the amount of data, in bytes, in the\nspecified cache level on which the priority is to be applied. The only supported value for the size operand is 128. Supported addressing modes for operand a are described in Addresses as Operands . a must be aligned to 128 bytes. If the data pointed to by address a is not already present in the specified cache level, then\nthe data will be prefetched before applying the specified priority. PTX ISA Notes Introduced in PTX ISA version 7.4. Target ISA Notes Requires sm_80 or higher. Examples applypriority.global.L2::evict_normal [ptr], 128; 9.7.8.16. Data Movement and Conversion Instructions: discard  discard Invalidate the data in cache at the specified address and cache level. Syntax discard{.global}.level  [a], size;\n\n.level = { .L2 }; Description The discard instruction invalidates the data at the address range [a .. a + (size - 1)] in\nthe cache level specified by the .level qualifier without writing back the data in the cache to\nthe memory. Therefore after the discard operation, the data at the address range [a .. a+ (size - 1)] has undetermined value. The operand size is an integer constant that specifies the amount of data, in bytes, in the\ncache level specified by the .level qualifier to be discarded. The only supported value for the size operand is 128. If no state space is specified then Generic Addressing is\nused. If the specified address does not fall within the address window of .global state space\nthen the behavior is undefined. Supported addressing modes for address operand a are described in Addresses as Operands . a must be aligned to 128 bytes. PTX ISA Notes Introduced in PTX ISA version 7.4. Target ISA Notes Requires sm_80 or higher. Examples discard.global.L2 [ptr], 128; 9.7.8.17. Data Movement and Conversion Instructions: createpolicy  createpolicy Create a cache eviction policy for the specified cache level. Syntax // Range-based policy\ncreatepolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy, [a], primary-size, total-size;\n\n// Fraction-based policy\ncreatepolicy.fractional.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy{, fraction};\n\n// Converting the access property from CUDA APIs\ncreatepolicy.cvt.L2.b64            cache-policy, access-property;\n\n.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,\n                               .L2::evict_first, .L2::evict_unchanged };\n.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged }; Description The createpolicy instruction creates a cache eviction policy for the specified cache level in an\nopaque 64-bit register specified by the destination operand cache-policy . The cache eviction\npolicy specifies how cache eviction priorities are applied to global memory addresses used in memory\noperations with .level::cache_hint qualifier. There are two types of cache eviction policies: Range-based policy The cache eviction policy created using createpolicy.range specifies the cache eviction\nbehaviors for the following three address ranges: [a .. a + (primary-size - 1)] referred to as primary range. [a + primary-size .. a + (total-size - 1)] referred to as trailing secondary range. [a - (total-size - primary-size) .. (a - 1)] referred to as preceding secondary range. When a range-based cache eviction policy is used in a memory operation with .level::cache_hint qualifier, the eviction priorities are applied as follows: If the memory address falls in the primary range, the eviction priority specified by .L2::primary_priority is applied. If the memory address falls in any of the secondary ranges, the eviction priority specified by .L2::secondary_priority is applied. If the memory address does not fall in either of the above ranges, then the applied eviction\npriority is unspecified. The 32-bit operand primary-size specifies the size, in bytes, of the primary range. The\n32-bit operand total-size specifies the combined size, in bytes, of the address range\nincluding primary and secondary ranges. The value of primary-size must be less than or equal\nto the value of total-size . Maximum allowed value of total-size is 4GB. If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged . If no state space is specified then Generic Addressing is\nused. If the specified address does not fall within the address window of .global state space\nthen the behavior is undefined. Fraction-based policy A memory operation with .level::cache_hint qualifier can use the fraction-based cache\neviction policy to request the cache eviction priority specified by .L2:primary_priority to\nbe applied to a fraction of cache accesses specified by the 32-bit floating point operand fraction . The remainder of the cache accesses get the eviction priority specified by .L2::secondary_priority . This implies that in a memory operation that uses a fraction-based\ncache policy, the memory access has a probability specified by the operand fraction of\ngetting the cache eviction priority specified by .L2::primary_priority . The valid range of values for the operand fraction is (0.0,.., 1.0] . If the operand fraction is not specified, it defaults to 1.0. If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged . The access property created using the CUDA APIs can be converted into cache eviction policy by the\ninstruction createpolicy.cvt . The source operand access-property is a 64-bit opaque\nregister. Refer to CUDA programming guide for more details. PTX ISA Notes Introduced in PTX ISA version 7.4. Target ISA Notes Requires sm_80 or higher. Examples createpolicy.fractional.L2::evict_last.b64                      policy, 1.0;\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;\n\ncreatepolicy.range.L2::evict_last.L2::evict_first.b64\n                                            policy, [ptr], 0x100000, 0x200000;\n\n// access-prop is created by CUDA APIs.\ncreatepolicy.cvt.L2.b64 policy, access-prop; 9.7.8.18. Data Movement and Conversion Instructions: isspacep  isspacep Query whether a generic address falls within a specified state space window. Syntax isspacep.space  p, a;    // result is .pred\n\n.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} }; Description Write predicate register p with 1 if generic address a falls within the specified state\nspace window and with 0 otherwise. Destination p has type .pred ; the source address\noperand must be of type .u32 or .u64 . isspacep.param{::entry} returns 1 if the generic address falls within the window of Kernel Function Parameters , otherwise returns 0 . If .param is specified without any sub-qualifiers then it defaults to .param::entry . isspacep.global returns 1 for Kernel Function Parameters as .param window is contained within the .global window. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Note ispacep.shared::cluster will return 1 for every shared memory address that is accessible to\nthe threads in the cluster, whereas ispacep.shared::cta will return 1 only if the address is\nof a variable declared in the executing CTA. PTX ISA Notes Introduced in PTX ISA version 2.0. isspacep.const introduced in PTX ISA version 3.1. isspacep.param introduced in PTX ISA version 7.7. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3. Target ISA Notes isspacep requires sm_20 or higher. isspacep.param{::entry} requires sm_70 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Examples isspacep.const           iscnst, cptr;\nisspacep.global          isglbl, gptr;\nisspacep.local           islcl,  lptr;\nisspacep.shared          isshrd, sptr;\nisspacep.param::entry    isparam, pptr;\nisspacep.shared::cta     isshrdcta, sptr;\nisspacep.shared::cluster ishrdany sptr; 9.7.8.19. Data Movement and Conversion Instructions: cvta  cvta Convert address from .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space to generic, or vice-versa. Take the generic address of a variable declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space. Syntax // convert const, global, local, or shared address to generic address\ncvta.space.size  p, a;        // source address in register a\ncvta.space.size  p, var;      // get generic address of var\ncvta.space.size  p, var+imm;  // generic address of var+offset\n\n// convert generic address to const, global, local, or shared address\ncvta.to.space.size  p, a;\n\n.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\n.size  = { .u32, .u64 }; Description Convert a const , Kernel Function Parameters ( .param ), global , local , or shared address to a generic address, or vice-versa. The\nsource and destination addresses must be the same size. Use cvt.u32.u64 or cvt.u64.u32 to\ntruncate or zero-extend addresses. For variables declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space, the generic address of the variable may be taken using cvta . The source is either a\nregister or a variable defined in const , Kernel Function Parameters ( .param ), global , local , or shared memory\nwith an optional offset. When converting a generic address into a const , Kernel Function Parameters ( .param ), global , local , or shared address, the resulting address is undefined in cases where the generic address does not fall within\nthe address window of the specified state space. A program may use isspacep to guard against\nsuch incorrect behavior. For cvta with .shared state space, the address must belong to the space specified by ::cta or ::cluster sub-qualifier, otherwise the behavior is undefined. If no sub-qualifier\nis specified with .shared state space, then ::cta is assumed by default. If .param is specified without any sub-qualifiers then it defaults to .param::entry . PTX ISA Notes Introduced in PTX ISA version 2.0. cvta.const and cvta.to.const introduced in PTX ISA version 3.1. cvta.param and cvta.to.param introduced in PTX ISA version 7.7. Note: The current implementation does not allow generic pointers to const space variables in\nprograms that contain pointers to constant buffers passed as kernel parameters. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3. Target ISA Notes cvta requires sm_20 or higher. cvta.param{::entry} and cvta.to.param{::entry} requires sm_70 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Examples cvta.const.u32   ptr,cvar;\ncvta.local.u32   ptr,lptr;\ncvta.shared::cta.u32  p,As+4;\ncvta.shared::cluster.u32 ptr, As;\ncvta.to.global.u32  p,gptr;\ncvta.param.u64   ptr,pvar;\ncvta.to.param::entry.u64  epptr, ptr; 9.7.8.20. Data Movement and Conversion Instructions: cvt  cvt Convert a value from one type to another. Syntax cvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding\ncvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding\ncvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;\ncvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;\ncvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;\ncvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;\ncvt.rna{.satfinite}.tf32.f32               d, a;\ncvt.frnd2{.relu}.tf32.f32                  d, a;\ncvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;\ncvt.rn.satfinite{.relu}.f8x2type.f16x2     d, a;\ncvt.rn.{.relu}.f16x2.f8x2type              d, a;\n\n.irnd   = { .rni, .rzi, .rmi, .rpi };\n.frnd   = { .rn,  .rz,  .rm,  .rp  };\n.frnd2  = { .rn,  .rz };\n.dtype = .atype = { .u8,   .u16, .u32, .u64,\n                    .s8,   .s16, .s32, .s64,\n                    .bf16, .f16, .f32, .f64 };\n.f8x2type = { .e4m3x2, .e5m2x2 }; Description Convert between different types and sizes. For .f16x2 and .bf16x2 instruction type, two inputs a and b of .f32 type are\nconverted into .f16 or .bf16 type and the converted values are packed in the destination\nregister d , such that the value converted from input a is stored in the upper half of d and the value converted from input b is stored in the lower half of d For .f16x2 instruction type, destination operand d has .f16x2 or .b32 type. For .bf16 instruction type, operand d has .b16 type. For .bf16x2 instruction type,\noperand d has .b32 type. For .tf32 instruction type, operand d has .b32 type. When converting to .e4m3x2 / .e5m2x2 data formats, the destination operand d has .b16 type. When converting two .f32 inputs to .e4m3x2 / .e5m2x2 , each input is converted to the\nspecified format, and the converted values are packed in the destination operand d such that the\nvalue converted from input a is stored in the upper 8 bits of d and the value converted from\ninput b is stored in the lower 8 bits of d . When converting an .f16x2 input to .e4m3x2 / .e5m2x2 , each .f16 input from operand a is converted to the specified\nformat. The converted values are packed in the destination operand d such that the value\nconverted from the upper 16 bits of input a is stored in the upper 8 bits of d and the value\nconverted from the lower 16 bits of input a is stored in the lower 8 bits of d . When converting from .e4m3x2 / .e5m2x2 to .f16x2 , source operand a has .b16 type. Each 8-bit input value in operand a is converted to .f16 type. The converted values\nare packed in the destination operand d such that the value converted from the upper 8 bits of a is stored in the upper 16 bits of d and the value converted from the lower 8 bits of a is stored in the lower 16 bits of d . Rounding modifier is mandatory in all of the following cases: float-to-float conversions, when destination type is smaller than source type All float-to-int conversions All int-to-float conversions All conversions involving .f16x2 , .e4m3x2, .e5m2x2, .bf16x2 and .tf32 instruction\ntypes. .satfinite modifier is only supported for conversions involving the following types: .e4m3x2 and .e5m2x2 destination types. .satfinite modifier is mandatory for such\nconversions. .f16 , .bf16 , .f16x2 , .bf16x2 as destination types. .tf32 as destination type with rounding mode specified as round to nearest, ties away from\nzero. Semantics if (/* inst type is .f16x2 or .bf16x2 */) {\n    d[31:16] = convert(a);\n    d[15:0]  = convert(b);\n} else {\n    d = convert(a);\n} Integer Notes Integer rounding is required for float-to-integer conversions, and for same-size float-to-float\nconversions where the value is rounded to an integer. Integer rounding is illegal in all other\ninstances. Integer rounding modifiers: .rni round to nearest integer, choosing even integer if source is equidistant between two integers .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity In float-to-integer conversion, NaN inputs are converted to 0. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float\nconversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier .ftz can only be specified when either .dtype or .atype is .f32 and applies only\nto single precision ( .f32 ) inputs and results. sm_1x For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving\nzero. The optional .ftz modifier may be specified in these cases for clarity. Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision\nsubnormal inputs or results to zero if the destination type size was 64-bits. The compiler will\npreserve this behavior for legacy PTX code. Saturation modifier: .sat For integer destination types, .sat limits the result to MININT..MAXINT for the size of\nthe operation. Note that saturation applies to both signed and unsigned integer types. The saturation modifier is allowed only in cases where the destination type’s value range is not\na superset of the source type’s value range; i.e., the .sat modifier is illegal in cases\nwhere saturation is not possible based on the source and destination types. For float-to-integer conversions, the result is clamped to the destination range by default; i.e, .sat is redundant. Floating Point Notes Floating-point rounding is required for float-to-float conversions that result in loss of precision,\nand for integer-to-float conversions. Floating-point rounding is illegal in all other instances. Floating-point rounding modifiers: .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity A floating-point value may be rounded to an integral value using the integer rounding modifiers (see\nInteger Notes). The operands must be of the same size. The result is an integral value, stored in\nfloating-point format. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. Modifier .ftz may be specified to flush\nsingle-precision subnormal inputs and results to sign-preserving zero. Modifier .ftz can only\nbe specified when either .dtype or .atype is .f32 and applies only to single\nprecision ( .f32 ) inputs and results. sm_1x Single-precision subnormal inputs and results are flushed to sign-preserving zero. The optional .ftz modifier may be specified in these cases for clarity. Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush\nsingle-precision subnormal inputs or results to zero if either source or destination type was .f64 . The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX\nISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to\nsign-preserving zero only for cvt.f32.f16 , cvt.f16.f32 , and cvt.f32.f32 instructions. Saturation modifier: .sat : For floating-point destination types, .sat limits the result to the range [0.0, 1.0]. NaN results are flushed to positive zero. Applies to .f16 , .f32 , and .f64 types. .relu : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination types, .relu clamps the result to 0 if negative. NaN results are converted to\ncanonical NaN . .satfinite : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination formats, if the input value is NaN , then the result is NaN in the specified\ndestination format. If the absolute value of input (ignoring sign) is greater than MAX_NORM of\nthe specified destination format, then the result is sign-preserved MAX_NORM of the destination\nformat. Notes A source register wider than the specified type may be used, except when the source operand has .bf16 or .bf16x2 format. The lower n bits corresponding to the instruction-type width\nare used in the conversion. See Operand Size Exceeding Instruction-Type Size for a description of these relaxed\ntype-checking rules. A destination register wider than the specified type may be used, except when the destination\noperand has .bf16 , .bf16x2 or .tf32 format. The result of conversion is sign-extended to\nthe destination register width for signed integers, and is zero-extended to the destination register\nwidth for unsigned, bit-size, and floating-point types. See Operand Size Exceeding Instruction-Type\nSize for a description of these relaxed\ntype-checking rules. For cvt.f32.bf16 , NaN input yields unspecified NaN . PTX ISA Notes Introduced in PTX ISA version 1.0. .relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats\nintroduced in PTX ISA version 7.0. cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16} , cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16 , and cvt.tf32.f32.{relu}.{rn/rz} introduced\nin PTX ISA 7.8. cvt with .e4m3x2 / .e5m2x2 for sm_90 or higher introduced in PTX ISA version 7.8. cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} for sm_90 or higher introduced in PTX ISA version 7.8. cvt with .e4m3x2 / .e5m2x2 for sm_89 introduced in PTX ISA version 8.1. cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} for sm_89 introduced in PTX ISA version 8.1. cvt.satfinite.{f16, bf16, f16x2, bf16x2, tf32}.f32 introduced in PTX ISA version 8.1. Target ISA Notes cvt to or from .f64 requires sm_13 or higher. .relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats require sm_80 or higher. cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16} , cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16 , and cvt.tf32.f32.{relu}.{rn/rz} require sm_90 or higher. cvt with .e4m3x2 / .e5m2x2 requires sm89 or higher. cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} requires sm_89 or higher. Examples cvt.f32.s32 f,i;\ncvt.s32.f64 j,r;     // float-to-int saturates by default\ncvt.rni.f32.f32 x,y; // round to nearest int, result is fp\ncvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets\ncvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode\ncvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs\ncvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output\ncvt.rn.bf16.f32          b, f;        // convert fp32 to bf16\ncvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation\ncvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs\ncvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output\ncvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format\ncvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format\ncvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format\ncvt.bf16.f16.rz          b, f         // convert f16 to bf16 format\ncvt.bf16.u64.rz          b, u         // convert u64 to bf16 format\ncvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format\ncvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format\ncvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output\ncvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu\n                                         // saturation on each output and pack as .e5m2x2\ncvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output 9.7.8.21. Data Movement and Conversion Instructions: cvt.pack  cvt.pack Convert two integer values from one integer type to another and pack the results. Syntax cvt.pack.sat.convertType.abType  d, a, b;\n    .convertType  = { .u16, .s16 }\n    .abType       = { .s32 }\n\ncvt.pack.sat.convertType.abType.cType  d, a, b, c;\n    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }\n    .abType       = { .s32 }\n    .cType        = { .b32 } Description Convert two 32-bit integers a and b into specified type and pack the results into d . Destination d is an unsigned 32-bit integer. Source operands a and b are integers of\ntype .abType and the source operand c is an integer of type .cType . The inputs a and b are converted to values of type specified by .convertType with\nsaturation and the results after conversion are packed into lower bits of d . If operand c is specified then remaining bits of d are copied from lower bits of c . Semantics ta = a < MIN(convertType) ? MIN(convertType) : a;\nta = a > MAX(convertType) ? MAX(convertType) : a;\ntb = b < MIN(convertType) ? MIN(convertType) : b;\ntb = b > MAX(convertType) ? MAX(convertType) : b;\n\nsize = sizeInBits(convertType);\ntd = tb ;\nfor (i = size; i <= 2 * size - 1; i++) {\n    td[i] = ta[i - size];\n}\n\nif (isU16(convertType) || isS16(convertType)) {\n    d = td;\n} else {\n    for (i = 0; i < 2 * size; i++) {\n        d[i] = td[i];\n    }\n    for (i = 2 * size; i <= 31; i++) {\n        d[i] = c[i - 2 * size];\n    }\n} .sat modifier limits the converted values to MIN(convertType) .. MAX(convertedType) (no\noverflow) if the corresponding inputs are not in the range of datatype specified as .convertType . PTX ISA Notes Introduced in PTX ISA version 6.5. Target ISA Notes Requires sm_72 or higher. Sub byte types ( .u4 / .s4 and .u2 / .s2 ) requires sm_75 or higher. Examples cvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion\ncvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion\ncvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }\ncvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion\ncvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion 9.7.8.22. Data Movement and Conversion Instructions: mapa  mapa Map the address of the shared variable in the target CTA. Syntax mapa{.space}.type          d, a, b;\n\n// Maps shared memory address in register a into CTA b.\nmapa.shared::cluster.type  d, a, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh + imm, b;\n\n// Maps generic address in register a into CTA b.\nmapa.type                  d, a, b;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 } Description Get address in the CTA specified by operand b which corresponds to the address specified by\noperand a . Instruction type .type indicates the type of the destination operand d and the source\noperand a . When space is .shared::cluster , source a is either a shared memory variable or a register\ncontaining a valid shared memory address and register d contains a shared memory address. When\nthe optional qualifier .space is not specified, both a and d are registers containing\ngeneric addresses pointing to shared memory. b is a 32-bit integer operand representing the rank of the target CTA. Destination register d will hold an address in CTA b corresponding to operand a . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples mapa.shared::cluster.u64 d1, %reg1, cta;\nmapa.shared::cluster.u32 d2, sh, 3;\nmapa.u64                 d3, %reg2, cta; 9.7.8.23. Data Movement and Conversion Instructions: getctarank  getctarank Generate the CTA rank of the address. Syntax getctarank{.space}.type d, a;\n\n// Get cta rank from source shared memory address in register a.\ngetctarank.shared::cluster.type d, a;\n\n// Get cta rank from shared memory variable.\ngetctarank.shared::cluster.type d, var;\n\n// Get cta rank from shared memory variable+offset.\ngetctarank.shared::cluster.type d, var + imm;\n\n// Get cta rank from generic address of shared memory variable in register a.\ngetctarank.type d, a;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 } Description Write the destination register d with the rank of the CTA which contains the address specified\nin operand a . Instruction type .type indicates the type of source operand a . When space is .shared::cluster , source a is either a shared memory variable or a register\ncontaining a valid shared memory address. When the optional qualifier .space is not specified, a is a register containing a generic addresses pointing to shared memory. Destination d is\nalways a 32-bit register which holds the rank of the CTA. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples getctarank.shared::cluster.u32 d1, addr;\ngetctarank.shared::cluster.u64 d2, sh + 4;\ngetctarank.u64                 d3, src; 9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copy  An asynchronous copy operation performs the underlying operation asynchronously in the background,\nthus allowing the issuing threads to perform subsequent tasks. An asynchronous copy operation can be a bulk operation that operates on a large amount of data, or\na non-bulk operation that operates on smaller sized data. The amount of data handled by a bulk\nasynchronous operation must be a multiple of 16 bytes. 9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operations  A thread must explicitly wait for the completion of an asynchronous copy operation in order to\naccess the result of the operation. Once an asynchronous copy operation is initiated, modifying the\nsource memory location or reading from the destination memory location before the asynchronous\noperation completes, will cause unpredictable results. This section describes two asynchronous copy operation completion mechanisms supported in PTX:\nAsync-group mechanism and mbarrier-based mechanism. Async-group mechanism When using the async-group completion mechanism, the issuing thread specifies a group of\nasynchronous operations, called async-group , using a commit operation and tracks the completion\nof this group using a wait operation. The thread issuing the asynchronous operation must create\nseparate async-groups for bulk and non-bulk asynchronous operations. A commit operation creates a per-thread async-group containing all prior asynchronous operations\ninitiated by the executing thread but none of the asynchronous operations following the commit\noperation. A committed asynchronous operation belongs to a single async-group . When an async-group completes, all the asynchronous operations belonging to that group are\ncomplete and the executing thread that initiated the asynchronous operations can read the result of\nthe asynchronous operations. All async-groups committed by an executing thread always complete in\nthe order in which they were committed. There is no ordering between asynchronous operations within\nan async-group . A typical pattern of using async-group as the completion mechanism is as follows: Initiate the asynchronous operations. Group the asynchronous operations into an async-group using a commit operation. Wait for the completion of the async-group using the wait operation. Once the async-group completes, access the results of all asynchronous operations in that async-group . Mbarrier-based mechanism A thread can track the completion of one or more asynchronous operations using the current phase of\nan mbarrier object . When the current phase of the mbarrier object is complete, it implies that\nall asynchronous operations tracked by this phase are complete, and all threads participating in\nthat mbarrier object can access the result of the asynchronous operations. The mbarrier object to be used for tracking the completion of an asynchronous operation can be\neither specified along with the asynchronous operation as part of its syntax, or as a separate\noperation. For a bulk asynchronous operation, the mbarrier object must be specified in the\nasynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous\noperation. A typical pattern of using mbarrier-based completion mechanism is as follows: Initiate the asynchronous operations. Set up an mbarrier object to track the asynchronous operations in its current phase, either as\npart of the asynchronous operation or as a separate operation. Wait for the mbarrier object to complete its current phase using mbarrier.test_wait or mbarrier.try_wait . Once the mbarrier.test_wait or mbarrier.try_wait operation returns True , access the\nresults of the asynchronous operations tracked by the mbarrier object . 9.7.8.24.2. Async Proxy  The cp{.reduce}.async.bulk operations are performed in the asynchronous proxy (or async\nproxy ). Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async proxy , fence.proxy.async should be used to synchronize memory between generic\nproxy and the async proxy . The completion of a cp{.reduce}.async.bulk operation is followed by an implicit generic-async proxy fence. So the result of the asynchronous operation is made visible to the generic proxy as\nsoon as its completion is observed. Async-group OR mbarrier-based completion mechanism must\nbe used to wait for the completion of the cp{.reduce}.async.bulk instructions. 9.7.8.24.3. Data Movement and Conversion Instructions: cp.async  cp.async Initiates an asynchronous copy operation from one state space to another. Syntax cp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, src-size}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, src-size}{, cache-policy} ;\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, ignore-src}{, cache-policy} ;\n\n.level::cache_hint =     { .L2::cache_hint }\n.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }\ncp-size =                { 4, 8, 16 } Description cp.async is a non-blocking instruction which initiates an asynchronous copy operation of data\nfrom the location specified by source address operand src to the location specified by\ndestination address operand dst . Operand src specifies a location in the global state space\nand dst specifies a location in the shared state space. Operand cp-size is an integer constant which specifies the size of data in bytes to be copied to\nthe destination dst . cp-size can only be 4, 8 and 16. Instruction cp.async allows optionally specifying a 32-bit integer operand src-size . Operand src-size represents the size of the data in bytes to be copied from src to dst and must\nbe less than cp-size . In such case, remaining bytes in destination dst are filled with\nzeros. Specifying src-size larger than cp-size results in undefined behavior. The optional and non-immediate predicate argument ignore-src specifies whether the data from the\nsource location src should be ignored completely. If the source data is ignored then zeros will\nbe copied to destination dst . If the argument ignore-src is not specified then it defaults\nto False . Supported alignment requirements and addressing modes for operand src and dst are described\nin Addresses as Operands . The mandatory .async qualifier indicates that the cp instruction will initiate the memory\ncopy operation asynchronously and control will return to the executing thread before the copy\noperation is complete. The executing thread can then use cp.async.wait_all or cp.async.wait_group or mbarrier instructions to wait for\ncompletion of the asynchronous copy operation. No other synchronization mechanisms described in Memory Consistency Model can be used to guarantee the\ncompletion of the asynchronous copy operations. There is no ordering guarantee between two cp.async operations if they are not explicitly\nsynchronized using cp.async.wait_all or cp.async.wait_group or mbarrier instructions . As described in Cache Operators , the .cg qualifier indicates\ncaching of data only at global level cache L2 and not at L1 whereas .ca qualifier indicates\ncaching of data at all levels including L1 cache. Cache operator are treated as performance hints\nonly. cp.async is treated as a weak memory operation in the Memory Consistency Model . The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively. The qualifier .level::prefetch_size may only be used with .global state space and with\ngeneric addressing where the address points to .global state space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. PTX ISA Notes Introduced in PTX ISA version 7.0. Support for .level::cache_hint and .level::prefetch_size qualifiers introduced in PTX ISA\nversion 7.4. Support for ignore-src operand introduced in PTX ISA version 7.5. Support for sub-qualifier ::cta introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Sub-qualifier ::cta requires sm_30 or higher. Examples cp.async.ca.shared.global  [shrd],    [gbl + 4], 4;\ncp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;\ncp.async.cg.shared.global  [%r2],     [%r3],     16;\n\ncp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;\ncp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;\ncp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;\ncp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;\n\ncp.async.ca.shared.global                   [shrd], [gbl], 4, p;\ncp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy; 9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_group  cp.async.commit_group Commits all prior initiated but uncommitted cp.async instructions into a cp.async-group . Syntax cp.async.commit_group ; Description cp.async.commit_group instruction creates a new cp.async-group per thread and batches all\nprior cp.async instructions initiated by the executing thread but not committed to any cp.async-group into the new cp.async-group . If there are no uncommitted cp.async instructions then cp.async.commit_group results in an empty cp.async-group. An executing thread can wait for the completion of all cp.async operations in a cp.async-group using cp.async.wait_group . There is no memory ordering guarantee provided between any two cp.async operations within the\nsame cp.async-group . So two or more cp.async operations within a cp.async-group copying data\nto the same location results in undefined behavior. PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Examples // Example 1:\ncp.async.ca.shared.global [shrd], [gbl], 4;\ncp.async.commit_group ; // Marks the end of a cp.async group\n\n// Example 2:\ncp.async.ca.shared.global [shrd1],   [gbl1],   8;\ncp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;\ncp.async.commit_group ; // Marks the end of cp.async group 1\n\ncp.async.ca.shared.global [shrd2],    [gbl2],    16;\ncp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;\ncp.async.commit_group ; // Marks the end of cp.async group 2 9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all  cp.async.wait_group/cp.async.wait_all Wait for completion of prior asynchronous copy operations. Syntax cp.async.wait_group N;\ncp.async.wait_all ; Description cp.async.wait_group instruction will cause executing thread to wait till only N or fewer of\nthe most recent cp.async-group s are pending and all the prior cp.async-group s committed by\nthe executing threads are complete. For example, when N is 0, the executing thread waits on all\nthe prior cp.async-group s to complete. Operand N is an integer constant. cp.async.wait_all is equivalent to : cp.async.commit_group;\ncp.async.wait_group 0; An empty cp.async-group is considered to be trivially complete. Writes performed by cp.async operations are made visible to the executing thread only after: The completion of cp.async.wait_all or The completion of cp.async.wait_group on the cp.async-group in which the cp.async belongs to or mbarrier.test_wait returns True on an mbarrier object which is tracking the completion of the cp.async operation. There is no ordering between two cp.async operations that are not synchronized with cp.async.wait_all or cp.async.wait_group or mbarrier objects . cp.async.wait_group and cp.async.wait_all does not provide any ordering and visibility\nguarantees for any other memory operation apart from cp.async . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Examples // Example of .wait_all:\ncp.async.ca.shared.global [shrd1], [gbl1], 4;\ncp.async.cg.shared.global [shrd2], [gbl2], 16;\ncp.async.wait_all;  // waits for all prior cp.async to complete\n\n// Example of .wait_group :\ncp.async.ca.shared.global [shrd3], [gbl3], 8;\ncp.async.commit_group;  // End of group 1\n\ncp.async.cg.shared.global [shrd4], [gbl4], 16;\ncp.async.commit_group;  // End of group 2\n\ncp.async.cg.shared.global [shrd5], [gbl5], 16;\ncp.async.commit_group;  // End of group 3\n\ncp.async.wait_group 1;  // waits for group 1 and group 2 to complete 9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulk  cp.async.bulk Initiates an asynchronous copy operation from one state space to another. Syntax cp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\ncp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n\n\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint} [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint =    { .L2::cache_hint } Description cp.async.bulk is a non-blocking instruction which initiates an asynchronous bulk-copy operation\nfrom the location specified by source address operand srcMem to the location specified by\ndestination address operand dstMem . The direction of bulk-copy is from the state space specified by the .src modifier to the state\nspace specified by the .dst modifiers. The 32-bit operand size specifies the amount of memory to be copied, in terms of number of\nbytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory\nspace and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory\nspace. Otherwise, the behavior is undefined. The addresses dstMem and srcMem must be aligned\nto 16 bytes. When the source of the copy is .shared::cta and the destination is .shared::cluster , the\ndestination has to be in the shared memory of a different CTA within the cluster. The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .shared::cluster .shared::cta .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk variant uses\nmbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.async.bulk variant uses bulk async-group based completion mechanism. The optional modifier .multicast::cluster allows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the\ncluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA. The source data is multicast to the same CTA-relative offset as dstMem in the shared memory of each destination CTA. The mbarrier signal is also multicast to the same\nCTA-relative offset as mbar in the shared memory of the destination CTA. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. The copy operation in cp.async.bulk is treated as a weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Notes .multicast::cluster qualifier is optimized for target architecture sm_90a and may have\nsubstantially reduced performance on other targets and hence .multicast::cluster is advised to\nbe used with .target sm_90a . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. .multicast::cluster qualifier advised to be used with .target sm_90a . Examples // .global -> .shared::cluster:\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                                             [dstMem], [srcMem], size, [mbar], ctaMask;\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n\n// .shared::cta -> .shared::cluster (strictly remote):\ncp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\n// .shared::cta -> .global:\ncp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy; 9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk  cp.reduce.async.bulk Initiates an asynchronous reduction operation. Syntax cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type\n              [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .b32, .u32, .s32, .b64, .u64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type\n               [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global      }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint    = { .L2::cache_hint }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type\n               [dstMem], [srcMem], size{, cache-policy}\n.dst  =                 { .global }\n.src  =                 { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.type =                 { .f16, .bf16 } Description cp.reduce.async.bulk is a non-blocking instruction which initiates an asynchronous reduction\noperation on an array of memory locations specified by the destination address operand dstMem with the source array whose location is specified by the source address operand srcMem . The size\nof the source and the destination array must be the same and is specified by the operand size . Each data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier .redOp . The type of each\ndata element in the source and the destination array is specified by the modifier .type . The source address operand srcMem is located in the state space specified by .src and the\ndestination address operand dstMem is located in the state specified by the .dst . The 32-bit operand size specifies the amount of memory to be copied from the source location and\nused in the reduction operation, in terms of number of bytes. size must be a multiple of 16. If\nthe value is not a multiple of 16, then the behavior is undefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory space. Otherwise, the behavior is\nundefined. The addresses dstMem and srcMem must be aligned to 16 bytes. The operations supported by .redOp are classified as follows: The bit-size operations are .and , .or , and .xor . The integer operations are .add , .inc , .dec , .min , and .max . The .inc and .dec operations return a result in the range [0..x] where x is the value at the source\nstate space. The floating point operation .add rounds to the nearest even. The current implementation of cp.reduce.async.bulk.add.f32 flushes subnormal inputs and results to sign-preserving zero. The cp.reduce.async.bulk.add.f16 and cp.reduce.async.bulk.add.bf16 operations require .noftz qualifier. It preserves input and result subnormals, and does not flush them to zero. The following table describes the valid combinations of .redOp and element type: .dst .redOp Element type .shared::cluster .add .u32 , .s32 , .u64 .min , .max .u32 , .s32 .inc , .dec .u32 .and , .or , .xor .b32 .global .add .u32 , .s32 , .u64 , .f32 , .f64 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .shared::cluster .shared::cta .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.reduce.async.bulk variant\nuses mbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.reduce.async.bulk variant uses bulk\nasync-group based completion mechanism. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. Each reduction operation performed by the cp.reduce.async.bulk has individually .relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk are treated as weak\nmemory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size; 9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch  cp.async.bulk.prefetch Provides a hint to the system to initiate the asynchronous prefetch of data to the cache. Syntax cp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}\n\n.src =                { .global }\n.level::cache_hint =  { .L2::cache_hint } Description cp.async.bulk.prefetch is a non-blocking instruction which may initiate an asynchronous prefetch\nof data from the location specified by source address operand srcMem , in .src statespace, to\nthe L2 cache. The 32-bit operand size specifies the amount of memory to be prefetched in terms of number of\nbytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory\nspace and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory\nspace. Otherwise, the behavior is undefined. The address srcMem must be aligned to 16 bytes. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.async.bulk.prefetch.L2.global                 [srcMem], size;\n\ncp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy; 9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor  cp.async.bulk.tensor Initiates an asynchronous copy operation on the tensor data from one state space to another. Syntax // global -> shared::cluster:\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colOffsets}\n                                   {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.load_mode =            { .tile, .im2col }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> global:\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}\n                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.level::cache_hint =    { .L2::cache_hint } Description cp.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous copy\noperation of tensor data from the location in .src state space to the location in the .dst state space. The operand dstMem specifies the location in the .dst state space into which the tensor data\nhas to be copied and srcMem specifies the location in the .src state space from which the\ntensor data has to be copied. The operand tensorMap is the generic address of the opaque tensor-map object which resides\neither in .param space or .const space or .global space. The operand tensorMap specifies\nthe properties of the tensor copy operation, as described in Tensor-map .\nThe tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating\nthe tensor-map objects on the host side. The dimension of the tensor data is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The number of tensor\ncoordinates in the vector argument tensorCoords should be equal to the dimension specified by\nthe modifier .dim . The individual tensor coordinates in tensorCoords are of type .s32 . The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk.tensor variant\nuses mbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.async.bulk.tensor variant uses bulk\nasync-group based completion mechanism. The qualifier .load_mode specifies how the data in the source location is copied into the\ndestination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column\nat the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least\n3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is\n.im2col. The length of the vector operand im2colOffsets is two less than the number of dimension .dim of the tensor operation. The modifier .im2col_no_offs is the same as .im2col mode\nexcept there is no im2colOffsets vector involved. The optional modifier .multicast::cluster allows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the\ncluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA. The source data is multicast to the same offset as dstMem in the shared\nmemory of each destination CTA. The mbarrier signal is also multicast to the same offset as mbar in the shared memory of the destination CTA. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. The copy operation in cp.async.bulk.tensor is treated as a weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Notes .multicast::cluster qualifier is optimized for target architecture sm_90a and may have\nsubstantially reduced performance on other targets and hence .multicast::cluster is advised to\nbe used with .target sm_90a . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. .multicast::cluster qualifier advised to be used with .target sm_90a . Examples .reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;\n\n@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;\n\n@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3]; 9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor  cp.reduce.async.bulk.tensor Initiates an asynchronous reduction operation on the tensor data. Syntax // shared::cta -> global:\ncp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}\n                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor} Description cp.reduce.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous\nreduction operation of tensor data in the .dst state space with tensor data in the .src state space. The operand srcMem specifies the location of the tensor data in the .src state space using\nwhich the reduction operation has to be performed. The operand tensorMap is the generic address of the opaque tensor-map object which resides\neither in .param space or .const space or .global space. The operand tensorMap specifies\nthe properties of the tensor copy operation, as described in Tensor-map .\nThe tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating\nthe tensor-map objects on the host side. Each element of the tensor data in the .dst state space is reduced inline with the corresponding\nelement from the tensor data in the .src state space. The modifier .redOp specifies the\nreduction operation used for the inline reduction. The type of each tensor data element in the\nsource and the destination tensor is specified in Tensor-map . The dimension of the tensor is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates of the tensor data in the\nglobal memory on which the reduce operation is to be performed. The number of tensor coordinates in\nthe vector argument tensorCoords should be equal to the dimension specified by the modifier .dim . The individual tensor coordinates are of the type .s32 . The following table describes the valid combinations of .redOp and element type: .redOp Element type .add .u32 , .s32 , .u64 , .f32 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. Value .bulk_group of the modifier .completion_mechanism specifies that cp.reduce.async.bulk.tensor instruction uses bulk async-group based completion mechanism. The qualifier .load_mode specifies how the data in the source location is copied into the\ndestination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col_no_offs mode, some dimensions of the source tensors are unrolled in a single dimensional\ncolumn at the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least\n3-dimensional. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. Each reduction operation performed by cp.reduce.async.bulk.tensor has individually .relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk.tensor are treated as weak memory operations and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group\n                                             [tensorMap0, {tc0}], [sMem0];\n\ncp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint\n                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;\n\ncp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group\n                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2] 9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor  cp.async.bulk.prefetch.tensor Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache. Syntax // global -> shared::cluster:\ncp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]\n                                                             {, im2colOffsets } {, cache-policy}\n\n.src =                { .global }\n.dim =                { .1d, .2d, .3d, .4d, .5d }\n.load_mode =          { .tile, .im2col }\n.level::cache_hint =  { .L2::cache_hint } Description cp.async.bulk.prefetch.tensor is a non-blocking instruction which may initiate an asynchronous\nprefetch of tensor data from the location in .src statespace to the L2 cache. The operand tensorMap is the generic address of the opaque tensor-map object which resides\neither in .param space or .const space or .global space. The operand tensorMap specifies\nthe properties of the tensor copy operation, as described in Tensor-map .\nThe tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating\nthe tensor-map objects on the host side. The dimension of the tensor data is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The number of tensor\ncoordinates in the vector argument tensorCoords should be equal to the dimension specified by\nthe modifier .dim . The individual tensor coordinates in tensorCoords are of type .s32 . The qualifier .load_mode specifies how the data in the source location is copied into the\ndestination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column\nat the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least\n3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is\n.im2col. The length of the vector operand im2colOffsets is two less than the number of dimension .dim of the tensor operation. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. cp.async.bulk.prefetch.tensor is treated as a weak memory operation in the Memory Consistency\nModel . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples .reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];\n\n@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];\n\n@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col\n                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint\n                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy; 9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group  cp.async.bulk.commit_group Commits all prior initiated but uncommitted cp.async.bulk instructions into a cp.async.bulk-group . Syntax cp.async.bulk.commit_group; Description cp.async.bulk.commit_group instruction creates a new per-thread bulk async-group and batches\nall prior cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions satisfying the following\nconditions into the new bulk async-group : The prior cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions use bulk_group based\ncompletion mechanism, and They are initiated by the executing thread but not committed to any bulk async-group . If there are no uncommitted cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions then cp.async.bulk.commit_group results in an empty bulk async-group . An executing thread can wait for the completion of all cp{.reduce}.async.bulk.{.prefetch}{.tensor} operations in a bulk async-group using cp.async.wait_group . There is no memory ordering guarantee provided between any two cp{.reduce}.async.bulk.{.prefetch}{.tensor} operations within the same bulk async-group . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.async.bulk.commit_group; 9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group  cp.async.bulk.wait_group Wait for completion of bulk async-groups . Syntax cp.async.bulk.wait_group{.read} N; Description cp.async.bulk.wait_group instruction will cause the executing thread to wait until only N or\nfewer of the most recent bulk async-groups are pending and all the prior bulk async-groups committed by the executing threads are complete. For example, when N is 0, the executing thread\nwaits on all the prior bulk async-groups to complete. Operand N is an integer constant. By default, cp.async.bulk.wait_group instruction will cause the executing thread to wait till\nall the bulk async operations in the specified bulk async-group have completed all of the\nfollowing: Reading from the source locations. Writing to their respective destination locations. Writes being made visible to the executing thread. The optional .read modifier indicates that the waiting has to be done until all the bulk async\noperations in the specified bulk async-group have completed reading from their source locations. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.async.bulk.wait_group.read   0;\ncp.async.bulk.wait_group        2; 9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace  tensormap.replace Modifies the field of a tensor-map object. Syntax tensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;\ntensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;\ntensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;\n\n.mode    = { .tile }\n.field1  = { .global_address, .rank }\n.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }\n.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .fill_mode }\n.ss      = { .global, .shared::cta }\n.type    = { .b32, .b64 } Description The tensormap.replace instruction replaces the field, specified by .field qualifier,\nof the tensor-map object at the location specified by the address operand addr with a\nnew value. The new value is specified by the argument new_val . Qualifier .mode specifies the mode of the tensor-map object\nlocated at the address operand addr . Instruction type .b1024 indicates the size of the tensor-map object, which is 1024 bits. Operand new_val has the type .type . When .field is specified as .global_address or .global_stride , .type must be .b64 . Otherwise, .type must be .b32 . The immediate integer operand ord specifies the ordinal of the field across the rank of the\ntensor which needs to be replaced in the tensor-map object. For field .rank , the operand new_val must be ones less than the desired tensor rank as\nthis field uses zero-based numbering. When .field3 is specified, the operand new_val must be an immediate and the Table 30 shows the mapping of the operand new_val across various fields. Table 30 Tensormap new_val validity  new_val .field3 .elemtype .interleave_layout .swizzle_mode .fill_mode 0 .u8 No interleave No swizzling Zero fill 1 .u16 16B interleave 32B swizzling OOB-NaN fill 2 .u32 32B interleave 64B swizzling x 3 .s32 x 128B swizzling x 4 .u64 x x x 5 .s64 x x x 6 .f16 x x x 7 .f32 x x x 8 .f32.ftz x x x 9 .f64 x x x 10 .bf16 x x x 11 .tf32 x x x 12 .tf32.ftz x x x If no state space is specified then Generic Addressing is used.\nIf the address specified by addr does not fall within the address window of .global or .shared::cta state space then the behavior is undefined. tensormap.replace is treated as a weak memory operation, on the entire 1024-bit opaque tensor-map object, in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_90a . Examples tensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val; 9.7.9. Texture Instructions  This section describes PTX instructions for accessing textures and samplers. PTX supports the\nfollowing operations on texture and sampler descriptors: Static initialization of texture and sampler descriptors. Module-scope and per-entry scope definitions of texture and sampler descriptors. Ability to query fields within texture and sampler descriptors. 9.7.9.1. Texturing Modes  For working with textures and samplers, PTX has two modes of operation. In the unified mode, texture and sampler information is accessed through a single .texref handle. In the independent\nmode , texture and sampler information each have their own handle, allowing them to be defined\nseparately and combined at the site of usage in the program. The advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior\nto sm_3x ), with the restriction that they correspond 1-to-1 with the 256 possible textures per\nkernel (128 for architectures prior to sm_3x ). The advantage of independent mode is that\ntextures and samplers can be mixed and matched, but the number of samplers is greatly restricted to\n32 per kernel (16 for architectures prior to sm_3x ). Table 31 summarizes the number of textures, samplers and\nsurfaces available in different texturing modes. Table 31 Texture, sampler and surface limits  Texturing mode Resource sm_1x , sm_2x sm_3x+ Unified mode Textures 128 256 Samplers 128 256 Surfaces 8 16 Independent mode Textures 128 256 Samplers 16 32 Surfaces 8 16 The texturing mode is selected using .target options texmode_unified and texmode_independent . A PTX module may declare only one texturing mode. If no texturing mode is\ndeclared, the module is assumed to use unified mode. Example : calculate an element’s power contribution as element’s power/total number of elements. .target texmode_independent\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n...\n.entry compute_power\n  ( .param .texref tex1 )\n{\n  txq.width.b32  r6, [tex1]; // get tex1's width\n  txq.height.b32 r5, [tex1]; // get tex1's height\n  tex.2d.v4.f32.f32  {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}];\n  mul.u32 r5, r5, r6;\n  add.f32 r1, r1, r2;\n  add.f32 r3, r3, r4;\n  add.f32 r1, r1, r3;\n  cvt.f32.u32 r5, r5;\n  div.f32 r1, r1, r5;\n} 9.7.9.2. Mipmaps  A mipmap is a sequence of textures, each of which is a progressively lower resolution\nrepresentation of the same image. The height and width of each image, or level of detail (LOD), in\nthe mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics\napplications to improve rendering speed and reduce aliasing artifacts. For example, a\nhigh-resolution mipmap image is used for objects that are close to the user; lower-resolution images\nare used as the object appears farther away. Mipmap filtering modes are provided when switching\nbetween two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity. Example: If the texture has a basic size of 256 by 256 pixels, then the associated mipmap set\nmay contain a series of eight images, each one-fourth the total area of the previous one: 128×128\npixels, 64×64, 32×32, 16×16, 8×8, 4×4, 2×2, 1×1 (a single pixel). If, for example, a scene is\nrendering this texture in a space of 40×40 pixels, then either a scaled up version of the 32×32\n(without trilinear interpolation) or an interpolation of the 64×64 and the 32×32 mipmaps (with\ntrilinear interpolation) would be used. The total number of LODs in a complete mipmap pyramid is calculated through the following equation: numLODs = 1 + floor(log2(max(w, h, d))) The finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st\nlevel, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller\nmipmap level has half the {width, height, depth} of the previous level, but if this half value is a\nfractional value, it’s rounded down to the next largest integer. Essentially, the size of a mipmap\nlevel can be specified as: max(1, floor(w_b / 2^i)) x\nmax(1, floor(h_b / 2^i)) x\nmax(1, floor(d_b / 2^i)) where i is the ith level beyond the 0th level (the base level). And w_b , h_b and d_b are the\nwidth, height and depth of the base level respectively. PTX support for mipmaps The PTX tex instruction supports three modes for specifying the LOD: base , level , and grad ient. In base mode, the instruction always picks level 0. In level mode, an additional\nargument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector\narguments provide partials (e.g., {ds/dx, dt/dx} and {ds/dy, dt/dy} for a 2d texture),\nwhich the tex instruction uses to compute the LOD. These instructions provide access to texture memory. tex tld4 txq 9.7.9.3. Texture Instructions: tex  tex Perform a texture memory lookup. Syntax tex.geom.v4.dtype.ctype  d, [a, c] {, e} {, f};\ntex.geom.v4.dtype.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\ntex.geom.v2.f16x2.ctype  d[|p], [a, c] {, e} {, f};\ntex.geom.v2.f16x2.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n// mipmaps\ntex.base.geom.v4.dtype.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v4.dtype.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v4.dtype.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\ntex.base.geom.v2.f16x2.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v2.f16x2.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v2.f16x2.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms };\n.dtype = { .u32, .s32, .f16,  .f32 };\n.ctype = {       .s32, .f32 };          // .cube, .acube require .f32\n                                        // .2dms, .a2dms require .s32 Description tex.{1d,2d,3d} Texture lookup using a texture coordinate vector. The instruction loads data from the texture named\nby operand a at coordinates given by operand c into destination d . Operand c is a\nscalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a\nfour-element vector for 3d textures, where the fourth element is ignored. An optional texture\nsampler b may be specified. If no sampler is specified, the sampler behavior is a property of\nthe named texture. The optional destination predicate p is set to True if data from texture\nat specified coordinates is resident in memory, False otherwise. When optional destination\npredicate p is set to False , data loaded will be all zeros. Memory residency of Texture Data\nat specified coordinates is dependent on execution environment setup using Driver API calls, prior\nto kernel launch. Refer to Driver API documentation for more details including any\nsystem/implementation specific behavior. An optional operand e may be specified. Operand e is a vector of .s32 values that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand e is a singleton tuple for 1d textures; is a two\nelement vector 2d textures; and is four-element vector for 3d textures, where the fourth element is\nignored. An optional operand f may be specified for depth textures . Depth textures are special type\nof textures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand f is .f32 scalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in f operand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. When using depth compare operand, the elements in texture coordinate vector c have .f32 type. Depth compare operand is not supported for 3d textures. The instruction returns a two-element vector for destination type .f16x2 . For all other\ndestination types, the instruction returns a four-element vector. Coordinates may be given in either\nsigned 32-bit integer or 32-bit floating point form. A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. tex.{a1d,a2d} Texture array selection, followed by texture lookup. The instruction first selects a texture from\nthe texture array named by operand a using the index given by the first element of the array\ncoordinate vector c . The instruction then loads data from the selected texture at coordinates\ngiven by the remaining elements of operand c into destination d . Operand c is a bit-size\ntype vector or tuple containing an index into the array of textures followed by coordinates within\nthe selected texture, as follows: For 1d texture arrays, operand c has type .v2.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the texture array, and the second element is interpreted as\na 1d texture coordinate of type .ctype . For 2d texture arrays, operand c has type .v4.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the texture array, and the next two elements are\ninterpreted as 2d texture coordinates of type .ctype . The fourth element is ignored. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. An optional operand e may be specified. Operand e is a vector of .s32 values that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand e is a singleton tuple for 1d texture arrays; and is\na two element vector 2d texture arrays. An optional operand f may be specified for depth textures arrays. Operand f is .f32 scalar value that specifies depth compare value for depth textures. When using depth compare\noperand, the coordinates in texture coordinate vector c have .f32 type. The instruction returns a two-element vector for destination type .f16x2 . For all other\ndestination types, the instruction returns a four-element vector. The texture array index is a\n32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point\nvalues. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.cube Cubemap texture lookup. The instruction loads data from the cubemap texture named by operand a at coordinates given by operand c into destination d . Cubemap textures are special\ntwo-dimensional layered textures consisting of six layers that represent the faces of a cube. All\nlayers in a cubemap are of the same size and are square (i.e., width equals height). When accessing a cubemap, the texture coordinate vector c has type .v4.f32 , and comprises\nthree floating-point coordinates ( s , t , r ) and a fourth padding argument which is\nignored. Coordinates ( s , t , r ) are projected onto one of the six cube faces. The ( s , t , r ) coordinates can be thought of as a direction vector emanating from the center of the\ncube. Of the three coordinates ( s , t , r ), the coordinate of the largest magnitude (the\nmajor axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by\nthe absolute value of the major axis to produce a new ( s , t ) coordinate pair to lookup into\nthe selected cube face. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. Offset vector operand e is not supported for cubemap textures. an optional operand f may be specified for cubemap depth textures. operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.acube Cubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture\nfrom the cubemap array named by operand a using the index given by the first element of the\narray coordinate vector c . The instruction then loads data from the selected cubemap texture at\ncoordinates given by the remaining elements of operand c into destination d . Cubemap array textures consist of an array of cubemaps, i.e., the total number of layers is a\nmultiple of six. When accessing a cubemap array texture, the coordinate vector c has type .v4.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the\ncubemap array, and the remaining three elements are interpreted as floating-point cubemap\ncoordinates ( s , t , r ), used to lookup in the selected cubemap as described above. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. Offset vector operand e is not supported for cubemap texture arrays. An optional operand f may be specified for cubemap depth texture arrays. Operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.2dms Multi-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of\nmultiple samples per data element. The instruction loads data from the texture named by operand a from sample number given by first element of the operand c , at coordinates given by\nremaining elements of operand c into destination d . When accessing a multi-sample texture,\ntexture coordinate vector c has type .v4.b32 . The first element in operand c is\ninterpreted as unsigned integer sample number ( .u32 ), and the next two elements are interpreted\nas signed integer ( .s32 ) 2d texture coordinates. The fourth element is ignored. An optional\ntexture sampler b may be specified. If no sampler is specified, the sampler behavior is a\nproperty of the named texture. An optional operand e may be specified. Operand e is a vector of type .v2.s32 that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Depth compare operand f is not supported for multi-sample textures. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.a2dms Multi-sample texture array selection, followed by multi-sample texture lookup. The instruction first\nselects a multi-sample texture from the multi-sample texture array named by operand a using the\nindex given by the first element of the array coordinate vector c . The instruction then loads\ndata from the selected multi-sample texture from sample number given by second element of the\noperand c , at coordinates given by remaining elements of operand c into destination d . When accessing a multi-sample texture array, texture coordinate vector c has type .v4.b32 . The first element in operand c is interpreted as unsigned integer sampler number, the\nsecond element is interpreted as unsigned integer index ( .u32 ) into the multi-sample texture\narray and the next two elements are interpreted as signed integer ( .s32 ) 2d texture\ncoordinates. An optional texture sampler b may be specified. If no sampler is specified, the\nsampler behavior is a property of the named texture. An optional operand e may be specified. Operand e is a vector of type .v2.s32 values\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Depth compare operand f is not supported for multi-sample texture arrays. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. Mipmaps .base (lod zero) Pick level 0 (base level). This is the default if no mipmap mode is specified. No additional arguments. .level (lod explicit) Requires an additional 32-bit scalar argument, lod , which contains the LOD to fetch from. The\ntype of lod follows .ctype (either .s32 or .f32 ). Geometries .2dms and .a2dms are not supported in this mode. .grad (lod gradient) Requires two .f32 vectors, dPdx and dPdy , that specify the partials. The vectors are\nsingletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are\nfour-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d\nand cube geometries. Geometries .2dms and .a2dms are not supported in this mode. For mipmap texture lookup, an optional operand e may be specified. Operand e is a vector of .s32 that specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Offset vector operand is not supported for cube\nand cubemap geometries. An optional operand f may be specified for mipmap textures. Operand f is .f32 scalar\nvalue that specifies depth compare value for depth textures. When using depth compare operand, the\ncoordinates in texture coordinate vector c have .f32 type. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. Depth compare operand is not supported for 3d textures. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture sm_20 or higher. In indirect access, operand a is a .u64 register holding\nthe address of a .texref variable. Notes For compatibility with prior versions of PTX, the square brackets are not required and .v4 coordinate vectors are allowed for any geometry, with the extra elements being ignored. PTX ISA Notes Unified mode texturing introduced in PTX ISA version 1.0. Extension using opaque .texref and .samplerref types and independent mode texturing introduced in PTX ISA version 1.5. Texture arrays tex.{a1d,a2d} introduced in PTX ISA version 2.3. Cubemaps and cubemap arrays introduced in PTX ISA version 3.0. Support for mipmaps introduced in PTX ISA version 3.1. Indirect texture access introduced in PTX ISA version 3.1. Multi-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2. Support for textures returning .f16 and .f16x2 data introduced in PTX ISA version 4.2. Support for tex.grad.{cube, acube} introduced in PTX ISA version 4.3. Offset vector operand introduced in PTX ISA version 4.3. Depth compare operand introduced in PTX ISA version 4.3. Support for optional destination predicate introduced in PTX ISA version 7.1. Target ISA Notes Supported on all target architectures. The cubemap array geometry ( .acube ) requires sm_20 or higher. Mipmaps require sm_20 or higher. Indirect texture access requires sm_20 or higher. Multi-sample textures and multi-sample texture arrays require sm_30 or higher. Texture fetch returning .f16 and .f16x2 data require sm_53 or higher. tex.grad.{cube, acube} requires sm_20 or higher. Offset vector operand requires sm_30 or higher. Depth compare operand requires sm_30 or higher. Support for optional destination predicate requires sm_60 or higher. Examples // Example of unified mode texturing\n // - f4 is required to pad four-element tuple and is ignored\n tex.3d.v4.s32.s32  {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}];\n\n // Example of independent mode texturing\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}];\n\n // Example of 1D texture array, independent texturing mode\n tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}];\n\n // Example of 2D texture array, unified texturing mode\n // - f3 is required to pad four-element tuple and is ignored\n tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}];\n\n // Example of cubemap array, unified textureing mode\n tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}];\n\n // Example of multi-sample texture, unified texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture, independent texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture array, unified texturing mode\n tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}];\n\n // Example of texture returning .f16 data\n tex.1d.v4.f16.f32  {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}];\n\n // Example of texture returning .f16x2 data\n tex.1d.v2.f16x2.f32  {h1,h2}, [tex_a,smpl_x,{f1}];\n\n // Example of 3d texture array access with tex.grad,unified texturing mode\n tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n// Example of cube texture array access with tex.grad,unified texturing mode\n tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n // Example of 1d texture lookup with offset, unified texturing mode\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a, {f1}], {r5};\n\n // Example of 2d texture array lookup with offset, unified texturing mode\n tex.a2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6};\n\n // Example of 2d mipmap texture lookup with offset, unified texturing mode\n tex.level.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}],\n                          flvl, {r7, r8};\n\n // Example of 2d depth texture lookup with compare, unified texturing mode\n tex.1d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a, {f1}], f0;\n\n // Example of depth 2d texture array lookup with offset, compare\n tex.a2d.v4.s32.f32  {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6;\n\n // Example of destination predicate use\n tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}]; 9.7.9.4. Texture Instructions: tld4  tld4 Perform a texture fetch of the 4-texel bilerp footprint. Syntax tld4.comp.2d.v4.dtype.f32    d[|p], [a, c] {, e} {, f};\ntld4.comp.geom.v4.dtype.f32  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n.comp  = { .r, .g, .b, .a };\n.geom  = { .2d, .a2d, .cube, .acube };\n.dtype = { .u32, .s32, .f32 }; Description Texture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction\nloads the bilerp footprint from the texture named by operand a at coordinates given by operand c into vector destination d . The texture component fetched for each texel sample is\nspecified by .comp . The four texel samples are placed into destination vector d in\ncounter-clockwise order starting at lower left. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. An optional operand f may be specified for depth textures . Depth textures are special type of\ntextures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand f is .f32 scalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in f operand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. tld4.2d For 2D textures, operand c specifies coordinates as a two-element, 32-bit floating-point vector. An optional operand e may be specified. Operand e is a vector of type .v2.s32 that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7. tld4.a2d Texture array selection, followed by tld4 texture fetch of 2d texture. For 2d texture arrays\noperand c is a four element, 32-bit vector. The first element in operand c is interpreted as an\nunsigned integer index ( .u32 ) into the texture array, and the next two elements are interpreted\nas 32-bit floating point coordinates of 2d texture. The fourth element is ignored. An optional operand e may be specified. Operand e is a vector of type .v2.s32 that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7. tld4.cube For cubemap textures, operand c specifies four-element vector which comprises three\nfloating-point coordinates (s, t, r) and a fourth padding argument which is ignored. Cubemap textures are special two-dimensional layered textures consisting of six layers that\nrepresent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e.,\nwidth equals height). Coordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be\nthought of as a direction vector emanating from the center of the cube. Of the three coordinates (s,\nt, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the\nother two coordinates (the minor axes) are divided by the absolute value of the major axis to\nproduce a new (s, t) coordinate pair to lookup into the selected cube face. Offset vector operand e is not supported for cubemap textures. tld4.acube Cubemap array selection, followed by tld4 texture fetch of cubemap texture. The first element in\noperand c is interpreted as an unsigned integer index ( .u32 ) into the cubemap texture array,\nand the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r),\nused to lookup in the selected cubemap. Offset vector operand e is not supported for cubemap texture arrays. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture sm_20 or higher. In indirect access, operand a is a .u64 register holding\nthe address of a .texref variable. PTX ISA Notes Introduced in PTX ISA version 2.2. Indirect texture access introduced in PTX ISA version 3.1. tld4.{a2d,cube,acube} introduced in PTX ISA version 4.3. Offset vector operand introduced in PTX ISA version 4.3. Depth compare operand introduced in PTX ISA version 4.3. Support for optional destination predicate introduced in PTX ISA version 7.1. Target ISA Notes tld4 requires sm_20 or higher. Indirect texture access requires sm_20 or higher. tld4.{a2d,cube,acube} requires sm_30 or higher. Offset vector operand requires sm_30 or higher. Depth compare operand requires sm_30 or higher. Support for optional destination predicate requires sm_60 or higher. Examples //Example of unified mode texturing\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}];\n\n// Example of independent mode texturing\ntld4.r.2d.v4.u32.f32  {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];\n\n// Example of unified mode texturing using offset\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};\n\n// Example of unified mode texturing using compare\ntld4.r.2d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;\n\n// Example of optional destination predicate\ntld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7; 9.7.9.5. Texture Instructions: txq  txq Query texture and sampler attributes. Syntax txq.tquery.b32         d, [a];       // texture attributes\ntxq.level.tlquery.b32  d, [a], lod;  // texture attributes\ntxq.squery.b32         d, [a];       // sampler attributes\n\n.tquery  = { .width, .height, .depth,\n             .channel_data_type, .channel_order,\n             .normalized_coords, .array_size,\n             .num_mipmap_levels, .num_samples};\n\n.tlquery = { .width, .height, .depth };\n\n.squery  = { .force_unnormalized_coords, .filter_mode,\n             .addr_mode_0, addr_mode_1, addr_mode_2 }; Description Query an attribute of a texture or sampler. Operand a is either a .texref or .samplerref variable, or a .u64 register. Query Returns .width .height .depth value in elements .channel_data_type Unsigned integer corresponding to source language’s channel data type\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both channel_data_type and channel_order queries. .channel_order Unsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both channel_data_type and channel_order queries. .normalized_coords 1 ( True ) or 0 ( False ). .force_unnormalized_coords 1 ( True) or 0 ( False). Defined only for .samplerref variables in independent texture mode. Overrides the normalized_coords field of a .texref variable used with a .samplerref in a tex instruction. .filter_mode Integer from enum { nearest, linear } .addr_mode_0 .addr_mode_1 .addr_mode_2 Integer from enum { wrap, mirror, clamp_ogl, clamp_to_edge, clamp_to_border } .array_size For a texture array, number of textures in array, 0 otherwise. .num_mipmap_levels For a mipmapped texture, number of levels of details (LOD), 0 otherwise. .num_samples For a multi-sample texture, number of samples, 0 otherwise. Texture attributes are queried by supplying a .texref argument to txq . In unified mode,\nsampler attributes are also accessed via a .texref argument, and in independent mode sampler\nattributes are accessed via a separate .samplerref argument. txq.level txq.level requires an additional 32bit integer argument, lod , which specifies LOD and\nqueries requested attribute for the specified LOD. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture sm_20 or higher. In indirect access, operand a is a .u64 register holding\nthe address of a .texref variable. PTX ISA Notes Introduced in PTX ISA version 1.5. Channel data type and channel order queries were added in PTX ISA version 2.1. The .force_unnormalized_coords query was added in PTX ISA version 2.2. Indirect texture access introduced in PTX ISA version 3.1. .array_size , .num_mipmap_levels , .num_samples samples queries were added in PTX ISA\nversion 4.1. txq.level introduced in PTX ISA version 4.3. Target ISA Notes Supported on all target architectures. Indirect texture access requires sm_20 or higher. Querying the number of mipmap levels requires sm_20 or higher. Querying the number of samples requires sm_30 or higher. txq.level requires sm_30 or higher. Examples txq.width.b32       %r1, [tex_A];\ntxq.filter_mode.b32 %r1, [tex_A];   // unified mode\ntxq.addr_mode_0.b32 %r1, [smpl_B];  // independent mode\ntxq.level.width.b32 %r1, [tex_A], %r_lod; 9.7.9.6. Texture Instructions: istypep  istypep Query whether a register points to an opaque variable of a specified type. Syntax istypep.type   p, a;  // result is .pred\n\n.type = { .texref, .samplerref, .surfref }; Description Write predicate register p with 1 if register a points to an opaque variable of the\nspecified type, and with 0 otherwise. Destination p has type .pred ; the source address\noperand must be of type .u64 . PTX ISA Notes Introduced in PTX ISA version 4.0. Target ISA Notes istypep requires sm_30 or higher. Examples istypep.texref istex, tptr;\nistypep.samplerref issampler, sptr;\nistypep.surfref issurface, surfptr; 9.7.10. Surface Instructions  This section describes PTX instructions for accessing surfaces. PTX supports the following\noperations on surface descriptors: Static initialization of surface descriptors. Module-scope and per-entry scope definitions of surface descriptors. Ability to query fields within surface descriptors. These instructions provide access to surface memory. suld sust sured suq 9.7.10.1. Surface Instructions: suld  suld Load from surface memory. Syntax suld.b.geom{.cop}.vec.dtype.clamp  d, [a, b];  // unformatted\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d };\n.cop   = { .ca, .cg, .cs, .cv };               // cache operation\n.vec   = { none, .v2, .v4 };\n.dtype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero }; Description suld.b.{1d,2d,3d} Load from surface memory using a surface coordinate vector. The instruction loads data from the\nsurface named by operand a at coordinates given by operand b into destination d . Operand a is a .surfref variable or .u64 register. Operand b is a scalar or singleton tuple\nfor 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d\nsurfaces, where the fourth element is ignored. Coordinate elements are of type .s32 . suld.b performs an unformatted load of binary data. The lowest dimension coordinate represents a\nbyte offset into the surface and is not scaled, and the size of the data transfer matches the size\nof destination operand d . suld.b.{a1d,a2d} Surface layer selection, followed by a load from the selected surface. The instruction first selects\na surface layer from the surface array named by operand a using the index given by the first\nelement of the array coordinate vector b . The instruction then loads data from the selected\nsurface at coordinates given by the remaining elements of operand b into destination d . Operand a is a .surfref variable or .u64 register. Operand b is a bit-size\ntype vector or tuple containing an index into the array of surfaces followed by coordinates within\nthe selected surface, as follows: For 1d surface arrays, operand b has type .v2.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the second element is interpreted as a\n1d surface coordinate of type .s32 . For 2d surface arrays, operand b has type .v4.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the next two elements are interpreted\nas 2d surface coordinates of type .s32 . The fourth element is ignored. A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp loads data at the nearest surface location (sized appropriately) .zero loads zero for out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes suld.b.trap introduced in PTX ISA version 1.5. Additional clamp modifiers and cache operations introduced in PTX ISA version 2.0. suld.b.3d and suld.b.{a1d,a2d} introduced in PTX ISA version 3.0. Indirect surface access introduced in PTX ISA version 3.1. Target ISA Notes suld.b supported on all target architectures. sm_1x targets support only the .trap clamping modifier. suld.3d and suld.{a1d,a2d} require sm_20 or higher. Indirect surface access requires sm_20 or higher. Cache operations require sm_20 or higher. Examples suld.b.1d.v4.b32.trap  {s1,s2,s3,s4}, [surf_B, {x}];\nsuld.b.3d.v2.b64.trap  {r1,r2}, [surf_A, {x,y,z,w}];\nsuld.b.a1d.v2.b32      {r0,r1}, [surf_C, {idx,x}];\nsuld.b.a2d.b32         r0, [surf_D, {idx,x,y,z}];  // z ignored 9.7.10.2. Surface Instructions: sust  sust Store to surface memory. Syntax sust.b.{1d,2d,3d}{.cop}.vec.ctype.clamp  [a, b], c;  // unformatted\nsust.p.{1d,2d,3d}.vec.b32.clamp          [a, b], c;  // formatted\n\nsust.b.{a1d,a2d}{.cop}.vec.ctype.clamp   [a, b], c;  // unformatted\n\n.cop   = { .wb, .cg, .cs, .wt };                     // cache operation\n.vec   = { none, .v2, .v4 };\n.ctype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero }; Description sust.{1d,2d,3d} Store to surface memory using a surface coordinate vector. The instruction stores data from operand c to the surface named by operand a at coordinates given by operand b . Operand a is\na .surfref variable or .u64 register. Operand b is a scalar or singleton tuple for 1d\nsurfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces,\nwhere the fourth element is ignored. Coordinate elements are of type .s32 . sust.b performs an unformatted store of binary data. The lowest dimension coordinate represents\na byte offset into the surface and is not scaled. The size of the data transfer matches the size of\nsource operand c . sust.p performs a formatted store of a vector of 32-bit data values to a surface sample. The\nsource vector elements are interpreted left-to-right as R , G , B , and A surface\ncomponents. These elements are written to the corresponding surface sample components. Source\nelements that do not occur in the surface sample are ignored. Surface sample components that do not\noccur in the source vector will be written with an unpredictable value. The lowest dimension\ncoordinate represents a sample offset rather than a byte offset. The source data interpretation is based on the surface sample format as follows: If the surface\nformat contains UNORM , SNORM , or FLOAT data, then .f32 is assumed; if the surface\nformat contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. The source data is then converted from this type to the surface\nsample format. sust.b.{a1d,a2d} Surface layer selection, followed by an unformatted store to the selected surface. The instruction\nfirst selects a surface layer from the surface array named by operand a using the index given by\nthe first element of the array coordinate vector b . The instruction then stores the data in\noperand c to the selected surface at coordinates given by the remaining elements of operand b . Operand a is a .surfref variable or .u64 register. Operand b is a bit-size type\nvector or tuple containing an index into the array of surfaces followed by coordinates within the\nselected surface, as follows: For 1d surface arrays, operand b has type .v2.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the second element is interpreted as\na 1d surface coordinate of type .s32 . For 2d surface arrays, operand b has type .v4.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the next two elements are\ninterpreted as 2d surface coordinates of type .s32 . The fourth element is ignored. A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp stores data at the nearest surface location (sized appropriately) .zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes sust.b.trap introduced in PTX ISA version 1.5. sust.p , additional clamp modifiers, and\ncache operations introduced in PTX ISA version 2.0. sust.b.3d and sust.b.{a1d,a2d} introduced in PTX ISA version 3.0. Indirect surface access introduced in PTX ISA version 3.1. Target ISA Notes sust.b supported on all target architectures. sm_1x targets support only the .trap clamping modifier. sust.3d and sust.{a1d,a2d} require sm_20 or higher. sust.p requires sm_20 or higher. Indirect surface access requires sm_20 or higher. Cache operations require sm_20 or higher. Examples sust.p.1d.v4.b32.trap  [surf_B, {x}], {f1,f2,f3,f4};\nsust.b.3d.v2.b64.trap  [surf_A, {x,y,z,w}], {r1,r2};\nsust.b.a1d.v2.b64      [surf_C, {idx,x}], {r1,r2};\nsust.b.a2d.b32         [surf_D, {idx,x,y,z}], r0;  // z ignored 9.7.10.3. Surface Instructions: sured  sured Reduce surface memory. Syntax sured.b.op.geom.ctype.clamp  [a,b],c; // byte addressing\nsured.p.op.geom.ctype.clamp  [a,b],c; // sample addressing\n\n.op    = { .add, .min, .max, .and, .or };\n.geom  = { .1d, .2d, .3d };\n.ctype = { .u32, .u64, .s32, .b32, .s64 };  // for sured.b\n.ctype = { .b32, .b64 };                    // for sured.p\n.clamp = { .trap, .clamp, .zero }; Description Reduction to surface memory using a surface coordinate vector. The instruction performs a reduction\noperation with data from operand c to the surface named by operand a at coordinates given by\noperand b . Operand a is a .surfref variable or .u64 register. Operand b is a\nscalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a\nfour-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of\ntype .s32 . sured.b performs an unformatted reduction on .u32 , .s32 , .b32 , .u64 , or .s64 data. The lowest dimension coordinate represents a byte offset into the surface and is not\nscaled. Operation add applies to .u32 , .u64 , and .s32 types; min and max apply to .u32 , .s32 , .u64 and .s64 types; operations and and or apply to .b32 type. sured.p performs a reduction on sample-addressed data. The lowest dimension coordinate\nrepresents a sample offset rather than a byte offset. The instruction type .b64 is restricted to min and max operations. For type .b32 , the data is interpreted as .u32 or .s32 based on the surface sample format as follows: if the surface format contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. For\ntype .b64 , if the surface format contains UINT data, then .u64 is assumed; if the\nsurface format contains SINT data, then .s64 is assumed. A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp stores data at the nearest surface location (sized appropriately) .zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes Introduced in PTX ISA version 2.0. Indirect surface access introduced in PTX ISA version 3.1. .u64 / .s64 / .b64 types with .min / .max operations introduced in PTX ISA version\n8.1. Target ISA Notes sured requires sm_20 or higher. Indirect surface access requires sm_20 or higher. .u64 / .s64 / .b64 types with .min / .max operations requires sm_50 or higher. Examples sured.b.add.2d.u32.trap  [surf_A, {x,y}], r1;\nsured.p.min.1d.u32.trap  [surf_B, {x}], r1;\nsured.b.max.1d.u64.trap  [surf_C, {x}], r1;\nsured.p.min.1d.b64.trap  [surf_D, {x}], r1; 9.7.10.4. Surface Instructions: suq  suq Query a surface attribute. Syntax suq.query.b32   d, [a];\n\n.query = { .width, .height, .depth,\n           .channel_data_type, .channel_order,\n           .array_size, .memory_layout }; Description Query an attribute of a surface. Operand a is a .surfref variable or a .u64 register. Query Returns .width .height .depth value in elements .channel_data_type Unsigned integer corresponding to source language’s channel data\ntype enumeration. If the source language combines channel data\ntype and channel order into a single enumeration type, that value\nis returned for both channel_data_type and channel_order queries. .channel_order Unsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and\nchannel order into a single enumeration type, that value is\nreturned for both channel_data_type and channel_order queries. .array_size For a surface array, number of surfaces in array, 0 otherwise. .memory_layout 1 for surface with linear memory layout; 0 otherwise Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes Introduced in PTX ISA version 1.5. Channel data type and channel order queries added in PTX ISA version 2.1. Indirect surface access introduced in PTX ISA version 3.1. The .array_size query was added in PTX ISA version 4.1. The .memory_layout query was added in PTX ISA version 4.2. Target ISA Notes Supported on all target architectures. Indirect surface access requires sm_20 or higher. Examples suq.width.b32       %r1, [surf_A]; 9.7.11. Control Flow Instructions  The following PTX instructions and syntax are for controlling execution in a PTX program: {} @ bra call ret exit 9.7.11.1. Control Flow Instructions: {}  {} Instruction grouping. Syntax { instructionList } Description The curly braces create a group of instructions, used primarily for defining a function body. The\ncurly braces also provide a mechanism for determining the scope of a variable: any variable declared\nwithin a scope is not available outside the scope. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples { add.s32  a,b,c; mov.s32  d,a; } 9.7.11.2. Control Flow Instructions: @  @ Predicated execution. Syntax @{!}p    instruction; Description Execute an instruction or instruction block for threads that have the guard predicate True . Threads with a False guard predicate do nothing. Semantics If {!}p then instruction PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples setp.eq.f32  p,y,0;     // is y zero?\n@!p div.f32      ratio,x,y  // avoid division by zero\n\n@q  bra L23;                // conditional branch 9.7.11.3. Control Flow Instructions: bra  bra Branch to a target and continue execution there. Syntax @p   bra{.uni}  tgt;           // tgt is a label\n     bra{.uni}  tgt;           // unconditional branch Description Continue execution at the target. Conditional branches are specified by using a guard predicate. The\nbranch target must be a label. bra.uni is guaranteed to be non-divergent, i.e. all active threads in a warp that are currently\nexecuting this instruction have identical values for the guard predicate and branch target. Semantics if (p) {\n    pc = tgt;\n} PTX ISA Notes Introduced in PTX ISA version 1.0. Unimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec. Target ISA Notes Supported on all target architectures. Examples bra.uni  L_exit;    // uniform unconditional jump\n@q  bra      L23;   // conditional branch 9.7.11.4. Control Flow Instructions: brx.idx  brx.idx Branch to a label indexed from a list of potential branch targets. Syntax @p    brx.idx{.uni} index, tlist;\n      brx.idx{.uni} index, tlist; Description Index into a list of possible destination labels, and continue execution from the chosen\nlabel. Conditional branches are specified by using a guard predicate. brx.idx.uni guarantees that the branch is non-divergent, i.e. all active threads in a warp that\nare currently executing this instruction have identical values for the guard predicate and the index argument. The index operand is a .u32 register. The tlist operand must be the label of a .branchtargets directive. It is accessed as a zero-based sequence using index . Behaviour is\nundefined if the value of index is greater than or equal to the length of tlist . The .branchtargets directive must be defined in the local function scope before it is used. It\nmust refer to labels within the current function. Semantics if (p) {\n    if (index < length(tlist)) {\n      pc = tlist[index];\n    } else {\n      pc = undefined;\n    }\n} PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples .function foo () {\n    .reg .u32 %r0;\n    ...\n    L1:\n    ...\n    L2:\n    ...\n    L3:\n    ...\n    ts: .branchtargets L1, L2, L3;\n    @p brx.idx %r0, ts;\n    ...\n} 9.7.11.5. Control Flow Instructions: call  call Call a function, recording the return location. Syntax // direct call to named function, func is a symbol\ncall{.uni} (ret-param), func, (param-list);\ncall{.uni} func, (param-list);\ncall{.uni} func;\n\n// indirect call via pointer, with full list of call targets\ncall{.uni} (ret-param), fptr, (param-list), flist;\ncall{.uni} fptr, (param-list), flist;\ncall{.uni} fptr, flist;\n\n// indirect call via pointer, with no knowledge of call targets\ncall{.uni} (ret-param), fptr, (param-list), fproto;\ncall{.uni} fptr, (param-list), fproto;\ncall{.uni} fptr, fproto; Description The call instruction stores the address of the next instruction, so execution can resume at that\npoint after executing a ret instruction. A call is assumed to be divergent unless the .uni suffix is present. The .uni suffix indicates that the call is guaranteed to be\nnon-divergent, i.e. all active threads in a warp that are currently executing this instruction have\nidentical values for the guard predicate and call target. For direct calls, the called location func must be a symbolic function name; for indirect calls,\nthe called location fptr must be an address of a function held in a register. Input arguments\nand return values are optional. Arguments may be registers, immediate constants, or variables in .param space. Arguments are pass-by-value. Indirect calls require an additional operand, flist or fproto , to communicate the list of\npotential call targets or the common function prototype of all call targets,\nrespectively. In the first case, flist gives a complete list of potential call targets and\nthe optimizing backend is free to optimize the calling convention. In the second case, where the\ncomplete list of potential call targets may not be known, the common function prototype is given\nand the call must obey the ABI’s calling convention. The flist operand is either the name of an array (call table) initialized to a list of function\nnames; or a label associated with a .calltargets directive, which declares a list of potential call targets. In both cases the fptr register holds the address of a function listed in the call\ntable or .calltargets list, and the call operands are type-checked against the type\nsignature of the functions indicated by flist . The fproto operand is the name of a label associated with a .callprototype directive. This\noperand is used when a complete list of potential targets is not known. The call operands are\ntype-checked against the prototype, and code generation will follow the ABI calling convention. If a\nfunction that doesn’t match the prototype is called, the behavior is undefined. Call tables may be declared at module scope or local scope, in either the constant or global state\nspace. The .calltargets and .callprototype directives must be declared within a function\nbody. All functions must be declared prior to being referenced in a call table initializer or .calltargets directive. PTX ISA Notes Direct call introduced in PTX ISA version 1.0. Indirect call introduced in PTX ISA version 2.1. Target ISA Notes Direct call supported on all target architectures. Indirect call requires sm_20 or higher. Examples // examples of direct call\n    call     init;    // call function 'init'\n    call.uni g, (a);  // call function 'g' with parameter 'a'\n@p  call     (d), h, (a, b);  // return value into register d\n\n// call-via-pointer using jump table\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n\n.global .u32 jmptbl[5] = { foo, bar, baz };\n      ...\n@p    ld.global.u32  %r0, [jmptbl+4];\n@p    ld.global.u32  %r0, [jmptbl+8];\n      call  (retval), %r0, (x, y), jmptbl;\n\n// call-via-pointer using .calltargets directive\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n      ...\n@p    mov.u32  %r0, foo;\n@q    mov.u32  %r0, baz;\nFtgt: .calltargets foo, bar, baz;\n      call  (retval), %r0, (x, y), Ftgt;\n\n// call-via-pointer using .callprototype directive\n.func dispatch (.reg .u32 fptr, .reg .u32 idx)\n{\n...\nFproto: .callprototype _ (.param .u32 _, .param .u32 _);\n      call  %fptr, (x, y), Fproto;\n... 9.7.11.6. Control Flow Instructions: ret  ret Return from function to instruction after call. Syntax ret{.uni}; Description Return execution to caller’s environment. A divergent return suspends threads until all threads are\nready to return to the caller. This allows multiple divergent ret instructions. A ret is assumed to be divergent unless the .uni suffix is present, indicating that the\nreturn is guaranteed to be non-divergent. Any values returned from a function should be moved into the return parameter variables prior to\nexecuting the ret instruction. A return instruction executed in a top-level entry routine will terminate thread execution. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples ret;\n@p  ret; 9.7.11.7. Control Flow Instructions: exit  exit Terminate a thread. Syntax exit; Description Ends execution of a thread. As threads exit, barriers waiting on all threads are checked to see if the exiting threads are the\nonly threads that have not yet made it to a barrier{.cta} for all threads in the CTA or to a barrier.cluster for all threads in the cluster. If the exiting threads are holding up the\nbarrier, the barrier is released. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples exit;\n@p  exit; 9.7.12. Parallel Synchronization and Communication Instructions  These instructions are: bar{.cta} , barrier{.cta} barrier.cluster bar.warp.sync membar atom red red.async vote match.sync activemask redux.sync griddepcontrol elect.sync mbarrier.init mbarrier.inval mbarrier.arrive mbarrier.arrive_drop mbarrier.test_wait mbarrier.try_wait mbarrier.pending_count cp.async.mbarrier.arrive tensormap.cp_fenceproxy 9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrier  bar{.cta}, barrier{.cta} Barrier synchronization. Syntax barrier{.cta}.sync{.aligned}      a{, b};\nbarrier{.cta}.arrive{.aligned}    a, b;\n\nbarrier{.cta}.red.popc{.aligned}.u32  d, a{, b}, {!}c;\nbarrier{.cta}.red.op{.aligned}.pred   p, a{, b}, {!}c;\n\nbar{.cta}.sync      a{, b};\nbar{.cta}.arrive    a, b;\n\nbar{.cta}.red.popc.u32  d, a{, b}, {!}c;\nbar{.cta}.red.op.pred   p, a{, b}, {!}c;\n\n.op = { .and, .or }; Description Performs barrier synchronization and communication within a CTA. Each CTA instance has sixteen\nbarriers numbered 0..15 . barrier{.cta} instructions can be used by the threads within the CTA for synchronization and\ncommunication. Operands a , b , and d have type .u32 ; operands p and c are predicates. Source\noperand a specifies a logical barrier resource as an immediate constant or register with value 0 through 15 . Operand b specifies the number of threads participating in the barrier. If\nno thread count is specified, all threads in the CTA participate in the barrier. When specifying a\nthread count, the value must be a multiple of the warp size. Note that a non-zero thread count is\nrequired for barrier{.cta}.arrive . Depending on operand b , either specified number of threads (in multiple of warp size) or all\nthreads in the CTA participate in barrier{.cta} instruction. The barrier{.cta} instructions\nsignal the arrival of the executing threads at the named barrier. barrier{.cta} instruction causes executing thread to wait for all non-exited threads from its\nwarp and marks warps’ arrival at barrier. In addition to signaling its arrival at the barrier, the barrier{.cta}.red and barrier{.cta}.sync instructions causes executing thread to wait for\nnon-exited threads of all other warps participating in the barrier to\narrive. barrier{.cta}.arrive does not cause executing thread to wait for threads of other\nparticipating warps. When a barrier completes, the waiting threads are restarted without delay, and the barrier is\nreinitialized so that it can be immediately reused. The barrier{.cta}.sync or barrier{.cta}.red or barrier{.cta}.arrive instruction\nguarantees that when the barrier completes, prior memory accesses requested by this thread are\nperformed relative to all threads participating in the barrier. The barrier{.cta}.sync and barrier{.cta}.red instruction further guarantees that no new memory access is requested by this\nthread before the barrier completes. A memory read (e.g., by ld or atom ) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread participating in the barrier. A\nmemory write (e.g., by st , red or atom ) has been performed when the value written has\nbecome visible to other threads participating in the barrier, that is, when the previous value can\nno longer be read. barrier{.cta}.red performs a reduction operation across threads. The c predicate (or its\ncomplement) from all threads in the CTA are combined using the specified reduction operator. Once\nthe barrier count is reached, the final value is written to the destination register in all threads\nwaiting at the barrier. The reduction operations for barrier{.cta}.red are population-count ( .popc ),\nall-threads-True ( .and ), and any-thread-True ( .or ). The result of .popc is the number of\nthreads with a True predicate, while .and and .or indicate if all the threads had a True predicate or if any of the threads had a True predicate. Instruction barrier{.cta} has optional .aligned modifier. When specified, it indicates that\nall threads in CTA will execute the same barrier{.cta} instruction. In conditionally executed\ncode, an aligned barrier{.cta} instruction should only be used if it is known that all threads\nin CTA evaluate the condition identically, otherwise behavior is undefined. Different warps may execute different forms of the barrier{.cta} instruction using the same\nbarrier name and thread count. One example mixes barrier{.cta}.sync and barrier{.cta}.arrive to implement producer/consumer models. The producer threads execute barrier{.cta}.arrive to\nannounce their arrival at the barrier and continue execution without delay to produce the next\nvalue, while the consumer threads execute the barrier{.cta}.sync to wait for a resource to be\nproduced. The roles are then reversed, using a different barrier, where the producer threads execute\na barrier{.cta}.sync to wait for a resource to consumed, while the consumer threads announce\nthat the resource has been consumed with barrier{.cta}.arrive . Care must be taken to keep a warp\nfrom executing more barrier{.cta} instructions than intended ( barrier{.cta}.arrive followed\nby any other barrier{.cta} instruction to the same barrier) prior to the reset of the\nbarrier. barrier{.cta}.red should not be intermixed with barrier{.cta}.sync or barrier{.cta}.arrive using the same active barrier. Execution in this case is unpredictable. The optional .cta qualifier simply indicates CTA-level applicability of the barrier and it\ndoesn’t change the semantics of the instruction. bar{.cta}.sync is equivalent to barrier{.cta}.sync.aligned . bar{.cta}.arrive is\nequivalent to barrier{.cta}.arrive.aligned . bar{.cta}.red is equivalent to barrier{.cta}.red.aligned . Note For .target sm_6x or below, barrier{.cta} instruction without .aligned modifier is equivalent to .aligned variant and has the same restrictions as of .aligned variant. All threads in warp (except for those have exited) must execute barrier{.cta} instruction\nin convergence. PTX ISA Notes bar.sync without a thread count introduced in PTX ISA version 1.0. Register operands, thread count, and bar.{arrive,red} introduced in PTX ISA version 2.0. barrier instruction introduced in PTX ISA version 6.0. .cta qualifier introduced in PTX ISA version 7.8. Target ISA Notes Register operands, thread count, and bar{.cta}.{arrive,red} require sm_20 or higher. Only bar{.cta}.sync with an immediate barrier number is supported for sm_1x targets. barrier{.cta} instruction requires sm_30 or higher. Examples // Use bar.sync to arrive at a pre-computed barrier number and\n// wait for all threads in CTA to also arrive:\n    st.shared [r0],r1;  // write my result to shared memory\n    bar.cta.sync  1;    // arrive, wait for others to arrive\n    ld.shared r2,[r3];  // use shared results from other threads\n\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for fixed number of cooperating threads to arrive:\n    #define CNT1 (8*12) // Number of cooperating threads\n\n    st.shared [r0],r1;     // write my result to shared memory\n    bar.cta.sync  1, CNT1; // arrive, wait for others to arrive\n    ld.shared r2,[r3];     // use shared results from other threads\n\n// Use bar.red.and to compare results across the entire CTA:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.and.pred r3,1,p; // r3=AND(p) forall threads in CTA\n\n// Use bar.red.popc to compute the size of a group of threads\n// that have a specific condition True:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.popc.u32 r3,1,p; // r3=SUM(p) forall threads in CTA\n\n/* Producer/consumer model. The producer deposits a value in\n * shared memory, signals that it is complete but does not wait\n * using bar.arrive, and begins fetching more data from memory.\n * Once the data returns from memory, the producer must wait\n * until the consumer signals that it has read the value from\n * the shared memory location. In the meantime, a consumer\n * thread waits until the data is stored by the producer, reads\n * it, and then signals that it is done (without waiting).\n */\n    // Producer code places produced value in shared memory.\n    st.shared   [r0],r1;\n    bar.arrive  0,64;\n    ld.global   r1,[r2];\n    bar.sync    1,64;\n    ...\n\n    // Consumer code, reads value from shared memory\n    bar.sync   0,64;\n    ld.shared  r1,[r0];\n    bar.arrive 1,64;\n    ...\n\n    // Examples of barrier.cta.sync\n    st.shared         [r0],r1;\n    barrier.cta.sync  0;\n    ld.shared         r1, [r0]; 9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.sync  bar.warp.sync Barrier synchronization for threads in a warp. Syntax bar.warp.sync      membermask; Description bar.warp.sync will cause executing thread to wait until all threads corresponding to membermask have executed a bar.warp.sync with the same membermask value before resuming\nexecution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s laneid . The behavior of bar.warp.sync is undefined if the executing thread is not in the membermask . bar.warp.sync also guarantee memory ordering among threads participating in barrier. Thus,\nthreads within warp that wish to communicate via memory can store to memory, execute bar.warp.sync , and then safely read values stored by other threads in warp. Note For .target sm_6x or below, all threads in membermask must execute the same bar.warp.sync instruction in convergence, and only threads belonging to some membermask can be active when the bar.warp.sync instruction is executed. Otherwise, the behavior is\nundefined. PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples st.shared.u32 [r0],r1;         // write my result to shared memory\nbar.warp.sync  0xffffffff;     // arrive, wait for others to arrive\nld.shared.u32 r2,[r3];         // read results written by other threads 9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.cluster  barrier.cluster Barrier synchronization within a cluster. Syntax barrier.cluster.arrive{.sem}{.aligned};\nbarrier.cluster.wait{.acquire}{.aligned};\n\n.sem = {.release, .relaxed} Description Performs barrier synchronization and communication within a cluster. barrier.cluster instructions can be used by the threads within the cluster for synchronization\nand communication. barrier.cluster.arrive instruction marks warps’ arrival at barrier without causing executing\nthread to wait for threads of other participating warps. barrier.cluster.wait instruction causes the executing thread to wait for all non-exited threads\nof the cluster to perform barrier.cluster.arrive . In addition, barrier.cluster instructions cause the executing thread to wait for all non-exited\nthreads from its warp. When all non-exited threads that executed barrier.cluster.arrive have executed barrier.cluster.wait , the barrier completes and is reinitialized so it can be reused\nimmediately. Each thread must arrive at the barrier only once before the barrier completes. The barrier.cluster.wait instruction guarantees that when it completes the execution, memory\naccesses (except asynchronous operations) requested, in program order, prior to the preceding barrier.cluster.arrive by all threads in the cluster are complete and visible to the executing\nthread. There is no memory ordering and visibility guarantee for memory accesses requested by the executing\nthread, in program order, after barrier.cluster.arrive and prior to barrier.cluster.wait . The optional .relaxed qualifier on barrier.cluster.arrive specifies that there are no memory\nordering and visibility guarantees provided for the memory accesses performed prior to barrier.cluster.arrive . The optional .sem and .acquire qualifiers on instructions barrier.cluster.arrive and barrier.cluster.wait specify the memory synchronization as described in the Memory Consistency\nModel . If the optional .sem qualifier is absent for barrier.cluster.arrive , .release is assumed by default. If the optional .acquire qualifier is absent for barrier.cluster.wait , .acquire is assumed by default. The optional .aligned qualifier indicates that all threads in the warp must execute the same barrier.cluster instruction. In conditionally executed code, an aligned barrier.cluster instruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 7.8. Support for .acquire , .relaxed , .release qualifiers introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples // use of arrive followed by wait\nld.shared::cluster.u32 r0, [addr];\nbarrier.cluster.arrive.aligned;\n...\nbarrier.cluster.wait.aligned;\nst.shared::cluster.u32 [addr], r1;\n\n// use memory fence prior to arrive for relaxed barrier\n@cta0 ld.shared::cluster.u32 r0, [addr];\nfence.cluster.acq_rel;\nbarrier.cluster.arrive.relaxed.aligned;\n...\nbarrier.cluster.wait.aligned;\n@cta1 st.shared::cluster.u32 [addr], r1; 9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fence  membar/fence Enforce an ordering of memory operations. Syntax // Thread fence :\nfence{.sem}.scope;\n\n// Operation fence :\nfence.op_restrict.release.cluster;\n\n// Proxy fence (bi-directional) :\nfence.proxy.proxykind;\n\n// Proxy fence (uni-directional) :\nfence.proxy.to_proxykind::from_proxykind.release.scope;\nfence.proxy.to_proxykind::from_proxykind.acquire.scope  [addr], size;\n\n// Old style membar :\nmembar.level;\nmembar.proxy.proxykind;\n\n.sem       = { .sc, .acq_rel };\n.scope     = { .cta, .cluster, .gpu, .sys };\n.level     = { .cta, .gl, .sys };\n.proxykind = { .alias, .async, async.global, .async.shared::{cta, cluster} };\n.op_restrict = { .mbarrier_init };\n.to_proxykind::from_proxykind = {.tensormap::generic}; Description The membar instruction guarantees that prior memory accesses requested by this thread ( ld , st , atom and red instructions) are performed at the specified level , before later\nmemory operations requested by this thread following the membar instruction. The level qualifier specifies the set of threads that may observe the ordering effect of this operation. A memory read (e.g., by ld or atom ) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread at the indicated level. A memory\nwrite (e.g., by st , red or atom ) has been performed when the value written has become\nvisible to other threads at the specified level, that is, when the previous value can no longer be\nread. The fence instruction establishes an ordering between memory accesses requested by this thread\n( ld , st , atom and red instructions) as described in the Memory Consistency Model . The scope qualifier specifies the set of threads that may\nobserve the ordering effect of this operation. fence.acq_rel is a light-weight fence that is sufficient for memory synchronization in most\nprograms. Instances of fence.acq_rel synchronize when combined with additional memory operations\nas described in acquire and release patterns in the Memory Consistency Model . If the optional .sem qualifier is absent, .acq_rel is assumed by default. fence.sc is a slower fence that can restore sequential consistency when used in sufficient\nplaces, at the cost of performance. Instances of fence.sc with sufficient scope always\nsynchronize by forming a total order per scope, determined at runtime. This total order can be\nconstrained further by other synchronization in the program. Qualifier .op_restrict restricts the class of prior memory operations for which the fence instruction provides the memory ordering guarantees. When .op_restrict is .mbarrier_init ,\nthe fence only applies to the prior mbarrier.init operations executed by the same thread on mbarrier objects in .shared::cta state space. The address operand addr and the operand size together specifies the memory range [addr, addr+size-1] on which the ordering guarantees on the memory accesses across the proxies is to be\nprovided. The only supported value for the size operand is 128. Generic Addressing is used unconditionally, and the address specified by the operand addr must fall within the .global state space. Otherwise, the behavior is undefined. On sm_70 and higher membar is a synonym for fence.sc 1 , and the membar levels cta , gl and sys are synonymous with the fence scopes cta , gpu and sys respectively. membar.proxy and fence.proxy instructions establish an ordering between memory accesses that\nmay happen through different proxies . A uni-directional proxy ordering from the from-proxykind to the to-proxykind establishes\nordering between a prior memory access performed via the from-proxykind and a subsequent memory access\nperformed via the to-proxykind . A bi-directional proxy ordering between two proxykinds establishes two uni-directional proxy orderings\n: one from the first proxykind to the second proxykind and the other from the second proxykind to the first\nproxykind. The .proxykind qualifier indicates the bi-directional proxy ordering that is established between the memory\naccesses done between the generic proxy and the proxy specified by .proxykind . Value .alias of the .proxykind qualifier refers to memory accesses performed using virtually\naliased addresses to the same memory location. Value .async of the .proxykind qualifier specifies\nthat the memory ordering is established between the async proxy and the generic proxy. The memory\nordering is limited only to the state space specified. If no state space is specified, then the memory\nordering applies on all state spaces. A .release proxy fence can form a release sequence that synchronizes with an acquire\nsequence that contains a .acquire proxy fence. The .to_proxykind and .from_proxykind qualifiers indicate the uni-directional proxy ordering that is established. On sm_70 and higher, membar.proxy is a synonym for fence.proxy . 1 The semantics of fence.sc introduced with sm_70 is a superset of the semantics of membar and the two are compatible; when executing on sm_70 or later architectures, membar acquires the full semantics of fence.sc . PTX ISA Notes membar.{cta,gl} introduced in PTX ISA version 1.4. membar.sys introduced in PTX ISA version 2.0. fence introduced in PTX ISA version 6.0. membar.proxy and fence.proxy introduced in PTX ISA version 7.5. .cluster scope qualifier introduced in PTX ISA version 7.8. .op_restrict qualifier introduced in PTX ISA version 8.0. fence.proxy.async is introduced in PTX ISA version 8.0. .to_proxykind::from_proxykind qualifier introduced in PTX ISA version 8.3. Target ISA Notes membar.{cta,gl} supported on all target architectures. membar.sys requires sm_20 or higher. fence requires sm_70 or higher. membar.proxy requires sm_60 or higher. fence.proxy requires sm_70 or higher. .cluster scope qualifier requires sm_90 or higher. .op_restrict qualifier requires sm_90 or higher. fence.proxy.async requires sm_90 or higher. .to_proxykind::from_proxykind qualifier requires sm_90 or higher. Examples membar.gl;\nmembar.cta;\nmembar.sys;\nfence.sc;\nfence.sc.cluster;\nfence.proxy.alias;\nmembar.proxy.alias;\nfence.mbarrier_init.release.cluster;\nfence.proxy.async;\nfence.proxy.async.shared::cta;\nfence.proxy.async.shared::cluster;\nfence.proxy.async.global;\n\ntensormap.replace.tile.global_address.global.b1024.b64   [gbl], new_addr;\nfence.proxy.tensormap::generic.release.gpu;\nfence.proxy.tensormap::generic.acquire.gpu [tmap], 128;\ncvta.global.u64  tmap, gbl;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [tmap, {tc0}], [mbar0]; 9.7.12.5. Parallel Synchronization and Communication Instructions: atom  atom Atomic reduction operations for thread-to-thread communication. Syntax Atomic operation with scalar type: atom{.sem}{.scope}{.space}.op{.level::cache_hint}.type d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.op.type d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b16 d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b128 d, [a], b, c {, cache-policy};\natom{.sem}{.scope}{.space}.exch{.level::cache_hint}.b128 d, [a], b {, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16     d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2   d, [a], b{, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16    d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2  d, [a], b{, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                { .relaxed, .acquire, .release, .acq_rel };\n.scope =              { .cta, .cluster, .gpu, .sys };\n\n.op =                 { .and, .or, .xor,\n                        .cas, .exch,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; Atomic operation with vector type: atom{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32                  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_16_bit.half_word_type  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type     d, [a], b{, cache-policy};\n\n.sem =               { .relaxed, .acquire, .release, .acq_rel };\n.scope =             { .cta, .cluster, .gpu, .sys };\n.op =                { .add, .min, .max };\n.half_word_type =    { .f16, .bf16 };\n.packed_type =       { .f16x2, .bf16x2 };\n.vec_16_bit =        { .v2, .v4, .v8 }\n.vec_32_bit =        { .v2, .v4 };\n.level::cache_hint = { .L2::cache_hint } Description Atomically loads the original value at location a into destination register d , performs a\nreduction operation with operand b and the value in location a , and stores the result of the\nspecified operation at location a , overwriting the original value. Operand a specifies a\nlocation in the specified state space. If no state space is given, perform the memory accesses using Generic Addressing . atom with scalar type may be used only\nwith .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. atom with vector type may be used only with .global space\nand with generic addressing where the address points to .global space. For atom with vector type, operands d and b are brace-enclosed vector expressions, size\nof which is equal to the size of vector qualifier. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory\nConsistency Model . If the .sem qualifier is absent, .relaxed is assumed by default. The optional .scope qualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is absent, .gpu scope is\nassumed by default. For atom with vector type, the supported combinations of vector qualifier and types, and atomic\noperations supported on these combinations are depicted in the following table: Vector qualifier Types .f16 / bf16 .f16x2 / bf16x2 .f32 .v2 .add , .min , .max .add , .min , .max .add .v4 .add , .min , .max .add , .min , .max .add .v8 .add , .min , .max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write. atom instruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch cases, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire atom is not guaranteed to be atomic as a single access. For sm_6x and earlier architectures, atom operations on .shared state space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory atomic\ninstructions, e.g., by inserting barriers between normal stores and atomic operations to a common\naddress, or by using atom.exch to store to locations accessed by other atomic operations. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands The bit-size operations are .and , .or , .xor , .cas (compare-and-swap), and .exch (exchange). The integer operations are .add , .inc , .dec , .min , .max . The .inc and .dec operations return a result in the range [0..b] . The floating-point operation .add operation rounds to nearest even. Current implementation of atom.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas atom.add.f32 on shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero. atom.add.f16 , atom.add.f16x2 , atom.add.bf16 and atom.add.bf16x2 operation requires\nthe .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to\nzero. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. Semantics atomic {\n    d = *a;\n    *a = (operation == cas) ? operation(*a, b, c)\n                            : operation(*a, b);\n}\nwhere\n    inc(r, s)  = (r >= s) ? 0 : r+1;\n    dec(r, s)  = (r==0 || r > s)  ? s : r-1;\n    exch(r, s) =  s;\n    cas(r,s,t) = (r == s) ? t : r; Notes Simple reductions may be specified by using the bit bucket destination operand _ . PTX ISA Notes 32-bit atom.global introduced in PTX ISA version 1.1. atom.shared and 64-bit atom.global.{add,cas,exch} introduced in PTX ISA 1.2. atom.add.f32 and 64-bit atom.shared.{add,cas,exch} introduced in PTX ISA 2.0. 64-bit atom.{and,or,xor,min,max} introduced in PTX ISA 3.1. atom.add.f64 introduced in PTX ISA 5.0. .scope qualifier introduced in PTX ISA 5.0. .sem qualifier introduced in PTX ISA version 6.0. atom.add.noftz.f16x2 introduced in PTX ISA 6.2. atom.add.noftz.f16 and atom.cas.b16 introduced in PTX ISA 6.3. Per-element atomicity of atom.f16x2 clarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2. Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4. atom.add.noftz.bf16 and atom.add.noftz.bf16x2 introduced in PTX ISA 7.8. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for vector types introduced in PTX ISA version 8.1. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes atom.global requires sm_11 or higher. atom.shared requires sm_12 or higher. 64-bit atom.global.{add,cas,exch} require sm_12 or higher. 64-bit atom.shared.{add,cas,exch} require sm_20 or higher. 64-bit atom.{and,or,xor,min,max} require sm_32 or higher. atom.add.f32 requires sm_20 or higher. atom.add.f64 requires sm_60 or higher. .scope qualifier requires sm_60 or higher. .sem qualifier requires sm_70 or higher. Use of generic addressing requires sm_20 or higher. atom.add.noftz.f16x2 requires sm_60 or higher. atom.add.noftz.f16 and atom.cas.b16 requires sm_70 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. atom.add.noftz.bf16 and atom.add.noftz.bf16x2 require sm_90 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for vector types requires sm_90 or higher. Support for .b128 type requires sm_90 or higher. Examples atom.global.add.s32  d,[a],1;\natom.shared::cta.max.u32  d,[x+4],0;\n@p  atom.global.cas.b32  d,[p],my_val,my_new_val;\natom.global.sys.add.u32 d, [a], 1;\natom.global.acquire.sys.inc.u32 ans, [gbl], %r0;\natom.add.noftz.f16x2 d, [a], b;\natom.add.noftz.f16   hd, [ha], hb;\natom.global.cas.b16  hd, [ha], hb, hc;\natom.add.noftz.bf16   hd, [a], hb;\natom.add.noftz.bf16x2 bd, [b], bb;\natom.add.shared::cluster.noftz.f16   hd, [ha], hb;\natom.shared.b128.cas d, a, b, c; // 128-bit atom\natom.global.b128.exch d, a, b;   // 128-bit atom\n\natom.global.cluster.relaxed.add.u32 d, [a], 1;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\natom.global.add.L2::cache_hint.s32  d, [a], 1, cache-policy;\n\natom.global.v8.f16.max.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v8.bf16.add.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v2.f16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v2.bf16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v4.b16x2.min.noftz  {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3};\natom.global.v4.f32.add  {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3};\natom.global.v2.f16x2.min.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.bf16x2.max.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1}; 9.7.12.6. Parallel Synchronization and Communication Instructions: red  red Reduction operations on global and shared memory. Syntax Reduction operation with scalar type: red{.sem}{.scope}{.space}.op{.level::cache_hint}.type          [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16    [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2  [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16\n                                                      [a], b {, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2\n                                                      [a], b {, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                {.relaxed, .release};\n.scope =              {.cta, .cluster, .gpu, .sys};\n\n.op =                 { .and, .or, .xor,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; Reduction operation with vector type: red{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy};\n\n.sem =                { .relaxed, .release };\n.scope =              { .cta, .cluster, .gpu, .sys };\n.op =                 { .add, .min, .max };\n.half_word_type =     { .f16, .bf16 };\n.packed_type =        { .f16x2,.bf16x2 };\n.vec_16_bit =         { .v2, .v4, .v8 }\n.vec_32_bit =         { .v2, .v4 };\n.level::cache_hint =  { .L2::cache_hint } Description Performs a reduction operation with operand b and the value in location a , and stores the\nresult of the specified operation at location a , overwriting the original value. Operand a specifies a location in the specified state space. If no state space is given, perform the memory\naccesses using Generic Addressing . red with scalar type may\nbe used only with .global and .shared spaces and with generic addressing, where the address\npoints to .global or .shared space. red with vector type may be used only with .global space and with generic addressing where the address points to .global space. For red with vector type, operand b is brace-enclosed vector expressions, size of which is\nequal to the size of vector qualifier. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory\nConsistency Model . If the .sem qualifier is absent, .relaxed is assumed by default. The optional .scope qualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is absent, .gpu scope is\nassumed by default. For red with vector type, the supported combinations of vector qualifier, types and reduction\noperations supported on these combinations are depicted in following table: Vector qualifier Types .f16 / bf16 .f16x2 / bf16x2 .f32 .v2 .add , .min , .max .add , .min , .max .add .v4 .add , .min , .max .add , .min , .max .add .v8 .add , .min , .max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write. red instruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch case, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire red is not guaranteed to be atomic as a single access. For sm_6x and earlier architectures, red operations on .shared state space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory reduction\ninstructions, e.g., by inserting barriers between normal stores and reduction operations to a common\naddress, or by using atom.exch to store to locations accessed by other reduction operations. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands The bit-size operations are .and , .or , and .xor . The integer operations are .add , .inc , .dec , .min , .max . The .inc and .dec operations return a result in the range [0..b] . The floating-point operation .add operation rounds to nearest even. Current implementation of red.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas red.add.f32 on shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero. red.add.f16 , red.add.f16x2 , red.add.bf16 and red.add.bf16x2 operation requires the .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to zero. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. Semantics *a = operation(*a, b);\n\nwhere\n    inc(r, s) = (r >= s) ? 0 : r+1;\n    dec(r, s) = (r==0 || r > s)  ? s : r-1; PTX ISA Notes Introduced in PTX ISA version 1.2. red.add.f32 and red.shared.add.u64 introduced in PTX ISA 2.0. 64-bit red.{and,or,xor,min,max} introduced in PTX ISA 3.1. red.add.f64 introduced in PTX ISA 5.0. .scope qualifier introduced in PTX ISA 5.0. .sem qualifier introduced in PTX ISA version 6.0. red.add.noftz.f16x2 introduced in PTX ISA 6.2. red.add.noftz.f16 introduced in PTX ISA 6.3. Per-element atomicity of red.f16x2 clarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2 Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4. red.add.noftz.bf16 and red.add.noftz.bf16x2 introduced in PTX ISA 7.8. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for vector types introduced in PTX ISA version 8.1. Target ISA Notes red.global requires sm_11 or higher red.shared requires sm_12 or higher. red.global.add.u64 requires sm_12 or higher. red.shared.add.u64 requires sm_20 or higher. 64-bit red.{and,or,xor,min,max} require sm_32 or higher. red.add.f32 requires sm_20 or higher. red.add.f64 requires sm_60 or higher. .scope qualifier requires sm_60 or higher. .sem qualifier requires sm_70 or higher. Use of generic addressing requires sm_20 or higher. red.add.noftz.f16x2 requires sm_60 or higher. red.add.noftz.f16 requires sm_70 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. red.add.noftz.bf16 and red.add.noftz.bf16x2 require sm_90 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for vector types requires sm_90 or higher. Examples red.global.add.s32  [a],1;\nred.shared::cluster.max.u32  [x+4],0;\n@p  red.global.and.b32  [p],my_val;\nred.global.sys.add.u32 [a], 1;\nred.global.acquire.sys.add.u32 [gbl], 1;\nred.add.noftz.f16x2 [a], b;\nred.add.noftz.bf16   [a], hb;\nred.add.noftz.bf16x2 [b], bb;\nred.global.cluster.relaxed.add.u32 [a], 1;\nred.shared::cta.min.u32  [x+4],0;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nred.global.and.L2::cache_hint.b32 [a], 1, cache-policy;\n\nred.global.v8.f16.add.noftz  [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v2.f16.add.noftz [gbl], {%h0, %h1};\nred.global.v2.bf16.add.noftz [gbl], {%h0, %h1};\nred.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3};\nred.global.v4.f32.add  [gbl], {%f0, %f1, %f2, %f3};\nred.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1}; 9.7.12.7. Parallel Synchronization and Communication Instructions: red.async  red.async Asynchronous reduction operation on shared memory. Syntax // Increment and Decrement reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.ss   =                 { .shared::cluster };\n.op   =                 { .inc, .dec };\n.type =                 { .u32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n\n// MIN and MAX reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.ss   = { .shared::cluster };\n.op   = { .min, .max };\n.type = { .u32, .s32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// Bitwise AND, OR and XOR reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.ss   = { .shared::cluster };\n.op   = { .and, .or, .xor };\n.type = { .b32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// ADD reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.add.type [a], b, [mbar];\n\n.ss   = { .shared::cluster };\n.type = { .u32, .s32, .u64 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes }; Description red.async is a non-blocking instruction which initiates an asynchronous reduction operation\nspecified by .op , with the operand b and the value at destination shared memory location\nspecified by operand a . The .inc and .dec operations return a result in the range [0..b] . The modifier .completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands . The shared memory addresses of destination operand a and the mbarrier object mbar , must\nmeet all of the following conditions: They Belong to the same CTA. They are different to the CTA of the executing thread but must be within the same cluster. Otherwise, the behavior is undefined. The state space of the address {.ss} , if specified, is applicable to both operands a and mbar . If not specified, then Generic Addressing is used for\nboth a and mbar . With .shared::cluster , if the addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined. The reduce operation in red.async is treated as a relaxed memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples red.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr]; 9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated)  vote (deprecated) Vote across thread group. Syntax vote.mode.pred  d, {!}a;\nvote.ballot.b32 d, {!}a;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni }; Deprecation Note The vote instruction without a .sync qualifier is deprecated in PTX ISA version 6.0. Support for this instruction with .target lower than sm_70 may be removed in a future PTX\nISA version. Removal Note Support for vote instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .target sm_70 or higher. Description Performs a reduction of the source predicate across all active threads in a warp. The destination\npredicate value is the same across all threads in the warp. The reduction modes are: .all True if source predicate is True for all active threads in warp. Negate the source\npredicate to compute .none . .any True if source predicate is True for some active thread in warp. Negate the source\npredicate to compute .not_all . .uni True if source predicate has the same value in all active threads in warp. Negating the\nsource predicate also computes .uni . In the ballot form, vote.ballot.b32 simply copies the predicate from each thread in a warp\ninto the corresponding bit position of destination register d , where the bit position\ncorresponds to the thread’s lane id. An inactive thread in warp will contribute a 0 for its entry when participating in vote.ballot.b32 . PTX ISA Notes Introduced in PTX ISA version 1.2. Deprecated in PTX ISA version 6.0 in favor of vote.sync . Not supported in PTX ISA version 6.4 for .target sm_70 or higher. Target ISA Notes vote requires sm_12 or higher. vote.ballot.b32 requires sm_20 or higher. vote is not supported on sm_70 or higher starting PTX ISA version 6.4. Release Notes Note that vote applies to threads in a single warp, not across an entire CTA. Examples vote.all.pred    p,q;\nvote.uni.pred    p,q;\nvote.ballot.b32  r1,p;  // get 'ballot' across warp 9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync  vote.sync Vote across thread group. Syntax vote.sync.mode.pred  d, {!}a, membermask;\nvote.sync.ballot.b32 d, {!}a, membermask;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni }; Description vote.sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed vote.sync with the same qualifiers and same membermask value\nbefore resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid . Operand a is a\npredicate register. In the mode form, vote.sync performs a reduction of the source predicate across all non-exited\nthreads in membermask . The destination operand d is a predicate register and its value is\nthe same across all threads in membermask . The reduction modes are: .all True if source predicate is True for all non-exited threads in membermask . Negate the\nsource predicate to compute .none . .any True if source predicate is True for some thread in membermask . Negate the source\npredicate to compute .not_all . .uni True if source predicate has the same value in all non-exited threads in membermask . Negating the source predicate also computes .uni . In the ballot form, the destination operand d is a .b32 register. In this form, vote.sync.ballot.b32 simply copies the predicate from each thread in membermask into the\ncorresponding bit position of destination register d , where the bit position corresponds to the\nthread’s lane id. A thread not specified in membermask will contribute a 0 for its entry in vote.sync.ballot.b32 . The behavior of vote.sync is undefined if the executing thread is not in the membermask . Note For .target sm_6x or below, all threads in membermask must execute the same vote.sync instruction in convergence, and only threads belonging to some membermask can be active when\nthe vote.sync instruction is executed. Otherwise, the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples vote.sync.all.pred    p,q,0xffffffff;\nvote.sync.ballot.b32  r1,p,0xffffffff;  // get 'ballot' across warp 9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync  match.sync Broadcast and compare a value across threads in warp. Syntax match.any.sync.type  d, a, membermask;\nmatch.all.sync.type  d[|p], a, membermask;\n\n.type = { .b32, .b64 }; Description match.sync will cause executing thread to wait until all non-exited threads from membermask have executed match.sync with the same qualifiers and same membermask value before resuming\nexecution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid. match.sync performs broadcast and compare of operand a across all non-exited threads in membermask and sets destination d and optional predicate p based on mode. Operand a has instruction type and d has .b32 type. Destination d is a 32-bit mask where bit position in mask corresponds to thread’s laneid. The matching operation modes are: .all d is set to mask corresponding to non-exited threads in membermask if all non-exited\nthreads in membermask have same value of operand a ; otherwise d is set\nto 0. Optionally predicate p is set to true if all non-exited threads in membermask have\nsame value of operand a ; otherwise p is set to false. The sink symbol ‘_’ may be used in\nplace of any one of the destination operands. .any d is set to mask of non-exited threads in membermask that have same value of operand a . The behavior of match.sync is undefined if the executing thread is not in the membermask . PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_70 or higher. Release Notes Note that match.sync applies to threads in a single warp, not across an entire CTA. Examples match.any.sync.b32    d, a, 0xffffffff;\nmatch.all.sync.b64    d|p, a, mask; 9.7.12.11. Parallel Synchronization and Communication Instructions: activemask  activemask Queries the active threads within a warp. Syntax activemask.b32 d; Description activemask queries predicated-on active threads from the executing warp and sets the destination d with 32-bit integer mask where bit position in the mask corresponds to the thread’s laneid . Destination d is a 32-bit destination register. An active thread will contribute 1 for its entry in the result and exited or inactive or\npredicated-off thread will contribute 0 for its entry in the result. PTX ISA Notes Introduced in PTX ISA version 6.2. Target ISA Notes Requires sm_30 or higher. Examples activemask.b32  %r1; 9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync  redux.sync Perform reduction operation on the data from each predicated active thread in the thread group. Syntax redux.sync.op.type dst, src, membermask;\n.op   = {.add, .min, .max}\n.type = {.u32, .s32}\n\nredux.sync.op.b32 dst, src, membermask;\n.op   = {.and, .or, .xor} Description redux.sync will cause the executing thread to wait until all non-exited threads corresponding to membermask have executed redux.sync with the same qualifiers and same membermask value\nbefore resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid . redux.sync performs a reduction operation .op of the 32 bit source register src across\nall non-exited threads in the membermask . The result of the reduction operation is written to\nthe 32 bit destination register dst . Reduction operation can be one of the bitwise operation in .and , .or , .xor or arithmetic\noperation in .add , .min , .max . For the .add operation result is truncated to 32 bits. The behavior of redux.sync is undefined if the executing thread is not in the membermask . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Release Notes Note that redux.sync applies to threads in a single warp, not across an entire CTA. Examples .reg .b32 dst, src, init, mask;\nredux.sync.add.s32 dst, src, 0xff;\nredux.sync.xor.b32 dst, src, mask; 9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol  griddepcontrol Control execution of dependent grids. Syntax griddepcontrol.action;\n\n.action   = { .launch_dependents, .wait } Description The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by\nthe runtime, to control execution in the following way: .launch_dependents modifier signals that specific dependents the runtime system designated to\nreact to this instruction can be scheduled as soon as all other CTAs in the grid issue the same\ninstruction or have completed. The dependent may launch before the completion of the current\ngrid. There is no guarantee that the dependent will launch before the completion of the current\ngrid. Repeated invocations of this instruction by threads in the current CTA will have no additional\nside effects past that of the first invocation. .wait modifier causes the executing thread to wait until all prerequisite grids in flight have\ncompleted and all the memory operations from the prerequisite grids are performed and made visible\nto the current grid. Note If the prerequisite grid is using griddepcontrol.launch_dependents , then the dependent grid\nmust use griddepcontrol.wait to ensure correct functional execution. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples griddepcontrol.launch_dependents;\ngriddepcontrol.wait; 9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync  elect.sync Elect a leader thread from a set of threads. Syntax elect.sync d|p, membermask; Description elect.sync elects one predicated active leader thread from among a set of threads specified by membermask . laneid of the elected thread is returned in the 32-bit destination operand d . The sink symbol ‘_’ can be used for destination operand d . The predicate destination p is set to True for the leader thread, and False for all other threads. Operand membermask specifies a 32-bit integer indicating the set of threads from which a leader\nis to be elected. The behavior is undefined if the executing thread is not in membermask . Election of a leader thread happens deterministically, i.e. the same leader thread is elected for\nthe same membermask every time. The mandatory .sync qualifier indicates that elect causes the executing thread to wait until\nall threads in the membermask execute the elect instruction before resuming execution. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples elect.sync    %r0|%p0, 0xffffffff; 9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier  mbarrier is a barrier created in shared memory that supports : Synchronizing any subset of threads within a CTA One-way synchronization of threads across CTAs of a cluster. As noted in mbarrier support with\nshared memory , threads can\nperform only arrive operations but not *_wait on an mbarrier located in shared::cluster space. Waiting for completion of asynchronous memory operations initiated by a thread and making them\nvisible to other threads. An mbarrier object is an opaque object in memory which can be initialized and invalidated using : mbarrier.init mbarrier.inval Operations supported on mbarrier object s are : mbarrier.expect_tx mbarrier.complete_tx mbarrier.arrive mbarrier.arrive_drop mbarrier.test_wait mbarrier.try_wait mbarrier.pending_count cp.async.mbarrier.arrive Performing any mbarrier operation except mbarrier.init on an uninitialized mbarrier object results in undefined behavior. Unlike bar{.cta} / barrier{.cta} instructions which can access a limited number of barriers\nper CTA, mbarrier objects are used defined and are only limited by the total shared memory size\navailable. mbarrier operations enable threads to perform useful work after the arrival at the mbarrier and\nbefore waiting for the mbarrier to complete. 9.7.12.15.1. Size and alignment of mbarrier object  An mbarrier object is an opaque object with the following type and alignment requirements : Type Alignment (bytes) Memory space .b64 8 .shared 9.7.12.15.2. Contents of the mbarrier object  An opaque mbarrier object keeps track of the following information : Current phase of the mbarrier object Count of pending arrivals for the current phase of the mbarrier object Count of expected arrivals for the next phase of the mbarrier object Count of pending asynchronous memory operations (or transactions) tracked by the current phase of\nthe mbarrier object . This is also referred to as tx-count . An mbarrier object progresses through a sequence of phases where each phase is defined by threads\nperforming an expected number of arrive-on operations. The valid range of each of the counts is as shown below: Count name Minimum value Maximum value Expected arrival count 1 2 20 - 1 Pending arrival count 0 2 20 - 1 tx-count -(2 20 - 1) 2 20 - 1 9.7.12.15.3. Lifecycle of the mbarrier object  The mbarrier object must be initialized prior to use. An mbarrier object is used to synchronize threads and asynchronous memory operations. An mbarrier object may be used to perform a sequence of such synchronizations. An mbarrier object must be invalidated to repurpose its memory. 9.7.12.15.4. Phase of the mbarrier object  The phase of an mbarrier object is the number of times the mbarrier object has been used to\nsynchronize threads and cp.async operations. In each phase {0, 1, 2, …}, threads perform in program order : arrive-on operations to complete the current phase and test_wait / try_wait operations to check for the completion of the current phase. An mbarrier object is automatically reinitialized upon completion of the current phase for\nimmediate use in the next phase. The current phase is incomplete and all prior phases are complete. For each phase of the mbarrier object, at least one test_wait or try_wait operation must be\nperformed which returns True for waitComplete before an arrive-on operation\nin the subsequent phase. 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object  Starting with the Hopper architecture ( sm_9x ), mbarrier object supports a new count, called tx-count , which is used for tracking the completion of asynchronous memory operations or\ntransactions. tx-count tracks the number of asynchronous transactions, in units specified by the\nasynchronous memory operation, that are outstanding and yet to be complete. The tx-count of an mbarrier object must be set to the total amount of asynchronous memory\noperations, in units as specified by the asynchronous operations, to be tracked by the current\nphase. Upon completion of each of the asynchronous operations, the complete-tx operation will be performed on the mbarrier object and thus progress the mbarrier towards the\ncompletion of the current phase. 9.7.12.15.5.1. expect-tx operation  The expect-tx operation, with an expectCount argument, increases the tx-count of an mbarrier object by the value specified by expectCount . This makes the current phase of the mbarrier object to expect and track the completion of additional asynchronous transactions. 9.7.12.15.5.2. complete-tx operation  The complete-tx operation, with an completeCount argument, on an mbarrier object consists of the following: mbarrier signaling Signals the completion of asynchronous transactions that were tracked by the current phase. As a\nresult of this, tx-count is decremented by completeCount . mbarrier potentially completing the current phase If the current phase has been completed then the mbarrier transitions to the next phase. Refer to Phase Completion of the mbarrier object for details on phase completion requirements and phase transition process. 9.7.12.15.6. Phase Completion of the mbarrier object  The requirements for completion of the current phase are described below. Upon completion of the\ncurrent phase, the phase transitions to the subsequent phase as described below. Current phase completion requirements An mbarrier object completes the current phase when all of the following conditions are met: The count of the pending arrivals has reached zero. The tx-count has reached zero. Phase transition When an mbarrier object completes the current phase, the following actions are performed\natomically: The mbarrier object transitions to the next phase. The pending arrival count is reinitialized to the expected arrival count. 9.7.12.15.7. Arrive-on operation on mbarrier object  An arrive-on operation, with an optional count argument, on an mbarrier object consists of the\nfollowing 2 steps : mbarrier signalling: Signals the arrival of the executing thread OR completion of the cp.async instruction which\nsignals the arrive-on operation initiated by the executing thread on the mbarrier object . As a\nresult of this, the pending arrival count is decremented by count . If the count argument is\nnot specified, then it defaults to 1. mbarrier potentially completing the current phase: If the current phase has been completed then the mbarrier transitions to the next phase. Refer to Phase Completion of the mbarrier object for details on phase completion requirements and phase transition process. 9.7.12.15.8. mbarrier support with shared memory  The following table summarizes the support of various mbarrier operations on mbarrier objects located at different shared memory locations: mbarrier operations .shared::cta .shared::cluster mbarrier.arrive Supported Supported, cannot return result mbarrier.expect_tx Supported Supported mbarrier.complete_tx Supported Supported Other mbarrier operations Supported Not supported 9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init  mbarrier.init Initialize the mbarrier object . Syntax mbarrier.init{.shared{::cta}}.b64 [addr], count; Description mbarrier.init initializes the mbarrier object at the location specified by the address operand addr with the unsigned 32-bit integer count . The value of operand count must be in the range\nas specified in Contents of the mbarrier object . Initialization of the mbarrier object involves : Initializing the current phase to 0. Initializing the expected arrival count to count . Initializing the pending arrival count to count . Initializing the tx-count to 0. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Examples .shared .b64 shMem, shMem2;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n\ncvta.shared.u64          addr, shMem2;\nmbarrier.init.b64        [addr],   %r1;\nbar.cta.sync             0;\n// ... other mbarrier operations on addr\n\nmbarrier.init.shared::cta.b64 [shMem], 12;\nbar.sync                 0;\n// ... other mbarrier operations on shMem 9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval  mbarrier.inval Invalidates the mbarrier object . Syntax mbarrier.inval{.shared{::cta}}.b64 [addr]; Description mbarrier.inval invalidates the mbarrier object at the location specified by the address\noperand addr . An mbarrier object must be invalidated before using its memory location for any other purpose. Performing any mbarrier operation except mbarrier.init on an invalidated mbarrier object\nresults in undefined behaviour. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Examples .shared .b64 shmem;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n.reg    .pred t0;\n\n// Example 1 :\nbar.sync                      0;\n@t0 mbarrier.init.b64     [addr], %r1;\n// ... other mbarrier operations on addr\nbar.sync                      0;\n@t0 mbarrier.inval.b64    [addr];\n\n\n// Example 2 :\nbar.cta.sync                  0;\nmbarrier.init.shared.b64           [shmem], 12;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared.b64      [shmem];\n\n// shmem can be reused here for unrelated use :\nbar.cta.sync                  0;\nst.shared.b64                      [shmem], ...;\n\n// shmem can be re-initialized as mbarrier object :\nbar.cta.sync                  0;\n@t0 mbarrier.init.shared.b64       [shmem], 24;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared::cta.b64 [shmem]; 9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx  mbarrier.expect_tx Perfoms expect-tx operation on the mbarrier object . Syntax mbarrier.expect_tx{.sem}{.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.expect_tx performs an expect-tx operation on the mbarrier object at the location specified by the address operand addr . The\n32-bit unsigned integer operand txCount specifies the expectCount argument to the expect-tx operation. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr are as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . This operation does not provide any memory ordering semantics and thus is a relaxed operation. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples mbarrier.expect_tx.b64                       [addr], 32;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj1], 512;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj2], 512; 9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx  mbarrier.complete_tx Perfoms complete-tx operation on the mbarrier object . Syntax mbarrier.complete_tx{.sem}{.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.complete_tx performs a complete-tx operation on the mbarrier object at the location specified by the address operand addr . The\n32-bit unsigned integer operand txCount specifies the completeCount argument to the complete-tx operation. mbarrier.complete_tx does not involve any asynchronous memory operations and only simulates the\ncompletion of an asynchronous memory operation and its side effect of signaling to the mbarrier\nobject . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr are as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . This operation does not provide any memory ordering semantics and thus is a relaxed operation. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples mbarrier.complete_tx.b64             [addr],     32;\nmbarrier.complete_tx.shared.b64      [mbarObj1], 512;\nmbarrier.complete_tx.relaxed.cta.b64 [addr2],    32; 9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive  mbarrier.arrive Performs arrive-on operation on the mbarrier object . Syntax mbarrier.arrive{.sem}{.scope}{.shared{::cta}}.b64           state, [addr]{, count};\nmbarrier.arrive{.sem}{.scope}{.shared::cluster}.b64         _, [addr] {,count}\nmbarrier.arrive.expect_tx{.sem}{.scope}{.shared{::cta}}.b64 state, [addr], txCount;\nmbarrier.arrive.expect_tx{.sem}{.scope}{.shared::cluster}.b64   _, [addr], txCount;\nmbarrier.arrive.noComplete{.sem}{.cta}{.shared{::cta}}.b64  state, [addr], count;\n\n.sem   = { .release }\n.scope = { .cta, .cluster } Description A thread executing mbarrier.arrive performs an arrive-on operation\non the mbarrier object at the location specified by the address operand addr . The 32-bit\nunsigned integer operand count specifies the count argument to the arrive-on operation. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . The optional qualifier .expect_tx specifies that an expect-tx operation is performed prior to the arrive-on operation. The 32-bit unsigned integer operand txCount specifies the expectCount argument to\nthe expect-tx operation. When both qualifiers .arrive and .expect_tx are specified, then\nthe count argument of the arrive-on operation is assumed to be 1. A mbarrier.arrive operation with .noComplete qualifier must not cause the mbarrier to\ncomplete its current phase, otherwise the behavior is undefined. The value of the operand count must be in the range as specified in Contents of the mbarrier\nobject . Note: for sm_8x , when the argument count is specified, the modifier .noComplete is\nrequired. mbarrier.arrive operation on an mbarrier object located in .shared::cta returns an opaque\n64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the\ndestination operand state. Contents of the state operand are implementation\nspecific. Optionally, sink symbol '_' can be used for the state argument. mbarrier.arrive operation on an mbarrier object located in .shared::cluster but not in .shared::cta cannot return a value. Sink symbol ‘_’ is mandatory for the destination operand for\nsuch cases. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory\nConsistency Model . If the .sem qualifier is absent, .release is assumed by default. The optional .scope qualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is not specified then it\ndefaults to .cta . In contrast, the .shared::<scope> indicates the state space where the\nmbarrier resides. PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Support for count argument without the modifier .noComplete introduced in PTX ISA version\n7.8. Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0. Support for qualifier .expect_tx is introduced in PTX ISA version 8.0. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes Requires sm_80 or higher. Support for count argument without the modifier .noComplete requires sm_90 or higher. Qualifier .expect_tx requires sm_90 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples .reg .b32 cnt, remoteAddr32, remoteCTAId, addr32;\n.reg .b64 %r<3>, addr, remoteAddr64;\n.shared .b64 shMem, shMem2;\n\ncvta.shared.u64            addr, shMem2;\nmov.b32                    addr32, shMem2;\nmapa.shared::cluster.u32   remoteAddr32, addr32, remoteCTAId;\nmapa.u64                   remoteAddr64, addr,   remoteCTAId;\n\ncvta.shared.u64          addr, shMem2;\n\nmbarrier.arrive.shared.b64                       %r0, [shMem];\nmbarrier.arrive.shared::cta.b64                  %r0, [shMem2];\nmbarrier.arrive.release.cta.shared::cluster.b64  _, [remoteAddr32];\nmbarrier.arrive.release.cluster.b64              _, [remoteAddr64], cnt;\nmbarrier.arrive.expect_tx.release.cluster.b64    _, [remoteAddr64], tx_count;\nmbarrier.arrive.noComplete.b64                   %r1, [addr], 2;\nmbarrier.arrive.b64                              %r2, [addr], cnt; 9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop  mbarrier.arrive_drop Decrements the expected count of the mbarrier object and performs arrive-on operation . Syntax mbarrier.arrive_drop{.sem}{.scope}{.shared{::cta}}.b64 state,           [addr]{, count};\nmbarrier.arrive_drop{.sem}{.scope}{.shared::cluster}.b64           _,   [addr] {,count};\nmbarrier.arrive_drop.expect_tx{.shared{::cta}}{.sem}{.scope}.b64 state, [addr], tx_count;\nmbarrier.arrive_drop.expect_tx{.shared::cluster}{.sem}{.scope}.b64   _, [addr], tx_count;\nmbarrier.arrive_drop.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state,  [addr], count;\n\n.sem   = { .release }\n.scope = { .cta, .cluster } Description A thread executing mbarrier.arrive_drop on the mbarrier object at the location specified by\nthe address operand addr performs the following steps: Decrements the expected arrival count of the mbarrier object by the value specified by the\n32-bit integer operand count . If count operand is not specified, it defaults to 1. Performs an arrive-on operation on the mbarrier object . The operand count specifies the count argument to the arrive-on\noperation . The decrement done in the expected arrivals count of the mbarrier object will be for all the\nsubsequent phases of the mbarrier object . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . The optional qualifier .expect_tx specifies that an expect-tx operation is performed prior to the arrive-on operation. The 32-bit unsigned integer operand txCount specifies the expectCount argument to\nthe expect-tx operation. When both qualifiers .arrive and .expect_tx are specified, then\nthe count argument of the arrive-on operation is assumed to be 1. mbarrier.arrive_drop operation forms the release pattern as described in the Memory\nConsistency Model and synchronizes with the acquire patterns. The optional .scope qualifier indicates the set of threads that an mbarrier.arrive_drop instruction can directly synchronize. If the .scope qualifier is not specified then it defaults\nto .cta . In contrast, the .shared::<scope> indicates the state space where the mbarrier\nresides. A mbarrier.arrive_drop with .noComplete qualifier must not complete the mbarrier, otherwise the behavior is undefined. The value of the operand count must be in the range as specified in Contents of the mbarrier\nobject . Note: for sm_8x , when the argument count is specified, the modifier .noComplete is\nrequired. A thread that wants to either exit or opt out of participating in the arrive-on operation can use mbarrier.arrive_drop to drop itself from the mbarrier . mbarrier.arrive_drop operation on an mbarrier object located in .shared::cta returns an\nopaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on\noperation in the destination operand state . Contents of the returned state are implementation\nspecific. Optionally, sink symbol '_' can be used for the state argument. mbarrier.arrive_drop operation on an mbarrier object located in .shared::cluster but not\nin .shared::cta cannot return a value. Sink symbol ‘_’ is mandatory for the destination operand\nfor such cases. PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Support for count argument without the modifier .noComplete introduced in PTX ISA version\n7.8. Support for qualifier .expect_tx is introduced in PTX ISA version 8.0. Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes Requires sm_80 or higher. Support for count argument without the modifier .noComplete requires sm_90 or higher. Qualifier .expect_tx requires sm_90 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples .reg .b32 cnt;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\n// Example 1\n@p mbarrier.arrive_drop.shared.b64 _, [shMem];\n@p exit;\n@p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a;\n@p2 exit;\n..\n@!p mbarrier.arrive.shared.b64   %r1, [shMem];\n@!p mbarrier.test_wait.shared.b64  q, [shMem], %r1;\n\n// Example 2\nmbarrier.arrive_drop.shared::cluster.b64 _, [addr];\nmbarrier.arrive_drop.shared::cta.release.cluster.b64     _, [addr], cnt;\n\n// Example 3\nmbarrier.arrive_drop.expect_tx.shared::cta.release.cta.b64 state, [addr], tx_count; 9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive  cp.async.mbarrier.arrive Makes the mbarrier object track all prior cp.async operations initiated by the\nexecuting thread. Syntax cp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr]; Description Causes an arrive-on operation to be\ntriggered by the system on the mbarrier object upon the completion of all prior cp.async operations initiated by the\nexecuting thread. The mbarrier object is at the location specified by the operand addr . The arrive-on operation is\nasynchronous to execution of cp.async.mbarrier.arrive . When .noinc modifier is not specified, the pending count of the mbarrier object is incremented\nby 1 prior to the asynchronous arrive-on operation . This\nresults in a zero-net change for the pending count from the asynchronous arrive-on operation\nduring the current phase. The pending count of the mbarrier object after the increment should not\nexceed the limit as mentioned in Contents of the mbarrier object . Otherwise,\nthe behavior is undefined. When the .noinc modifier is specified, the increment to the pending count of the mbarrier\nobject is not performed. Hence the decrement of the pending count done by the asynchronous arrive-on operation must be\naccounted for in the initialization of the mbarrier object . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Examples // Example 1: no .noinc\nmbarrier.init.shared.b64 [shMem], threadCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n....\n// Absence of .noinc accounts for arrive-on from completion of prior cp.async operations.\n// So mbarrier.init must only account for arrive-on from mbarrier.arrive.\ncp.async.mbarrier.arrive.shared.b64 [shMem];\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n\n\n\n// Example 2: with .noinc\n\n// Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.\n\n// All threads participating in the mbarrier perform cp.async\nmov.b32 copyOperationCnt, threadCount;\n\n// 3 arrive-on operations will be triggered per-thread\nmul.lo.u32 copyArrivalCnt, copyOperationCnt, 3;\n\nadd.u32 totalCount, threadCount, copyArrivalCnt;\n\nmbarrier.init.shared.b64 [shMem], totalCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n...\n// Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance\n....\ncp.async.ca.shared.global [shard3], [gbl3], 4;\ncp.async.ca.shared.global [shard4], [gbl4], 16;\ncp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance\n....\ncp.async.ca.shared.global [shard5], [gbl5], 4;\ncp.async.cg.shared.global [shard6], [gbl6], 16;\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop; 9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait  mbarrier.test_wait/mbarrier.try_wait Checks whether the mbarrier object has completed the phase. Syntax mbarrier.test_wait{.sem}{.scope}{.shared{::cta}}.b64        waitComplete, [addr], state;\nmbarrier.test_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity;\n\nmbarrier.try_wait{.sem}{.scope}{.shared{::cta}}.b64         waitComplete, [addr], state\n                                                               {, suspendTimeHint};\n\nmbarrier.try_wait.parity{.sem}{.scope}{.shared{::cta}}.b64  waitComplete, [addr], phaseParity\n                                                               {, suspendTimeHint};\n\n.sem   = { .acquire }\n.scope = { .cta, .cluster } Description The test_wait and try_wait operations test for the completion of the current or the immediately\npreceding phase of an mbarrier object at the location specified by the operand addr . mbarrier.test_wait is a non-blocking instruction which tests for the completion of the phase. mbarrier.try_wait is a potentially blocking instruction which tests for the completion of the\nphase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes\nexecution when the specified phase completes OR before the phase completes following a\nsystem-dependent time limit. The optional 32-bit unsigned integer operand suspendTimeHint specifies the time limit, in nanoseconds, that may be used for the time limit instead of the\nsystem-dependent limit. mbarrier.test_wait and mbarrier.try_wait test for completion of the phase : Specified by the operand state , which was returned by an mbarrier.arrive instruction on\nthe same mbarrier object during the current or the immediately preceding phase. Or Indicated by the operand phaseParity , which is the integer parity of either the current phase\nor the immediately preceding phase of the mbarrier object . The .parity variant of the instructions test for the completion of the phase indicated by the\noperand phaseParity , which is the integer parity of either the current phase or the immediately\npreceding phase of the mbarrier object . An even phase has integer parity 0 and an odd phase has\ninteger parity of 1. So the valid values of phaseParity operand are 0 and 1. Note: the use of the .parity variants of the instructions requires tracking the phase of an mbarrier object throughout its lifetime. The test_wait and try_wait operations are valid only for : the current incomplete phase, for which waitComplete returns False . the immediately preceding phase, for which waitComplete returns True . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . When mbarrier.test_wait and mbarrier.try_wait operations return True , they form the acquire pattern as described in the Memory Consistency Model . The optional .scope qualifier indicates the set of threads that the mbarrier.test_wait and mbarrier.try_wait instructions can directly synchronize. If the .scope qualifier is not\nspecified then it defaults to .cta . In contrast, the .shared::<scope> indicates the state\nspace where the mbarrier resides. The following ordering of memory operations hold for the executing thread when mbarrier.test_wait or mbarrier.try_wait returns True : All memory accesses (except async operations ) requested prior, in program\norder, to mbarrier.arrive during the completed phase by the participating threads of the CTA\nare performed and are visible to the executing thread. All cp.async operations\nrequested prior, in program order, to cp.async.mbarrier.arrive during the completed phase by\nthe participating threads of the CTA are performed and made visible to the executing thread. All cp.async.bulk asynchronous operations using the same mbarrier object requested prior,\nin program order, to mbarrier.arrive during the completed phase by the participating threads\nof the CTA are performed and made visible to the executing thread. All memory accesses requested after the mbarrier.test_wait or mbarrier.try_wait , in\nprogram order, are not performed and not visible to memory accesses performed prior to mbarrier.arrive , in program order, by other threads participating in the mbarrier . There is no ordering and visibility guarantee for memory accesses requested by the thread after mbarrier.arrive and prior to mbarrier.test_wait , in program order. PTX ISA Notes mbarrier.test_wait introduced in PTX ISA version 7.0. Modifier .parity is introduced in PTX ISA version 7.1. mbarrier.try_wait introduced in PTX ISA version 7.8. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes mbarrier.test_wait requires sm_80 or higher. mbarrier.try_wait requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples // Example 1a, thread synchronization with test_wait:\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.test_wait.shared.b64    complete, [shMem], %r1;\n@!complete nanosleep.u32 20;\n@!complete bra waitLoop;\n\n// Example 1b, thread synchronization with try_wait :\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.try_wait.shared.b64    complete, [shMem], %r1;\n@!complete bra waitLoop;\n\n\n// Example 2, thread synchronization using phase parity :\n\n.reg .b32 i, parArg;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmov.b32 i, 0;\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nloopStart :                           // One phase per loop iteration\n    ...\n    mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads\n    ...\n    and.b32 parArg, i, 1;\n    waitLoop:\n    mbarrier.test_wait.parity.shared.b64  complete, [shMem], parArg;\n    @!complete nanosleep.u32 20;\n    @!complete bra waitLoop;\n    ...\n    add.u32 i, i, 1;\n    setp.lt.u32 p, i, IterMax;\n@p bra loopStart;\n\n\n// Example 3, Asynchronous copy completion waiting :\n\n.reg .b64 state;\n.shared .b64 shMem2;\n.shared .b64 shard1, shard2;\n.global .b64 gbl1, gbl2;\n\nmbarrier.init.shared.b64 [shMem2], threadCount;\n...\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n\n// Absence of .noinc accounts for arrive-on from prior cp.async operation\ncp.async.mbarrier.arrive.shared.b64 [shMem2];\n...\nmbarrier.arrive.shared.b64 state, [shMem2];\n\nwaitLoop:\nmbarrier.test_wait.shared::cta.b64 p, [shMem2], state;\n@!p bra waitLoop;\n\n// Example 4, Synchronizing the CTA0 threads with cluster threads\n.reg .b64 %r1, addr, remAddr;\n.shared .b64 shMem;\n\ncvta.shared.u64          addr, shMem;\nmapa.u64                 remAddr, addr, 0;     // CTA0’s shMem instance\n\n// One thread from CTA0 executing the below initialization operation\n@p0 mbarrier.init.shared::cta.b64 [shMem], N;  // N = no of cluster threads\n\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Entire cluster executing the below arrive operation\nmbarrier.arrive.release.cluster.b64              _, [remAddr];\n\n// computation not requiring mbarrier synchronization ...\n\n// Only CTA0 threads executing the below wait operation\nwaitLoop:\nmbarrier.try_wait.parity.acquire.cluser.shared::cta.b64  complete, [shMem], 0;\n@!complete bra waitLoop; 9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count  mbarrier.pending_count Query the pending arrival count from the opaque mbarrier state. Syntax mbarrier.pending_count.b64 count, state; Description The pending count can be queried from the opaque mbarrier state using mbarrier.pending_count . The state operand is a 64-bit register that must be the result of a prior mbarrier.arrive.noComplete or mbarrier.arrive_drop.noComplete instruction. Otherwise, the\nbehavior is undefined. The destination register count is a 32-bit unsigned integer representing the pending count of\nthe mbarrier object prior to the arrive-on operation from\nwhich the state register was obtained. PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Examples .reg .b32 %r1;\n.reg .b64 state;\n.shared .b64 shMem;\n\nmbarrier.arrive.noComplete.b64 state, [shMem], 1;\nmbarrier.pending_count.b64 %r1, state; 9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy  tensormap.cp_fenceproxy A fused copy and fence operation. Syntax tensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned  [dst], [src], size;\n\n.cp_qualifiers    = { .global.shared::cta }\n.fence_qualifiers = { .to_proxy::from_proxy.release.scope }\n.to_proxy::from_proxy  = { .tensormap::generic }\n.scope            = { .cta, .cluster, .gpu , .sys } Description The tensormap.cp_fence instructions perform the following operations in order : Copies data of size specified by the size argument, in bytes, from the location specified\nby the address operand src in shared memory to the location specified by the address operand dst in the global memory, in the generic proxy. Establishes a uni-directional proxy release pattern on the ordering from the copy operation\nto the subsequent access performed in the tensormap proxy on the address dst . The valid value of size operand is 128. The operands src and dst specify non-generic addresses in shared::cta and global state space respectively. The optional .scope qualifier specifies the set of threads that can directly observe the proxy\nsynchronizing effect of this operation, as described in Memory Consistency Model . The mandatory .sync qualifier indicates that tensormap.cp_fenceproxy causes the executing\nthread to wait until all threads in the warp execute the same tensormap.cp_fenceproxy instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tensormap.cp_fenceproxy instruction. In conditionally executed code, an aligned tensormap.cp_fenceproxy instruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_90 or higher. Examples // Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor\n\n.reg .b64 new_addr;\n.global .align 128 .b8 gbl[128];\n.shared .align 128 .b8 sMem[128];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar];\n...\ntry_wait_loop:\nmbarrier.try_wait.shared.b64 p, [mbar], state;\n@!p bra try_wait loop;\n\ntensormap.replace.tile.global_address.shared.b1024.b64   [sMem], new_addr;\ntensormap.cp_fenceproxy.global.shared::cta.proxy.tensormap::generic.release.gpu\n                                    .sync.aligned        [gbl], [sMem], 128;\nfence.proxy.tensormap::generic.acquire.gpu [gbl], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [gbl, {tc0}], [mbar0]; 9.7.13. Warp Level Matrix Multiply-Accumulate Instructions  The matrix multiply and accumulate operation has the following form: D = A * B + C where D and C are called accumulators and may refer to the same matrix. PTX provides two ways to perform matrix multiply-and-accumulate computation: Using wmma instructions: This warp-level computation is performed collectively by all threads in the warp as follows: Load matrices A, B and C from memory into registers using the wmma.load operation. When\nthe operation completes, the destination registers in each thread hold a fragment of the\nloaded matrix. Perform the matrix multiply and accumulate operation using the wmma.mma operation on the\nloaded matrices. When the operation completes, the destination registers in each thread hold\na fragment of the result matrix returned by the wmma.mma operation. Store result Matrix D back to memory using the wmma.store operation. Alternately, result\nmatrix D can also be used as argument C for a subsequent wmma.mma operation. The wmma.load and wmma.store instructions implicitly handle the organization of matrix\nelements when loading the input matrices from memory for the wmma.mma operation and when\nstoring the result back to memory. Using mma instruction: Similar to wmma , mma also requires computation to be performed collectively by all\nthreads in the warp however distribution of matrix elements across different threads in warp\nneeds to be done explicitly before invoking the mma operation. The mma instruction\nsupports both dense as well as sparse matrix A. The sparse variant can be used when A is a\nstructured sparse matrix as described in Sparse matrix storage . 9.7.13.1. Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices. The following matrix shapes are supported for the specified types: Instruction Sparsity Multiplicand Data-type Shape PTX ISA version wmma Dense Floating-point - .f16 .m16n16k16 , .m8n32k16 ,\nand .m32n8k16 PTX ISA version 6.0 wmma Dense Alternate floating-point format - .bf16 .m16n16k16 , .m8n32k16 ,\nand .m32n8k16 PTX ISA version 7.0 wmma Dense Alternate floating-point format - .tf32 .m16n16k8 PTX ISA version 7.0 wmma Dense Integer - .u8 / .s8 .m16n16k16 , .m8n32k16 ,\nand .m32n8k16 PTX ISA version 6.3 wmma Dense Sub-byte integer - .u4 / .s4 .m8n8k32 PTX ISA version 6.3\n(preview feature) wmma Dense Single-bit - .b1 .m8n8k128 PTX ISA version 6.3\n(preview feature) mma Dense Floating-point - .f64 .m8n8k4 PTX ISA version 7.0 .m16n8k4 , .m16n8k8 ,\nand .m16n8k16 PTX ISA version 7.8 mma Dense Floating-point - .f16 .m8n8k4 PTX ISA version 6.4 .m16n8k8 PTX ISA version 6.5 .m16n8k16 PTX ISA version 7.0 mma Dense Alternate floating-point format - .bf16 .m16n8k8 and .m16n8k16 PTX ISA version 7.0 mma Dense Alternate floating-point format - .tf32 .m16n8k4 and .m16n8k8 PTX ISA version 7.0 mma Dense Integer - .u8 / .s8 .m8n8k16 PTX ISA version 6.5 .m16n8k16 and .m16n8k32 PTX ISA version 7.0 mma Dense Sub-byte integer - .u4 / .s4 .m8n8k32 PTX ISA version 6.5 .m16n8k32 and .m16n8k64 PTX ISA version 7.0 mma Dense Single-bit - .b1 .m8n8k128 , .m16n8k128 ,\nand .m16n8k256 PTX ISA version 7.0 mma Dense Alternate floating-point format - .e4m3 / .e5m2 .m16n8k32 PTX ISA version 8.4 mma Sparse Floating-point - .f16 .m16n8k16 and .m16n8k32 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .bf16 .m16n8k16 and .m16n8k32 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .tf32 .m16n8k8 and .m16n8k16 PTX ISA version 7.1 mma Sparse Integer - .u8 / .s8 .m16n8k32 and .m16n8k64 PTX ISA version 7.1 mma Sparse Sub-byte integer - .u4 / .s4 .m16n8k64 and .m16n8k128 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .e4m3 / .e5m2 .m16n8k64 PTX ISA version 8.4 mma Sparse\nwith\nordered\nmetadata Floating-point - .f16 .m16n8k16 and .m16n8k32 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Alternate floating-point format - .bf16 .m16n8k16 and .m16n8k32 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Alternate floating-point format - .tf32 .m16n8k8 and .m16n8k16 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Integer - .u8 / .s8 .m16n8k32 and .m16n8k64 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Sub-byte integer - .u4 / .s4 .m16n8k64 and .m16n8k128 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Alternate floating-point format - .e4m3 / .e5m2 .m16n8k64 PTX ISA version 8.5 9.7.13.2. Matrix Data-types  The matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point. For floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later. Data-type Multiplicands (A or B) Accumulators (C or D) Integer .u8 , .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 or .e5m2 .f32 Floating Point .f64 .f64 Sub-byte integer both .u4 or both .s4 .s32 Single-bit integer .b1 .s32 9.7.13.3. Matrix multiply-accumulate operation using wmma instructions  This section describes warp level wmma.load, wmma.mma and wmma.store instructions and the\norganization of various matrices invovled in these instruction. 9.7.13.3.1. Matrix Fragments for WMMA  Each thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the\nthreads in a warp is unspecified and is target architecture dependent, and hence the identity of the\nfragment within the matrix is also unspecified and is target architecture dependent. The fragment\nreturned by a wmma operation can be used as an operand for another wmma operation if the\nshape, layout and element type of the underlying matrix matches. Since fragment layout is\narchitecture dependent, using the fragment returned by a wmma operation in one function as an\noperand for a wmma operation in a different function may not work as expected if the two\nfunctions are linked together but were compiled for different link-compatible SM architectures. Note\npassing wmma fragment to a function having .weak linkage is unsafe since at link time\nreferences to such function may get resolved to a function in different compilation module. Each fragment is a vector expression whose contents are determined as follows. The identity of\nindividual matrix elements in the fragment is unspecified. Integer fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .u8 or .s8 .m16n16k16 A A vector expression of two .b32 registers, with each\nregister containing four elements from the matrix. B A vector expression of two .b32 registers, with each\nregister containing four elements from the matrix. .m8n32k16 A A vector expression containing a single .b32 register\ncontaining four elements from the matrix. B A vector expression of four .b32 registers, with each\nregister containing four elements from the matrix. .m32n8k16 A A vector expression of four .b32 registers, with each\nregister containing four elements from the matrix. B A vector expression containing single .b32 register,\nwith each containing four elements from the matrix. Accumulators (C or D): Data-type Shape Fragment .s32 .m16n16k16 A vector expression of eight .s32 registers. .m8n32k16 .m32n8k16 Floating point fragments Data-type Matrix Fragment .f16 A or B A vector expression of eight .f16x2 registers. .f16 C or D A vector expression of four .f16x2 registers. .f32 A vector expression of eight .f32 registers. Floating point fragments for .bf16 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .bf16 .m16n16k16 A A vector expression of four .b32 registers, with each\nregister containing two elements from the matrix. B .m8n32k16 A A vector expression containing a two .b32 registers,\nwith containing two elements from the matrix. B A vector expression of eight .b32 registers, with\neach register containing two elements from the matrix. .m32n8k16 A A vector expression of eight .b32 registers, with\neach register containing two elements from the matrix. B A vector expression containing two .b32 registers,\nwith each containing two elements from the matrix. Accumulators (C or D): Data-type Matrix Fragment .f32 C or D A vector expression containing eight .f32 registers. Floating point fragments for .tf32 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .tf32 .m16n16k8 A A vector expression of four .b32 registers. B A vector expression of four .b32 registers. Accumulators (C or D): Data-type Shape Matrix Fragment .f32 .m16n16k8 C or D A vector expression containing eight .f32 registers. Double precision floating point fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .f64 .m8n8k4 A or B A vector expression of single .f64 register. Accumulators (C or D): Data-type Shape Matrix Fragment .f64 .m8n8k4 C or D A vector expression containing single .f64 register. Sub-byte integer and single-bit fragments Multiplicands (A or B): Data-type Shape Fragment .u4 or .s4 .m8n8k32 A vector expression containing a single .b32 register, containing eight elements from the matrix. .b1 .m8n8k128 A vector expression containing a single .b32 register, containing 32 elements from the matrix. Accumulators (C or D): Data-type Shape Fragment .s32 .m8n8k32 A vector expression of two .s32 registers. .m8n8k128 A vector expression of two .s32 registers. Manipulating fragment contents The contents of a matrix fragment can be manipulated by reading and writing to individual\nregisters in the fragment, provided the following conditions are satisfied: All matrix element in the fragment are operated on uniformly across threads, using the same\nparameters. The order of the matrix elements is not changed. For example, if each register corresponding to a given matrix is multiplied by a uniform constant\nvalue, then the resulting matrix is simply the scaled version of the original matrix. Note that type conversion between .f16 and .f32 accumulator fragments is not supported in\neither direction. The result is undefined even if the order of elements in the fragment remains\nunchanged. 9.7.13.3.2. Matrix Storage for WMMA  Each matrix can be stored in memory with a row-major or column-major layout. In a row-major format, consecutive elements of each row are stored in contiguous memory locations, and the row is\ncalled the leading dimension of the matrix. In a column-major format, consecutive elements of\neach column are stored in contiguous memory locations and the column is called the leading\ndimension of the matrix. Consecutive instances of the leading dimension (rows or columns) need not be stored contiguously\nin memory. The wmma.load and wmma.store operations accept an optional argument stride that specifies the offset from the beginning of each row (or column) to the next, in terms of matrix\nelements (and not bytes). For example, the matrix being accessed by a wmma operation may be a\nsubmatrix from a larger matrix stored in memory. This allows the programmer to compose a\nmultiply-and-accumulate operation on matrices that are larger than the shapes supported by the wmma operation. Address Alignment: The starting address of each instance of the leading dimension (row or column) must be aligned\nwith the size of the corresponding fragment in bytes. Note that the starting address is\ndetermined by the base pointer and the optional stride . Consider the following instruction as an example: wmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s; Fragment size in bytes = 32 (eight elements of type .f16x2 ) Actual stride in bytes = 2 * s (since stride is specified in terms of .f16 elements, not bytes) For each row of this matrix to be aligned at fragment size the following must be true: p is a multiple of 32. 2*s is a multiple of 32. Default value for stride: The default value of the stride is the size of the leading dimension of the matrix. For\nexample, for an MxK matrix, the stride is K for a row-major layout and M for a column-major layout. In particular, the default strides for the supported matrix shapes are as\nfollows: Shape A (row) A (column) B (row) B (column) Accumulator (row) Accumulator (column) 16x16x16 16 16 16 16 16 16 8x32x16 16 8 32 16 32 8 32x8x16 16 32 8 16 8 32 8x8x32 32 8 8 32 8 8 8x8x128 128 8 8 128 8 8 16x16x8 8 16 16 8 16 16 8x8x4 4 8 8 4 8 8 9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load  wmma.load Collectively load a matrix from memory for WMMA Syntax Floating point format .f16 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride};\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride};\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f16, .s8, .u8};\n.btype  = {.f16, .s8, .u8};\n.ctype  = {.f16, .f32, .s32}; Alternate floating point format .bf16 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.bf16 };\n.btype  = {.bf16 };\n.ctype  = {.f32 }; Alternate floating point format .tf32 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.tf32 };\n.btype  = {.tf32 };\n.ctype  = {.f32 }; Double precision Floating point .f64 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f64 };\n.btype  = {.f64 };\n.ctype  = {.f64 }; Sub-byte loads: wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\n.ctype  = {.s32}; Single-bit loads: wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.b1};\n.btype  = {.b1};\n.ctype  = {.s32}; Description Collectively load a matrix across all threads in a warp from the location indicated by address\noperand p in the specified state space into destination register r . If no state space is given, perform the memory accesses using Generic Addressing . wmma.load operation may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. The mutually exclusive qualifiers .a , .b and .c indicate whether matrix A, B or C is\nbeing loaded respectively for the wmma computation. The destination operand r is a brace-enclosed vector expression that can hold the fragment\nreturned by the load operation, as described in Matrix Fragments for WMMA . The .shape qualifier indicates the dimensions of all the matrix arguments involved in the\nintended wmma computation. The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or column-major format. stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the leading dimension (rows or columns). The default\nvalue of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior. The required alignment for address p and stride is described in the Matrix Storage for WMMA . The mandatory .sync qualifier indicates that wmma.load causes the executing thread to wait\nuntil all threads in the warp execute the same wmma.load instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.load instruction. In conditionally executed code, a wmma.load instruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined. The behavior of wmma.load is undefined if all threads do not use the same qualifiers and the\nsame values of p and stride , or if any thread in the warp has exited. wmma.load is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.0. .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. .m8n8k4 and .m16n16k8 on wmma introduced in PTX ISA version 7.0. Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. All\ndetails are subject to change with no guarantees of backward compatibility on future PTX ISA\nversions or SM architectures. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher. Double precision and alternate floating point precision wmma requires sm_80 or higher. Examples // Load elements from f16 row-major matrix B\n.reg .b32 x<8>;\n\nwmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from f32 column-major matrix C and scale the values:\n.reg .b32 x<8>;\n\nwmma.load.c.sync.aligned.m16n16k16.col.f32\n                 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr];\n\nmul.f32 x0, x0, 0.1;\n// repeat for all registers x<8>;\n...\nmul.f32 x7, x7, 0.1;\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from integer matrix A:\n.reg .b32 x<4>\n// destination registers x<4> contain four packed .u8 values each\nwmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];\n\n// Load elements from sub-byte integer matrix A:\n.reg .b32 x0;\n// destination register x0 contains eight packed .s4 values\nwmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];\n\n// Load elements from .bf16 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k16.row.bf16\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .tf32 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k8.row.tf32\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .f64 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m8n8k4.row.f64\n                {x0}, [ptr]; 9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store  wmma.store Collectively store a matrix into memory for WMMA Syntax wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f16, .f32, .s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32, .m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.type   = {.s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.type   = {.f64}; Description Collectively store a matrix across all threads in a warp at the location indicated by address\noperand p in the specified state space from source register r . If no state space is given, perform the memory accesses using Generic Addressing . wmma.load operation may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. The source operand r is a brace-enclosed vector expression that matches the shape of the\nfragment expected by the store operation, as described in Matrix Fragments for WMMA . The .shape qualifier indicates the dimensions of all the matrix arguments involved in the\nintended wmma computation. It must match the .shape qualifier specified on the wmma.mma instruction that produced the D matrix being stored. The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or column-major format. stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the leading dimension (rows or columns). The default\nvalue of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior. The required alignment for address p and stride is described in the Matrix Storage for WMMA . The mandatory .sync qualifier indicates that wmma.store causes the executing thread to wait\nuntil all threads in the warp execute the same wmma.store instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.store instruction. In conditionally executed code, a wmma.store instruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined. The behavior of wmma.store is undefined if all threads do not use the same qualifiers and the\nsame values of p and stride , or if any thread in the warp has exited. wmma.store is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.0. .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. .m16n16k8 introduced in PTX ISA version 7.0. Double precision wmma introduced in PTX ISA version 7.0. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. All\ndetails are subject to change with no guarantees of backward compatibility on future PTX ISA\nversions or SM architectures. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher. Double precision wmma and shape .m16n16k8 requires sm_80 or higher. Examples // Storing f32 elements computed by a wmma.mma\n.reg .b32 x<8>;\n\nwmma.mma.sync.m16n16k16.row.col.f32.f32\n              {d0, d1, d2, d3, d4, d5, d6, d7}, ...;\nwmma.store.d.sync.m16n16k16.row.f32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m16n16k16 shape:\n.reg .b32 d<8>;\nwmma.store.d.sync.aligned.m16n16k16.row.s32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m8n8k128 shape:\n.reg .b32 d<2>\nwmma.store.d.sync.aligned.m8n8k128.row.s32\n[ptr], {d0, d1};\n\n// Store f64 accumulator for m8n8k4 shape:\n.reg .f64 d<2>;\nwmma.store.d.sync.aligned.m8n8k4.row.f64\n              [ptr], {d0, d1}; 9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma  wmma.mma Perform a single matrix multiply-and-accumulate operation across a warp Syntax // Floating point (.f16 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c;\n\n// Integer (.u8/.s8 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape  =  {.m16n16k16, .m8n32k16, .m32n8k16};\n.dtype   = {.f16, .f32};\n.atype   = {.s8, .u8};\n.btype   = {.s8, .u8};\n.ctype   = {.f16, .f32}; Floating point format .bf16 wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k16, .m8n32k16, .m32n8k16};\n.atype   = {.bf16 };\n.btype   = {.bf16}; Floating point format .tf32 wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k8 };\n.atype   = {.tf32 };\n.btype   = {.tf32}; Floating point Double precision wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m8n8k4 };\n.rnd = { .rn, .rz, .rm, .rp }; Sub-byte ( .u4 / .s4 multiplicands) wmma.mma : wmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n.shape  = {.m8n8k32};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4}; Single-bit ( .b1 multiplicands) wmma.mma : wmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c;\n.shape  = {.m8n8k128};\n.atype  = {.b1};\n.btype  = {.b1};\n.op     = {.xor, .and} Description Perform a warp-level matrix multiply-and-accumulate computation D = A * B + C using matrices A,\nB and C loaded in registers a , b and c respectively, and store the result matrix in\nregister d . The register arguments a , b , c and d hold unspecified fragments of\nthe corresponding matrices as described in Matrix Fragments for WMMA The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the\nelements in the matrices D, A, B and C respectively. For wmma.mma without explicit .atype and .btype : .atype and .btype are\nimplicitly set to .f16 . For integer wmma , .ctype and .dtype must be specified as .s32 . Also, the values for .atype and .btype must be the same, i.e., either both are .s8 or both are .u8 . For sub-byte single-bit wmma , .ctype and .dtype must be specified as .s32 . Also, the\nvalues for .atype and .btype must be the same; i.e., either both are .s4 , both are .u4 , or both are .b1 . For single-bit wmma , multiplication is replaced by a sequence of logical operations;\nspecifically, wmma.xor.popc and wmma.and.popc computes the XOR, AND respectively of a\n128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result\n( popc ). This result is added to the corresponding element of C and written into D. The qualifiers .alayout and .blayout must match the layout specified on the wmma.load instructions that produce the contents of operands a and b respectively. Similarly, the\nqualifiers .atype , .btype and .ctype must match the corresponding qualifiers on the wmma.load instructions that produce the contents of operands a , b and c respectively. The .shape qualifier must match the .shape qualifier used on the wmma.load instructions\nthat produce the contents of all three input operands a , b and c respectively. The destination operand d is a brace-enclosed vector expression that matches the .shape of\nthe fragment computed by the wmma.mma instruction. Saturation at the output: The optional qualifier .satfinite indicates that the final values in the destination register\nare saturated as follows: The output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the\naccumulation would overflow, the value wraps. Precision and rounding for .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .ctype or .dtype is .f32 , accumulation of the intermediate values is performed with\nat least single precision. When both .ctype and .dtype are specified as .f16 , the\naccumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs is unspecified. Precision and rounding for .bf16 , .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision. The accumulation order, rounding and handling of subnormal inputs is unspecified. Rounding modifiers on double precision wmma.mma (default is .rn ): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The mandatory .sync qualifier indicates that wmma.mma causes the executing thread to wait\nuntil all threads in the warp execute the same wmma.mma instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.mma instruction. In conditionally executed code, a wmma.mma instruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined. The behavior of wmma.mma is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited. PTX ISA Notes Introduced in PTX ISA version 6.0. .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0. Support for .and operation in single-bit wmma introduced in PTX ISA version 7.1. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3. Support for .satfinite on floating point wmma.mma is deprecated in PTX ISA version 6.4 and\nis removed from PTX ISA version 6.5. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA. All details are\nsubject to change with no guarantees of backward compatibility on future PTX ISA versions or SM\narchitectures. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher. Double precision, alternate floating point precision wmma require sm_80 or higher. .and operation in single-bit wmma requires sm_80 or higher. Examples .global .align 32 .f16 A[256], B[256];\n.global .align 32 .f32 C[256], D[256];\n.reg .b32 a<8> b<8> c<8> d<8>;\n\nwmma.load.a.sync.aligned.m16n16k16.global.row.f16\n        {a0, a1, a2, a3, a4, a5, a6, a7}, [A];\nwmma.load.b.sync.aligned.m16n16k16.global.col.f16\n        {b0, b1, b2, b3, b4, b5, b6, b7}, [B];\n\nwmma.load.c.sync.aligned.m16n16k16.global.row.f32\n        {c0, c1, c2, c3, c4, c5, c6, c7}, [C];\n\nwmma.mma.sync.aligned.m16n16k16.row.col.f32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3, a4, a5, a6, a7},\n        {b0, b1, b2, b3, b4, b5, b6, b7},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\nwmma.store.d.sync.aligned.m16n16k16.global.col.f32\n        [D], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Compute an integer WMMA:\n.reg .b32  a, b<4>;\n.reg .b32 c<8>, d<8>;\nwmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a}, {b0, b1, b2,  b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\n// Compute sub-byte WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute single-bit type WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute double precision wmma\n.reg .f64 a, b, c<2>, d<2>;\nwmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute alternate floating point precision wmma\n.reg .b32 a<2>, b<2>, c<8>, d<8>;\nwmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3}, {b0, b1, b2, b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7}; 9.7.13.4. Matrix multiply-accumulate operation using mma instruction  This section describes warp-level mma , ldmatrix , stmatrix , and movmatrix instructions and the organization of various matrices involved in these instructions. 9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type  A warp executing mma.m8n8k4 with .f16 floating point type will compute 4 MMA operations of shape .m8n8k4 . Elements of 4 matrices need to be distributed across the threads in a warp. The following table\nshows distribution of matrices for MMA operations. MMA Computation Threads participating in MMA computation MMA computation 1 Threads with %laneid 0-3 (low group) and 16-19 (high group) MMA computation 2 Threads with %laneid 4-7 (low group) and 20-23 (high group) MMA computation 3 Threads with %laneid 8-11 (low group) and 24-27 (high group) MMA computation 4 Threads with %laneid 12-15 (low group) and 28-31 (high group) For each of the individual MMA computation shown above, each of the required thread holds a fragment\nof the matrix for performing mma operation as follows: Multiplicand A: .atype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers,\nwith each register containing two .f16 elements from\nthe matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix A is shown in Figure 21 . Figure 21 MMA .m8n8k4 fragment layout for row-major matrix A with .f16 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid < 16 ( % laneid % 4 ) + 4 otherwise col = i for ai where i = { 0 ,.., 3 } Fragment layout for Column Major matrix A is shown in Figure 22 . The layout of the fragments held by different threads is shown below: Figure 22 MMA .m8n8k4 fragment layout for column-major matrix A with .f16 type  The row and column of a matrix fragment can be computed as: row = i % 4 for ai where i = { 0 ,.., 3 } if % laneid < 16 ( i % 4 ) + 4 for ai where i = { 0 ,.., 3 } otherwise col = % laneid % 4 Multiplicand B: .btype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix B is shown in Figure 23 . Figure 23 MMA .m8n8k4 fragment layout for row-major matrix B with .f16 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = i for bi where i = { 0 ,.., 3 } if % laneid < 16 i + 4 for bi where i = { 0 ,.., 3 } otherwise Fragment layout for Column Major matrix B is shown in Figure 24 . Figure 24 MMA .m8n8k4 fragment layout for column-major matrix B with .f16 type  The row and column of a matrix fragment can be computed as: row = i for bi where i = { 0 ,.., 3 } col = % laneid % 4 if % laneid < 16 ( % laneid % 4 ) + 4 otherwise Accumulators C (or D): .ctype / .dtype Fragment Elements (low to high) .f16 A vector expression containing four .f16x2 registers, with\neach register containing two .f16 elements from the matrix\nC (or D). c0, c1, c2, c3, c4, c5, c6, c7 .f32 A vector expression of eight .f32 registers. The layout of the fragments held by different threads is shown below: Fragment layout for accumulator matrix when .ctype is .f16 is shown in Figure 25 . Figure 25 MMA .m8n8k4 fragment layout for matrix C/D with .ctype = .f16  The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid < 16 ( % laneid % 4 ) + 4 otherwise col = i for ci where i = { 0 ,.., 7 } Fragment layout for accumulator matrix when .ctype is .f32 is shown in Figure 26 and Figure 27 . Figure 26 MMA .m8n8k4 computation 1 and 2 fragment layout for matrix C/D with .ctype = .f32  Figure 27 MMA .m8n8k4 computation 3 and 4 fragment layout for matrix C/D with .ctype = .f32  The row and column of a matrix fragment can be computed as: row = X if % laneid < 16 X + 4 otherwise where X = ( % laneid & 0b1 ) + ( i & 0b10 ) for ci where i = { 0 ,.., 7 } col = ( i & 0b100 ) + ( % laneid & 0b10 ) + ( i & 0b1 ) for ci where i = { 0 ,.., 7 } 9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point type  A warp executing mma.m8n8k4 with .f64 floating point type will compute an MMA operation of\nshape .m8n8k4 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .f64 A vector expression containing a single .f64 register, containing\nsingle .f64 element from the matrix A. a0 The layout of the fragments held by different threads is shown in Figure 28 . Figure 28 MMA .m8n8k4 fragment layout for matrix A  with .f64 type  The row and column of a matrix fragment can be computed as: row = % laneid >> 2 col = % laneid % 4 Multiplicand B: .btype Fragment Elements (low to high) .f64 A vector expression containing a single .f64 register, containing a single .f64 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 29 . Figure 29 MMA .m8n8k4 fragment layout for matrix B  with .f64 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = % laneid >> 2 Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing of two .f64 registers containing two .f64 elements from the matrix C. c0, c1 The layout of the fragments held by different threads is shown in Figure 30 . Figure 30 MMA .m8n8k4 fragment layout for accumulator matrix C/D  with .f64 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 } 9.7.13.4.3. Matrix Fragments for mma.m8n8k16  A warp executing mma.m8n8k16 will compute an MMA operation of shape .m8n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing\nfour .s8 or .u8 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 31 . Figure 31 MMA .m8n8k16 fragment layout for matrix A with .u8 / .s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 4 ) + i for ai where i = { 0 ,.., 3 } Multiplicand B: .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing\nfour .s8 or .u8 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 32 . Figure 32 MMA .m8n8k16 fragment layout for matrix B with .u8 / .s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing of two .s32 registers. c0, c1 The layout of the fragments held by different threads is shown in Figure 33 . Figure 33 MMA .m8n8k16 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.4. Matrix Fragments for mma.m8n8k32  A warp executing mma.m8n8k32 will compute an MMA operation of shape .m8n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing\neight .s4 or .u4 elements from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 34 . Figure 34 MMA .m8n8k32 fragment layout for matrix A with .u4 / .s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 8 ) + i for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing\neight .s4 or .u4 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 35 . Figure 35 MMA .m8n8k32 fragment layout for matrix B with .u4 / .s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + i for bi where i = { 0 ,.., 7 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression of two .s32 registers. c0, c1 The layout of the fragments held by different threads is shown in Figure 36 : Figure 36 MMA .m8n8k32 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.5. Matrix Fragments for mma.m8n8k128  A warp executing mma.m8n8k128 will compute an MMA operation of shape .m8n8k128 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty\ntwo .b1 elements from the matrix A. a0, a1, … a30, a31 The layout of the fragments held by different threads is shown in Figure 37 . Figure 37 MMA .m8n8k128 fragment layout for matrix A with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 32 ) + i for ai where i = { 0 ,.., 31 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty\ntwo .b1 elements from the matrix B. b0, b1, …, b30, b31 The layout of the fragments held by different threads is shown in Figure 38 . Figure 38 MMA .m8n8k128 fragment layout for matrix B with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,.., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing two .s32 registers, containing two .s32 elements from the matrix C (or D). c0, c1 The layout of the fragments held by different threads is shown in Figure 39 . Figure 39 MMA .m8n8k128 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.6. Matrix Fragments for mma.m16n8k4  A warp executing mma.m16n8k4 will compute an MMA operation of shape .m16n8k4 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix A. a0, a1 The layout of the fragments held by different threads is shown in Figure 40 . Figure 40 MMA .m16n8k4 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix A. a0, a1 The layout of the fragments held by different threads is shown in Figure 41 . Figure 41 MMA .m16n8k4 fragment layout for matrix A with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group Multiplicand B: .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression of a single .b32 register, containing a single .tf32 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 42 . Figure 42 MMA .m16n8k4 fragment layout for matrix B with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression of a single .f64 register, containing a single .f64 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 43 . Figure 43 MMA .m16n8k4 fragment layout for matrix B with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID Accumulators (C or D): .tf32 : .ctype / .dtype Fragment Elements (low to high) .f32 A vector expression containing four .f32 registers, containing four .f32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 44 . Figure 44 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 45 . Figure 45 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.7. Matrix Fragments for mma.m16n8k8  A warp executing mma.m16n8k8 will compute an MMA operation of shape .m16n8k8 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 / .bf16 elements from the\nmatrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 46 . Figure 46 MMA .m16n8k8 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = threadID_in_group * 2 + ( i & 0x1 ) for ai where i = { 0 ,.., 3 } .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, containing four .tf32 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 47 . Figure 47 MMA .m16n8k8 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 48 . Figure 48 MMA .m16n8k8 fragment layout for matrix A with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing a single .f16x2 register, containing\ntwo .f16 / .bf16 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 49 . Figure 49 MMA .m16n8k8 fragment layout for matrix B with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + i for bi where i = { 0 , 1 } col = groupID .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 50 . Figure 50 MMA .m16n8k8 fragment layout for matrix B with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 51 . Figure 51 MMA .m16n8k8 fragment layout for matrix B with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID Accumulators (C or D): .f16 , .bf16 and .tf32 : .ctype / .dtype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression of four .f32 registers. The layout of the fragments held by different threads is shown in Figure 52 . Figure 52 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f16x2 / .f32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression of four .f64 registers containing four .f64 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 53 . Figure 53 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type  A warp executing mma.m16n8k16 floating point types will compute an MMA operation of shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with\neach register containing two .f16 / .bf16 elements\nfrom the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 54 . Figure 54 MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 2 || 4 <= i < 6 groupID + 8 Otherwise col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ai where i < 4 ( threadID_in_group * 2 ) + ( i & 0x1 ) + 8 for ai where i >= 4 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing eight .f64 registers, with each\nregister containing one .f64 element from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 55 . Figure 55 MMA .m16n8k16 fragment layout for matrix A with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i % 2 = 0 groupID + 8 Otherwise col = ( i * 2 ) + threadID_in_group for ai where i % 2 = 0 ( i * 2 ) - 2 + ( threadID_in_group Otherwise Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with\neach register containing two .f16 / .bf16 elements\nfrom the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 56 . Figure 56 MMA .m16n8k16 fragment layout for matrix B with .f16 / .bf16 type.  where the row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + ( i & 0x1 ) for bi where i < 2 ( threadID_in_group * 2 ) + ( i & 0x1 ) + 8 for bi where i >= 2 col = groupID .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, with each\nregister containing one .f64 element from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 57 . Figure 57 MMA .m16n8k16 fragment layout for matrix B with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group + ( i * 4 ) for bi where i < 4 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers containing .f64 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression containing four .f32 registers containing\nfour .f32 elements from the matrix C (or D). .f16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 elements from the matrix C (or D). The layout of the fragments held by different threads is shown in Figure 58 . Figure 58 MMA .m16n8k16 fragment layout for accumulator matrix matrix C/D.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type  A warp executing mma.m16n8k16 with .u8 or .s8 integer type will compute an MMA operation\nof shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .u8 / .s8 A vector expression containing two .b32 registers, with each\nregister containing four .u8 / .s8 elements from the\nmatrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 59 . Figure 59 MMA .m16n8k16 fragment layout for matrix A with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i < 4 groupID + 8 for ai where i >= 4 col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing a single .b32 register, containing\nfour .u8 / .s8 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 60 . Figure 60 MMA .m16n8k16 fragment layout for matrix B with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 61 . Figure 61 MMA .m16n8k16 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.10. Matrix Fragments for mma.m16n8k32  A warp executing mma.m16n8k32 will compute an MMA operation of shape .m16n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .s4 or .u4 : .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each\nregister containing eight .u4 / .s4 elements from the\nmatrix A. a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 62 . Figure 62 MMA .m16n8k32 fragment layout for matrix A with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i < 8 groupID + 8 for ai where i >= 8 col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i = { 0 ,.., 15 } .s8 or .u8 or .e4m3 or .e5m2 : .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each\nregister containing four .s8 / .u8 elements from the\nmatrix A. a0, a1, .., a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each\nregister containing four .e4m3 / .e5m2 elements from\nthe matrix A. a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 63 . Figure 63 MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 4 || 8 <= i < 12 groupID + 8 otherwise col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i < 8 ( threadID_in_group * 4 ) + ( i & 0x3 ) + 16 for ai where i >= 8 Multiplicand B: .s4 or .u4 : .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register,\ncontaining eight .s4 / .u4 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 64 . Figure 64 MMA .m16n8k32 fragment layout for matrix B with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = { 0 ,.., 7 } col = groupID .s8 or .u8 or .e4m3 or .e5m2 : .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing two .b32 registers, with each\nregister containing four .s8 / .u8 elements from the\nmatrix B. b0, b1, b2, b3, b4, b5, b6, b7 .e4m3 / .e5m2 A vector expression containing two .b32 registers, with each\nregister containing four .e4m3 / .e5m2 elements from the\nmatrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 65 and Figure 66 . Figure 65 MMA .m16n8k32 fragment layout for rows 0–15 of matrix B with .u8 / .s8 type.  Figure 66 MMA .m16n8k32 fragment layout for rows 16–31 of matrix B with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + ( i & 0x3 ) for bi where i < 4 ( threadID_in_group * 4 ) + ( i & 0x3 ) + 16 for bi where i >= 4 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression containing four .f32 registers, containing\nfour .f32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 67 . Figure 67 MMA .m16n8k32 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.11. Matrix Fragments for mma.m16n8k64  A warp executing mma.m16n8k64 will compute an MMA operation of shape .m16n8k64 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing four .b32 registers, with each\nregister containing eight .s4 / .u4 elements from the\nmatrix A. a0, a1, …, a30, a31 The layout of the fragments held by different threads is shown in Figure 68 . Figure 68 MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 8 || 16 <= i < 24 groupID + 8 otherwise col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i < 16 ( threadID_in_group * 8 ) + ( i & 0x7 ) + 32 for ai where i >= 16 Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each\nregister containing eight .s4 / .u4 elements from the\nmatrix B. b0, b1, …, b14, b15 The layout of the fragments held by different threads is shown in Figure 69 and Figure 70 . Figure 69 MMA .m16n8k64 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type.  Figure 70 MMA .m16n8k64 fragment layout for rows 32–63 of matrix B with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i < 8 ( threadID_in_group * 8 ) + ( i & 0x7 ) + 32 for bi where i >= 8 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 71 . Figure 71 MMA .m16n8k64 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.12. Matrix Fragments for mma.m16n8k128  A warp executing mma.m16n8k128 will compute an MMA operation of shape .m16n8k128 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register containing\nthirty two .b1 elements from the matrix A. a0, a1, …, a62, a63 The layout of the fragments held by different threads is shown in Figure 72 . Figure 72 MMA .m16n8k128 fragment layout for matrix A with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i < 32 groupID + 8 for ai where i >= 32 col = ( threadID_in_group * 32 ) + ( i & 0x1F ) for ai where i = { 0 , ..., 63 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register containing thirty\ntwo .b1 elements from the matrix B. b0, b1, … , b30, b31 The layout of the fragments held by different threads is shown in Figure 73 . Figure 73 MMA .m16n8k128 fragment layout for matrix B with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,..., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 74 . Figure 74 MMA .m16n8k128 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.13. Matrix Fragments for mma.m16n8k256  A warp executing mma.m16n8k256 will compute an MMA operation of shape .m16n8k256 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each register\ncontaining thirty two .b1 elements from the matrix A. a0, a1, …, a126, a127 The layout of the fragments held by different threads is shown in Figure 75 . Figure 75 MMA .m16n8k256 fragment layout for matrix A with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 32 || 64 <= i < 96 groupID + 8 otherwise col = ( threadID_in_group * 32 ) + i for ai where i < 64 ( threadID_in_group * 32 ) + ( i & 0x1F ) + 128 for ai where i >= 64 Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register\ncontaining thirty two .b1 elements from the matrix B. b0, b1, …, b62, b63 The layout of the fragments held by different threads is shown in Figure 76 and Figure 77 . Figure 76 MMA .m16n8k256 fragment layout for rows 0–127 of matrix B with .b1 type.  Figure 77 MMA .m16n8k256 fragment layout for rows 128–255 of matrix B with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + ( i & 0x1F ) for bi where i < 32 ( threadID_in_group * 32 ) + ( i & 0x1F ) + 128 for bi where i >= 32 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 78 . Figure 78 MMA .m16n8k256 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.14. Multiply-and-Accumulate Instruction: mma  mma Perform matrix multiply-and-accumulate operation Syntax Half precision floating point type: mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.ctype   = {.f16, .f32};\n.dtype   = {.f16, .f32}; Alternate floating point type : mma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32        d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32      d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32       d, a, b, c;\nmma.sync.aligned.m16n8k32.row.col.f32.f8type.f8type.f32   d, a, b, c;\n\n.atype  = {.bf16, .tf32};\n.btype  = {.bf16, .tf32};\n.f8type = {.e4m3, .e5m2}; Double precision floating point type: mma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c;\n\n.shape   = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16}; Integer type: mma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k16, .m16n8k16, .m16n8k32}\n.atype   = {.u8, .s8};\n.btype   = {.u8, .s8};\n\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k32, .m16n8k32, .m16n8k64}\n.atype   = {.u4, .s4};\n.btype   = {.u4, .s4}; Single bit: mma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c;\n\n.bitOp = {.xor, .and}\n.shape = {.m8n8k128, .m16n8k128, .m16n8k256} Description Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C , where the A matrix is MxK , the B matrix is KxN , and the C and D matrices are MxN . A warp executing mma.sync.m8n8k4 instruction computes 4 matrix multiply and accumulate\noperations. Rest of the mma.sync operations compute a single matrix mutliply and accumulate\noperation per warp. For single-bit mma.sync , multiplication is replaced by a sequence of logical operations;\nspecifically, mma.xor.popc and mma.and.popc computes the XOR, AND respectively of a k-bit\nrow of A with a k-bit column of B, then counts the number of set bits in the result ( popc ). This\nresult is added to the corresponding element of C and written into D. Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp. The registers in each thread hold a fragment of matrix as described in Matrix multiply-accumulate\noperation using mma instruction . The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the\nelements in the matrices D, A, B and C respectively. Specific shapes have type restrictions : .m8n8k4 : When .ctype is .f32 , .dtype must also be .f32 . .m16n8k8 : .dtype must be the same as .ctype . .atype must be the same as .btype . The qualifiers .alayout and .blayout indicate the row-major or column-major layouts of\nmatrices A and B respectively. Precision and rounding : .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single\nprecision. When .ctype or .dtype is .f32 , accumulation of the intermediate values\nis performed with at least single precision. When both .ctype and .dtype are specified\nas .f16 , the accumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .e4m3 and .e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .f64 floating point operations : Precision of the element-wise multiplication and addition operation is identical to that of .f64 precision fused multiply-add. Supported rounding modifiers are : .rn : mantissa LSB rounds to nearest even. This is the default. .rz : mantissa LSB rounds towards zero. .rm : mantissa LSB rounds towards negative infinity. .rp : mantissa LSB rounds towards positive infinity. Integer operations : The integer mma operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that mma instruction causes the executing thread to\nwait until all threads in the warp execute the same mma instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma instruction. In conditionally executed code, a mma instruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined. The behavior of mma instruction is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited. Notes Programs using double precision floating point mma instruction with shapes .m16n8k4 , .m16n8k8 , and .m16n8k16 require at least 64 registers for compilation. PTX ISA Notes Introduced in PTX ISA version 6.4. .f16 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version\n6.4. .f16 floating point type mma operation with .m16n8k8 shape introduced in PTX ISA version\n6.5. .u8/.s8 integer type mma operation with .m8n8k16 shape introduced in PTX ISA version\n6.5. .u4/.s4 integer type mma operation with .m8n8k32 shape introduced in PTX ISA version\n6.5. .f64 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version\n7.0. .f16 floating point type mma operation with .m16n8k16 shape introduced in PTX ISA\nversion 7.0. .bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes\nintroduced in PTX ISA version 7.0. .tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes\nintroduced in PTX ISA version 7.0. .u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes introduced in\nPTX ISA version 7.0. .u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes introduced in\nPTX ISA version 7.0. .b1 single-bit integer type mma operation with .m8n8k128 , .m16n8k128 and .m16n8k256 shapes introduced in PTX ISA version 7.0. Support for .and operation in single-bit mma introduced in PTX ISA version 7.1. .f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes introduced in PTX ISA version 7.8. Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in\nPTX ISA version 8.4. Target ISA Notes Requires sm_70 or higher. .f16 floating point type mma operation with .m8n8k4 shape requires sm_70 or higher. Note mma.sync.m8n8k4 is optimized for target architecture sm_70 and may have substantially\nreduced performance on other target architectures. .f16 floating point type mma operation with .m16n8k8 shape requires sm_75 or higher. .u8/.s8 integer type mma operation with .m8n8k16 shape requires sm_75 or higher. .u4/.s4 integer type mma operation with .m8n8k32 shape sm_75 or higher. .b1 single-bit integer type mma operation with .m8n8k128 shape sm_75 or higher. .f64 floating point type mma operation with .m8n8k4 shape requires sm_80 or higher. .f16 floating point type mma operation with .m16n8k16 shape requires sm_80 or\nhigher. .bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes\nrequires sm_80 or higher. .tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes\nrequires sm_80 or higher. .u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes requires sm_80 or higher. .u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes requires sm_80 or higher. .b1 single-bit integer type mma operation with .m16n8k128 and .m16n8k256 shapes\nrequires sm_80 or higher. .and operation in single-bit mma requires sm_80 or higher. .f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes require sm_90 or higher. .e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher. Examples of half precision floating point type // f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4> %Rd<4>\nmma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n\n// f16 elements in C and f32 elements in D\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4>\n.reg .f32 %Rd<8>\nmma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n // f32 elements in C and D\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<4>, %Rb<2>, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .f16 %Ra<4>, %Rb<2>;\n.reg .f32 %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of alternate floating point type .reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Rb2, %Rb3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of integer type .reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\n\n// s8 elements in A and u8 elements in B\nmma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// u4 elements in A and B matrix\nmma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// s8 elements in A and u8 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u4 elements in A and s4 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// s8 elements in A and s8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u8 elements in A and u8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1 },\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of single bit type // b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of .f64 floating point type .reg .f64 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .f64 %Ra<8>, %Rb<4>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; 9.7.13.4.15. Warp-level matrix load instruction: ldmatrix  ldmatrix Collectively load one or more matrices from shared memory for mma instruction Syntax ldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];\n\n.shape  = {.m8n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16}; Description Collectively load one or more matrices across all threads in a warp from the location indicated by\nthe address operand p , from .shared state space into destination register r . If no state\nspace is provided, generic addressing is used, such that the address in p points into .shared space. If the generic address doesn’t fall in .shared state space, then the behavior\nis undefined. The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit data as indicated by the .type qualifier. The values .x1 , .x2 and .x4 for .num indicate one, two or four matrices\nrespectively. The mandatory .sync qualifier indicates that ldmatrix causes the executing thread to wait\nuntil all threads in the warp execute the same ldmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same ldmatrix instruction. In conditionally executed code, an ldmatrix instruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined. The behavior of ldmatrix is undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited. The destination operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of .num . Each component of the vector expression holds a fragment\nfrom the corresponding matrix. Supported addressing modes for p are described in Addresses as Operands . Consecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of .num as shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on. .num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 Note For .target sm_75 or below, all threads must contain valid addresses. Otherwise, the behavior\nis undefined. For .num = .x1 and .num = .x2 , addresses contained in lower threads can be\ncopied to higher threads to achieve the expected behavior. When reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses\nmust be naturally aligned accordingly. Each thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its\nregister r , and so on. A group of four threads loads an entire row of the matrix as shown in Figure 79 . Figure 79 ldmatrix fragment layout  When .num = .x2 , the elements of the second matrix are loaded in the next destination\nregister in each thread as per the layout in above table. Similarly, when .num = .x4 ,\nelements of the third and fourth matrices are loaded in the subsequent destination registers in each\nthread. Optional qualifier .trans indicates that the matrix is loaded in column-major format. The ldmatrix instruction is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.5. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. Examples // Load a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 d;\nldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];\n\n// Load two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];\n\n// Load four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr]; 9.7.13.4.16. Warp-level matrix store instruction: stmatrix  stmatrix Collectively store one or more matrices to shared memory. Syntax stmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r;\n\n.shape  = {.m8n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16}; Description Collectively store one or more matrices across all threads in a warp to the location indicated by\nthe address operand p , in .shared state space. If no state space is provided, generic\naddressing is used, such that the address in p points into .shared space. If the generic\naddress doesn’t fall in .shared state space, then the behavior is undefined. The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit data as indicated by the .type qualifier. The values .x1 , .x2 and .x4 for .num indicate one, two or four matrices\nrespectively. The mandatory .sync qualifier indicates that stmatrix causes the executing thread to wait\nuntil all threads in the warp execute the same stmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same stmatrix instruction. In conditionally executed code, an stmatrix instruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined. The behavior of stmatrix is undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited. The source operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of .num . Each component of the vector expression holds a fragment\nfrom the corresponding matrix. Supported addressing modes for p are described in Addresses as Operands . Consecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of .num as shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on. .num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 When storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. The matrix addresses\nmust be naturally aligned accordingly. Each thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its\nregister r , and so on. A group of four threads stores an entire row of the matrix as shown in Figure 80 . Figure 80 stmatrix fragment layout  When .num = .x2 , the elements of the second matrix are storedd from the next source register\nin each thread as per the layout in above table. Similarly, when .num = .x4 , elements of the\nthird and fourth matrices are stored from the subsequent source registers in each thread. Optional qualifier .trans indicates that the matrix is stored in column-major format. The stmatrix instruction is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples // Store a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};\n\n// Store two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};\n\n// Store four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3}; 9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix  movmatrix Transpose a matrix in registers across the warp. Syntax movmatrix.sync.aligned.shape.trans.type d, a;\n\n.shape  = {.m8n8};\n.type   = {.b16}; Description Move a row-major matrix across all threads in a warp, reading elements from source a , and\nwriting the transposed elements to destination d . The .shape qualifier indicates the dimensions of the matrix being transposed. Each matrix\nelement holds 16-bit data as indicated by the .type qualifier. The mandatory .sync qualifier indicates that movmatrix causes the executing thread to wait\nuntil all threads in the warp execute the same movmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same movmatrix instruction. In conditionally executed code, a movmatrix instruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nthe behavior is undefined. Operands a and d are 32-bit registers containing fragments of the input matrix and the\nresulting matrix respectively. The mandatory qualifier .trans indicates that the resulting\nmatrix in d is a transpose of the input matrix specified by a . Each thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first\nfragment in register a , and so on. A group of four threads holds an entire row of the input\nmatrix as shown in Figure 81 . Figure 81 movmatrix source matrix fragment layout  Each thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the\nfirst fragment in register d , and so on. A group of four threads holds an entire column of the\nresult matrix as shown in Figure 82 . Figure 82 movmatrix result matrix fragment layout  PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. Examples .reg .b32 d, a;\nmovmatrix.sync.aligned.m8n8.trans.b16 d, a; 9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A  This section describes warp-level mma.sp{::ordered_metadata} instruction with sparse matrix A.\nThis variant of the mma operation can be used when A is a structured sparse matrix with 50%\nzeros in each row distributed in a shape-specific granularity. For an MxNxK sparse mma.sp{::ordered_metadata} operation, the MxK matrix A is packed into MxK/2 elements.\nFor each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements\nare packed in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata. 9.7.13.5.1. Sparse matrix storage  Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a 16x16 matrix A, sparsity is expected to be at 2:4\ngranularity, i.e. each 4-element vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row\ncontains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata\noperand. Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and\nwill result in undefined behavior. In a group of four consecutive threads, one or more threads store\nthe metadata for the whole group depending upon the matrix shape. These threads are specified using\nan additional sparsity selector operand. Figure 83 shows an example of a 16x16 matrix A represented in sparse format and sparsity\nselector indicating which thread in a group of four consecutive threads stores the metadata. Figure 83 Sparse MMA storage example  Granularities for different matrix shapes and data types are described below. Sparse mma.sp{::ordered_metadata} with half-precision and .bf16 type For the .m16n8k16 and .m16n8k32 mma.sp{::ordered_metadata} operations, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements\nin a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are\nstored in the operand representing matrix A and their positions in the four-wide chunk in matrix\nA are indicated by two 2-bit indices in the metadata operand. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values\nof indices; any other values result in an undefined behavior. Figure 84 Sparse MMA metadata example for .f16 / .bf16 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k16 : One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}. m16n8k32 : A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior. Sparse mma.sp{::ordered_metadata} with .tf32 type When matrix A has .tf32 elements, matrix A is structured sparse at a granularity of 1:2. In\nother words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero\nelement. Only the non-zero elements are stored in the operand for matrix A and their positions in a\ntwo-wide chunk in matrix A are indicated by the 4-bit index in the metadata. 0b1110 and 0b0100 are the only meaningful index values; any other values result in an undefined behavior. Figure 85 Sparse MMA metadata example for .tf32 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k8 : One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}. m16n8k16 : A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior. Sparse mma.sp{::ordered_metadata} with integer type When matrices A and B have .u8 / .s8 elements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes\nand two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their\npositions in the four-wide chunk are indicated by two 2-bit indices in the metadata. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior. Figure 86 Sparse MMA metadata example for .u8 / .s8 type.  when matrices A and B have .u4 / .s4 elements, matrix A is pair-wise structured sparse at a\ngranularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has\nfour zeroes and four non-zero values. Further, the zero and non-zero values are clustered in\nsub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within\nthe eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored\nin sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the\neight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior. Figure 87 Sparse MMA metadata example for .u4 / .s4 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k32 with .u8 / .s8 type and m16n8k64 with .u4 / .s4 type: A thread-pair\nwithin a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity\nselector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an\nundefined behavior. m16n8k64 with .u8 / .s8 type and m16n8k128 with .u4 / .s4 type: All threads\nwithin a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity\nselector in this case must be 0. Any other value of sparsity selector results in an undefined\nbehavior. Sparse mma.sp{::ordered_metadata} with .e4m3 / .e5m2 type When matrices A and B have .e4m3 / .e5m2 elements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes and\ntwo non-zero elements. Only the two non-zero elements are stored in sparse matrix and their positions\nin the four-wide chunk are indicated by two 2-bit indices in the metadata. 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values\nresult in an undefined behavior. Figure 88 Sparse MMA metadata example for .e4m3 / .e5m2 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k64 : All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior. 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of\nvarious matrices and the sparsity metadata. The following conventions are used throughout this\nsection: For matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data. For matrix B, when the combination of matrix dimension and the supported data type is not already\ncovered in Matrix multiply-accumulate operation using mma instruction , a pictorial representation of matrix\nfragments is provided. For matrices C and D, since the matrix dimension - data type combination is the same for all\nsupported shapes, and is already covered in Matrix multiply-accumulate operation using mma\ninstruction , the pictorial representations\nof matrix fragments are not included in this section. For the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present\nin cell [x][y..z] indicates that bits m through n (with m being higher) in the\nmetadata operand of thread with %laneid=k contains the indices of the non-zero elements from\nthe chunk [x][y]..[x][z] of matrix A. 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types  A warp executing sparse mma.m16n8k16 with .f16 / .bf16 floating point type will compute\nan MMA operation of shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing two .b32 registers,\nwith each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from\nmatrix A. Mapping of the non-zero elements is as\ndescribed in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 89 . Figure 89 Sparse MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 lastcol = firstcol + 3 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k16 with floating point type for .f16 / .b16 formats. Metadata: A .b32 register containing 16 2-bit vectors each storing the index of a non-zero\nelement of a 4-wide chunk of matrix A as shown in Figure 90 . Figure 90 Sparse MMA .m16n8k16 metadata layout for .f16 / .bf16 type.  9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types  A warp executing sparse mma.m16n8k32 with .f16 / .bf16 floating point type will compute\nan MMA operation of shape .m16n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing four .b32 registers,\nwith each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from\nmatrix A. Mapping of the non-zero elements is as\ndescribed in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 91 . Figure 91 Sparse MMA .m16n8k32 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 2 || 4 <= i < 6 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 For ai where i < 4 ( threadID_in_group * 4 ) + 16 for ai where i >= 4 lastcol = firstcol + 3 Multiplicand B: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .b32 registers, each\ncontaining two .f16 / .bf16 elements from matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 92 . Figure 92 Sparse MMA .m16n8k32 fragment layout for matrix B with .f16 / .bf16 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k16 with floating point type for .f16 / .b16 formats. Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero element from a 4-wide chunk of matrix A as shown in Figure 93 . Figure 93 Sparse MMA .m16n8k32 metadata layout for .f16 / .bf16 type.  9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type  A warp executing sparse mma.m16n8k16 with .tf32 floating point type will compute an MMA\noperation of shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing four .b32 registers, with each\nregister containing one non-zero .tf32 element out of 2\nconsecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 94 . Figure 94 Sparse MMA .m16n8k16 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 for a0 and a1 ( threadID_in_group * 2 ) + 8 for a2 and a3 lastcol = firstcol + 1 Multiplicand B: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, each\ncontaining four .tf32 elements from matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 95 . Figure 95 Sparse MMA .m16n8k16 fragment layout for matrix B with .tf32 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k16 with floating point type . Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in Figure 96 . Figure 96 Sparse MMA .m16n8k16 metadata layout for .tf32 type.  9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type  A warp executing sparse mma.m16n8k8 with .tf32 floating point type will compute an MMA\noperation of shape .m16n8k8 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing two .b32 registers, each\ncontaining one non-zero .tf32 element out of 2\nconsecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 97 . Figure 97 Sparse MMA .m16n8k8 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 lastcol = firstcol + 1 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k8 for .tf32 format. Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in Figure 98 . Figure 98 Sparse MMA .m16n8k8 metadata layout for .tf32 type.  9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type  A warp executing sparse mma.m16n8k32 with .u8 / .s8 integer type will compute an MMA\noperation of shape .m16n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u8 / .s8 A vector expression containing two .b32 registers, with each\nregister containing four non-zero .u8 / .s8 elements out\nof 8 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 99 . Figure 99 Sparse MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 4 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 8 lastcol = firstcol + 7 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k32 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 100 . Figure 100 Sparse MMA .m16n8k32 metadata layout for .u8 / .s8 type.  9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 type  A warp executing sparse mma.m16n8k64 with .u8 / .s8 / .e4m3 / .e5m2 type will compute an MMA\noperation of shape .m16n8k64 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u8 / .s8 A vector expression containing four .b32 registers, with each\nregister containing four non-zero .u8 / .s8 elements out\nof 8 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each\nregister containing four non-zero .e4m3 / .e5m2 elements\nout of 8 consecutive elements from matrix A. The layout of the fragments held by different threads is shown in Figure 101 and Figure 102 . Figure 101 Sparse MMA .m16n8k64 fragment layout for columns 0–31 of matrix A with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 102 Sparse MMA .m16n8k64 fragment layout for columns 32–63 of matrix A with .u8 / .s8 / .e4m3 / .e5m2 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 4 || 8 <= i < 12 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 8 For ai where i < 8 ( threadID_in_group * 8 ) + 32 For ai where i >= 8 lastcol = firstcol + 7 Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing four .b32 registers,\neach containing four .u8 / .s8 elements from\nmatrix B. b0, b1, b2, b3, …, b15 .e4m3 / .e5m2 A vector expression containing four .b32 registers,\neach containing four .e4m3 / .e5m2 elements from\nmatrix B. The layout of the fragments held by different threads is shown in Figure 103 , Figure 104 , Figure 105 and Figure 106 . Figure 103 Sparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 104 Sparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 105 Sparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 106 Sparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k16 with integer type . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 107 and Figure 108 . Figure 107 Sparse MMA .m16n8k64 metadata layout for columns 0–31 for .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 108 Sparse MMA .m16n8k64 metadata layout for columns 32–63 for .u8 / .s8 / .e4m3 / .e5m2 type.  9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type  A warp executing sparse mma.m16n8k64 with .u4 / .s4 integer type will compute an MMA\noperation of shape .m16n8k64 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u4 / .s4 A vector expression containing two .b32 registers, with each\nregister containing eight non-zero .u4 / .s4 elements\nout of 16 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 109 . Figure 109 Sparse MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 8 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 16 lastcol = firstcol + 15 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k64 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 110 . Figure 110 Sparse MMA .m16n8k64 metadata layout for .u4 / .s4 type.  9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer type  A warp executing sparse mma.m16n8k128 with .u4 / .s4 integer type will compute an MMA\noperation of shape .m16n8k128 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u4 / .s4 A vector expression containing four .b32 registers, with each\nregister containing eight non-zero .u4 / .s4 elements out\nof 16 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 111 and Figure 112 . Figure 111 Sparse MMA .m16n8k128 fragment layout for columns 0–63 of matrix A with .u4 / .s4 type.  Figure 112 Sparse MMA .m16n8k128 fragment layout for columns 64–127 of matrix A with .u4 / .s4 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 8 || 16 <= i < 24 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 16 For ai where i < 16 ( threadID_in_group * 16 ) + 64 For ai where i >= 16 lastcol = firstcol + 15 Multiplicand B: .atype Fragment Elements (low to high) .u4 / .s4 A vector expression containing four .b32 registers, each containing\neight .u4 / .s4 elements from matrix B. b0, b1, b2, b3, …, b31 The layout of the fragments held by different threads is shown in Figure 113 , Figure 114 , Figure 115 , Figure 116 . Figure 113 Sparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type.  Figure 114 Sparse MMA .m16n8k128 fragment layout for rows 31–63 of matrix B with .u4 / .s4 type.  Figure 115 Sparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with .u4 / .s4 type.  Figure 116 Sparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with .u4 / .s4 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k64 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 117 and Figure 118 . Figure 117 Sparse MMA .m16n8k128 metadata layout for  columns 0–63 for .u4 / .s4 type.  Figure 118 Sparse MMA .m16n8k128 metadata layout for  columns 64–127 for .u4 / .s4 type.  9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata  mma.sp/mma.sp::ordered_metadata Perform matrix multiply-and-accumulate operation with sparse matrix A Syntax Half precision floating point type: mma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\n\n.ctype     = {.f16, .f32};\n.dtype     = {.f16, .f32};\n.spvariant = {.sp, .sp::ordered_metadata}; Alternate floating point type : mma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32      d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f;\n\n.f8type    = {.e4m3, .e5m2};\n.spvariant = {.sp, .sp::ordered_metadata}; Integer type: mma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k32, .m16n8k64}\n.atype     = {.u8, .s8};\n.btype     = {.u8, .s8};\n.spvariant = {.sp, .sp::ordered_metadata};\n\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k64, .m16n8k128}\n.atype     = {.u4, .s4};\n.btype     = {.u4, .s4};\n.spvariant = {.sp, .sp::ordered_metadata}; Description Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C , where the A matrix is MxK , the B matrix is KxN , and the C and D matrices are MxN . A warp executing mma.sp.sync/mma.sp::ordered_metadata.sync instruction compute a single matrix\nmutliply and accumulate operation. Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp. Matrix A\nis structured sparse as described in Sparse matrix storage Operands e and f represent sparsity\nmetadata and sparsity selector respectively. Operand e is a 32-bit integer and operand f is\na 32-bit integer constant with values in the range 0..3 Instruction mma.sp::ordered_metadata requires the indices in the sparsity metadata to be sorted\nin an increasing order starting from LSB, otherwise behavior is undefined. The registers in each thread hold a fragment of matrix as described in Matrix fragments for\nmultiply-accumulate operation with sparse matrix A . The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the\nelements in the matrices D, A, B and C respectively. In case of shapes .m16n8k16 and .m16n8k32 , .dtype must be the same as .ctype Precision and rounding : .f16 floating point operations : Element-wise multiplication of matrix A and B is performed with at least single\nprecision. When .ctype or .dtype is .f32 , accumulation of the intermediate values\nis performed with at least single precision. When both .ctype and .dtype are specified\nas .f16 , the accumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .e4m3 and .e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. Integer operations : The integer mma.sp/mma.sp::ordered_metadata operation is performed with .s32 accumulators.\nThe .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that mma.sp/mma.sp::ordered_metadata instruction causes\nthe executing thread to wait until all threads in the warp execute the same mma.sp/mma.sp::ordered_metadata instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma.sp/mma.sp::ordered_metadata instruction. In conditionally executed code, a mma.sp/mma.sp::ordered_metadata instruction should only be used if it is known that all threads in the warp evaluate the condition identically,\notherwise behavior is undefined. The behavior of mma.sp/mma.sp::ordered_metadata instruction is undefined if all threads in the same warp\ndo not use the same qualifiers, or if any thread in the warp has exited. Notes mma.sp instruction may have substantially reduced performance on some target architectures.\nHence, it is advised to use mma.sp::ordered_metadata instruction. PTX ISA Notes Introduced in PTX ISA version 7.1. Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in\nPTX ISA version 8.4. mma.sp::ordered_metadata introduced in PTX ISA version 8.5. Target ISA Notes Requires sm_80 or higher. .e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher. mma.sp::ordered_metadata requires sm_80 or higher. Examples of half precision floating point type // f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\n\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\n\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1; Examples of alternate floating point type .reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; Examples of integer type .reg .b32 %Ra<4>, %Rb<4>, %Rc<4>, %Rd<4>;\n.reg .u32 %Re;\n\n// u8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// s8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// s8 elements in A and B matrix with ordered metadata\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0; 9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions  The warpgroup level matrix multiply and accumulate operation has either of the following forms,\nwhere matrix D is called accumulator: D = A * B + D D = A * B , where the input from accumulator D is disabled. The wgmma instructions perform warpgroup level matrix multiply-and-accumulate operation by\nhaving all threads in a warpgroup collectively perform the following actions: Load matrices A, B and D into registers or into shared memory. Perform the following fence operations: wgmma.fence operations to indicate that the register/shared-memory across the warpgroup\nhave been written into. fence.proxy.async operation to make the generic proxy operations visible to the async\nproxy. Issue the asynchronous matrix multiply and accumulate operations using the wgmma.mma_async operation on the input matrices. The wgmma.mma_async operation is performed in the async\nproxy. Create a wgmma-group and commit all the prior outstanding wgmma.mma_async operations into the\ngroup, by using wgmma.commit_group operation. Wait for the completion of the required wgmma-group. Once the wgmma-group completes, all the wgmma.mma_async operations have been performed and\ncompleted. 9.7.14.1. Warpgroup  A warpgroup is a set of four contiguous warps such that the warp-rank of the first warp is a\nmultiple of 4. warp-rank of a warp is defined as: (%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32 9.7.14.2. Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while D is a MxN matrix. The following matrix shapes are supported for the specified types for the wgmma.mma_async operation: Multiplicand Data type Sparsity Shape Floating-point - .f16 Dense .m64n8k16 , .m64n16k16 , .m64n24k16 , .m64n32k16 , .m64n40k16 , .m64n48k16 , .m64n56k16 , .m64n64k16 , .m64n72k16 , .m64n80k16 , .m64n88k16 , .m64n96k16 , .m64n104k16 , .m64n112k16 , .m64n120k16 , .m64n128k16 , .m64n136k16 , .m64n144k16 , .m64n152k16 , .m64n160k16 , .m64n168k16 , .m64n176k16 , .m64n184k16 , .m64n192k16 , .m64n200k16 , .m64n208k16 , .m64n216k16 , .m64n224k16 , .m64n232k16 , .m64n240k16 , .m64n248k16 , .m64n256k16 Alternate floating-point\nformat - .bf16 Alternate floating-point\nformat - .tf32 Sparse Alternate floating-point\nformat - .tf32 Dense .m64n8k8 , .m64n16k8 , .m64n24k8 , .m64n32k8 , .m64n40k8 , .m64n48k8 , .m64n56k8 , .m64n64k8 , .m64n72k8 , .m64n80k8 , .m64n88k8 , .m64n96k8 , .m64n104k8 , .m64n112k8 , .m64n120k8 , .m64n128k8 , .m64n136k8 , .m64n144k8 , .m64n152k8 , .m64n160k8 , .m64n168k8 , .m64n176k8 , .m64n184k8 , .m64n192k8 , .m64n200k8 , .m64n208k8 , .m64n216k8 , .m64n224k8 , .m64n232k8 , .m64n240k8 , .m64n248k8 , .m64n256k8 Alternate floating-point\nformat - .e4m3 / .e5m2 Dense .m64n8k32 , .m64n16k32 , .m64n24k32 , .m64n32k32 , .m64n40k32 , .m64n48k32 , .m64n56k32 , .m64n64k32 , .m64n72k32 , .m64n80k32 , .m64n88k32 , .m64n96k32 , .m64n104k32 , .m64n112k32 , .m64n120k32 , .m64n128k32 , .m64n136k32 , .m64n144k32 , .m64n152k32 , .m64n160k32 , .m64n168k32 , .m64n176k32 , .m64n184k32 , .m64n192k32 , .m64n200k32 , .m64n208k32 , .m64n216k32 , .m64n224k32 , .m64n232k32 , .m64n240k32 , .m64n248k32 , .m64n256k32 Floating point - .f16 Sparse Altername floating-point\nformat - .bf16 Integer - .u8 / .s8 Dense .m64n8k32 , .m64n16k32 , .m64n24k32 , .m64n32k32 , .m64n48k32 , .m64n64k32 , .m64n80k32 , .m64n96k32 , .m64n112k32 , .m64n128k32 , .m64n144k32 , .m64n160k32 , .m64n176k32 , .m64n192k32 , .m64n208k32 , .m64n224k32 , .m64n240k32 , .m64n256k32 Alternate floating-point\nformat - .e4m3 / .e5m2 Sparse .m64n8k64 , .m64n16k64 , .m64n24k64 , .m64n32k64 , .m64n40k64 , .m64n48k64 , .m64n56k64 , .m64n64k64 , .m64n72k64 , .m64n80k64 , .m64n88k64 , .m64n96k64 , .m64n104k64 , .m64n112k64 , .m64n120k64 , .m64n128k64 , .m64n136k64 , .m64n144k64 , .m64n152k64 , .m64n160k64 , .m64n168k64 , .m64n176k64 , .m64n184k64 , .m64n192k64 , .m64n200k64 , .m64n208k64 , .m64n216k64 , .m64n224k64 , .m64n232k64 , .m64n240k64 , .m64n248k64 , .m64n256k64 Integer - .u8 / .s8 Sparse .m64n8k64 , .m64n16k64 , .m64n24k64 , .m64n32k64 , .m64n48k64 , .m64n64k64 , .m64n80k64 , .m64n96k64 , .m64n112k64 , .m64n128k64 , .m64n144k64 , .m64n160k64 , .m64n176k64 , .m64n192k64 , .m64n208k64 , .m64n224k64 , .m64n240k64 , .m64n256k64 Single-bit - .b1 Dense .m64n8k256 , .m64n16k256 , .m64n24k256 , .m64n32k256 , .m64n48k256 , .m64n64k256 , .m64n80k256 , .m64n96k256 , .m64n112k256 , .m64n128k256 , .m64n144k256 , .m64n160k256 , .m64n176k256 , .m64n192k256 , .m64n208k256 , .m64n224k256 , .m64n240k256 , .m64n256k256 9.7.14.3. Matrix Data-types  The matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point. For floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later. For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have\nelements of the same data-type, e.g. both signed integer or both unsigned integer. Data-type Multiplicands (A or B) Accumulator (D) Integer both .u8 or both .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 , .e5m2 .f16 , .f32 Single-bit integer .b1 .s32 9.7.14.4. Async Proxy  The wgmma.mma_async operations are performed in the asynchronous proxy (or async proxy). Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async\nproxy, fence.proxy.async should be used to synchronize memory between generic proxy and the\nasync proxy. The completion of a wgmma.mma_async operation is followed by an implicit generic-async proxy\nfence. So the result of the asynchronous operation is made visible to the generic proxy as soon as\nits completion is observed. wgmma.commit_group and wgmma.wait_group operations must be used\nto wait for the completion of the wgmma.mma_async instructions. 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction  This section describes warpgroup level wgmma.mma_async instruction and the organization of\nvarious matrices involved in this instruction. 9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts  The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared\nmemory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This\nsection describes the layouts of register fragments and shared memory expected by the warpgroup MMA\ninstructions. When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes. 9.7.14.5.1.1. Register Fragments  This section describes the organization of various matrices located in register operands of the wgmma.mma_async instruction. 9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16  A warpgroup executing wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with each\nregister containing two .f16 / .bf16 elements from matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 119 . Figure 119 WGMMA .m64nNk16 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) .f16 A vector expression containing N/4 number of .f16x2 registers, with each register containing two .f16 elements from matrix D. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ... , 32} .f32 A vector expression containing N/2 number of .f32 registers. The layout of the fragments held by different threads is shown in Figure 120 . Figure 120 WGMMA .m64nNk16 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8  A warpgroup executing wgmma.mma_async.m64nNk8 will compute an MMA operation of shape .m64nNk8 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers containing\nfour .tf32 elements from matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 121 . Figure 121 WGMMA .m64nNk8 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) .f32 A vector expression containing N/2 number of .f32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ... , 32} The layout of the fragments held by different threads is shown in Figure 122 . Figure 122 WGMMA .m64nNk8 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32  A warpgroup executing wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each\nregister containing four .u8 / .s8 elements from matrix A. a0, a1, a2, a3, … , a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each\nregister containing four .e4m3 / .e5m2 elements from\nmatrix A. The layout of the fragments held by different threads is shown in Figure 123 . Figure 123 WGMMA .m64nNk32 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) Miscellaneous Information .s32 A vector expression containing\nN/2 number of .s32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N depends on .dtype, as\ndescribed in the next column. N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} .f32 A vector expression containing\nN/2 number of .f32 registers. N = 8*i where i = {1, 2, ... , 32} .f16 A vector expression containing\nN/4 number of .f16x2 registers, with each register\ncontaining two .f16 elements from matrix D. The layout of the fragments held by different threads is shown in Figure 124 . Figure 124 WGMMA .m64nNk32 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256  A warpgroup executing wgmma.mma_async.m64nNk256 will compute an MMA operation of shape .m64nNk256 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each\nregister containing thirty two .b1 element from matrix A. a0, a1, a2, …, a127 The layout of the fragments held by different threads is shown in Figure 125 . Figure 125 WGMMA .m64nNk256 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) .s32 A vector expression containing N/2 number of .s32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} The layout of the fragments held by different threads is shown in Figure 126 . Figure 126 WGMMA .m64nNk256 register fragment layout for accumulator matrix D.  9.7.14.5.1.2. Shared Memory Matrix Layout  Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each\ncore matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy\ncontiguous space in shared memory. Matrix A is made up of 8x2 core matrices and Matrix B is made up of 2x(N/8) core matrices. This\nsection describes the layout of the core matrices for each shape. 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of eight .f16 / .bf16 elements. 8x8 B Each column is made up of eight .f16 / .bf16 elements. 8x8 Matrices A and B consist of core matrices as shown in Figure 127 .\nEach colored cell represents a core matrix. Figure 127 WGMMA .m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 128 . Each numbered\ncell represents an individual element of the core matrix. Figure 128 WGMMA .m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 129 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 129 WGMMA .m64nNk16 core matrix layout for B  9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of four .tf32 elements. 8x4 B Each row is made up of four .tf32 elements. 4x8 Matrices A and B consist of core matrices as shown in Figure 130 . Each\ncolored cell represents a core matrix. Figure 130 WGMMA .m64nNk8 core matrices for A and B  Layout of core matrices of A is shown in Figure 131 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 131 WGMMA .m64nNk8 core matrix layout for A  Layout of core matrices of B is shown in Figure 132 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 132 WGMMA .m64nNk8 core matrix layout for B  9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32  Core matrices of A and B are as follows: .atype/ .btype Core matrix Matrix description Matrix size .s8 / .u8 A Each row is made up of sixteen .s8 / .u8 elements. 8x4 .e4m3 / .e5m2 Each row is made up of sixteen .e4m3 / .e5m2 elements. .s8 / .u8 B Each column is made up of sixteen .s8 / .u8 elements. 4x8 .e4m3 / .e5m2 Each column is made up of sixteen .e4m3 / .e5m2 elements. Matrices A and B consist of core matrices as shown in Figure 133 . Each\ncolored cell represents a core matrix. Figure 133 WGMMA .m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 134 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 134 WGMMA .m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 135 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 135 WGMMA .m64nNk32 core matrix layout for B  9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of 256 .b1 elements. 8x128 B Each column is made up of 256 .b1 elements. 128x8 Matrices A and B consist of core matrices as shown in Figure 136 . Each\ncolored cell represents a core matrix. Figure 136 WGMMA .m64nNk256 core matrices for A and B  Layout of core matrices of A is shown in Figure 137 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 137 WGMMA .m64nNk256 core matrix layout for A  Layout of core matrices of B is shown in Figure 138 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 138 WGMMA .m64nNk256 core matrix layout for B  9.7.14.5.1.2.5. Strides  Leading dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent\ncore matrices in the K dimension. Stride dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core\nmatrices in the M or N dimension. Figure 139 and Figure 140 show the leading dimension byte offset and the stride dimension byte offsets for A and B matrices. Matrix A: Figure 139 WGMMA stride and leading dimension byte offset for matrix A  Matrix B: Figure 140 WGMMA stride and leading dimension byte offset for matrix B  Leading dimension byte offset and stride dimension byte offset must be specified in the matrix\ndescriptor as described in Matrix Descriptor Format . 9.7.14.5.1.2.6. Swizzling Modes  The core matrices can be swizzled in the shared memory by specifying one of the following swizzling\nmodes: No swizzling: All the elements of the entire core matrix are adjacent to each other and there is\nno swizzling. Figure 141 illustrates this: Figure 141 WGMMA core matrices with no swizzling  32-Byte swizzling: A group of two adjacent core matrices are swizzled as shown in Figure 142 . The\nswizzling pattern repeats for the remaining core matrices. Figure 142 WGMMA core matrices with 32-byte swizzling  64-Byte swizzling: A group of four adjacent core matrices are swizzled as shown in Figure 143 . The\nswizzling pattern repeats for the remaining core matrices. Figure 143 WGMMA core matrices with 64-byte swizzling  128-Byte swizzling: A group of eight adjacent core matrices are swizzled as shown in Figure 144 . The\nswizzling pattern repeats for the remaining core matrices. Figure 144 WGMMA core matrices with 128-byte swizzling  9.7.14.5.1.2.7. Matrix Descriptor Format  Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in\nthe matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the\nfollowing layout: Bit-field Size in bits Description 13–0 14 matrix-descriptor-encode(Matrix start address) 29–16 14 matrix-descriptor-encode(Leading dimension byte offset) 45–32 14 matrix-descriptor-encode(Stride dimension byte offset) 51–49 3 Matrix base offset. This is valid for all swizzling modes except the no-swizzle mode. 63–62 2 Specifies the swizzling mode to be used: 0: No swizzle 1: 128-Byte swizzle 2: 64-Byte swizzle 3: 32-Byte swizzle where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 0x4 The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as\nper the below table: Swizzling mode Starting address of the repeating pattern 128-Byte swizzle 1024-Byte boundary 64-Byte swizzle 512-Byte boundary 32-Byte swizzle 256-Byte boundary Otherwise, the base offset must be a non-zero value, computed using the following formula: base offset = (pattern start addr >> 0x7) & 0x7 9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async  wgmma.mma_async Perform matrix multiply-and-accumulate operation across warpgroup Syntax Half precision floating point type: wgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype   = {.f16, .f32}; Alternate floating point type : .bf16 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\n.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,\n            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,\n            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,\n            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,\n            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,\n            .m64n168k8, .m648176k8, .m64n184k8, .m64n192k8,\n            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,\n            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32}; Integer type: wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,\n            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,\n            .m648176k32, .m64n192k32, .m64n208k32, .m64n224k32};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8}; Single bit: wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,\n            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,\n            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,\n            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,\n            .m64n240k256, .m64n256k256};\n.op  = {.and}; Description Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D = A*B+D , where the A matrix is MxK , the B matrix is KxN , and the D matrix is MxN . The operation of the form D = A*B is issued when the input predicate argument scale-d is\nfalse. wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async instruction from their prior accesses. Otherwise, the behavior is undefined. wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed. Register operand d represents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand a represents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the warps\nin the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format . Matrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0\nand 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16 / .bf16 types on matrices accessed from shared memory using matrix descriptors. For the floating point variants of the wgmma.mma_async operation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of imm-scale-a and imm-scale-b are -1 and 1. The qualifiers .dtype , .atype and .btype indicate the data type of the elements in\nmatrices D, A and B respectively. .atype and .btype must be the same for all floating point wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the wgmma.mma_async operation are as follows: Matrices A and B have 8-bit data elements when .atype / .btype is .e4m3 / .e5m2 . Matrices A and B have 16-bit data elements when .atype / .btype is .bf16 . Matrices A and B have 32-bit data elements when .atype / .btype is .tf32 . Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .dtype is .f32 , accumulation of the intermediate values is performed with at least single\nprecision. When .dtype is .f16 , the accumulation is performed with at least half\nprecision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified\nprecision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. Integer operations: The integer wgmma.mma_async operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the\nrange MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.mma_async instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4. Target ISA Notes Requires sm_90a . Examples of half precision floating point type .reg .f16x2 f16a<40>, f16d<40>;\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n.reg .pred  scaleD;\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16\n  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,\n   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},\n  descA,\n  descB,\n  scaleD, -1, 1, 1, 0; Examples of alternate floating point type .reg .f32   f32d<40>;\n.reg .b32   bf16a<40>\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,\n   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,\n   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,\n   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,\n   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,\n   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},\n  {bf16a0, bf16a1, bf16a2, bf16a3},\n  descB,\n  scaleD, -1, -1, 0;\n\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},\n  descA,\n  descB,\n  0, -1, -1;\n\n.reg .b32 f16d<8>, f16a<8>;\n.reg .f32 f32d<8>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2\n  {f16d0, f16d1},\n  descA,\n  descB,\n  scaleD, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1; Examples of integer type .reg .s32 s32d<8>, s32a<8>;\n.reg .u32 u32a<8>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD; Examples of single bit type .reg .s32 s32d<4>;\n.reg .b32 b32a<4>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\n\nwgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc\n  {s32d0, s32d1, s32d2, s32d3},\n  {b32a0, b32a1, b32a2, b32a3},\n  descB,\n  scaleD; 9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction  This section describes warp-level wgmma.mma_async.sp instruction with sparse matrix A. This\nvariant of the wgmma.mma_async operation can be used when A is a structured sparse matrix with\n50% zeros in each row distributed in a shape-specific granularity. For an MxNxK sparse wgmma.mma_async.sp operation, the MxK matrix A is packed into MxK/2 elements. For each\nK-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements are\npacked in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata. 9.7.14.6.1. Sparse matrix storage  Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a 64x32 matrix A used in floating point wgmma.mma_async operations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element\nvector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each\nnon-zero element in a sub-chunk is stored in the metadata operand. Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and will result in undefined behavior. In a\ngroup of four consecutive threads, one or more threads store the metadata for the whole group\ndepending upon the matrix shape. These threads are specified using an additional sparsity selector operand. Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in Figure 83 , with an appropriate matrix size. Granularities for different matrix shapes and data types are described below. Sparse wgmma.mma_async.sp with half-precision and .bf16 type For .f16 and .bf16 types, for all supported 64xNx32 shapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand. Figure 145 Sparse WGMMA metadata example for .f16 / .bf16 type.  The sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior. Sparse wgmma.mma_async.sp with .tf32 type For .tf32 type, for all supported 64xNx16 shapes, matrix A is structured sparse at a\ngranularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have\none zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and\nthe 4-bit index in the metadata indicates the position of the non-zero element in the two-wide\nchunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in\nan undefined behavior. Figure 146 Sparse WGMMA metadata example for .tf32 type.  The sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior. Sparse wgmma.mma_async.sp with .e4m3 and .e5m2 floating point type For .e4m3 and .e5m2 types, for all supported 64xNx64 shapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand. Figure 147 Sparse WGMMA metadata example for .e4m3 / .e5m2 type.  All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior. Sparse wgmma.mma_async.sp with integer type For the integer type, for all supported 64xNx64 shapes, matrix A is structured sparse at a\ngranularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have\ntwo zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two\n2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide\nchunk. Figure 148 Sparse WGMMA metadata example for .u8 / .s8 type.  All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior. 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of A\nmatrix and the sparsity metadata. Each warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following\ntable shows the assignment of warps to rows of matrix A: Warp Sparsity information for rows of matrix A %warpid % 4 = 3 48-63 %warpid % 4 = 2 32-47 %warpid % 4 = 1 16-31 %warpid % 4 = 0 0-15 The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data. For matrix D, since the matrix dimension - data type combination is the same for all supported\nshapes, and is already covered in Matrix multiply-accumulate operation using wgmma instruction , the pictorial\nrepresentations of matrix fragments are not included in this section. For the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present\nin cell [x][y..z] indicates that bits m through n (with m being higher) in the\nmetadata operand of thread with %laneid=k contains the indices of the non-zero elements from\nthe chunk [x][y]..[x][z] of matrix A. 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32  A warpgroup executing sparse wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A, from shared memory is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk32 . Multiplicand A, from registers: .atype Fragments Elements .f16 / .bf16 A vector expression containing four .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A. Non-zero elements: a0, a1, a2, a3, a4, a5, a6, a7 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 149 . Figure 149 Sparse WGMMA .m64nNk32 fragment layout for matrix A with .f16 / .bf16 type.  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32\nwith floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk32 . Metadata operand is a .b32 register containing 16 2-bit vectors each storing the index of a\nnon-zero element of a 4-wide chunk of matrix A. Figure 150 shows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable i represents the value of the sparsity\nselector operand. Figure 150 Sparse WGMMA .m64nNk32 metadata layout for .f16 / .bf16 type.  9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16  A warpgroup executing sparse wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A, from shared memory is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk16 . Multiplicand A, from registers: .atype Fragments Elements .tf32 A vector expression containing four .b32 registers, containing four non-zero .tf32 elements out of eight consecutive elements from matrix A. Non-zero elements: a0, a1, a2, a3 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 151 . Figure 151 Sparse WGMMA .m64nNk16 fragment layout for matrix A with .tf32 type.  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk8\nwith floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk16 . Metadata operand is a .b32 register containing eight 4-bit vectors each storing the index of a\nnon-zero element of a 2-wide chunk of matrix A. Figure 152 shows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable i represents the value of the sparsity\nselector operand. Figure 152 Sparse WGMMA .m64nNk16 metadata layout for .tf32 type.  9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64  A warpgroup executing sparse wgmma.mma_async.m64nNk64 will compute an MMA operation of shape .m64nNk64 where N is a valid n dimension as listed in Matrix shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A, from shared memory is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk64 . Multiplicand A, from registers: .atype Fragments Elements .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four non-zero .e4m3 / .e5m2 elements out of eight consecutive elements from matrix A. Non-zero elements: a0, a1, a2, … , a15 Mapping of the non-zero elements is as described in Sparse matrix storage .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four non-zero .s8 / .u8 elements out of eight consecutive elements from matrix A. The layout of the fragments held by different threads is shown in Figure 153 . Figure 153 Sparse WGMMA .m64nNk64 fragment layout for matrix A with .e4m3 / .e5m2 / .s8 / .u8 type.  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32\nwith floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk64 . Metadata operand is a .b32 register containing 16 4-bit vectors each storing the indices of\ntwo non-zero elements of a 4-wide chunk of matrix A. Figure 154 shows the mapping of the metadata\nbits to the elements of columns 0–31 of matrix A. Figure 154 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 0–31  Figure 155 shows the mapping of the metadata\nbits to the elements of columns 32–63 of matrix A. Figure 155 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 32–63  9.7.14.6.3. Shared Memory Matrix Layout  Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each\ncore matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy\ncontiguous space in shared memory. Matrix A is made up of 8x2 packed core matrices and Matrix B is made up of 4x (N/8) core\nmatrices. This section describes the layout of the core matrices for each shape. 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of sixteen .f16 / .bf16 elements,\nwith two non-zero elements out of four consecutive elements. 8x16 B Each column is made up of eight .f16 / .bf16 elements. 8x8 Matrices A and B consist of core matrices as shown in Figure 156 .\nEach colored cell represents a core matrix. Figure 156 Sparse WGMMA .m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 157 . Figure 157 Sparse WGMMA .m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 158 . Figure 158 Sparse WGMMA .m64nNk32 core matrix layout for B  9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of eight .tf32 elements\nwith a non-zero element out of two consecutive elements. 8x8 B Each column is made up of four .tf32 elements. 4x8 Matrices A and B consist of core matrices as shown in Figure 159 .\nEach colored cell represents a core matrix. Figure 159 Sparse WGMMA .m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 160 . Figure 160 Sparse WGMMA .m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 161 . Figure 161 Sparse WGMMA .m64nNk16 core matrix layout for B  9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of thirty-two .e4m3 / .e5m2 elements,\nwith two non-zero elements out of four consecutive elements. 8x32 B Each column is made up of eight .f16 / .bf16 elements. 16x8 Matrices A and B consist of core matrices as shown in Figure 162 .\nEach colored cell represents a core matrix. Figure 162 Sparse WGMMA .m64nNk64 core matrices for A and B  Layout of core matrices of A is shown in Figure 163 . Figure 163 Sparse WGMMA .m64nNk64 core matrix layout for A  Layout of core matrices of B is shown in Figure 164 . Figure 164 Sparse WGMMA .m64nNk64 core matrix layout for B  9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp  wgmma.mma_async.sp Perform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup Syntax Half precision floating point type: wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype   = {.f16, .f32}; Alternate floating point type : .bf16 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,\n            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,\n            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,\n            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,\n            .m64n168k64, .m648176k64, .m64n184k64, .m64n192k64,\n            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,\n            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32}; Integer type: wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;\n\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,\n            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,\n            .m648176k64, .m64n192k64, .m64n208k64, .m64n224k64,\n            .m64n240k64, .m64n256k64};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8}; Description Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D = A*B+D , where the A matrix is MxK , the B matrix is KxN , and the D matrix is MxN . The matrix A is stored in the packed format Mx(K/2) as described in Matrix multiply-accumulate\noperation using wgmma.mma_async.sp instruction with sparse matrix A . The operation of the form D = A*B is issued when the input predicate argument scale-d is\nfalse. wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async instruction from their prior accesses. Otherwise, the behavior is undefined. wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed. Register operand d represents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand a represents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the\nwarps in the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format . Matrix A is\nstructured sparse as described in Sparse matrix storage .  Operands sp-meta and sp-sel represent sparsity metadata and sparsity selector respectively. Operand sp-meta is a 32-bit\ninteger and operand sp-sel is a 32-bit integer constant with values in the range 0..3. The valid values of sp-meta and sp-sel for each shape is specified in Matrix\nmultiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A and are summarized here : Matrix shape .atype Valid values of sp-meta Valid values of sp-sel .m64nNk16 .tf32 0b1110 , 0b0100 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk32 .f16 / .bf16 0b00, 0b01, 0b10, 0b11 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk64 .e4m3 / .e5m2 / .s8 / .u8 0b00, 0b01, 0b10, 0b11 0 (all threads contribute) Matrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0\nand 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16 / .bf16 types on matrices accessed from shared memory using matrix descriptors. For the floating point variants of the wgmma.mma_async operation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of imm-scale-a and imm-scale-b are -1 and 1. The qualifiers .dtype , .atype and .btype indicate the data type of the elements in\nmatrices D, A and B respectively. .atype and .btype must be the same for all floating point wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the wgmma.mma_async operation are as follows: Matrices A and B have 8-bit data elements when .atype / .btype is .e4m3 / .e5m2 . Matrices A and B have 16-bit data elements when .atype / .btype is .bf16 . Matrices A and B have 32-bit data elements when .atype / .btype is .tf32 . Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .dtype is .f32 , accumulation of the intermediate values is performed with at least single\nprecision. When .dtype is .f16 , the accumulation is performed with at least half\nprecision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified\nprecision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. Integer operations: The integer wgmma.mma_async operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the\nrange MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.mma_async instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.2. Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4. Target ISA Notes Requires sm_90a . Examples of integer type wgmma.fence.sync.aligned;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0; 9.7.14.7. Asynchronous wgmma Proxy Operations  This section describes warpgroup level wgmma.fence , wgmma.commit_group and wgmma.wait_group instructions. 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence  wgmma.fence Enforce an ordering of register accesses between wgmma.mma_async and other operations. Syntax wgmma.fence.sync.aligned; Description wgmma.fence instruction establishes an ordering between prior accesses to any warpgroup\nregisters and subsequent accesses to the same registers by a wgmma.mma_async instruction. Only\nthe accumulator register and the input registers containing the fragments of matrix A require this\nordering. The wgmma.fence instruction must be issued by all warps of the warpgroup at the following\nlocations: Before the first wgmma.mma_async operation in a warpgroup. Between a register access by a thread in the warpgroup and any wgmma.mma_async instruction\nthat accesses the same registers, either as accumulator or input register containing fragments of\nmatrix A, except when these are accumulator register accesses across multiple wgmma.mma_async instructions of the same shape. In the latter case, an ordering guarantee is provided by default. Otherwise, the behavior is undefined. An async proxy fence must be used to establish an ordering between prior writes to shared memory\nmatrices and subsequent reads of the same matrices in a wgmma.mma_async instruction. The mandatory .sync qualifier indicates that wgmma.fence instruction causes the executing\nthread to wait until all threads in the warp execute the same wgmma.fence instruction before\nresuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.fence instruction. In conditionally executed code, an wgmma.fence instruction\nshould only be used if it is known that all threads in the warpgroup evaluate the condition\nidentically, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples // Example 1, first use example:\nwgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n\n// Example 2, use-case with the input value updated in between:\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\n...\nmov.b32 s32d0, new_val;\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},\n                                                 {s32d0, s32d1, s32d2, s32d3},\n                                                  descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0; 9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group  wgmma.commit_group Commits all prior uncommitted wgmma.mma_async operations into a wgmma-group . Syntax wgmma.commit_group.sync.aligned; Description wgmma.commit_group instruction creates a new wgmma-group per warpgroup and batches all prior wgmma.mma_async instructions initiated by the executing warp but not committed to any\nwgmma-group into the new wgmma-group. If there are no uncommitted wgmma.mma_async instructions\nthen wgmma.commit_group results in an empty wgmma-group. An executing thread can wait for the completion of all wgmma.mma_async operations in a\nwgmma-group by using wgmma.wait_group . The mandatory .sync qualifier indicates that wgmma.commit_group instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.commit_group instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.commit_group instruction. In conditionally executed code, an wgmma.commit_group instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples wgmma.commit_group.sync.aligned; 9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group  wgmma.wait_group Signal the completion of a preceding warpgroup operation. Syntax wgmma.wait_group.sync.aligned N; Description wgmma.wait_group instruction will cause the executing thread to wait until only N or fewer of\nthe most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing\nthreads are complete. For example, when N is 0, the executing thread waits on all the prior\nwgmma-groups to complete. Operand N is an integer constant. Accessing the accumulator register or the input register containing the fragments of matrix A of a wgmma.mma_async instruction without first performing a wgmma.wait_group instruction that\nwaits on a wgmma-group including that wgmma.mma_async instruction is undefined behavior. The mandatory .sync qualifier indicates that wgmma.wait_group instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.wait_group instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.wait_group instruction. In conditionally executed code, an wgmma.wait_group instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples wgmma.fence.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},\n                                                  {f16a0, f16a1, f16a2, f16a3},\n                                                   descB, 1, -1, -1, 1;\nwgmma.commit_group.sync.aligned;\n\nwgmma.wait_group.sync.aligned 0; 9.7.15. Stack Manipulation Instructions  The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the\nstack frame of the current function. The stack manipulation instrucitons are: stacksave stackrestore alloca 9.7.15.1. Stack Manipulation Instructions: stacksave  stacksave Save the value of stack pointer into a register. Syntax stacksave.type  d;\n\n.type = { .u32, .u64 }; Description Copies the current value of stack pointer into the destination register d . Pointer returned by stacksave can be used in a subsequent stackrestore instruction to restore the stack\npointer. If d is modified prior to use in stackrestore instruction, it may corrupt data in\nthe stack. Destination operand d has the same type as the instruction type. Semantics d = stackptr; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: stacksave is a preview feature in PTX ISA version 7.3. All details are subject to change with\nno guarantees of backward compatibility on future PTX ISA versions or SM architectures. Target ISA Notes stacksave requires sm_52 or higher. Examples .reg .u32 rd;\nstacksave.u32 rd;\n\n.reg .u64 rd1;\nstacksave.u64 rd1; 9.7.15.2. Stack Manipulation Instructions: stackrestore  stackrestore Update the stack pointer with a new value. Syntax stackrestore.type  a;\n\n.type = { .u32, .u64 }; Description Sets the current stack pointer to source register a . When stackrestore is used with operand a written by a prior stacksave instruction, it\nwill effectively restore the state of stack as it was before stacksave was executed. Note that\nif stackrestore is used with an arbitrary value of a , it may cause corruption of stack\npointer. This implies that the correct use of this feature requires that stackrestore.type a is\nused after stacksave.type a without redefining the value of a between them. Operand a has the same type as the instruction type. Semantics stackptr = a; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: stackrestore is a preview feature in PTX ISA version 7.3. All details are subject to change\nwith no guarantees of backward compatibility on future PTX ISA versions or SM architectures. Target ISA Notes stackrestore requires sm_52 or higher. Examples .reg .u32 ra;\nstacksave.u32 ra;\n// Code that may modify stack pointer\n...\nstackrestore.u32 ra; 9.7.15.3. Stack Manipulation Instructions: alloca  alloca Dynamically allocate memory on stack. Syntax alloca.type  ptr, size{, immAlign};\n\n.type = { .u32, .u64 }; Description The alloca instruction dynamically allocates memory on the stack frame of the current function\nand updates the stack pointer accordingly. The returned pointer ptr points to local memory and\ncan be used in the address operand of ld.local and st.local instructions. If sufficient memory is unavailable for allocation on the stack, then execution of alloca may\nresult in stack overflow. In such cases, attempting to access the allocated memory with ptr will\nresult in undefined program behavior. The memory allocated by alloca is deallocated in the following ways: It is automatically deallocated when the function exits. It can be explicitly deallocated using stacksave and stackrestore instructions: stacksave can be used to save the value of stack pointer before executing alloca , and stackrestore can be used after alloca to restore stack pointer to the original value which\nwas previously saved with stacksave . Note that accessing deallocated memory after executing stackrestore results in undefined behavior. size is an unsigned value which specifies the amount of memory in number of bytes to be\nallocated on stack. size = 0 may not lead to a valid memory allocation. Both ptr and size have the same type as the instruction type. immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the\nmemory allocated by alloca . It is an integer constant, must be a power of 2 and must not exceed\n2^23. immAlign is an optional argument with default value being 8 which is the minimum\nguaranteed alignment. Semantics alloca.type ptr, size, immAlign:\n\na = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment\n\n// Allocate size bytes of stack memory with alignment a and update the stack pointer.\n// Since the stack grows down, the updated stack pointer contains a lower address.\nstackptr = alloc_stack_mem(size, a);\n\n// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory\n// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).\nstacksave ptr; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: alloca is a preview feature in PTX ISA version 7.3. All details are subject to change with no\nguarantees of backward compatibility on future PTX ISA versions or SM architectures. Target ISA Notes alloca requires sm_52 or higher. Examples .reg .u32 ra, stackptr, ptr, size;\n\nstacksave.u32 stackptr;     // Save the current stack pointer\nalloca ptr, size, 8;        // Allocate stack memory\nst.local.u32 [ptr], ra;     // Use the allocated stack memory\nstackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer 9.7.16. Video Instructions  All video instructions operate on 32-bit register operands. However, the video instructions may be\nclassified as either scalar or SIMD based on whether their core operation applies to one or multiple\nvalues. The video instructions are: vadd , vadd2 , vadd4 vsub , vsub2 , vsub4 vmad vavrg2 , vavrg4 vabsdiff , vabsdiff2 , vabsdiff4 vmin , vmin2 , vmin4 vmax , vmax2 , vmax4 vshl vshr vset , vset2 , vset4 9.7.16.1. Scalar Video Instructions  All scalar video instructions operate on 32-bit register operands. The scalar video instructions\nare: vadd vsub vabsdiff vmin vmax vshl vshr vmad vset The scalar video instructions execute the following stages: Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to\nproduce signed 33-bit input values. Perform a scalar arithmetic operation to produce a signed 34-bit result. Optionally clamp the result to the range of the destination type. Optionally perform one of the following: apply a second operation to the intermediate result and a third operand, or truncate the intermediate result to a byte or half-word value and merge into a specified\nposition in the third operand to produce the final result. The general format of scalar video instructions is as follows: // 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;\n\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.secop = { .add, .min, .max }; The source and destination operands are all 32-bit registers. The type of each operand ( .u32 or .s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are\nextracted and sign- or zero-extended internally to .s33 values. The primary operation is then\nperformed to produce an .s34 intermediate result. The sign of the intermediate result depends on\ndtype. The intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging. .s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {\n    if ( !sat )  return tmp;\n\n    switch ( dsel ) {\n        case .b0, .b1, .b2, .b3:\n            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );\n            else         return CLAMP( tmp, U8_MAX, U8_MIN );\n        case .h0, .h1:\n            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );\n            else         return CLAMP( tmp, U16_MAX, U16_MIN );\n        default:\n            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );\n            else         return CLAMP( tmp, U32_MAX, U32_MIN );\n    }\n} This intermediate result is then optionally combined with the third source operand using a secondary\narithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the\nthird operand is based on dtype . .s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {\n    switch ( secop ) {\n        .add:     return tmp + c;\n        .min:     return MIN(tmp, c);\n        .max      return MAX(tmp, c);\n        default:  return tmp;\n    }\n} .s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {\n    switch ( dsel ) {\n        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);\n        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);\n        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);\n        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);\n        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);\n        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);\n        default:   return tmp;\n    }\n} The lower 32-bits are then written to the destination operand. 9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax  vadd, vsub Integer byte/half-word/word addition/subtraction. vabsdiff Integer byte/half-word/word absolute value of difference. vmin, vmax Integer byte/half-word/word minimum/maximum. Syntax // 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vadd, vsub, vabsdiff, vmin, vmax };\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max }; Description Perform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\n\nswitch ( vop ) {\n    case vadd:     tmp = ta + tb;\n    case vsub:     tmp = ta - tb;\n    case vabsdiff: tmp = | ta - tb |;\n    case vmin:     tmp = MIN( ta, tb );\n    case vmax:     tmp = MAX( ta, tb );\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vadd , vsub , vabsdiff , vmin , vmax require sm_20 or higher. Examples vadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;\nvsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;\nvabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;\nvmin.s32.s32.s32.sat.add  r1, r2, r3, c; 9.7.16.1.2. Scalar Video Instructions: vshl, vshr  vshl, vshr Integer byte/half-word/word left/right shift. Syntax // 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};\nvop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vshl, vshr };\n.dtype = .atype = { .u32, .s32 };\n.mode  = { .clamp, .wrap };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max }; Description vshl Shift a left by unsigned amount in b with optional saturate, and optional secondary\narithmetic operation or subword data merge. Left shift fills with zero. vshr Shift a right by unsigned amount in b with optional saturate, and optional secondary\narithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift\nfills with zero. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a,atype, asel );\ntb = partSelectSignExtend( b, .u32, bsel );\nif ( mode == .clamp  && tb > 32 )  tb = 32;\nif ( mode == .wrap )                       tb = tb & 0x1f;\nswitch ( vop ){\n   case vshl:  tmp = ta << tb;\n   case vshr:  tmp = ta >> tb;\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vshl , vshr require sm_20 or higher. Examples vshl.s32.u32.u32.clamp  r1, r2, r3;\nvshr.u32.u32.u32.wrap   r1, r2, r3.h1; 9.7.16.1.3. Scalar Video Instructions: vmad  vmad Integer byte/half-word/word multiply-accumulate. Syntax // 32-bit scalar operation\nvmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},\n                                         {-}c;\nvmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.scale = { .shr7, .shr15 }; Description Calculate (a*b) + c , with optional operand negates, plus one mode, and scaling. The source operands support optional negation with some restrictions. Although PTX syntax allows\nseparate negation of the a and b operands, internally this is represented as negation of the\nproduct (a*b) . That is, (a*b) is negated if and only if exactly one of a or b is\nnegated. PTX allows negation of either (a*b) or c . The plus one mode ( .po ) computes (a*b) + c + 1 , which is used in computing averages. Source\noperands may not be negated in .po mode. The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product (a*b) is not negated; otherwise, the intermediate result is signed. Input c has the same\nsign as the intermediate result. The final result is unsigned if the intermediate result is unsigned and c is not negated. Depending on the sign of the a and b operands, and the operand negates, the following\ncombinations of operands are supported for VMAD: (u32 * u32) + u32  // intermediate unsigned; final unsigned\n-(u32 * u32) + s32  // intermediate   signed; final   signed\n (u32 * u32) - u32  // intermediate unsigned; final   signed\n (u32 * s32) + s32  // intermediate   signed; final   signed\n-(u32 * s32) + s32  // intermediate   signed; final   signed\n (u32 * s32) - s32  // intermediate   signed; final   signed\n (s32 * u32) + s32  // intermediate   signed; final   signed\n-(s32 * u32) + s32  // intermediate   signed; final   signed\n (s32 * u32) - s32  // intermediate   signed; final   signed\n (s32 * s32) + s32  // intermediate   signed; final   signed\n-(s32 * s32) + s32  // intermediate   signed; final   signed\n (s32 * s32) - s32  // intermediate   signed; final   signed The intermediate result is optionally scaled via right-shift; this result is sign-extended if the\nfinal result is signed, and zero-extended otherwise. The final result is optionally saturated to the appropriate 32-bit range based on the type (signed\nor unsigned) of the final result. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\nsignedFinal = isSigned(atype) || isSigned(btype) ||\n                                 (a.negate ^ b.negate) || c.negate;\ntmp[127:0] = ta * tb;\n\nlsb = 0;\nif ( .po )                  {              lsb = 1; } else\nif ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else\nif ( c.negate )             { c   = ~c;    lsb = 1; }\n\nc128[127:0] = (signedFinal) sext32( c ) : zext ( c );\ntmp = tmp + c128 + lsb;\nswitch( scale ) {\n   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;\n   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;\n}\nif ( .sat ) {\n     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);\n     else             result = CLAMP(result, U32_MAX, U32_MIN);\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vmad requires sm_20 or higher. Examples vmad.s32.s32.u32.sat    r0, r1, r2, -r3;\nvmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3; 9.7.16.1.4. Scalar Video Instructions: vset  vset Integer byte/half-word/word comparison. Syntax // 32-bit scalar operation, with optional secondary operation\nvset.atype.btype.cmp       d, a{.asel}, b{.bsel};\nvset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max }; Description Compare input values using specified comparison, with optional secondary arithmetic operation or\nsubword data merge. The intermediate result of the comparison is always unsigned, and therefore destination d and\noperand c are also unsigned. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\ntmp = compare( ta, tb, cmp ) ? 1 : 0;\nd = optSecondaryOp( op2, tmp, c );    // optional secondary operation\nd = optMerge( dsel, tmp, c );         // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vset requires sm_20 or higher. Examples vset.s32.u32.lt    r1, r2, r3;\nvset.u32.u32.ne    r1, r2, r3.h1; 9.7.16.2. SIMD Video Instructions  The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values. The SIMD video instructions are: vadd2 , vadd4 vsub2 , vsub4 vavrg2 , vavrg4 vabsdiff2 , vabsdiff4 vmin2 , vmin4 vmax2 , vmax4 vset2 , vset4 PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit\nvalues. The SIMD video instructions execute the following stages: Form input vectors by extracting and sign- or zero-extending byte or half-word values from the\nsource operands, to form pairs of signed 17-bit values. Perform a SIMD arithmetic operation on the input pairs. Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the\ndestination type. Optionally perform one of the following: perform a second SIMD merge operation, or apply a scalar accumulate operation to reduce the intermediate SIMD results to a single\nscalar. The general format of dual half-word SIMD video instructions is as follows: // 2-way SIMD operation, with second SIMD merge or accumulate\nvop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };\n.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } }; The general format of quad byte SIMD video instructions is as follows: // 4-way SIMD operation, with second SIMD merge or accumulate\nvop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 }; The source and destination operands are all 32-bit registers. The type of each operand ( .u32 or .s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are\nextracted and sign- or zero-extended internally to .s33 values. The primary operation is then\nperformed to produce an .s34 intermediate result. The sign of the intermediate result depends on dtype . The intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging. 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2  vadd2, vsub2 Integer dual half-word SIMD addition/subtraction. vavrg2 Integer dual half-word SIMD average. vabsdiff2 Integer dual half-word SIMD absolute value of difference. vmin2, vmax2 Integer dual half-word SIMD minimum/maximum. Syntax // SIMD instruction with secondary SIMD merge operation\nvop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32 Description Two-way SIMD parallel arithmetic operation with secondary operation. Elements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands a and b using the asel and bsel modifiers. The selected half-words are then operated on in parallel. The results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation. For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into\ndestination d . For all other positions, the corresponding half-word from source operand c is copied to d . For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to operand c , producing a result in d . Semantics // extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\n\nfor (i=0; i<2; i++) {\n    switch ( vop2 ) {\n       case vadd2:             t[i] = Va[i] + Vb[i];\n       case vsub2:             t[i] = Va[i] - Vb[i];\n       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;\n       case vmin2:             t[i] = MIN( Va[i], Vb[i] );\n       case vmax2:             t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );\n        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vadd2 , vsub2 , varvg2 , vabsdiff2 , vmin2 , vmax2 require sm_30 or higher. Examples vadd2.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;\nvmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1; 9.7.16.2.2. SIMD Video Instructions: vset2  vset2 Integer dual half-word SIMD comparison. Syntax // SIMD instruction with secondary SIMD merge operation\nvset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32 Description Two-way SIMD parallel comparison with secondary operation. Elements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands a and b using the asel and bsel modifiers. The selected half-words are then compared in parallel. The intermediate result of the comparison is always unsigned, and therefore the half-words of\ndestination d and operand c are also unsigned. For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into\ndestination d . For all other positions, the corresponding half-word from source operand b is copied to d . For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to operand c , producing a result in d . Semantics // extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\nfor (i=0; i<2; i++) {\n    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vset2 requires sm_30 or higher. Examples vset2.s32.u32.lt      r1, r2, r3, r0;\nvset2.u32.u32.ne.add  r1, r2, r3, r0; 9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4  vadd4, vsub4 Integer quad byte SIMD addition/subtraction. vavrg4 Integer quad byte SIMD average. vabsdiff4 Integer quad byte SIMD absolute value of difference. vmin4, vmax4 Integer quad byte SIMD minimum/maximum. Syntax // SIMD instruction with secondary SIMD merge operation\nvop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\nvop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654 Description Four-way SIMD parallel arithmetic operation with secondary operation. Elements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands a and b using the asel and bsel modifiers. The selected bytes are then operated on in parallel. The results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation. For instructions with a secondary SIMD merge operation: For byte positions indicated in mask, the selected byte results are copied into destination d . For all other positions, the corresponding byte from source operand c is copied to d . For instructions with a secondary accumulate operation: For byte positions indicated in mask, the selected byte results are added to operand c ,\nproducing a result in d . Semantics // extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    switch ( vop4 ) {\n        case vadd4:            t[i] = Va[i] + Vb[i];\n        case vsub4:            t[i] = Va[i] - Vb[i];\n        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;\n        case vmin4:            t[i] = MIN( Va[i], Vb[i] );\n        case vmax4:            t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );\n        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vadd4 , vsub4 , varvg4 , vabsdiff4 , vmin4 , vmax4 require sm_30 or higher. Examples vadd4.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;\nvmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1; 9.7.16.2.4. SIMD Video Instructions: vset4  vset4 Integer quad byte SIMD comparison. Syntax // SIMD instruction with secondary SIMD merge operation\nvset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654 Description Four-way SIMD parallel comparison with secondary operation. Elements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands a and b using the asel and bsel modifiers. The selected bytes are then compared in parallel. The intermediate result of the comparison is always unsigned, and therefore the bytes of destination d and operand c are also unsigned. For instructions with a secondary SIMD merge operation: For byte positions indicated in mask, the selected byte results are copied into destination d . For all other positions, the corresponding byte from source operand b is copied to d . For instructions with a secondary accumulate operation: For byte positions indicated in mask, the selected byte results are added to operand c ,\nproducing a result in d . Semantics // extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vset4 requires sm_30 or higher. Examples vset4.s32.u32.lt      r1, r2, r3, r0;\nvset4.u32.u32.ne.max  r1, r2, r3, r0; 9.7.17. Miscellaneous Instructions  The Miscellaneous instructions are: brkpt nanosleep pmevent trap setmaxnreg 9.7.17.1. Miscellaneous Instructions: brkpt  brkpt Breakpoint. Syntax brkpt; Description Suspends execution. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes brkpt requires sm_11 or higher. Examples brkpt;\n@p  brkpt; 9.7.17.2. Miscellaneous Instructions: nanosleep  nanosleep Suspend the thread for an approximate delay given in nanoseconds. Syntax nanosleep.u32 t; Description Suspends the thread for a sleep duration approximately close to the delay t , specified in\nnanoseconds. t may be a register or an immediate value. The sleep duration is approximated, but guaranteed to be in the interval [0, 2*t] . The maximum\nsleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual\nthreads within a warp such that all sleeping threads in the warp wake up together. PTX ISA Notes nanosleep introduced in PTX ISA 6.3. Target ISA Notes nanosleep requires sm_70 or higher. Examples .reg .b32 r;\n.reg .pred p;\n\nnanosleep.u32 r;\nnanosleep.u32 42;\n@p nanosleep.u32 r; 9.7.17.3. Miscellaneous Instructions: pmevent  pmevent Trigger one or more Performance Monitor events. Syntax pmevent       a;    // trigger a single performance monitor event\npmevent.mask  a;    // trigger one or more performance monitor events Description Triggers one or more of a fixed number of performance monitor events, with event index or mask\nspecified by immediate operand a . pmevent (without modifier .mask ) triggers a single performance monitor event indexed by\nimmediate operand a , in the range 0..15 . pmevent.mask triggers one or more of the performance monitor events. Each bit in the 16-bit\nimmediate operand a controls an event. Programmatic performance moniter events may be combined with other hardware events using Boolean\nfunctions to increment one of the four performance counters. The relationship between events and\ncounters is programmed via API calls from the host. Notes Currently, there are sixteen performance monitor events, numbered 0 through 15. PTX ISA Notes pmevent introduced in PTX ISA version 1.4. pmevent.mask introduced in PTX ISA version 3.0. Target ISA Notes pmevent supported on all target architectures. pmevent.mask requires sm_20 or higher. Examples pmevent      1;\n@p  pmevent      7;\n@q  pmevent.mask 0xff; 9.7.17.4. Miscellaneous Instructions: trap  trap Perform trap operation. Syntax trap; Description Abort execution and generate an interrupt to the host CPU. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples trap;\n@p  trap; 9.7.17.5. Miscellaneous Instructions: setmaxnreg  setmaxnreg Hint to change the number of registers owned by the warp. Syntax setmaxnreg.action.sync.aligned.u32 imm-reg-count;\n\n.action = { .inc, .dec }; Description setmaxnreg provides a hint to the system to update the maximum number of per-thread registers\nowned by the executing warp to the value specified by the imm-reg-count operand. Qualifier .dec is used to release extra registers such that the absolute per-thread maximum\nregister count is reduced from its current value to imm-reg-count . Qualifier .inc is used to\nrequest additional registers such that the absolute per-thread maximum register count is increased\nfrom its current value to imm-reg-count . A pool of available registers is maintained per-CTA. Register adjustments requested by the setmaxnreg instructions are handled by supplying extra registers from this pool to the\nrequesting warp or by releasing extra registers from the requesting warp to this pool, depending\nupon the value of the .action qualifier. The setmaxnreg.inc instruction blocks the execution until enough registers are available in the\nCTA’s register pool. After the instruction setmaxnreg.inc obtains new registers from the CTA\npool, the initial contents of the new registers are undefined. The new registers must be initialized\nbefore they are used. The same setmaxnreg instruction must be executed by all warps in a warpgroup . After executing a setmaxnreg instruction, all warps in the warpgroup must synchronize explicitly before\nexecuting subsequent setmaxnreg instructions. If a setmaxnreg instruction is not executed by all\nwarps in the warpgroup , then the behavior is undefined. Operand imm-reg-count is an integer constant. The value of imm-reg-count must be in the\nrange 24 to 256 (both inclusive) and must be a multiple of 8. Changes to the register file of the warp always happen at the tail-end of the register file. The setmaxnreg instruction requires that the kernel has been launched with a valid value of\nmaximum number of per-thread registers specified via the appropriate compilation via the appropriate\ncompile-time option or the appropriate performance tuning directive. Otherwise, the setmaxnreg instruction may have no effect. When qualifier .dec is specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of setmaxnreg instruction should be greater than or equal to the imm-reg-count . Otherwise, the behaviour is undefined. When qualifier .inc is specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of setmaxnreg instruction should be less than or equal to the imm-reg-count . Otherwise, the behaviour is undefined. The mandatory .sync qualifier indicates that setmaxnreg instruction causes the executing\nthread to wait until all threads in the warp execute the same setmaxnreg instruction before\nresuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame setmaxnreg instruction. In conditionally executed code, setmaxnreg instruction should\nonly be used if it is known that all threads in warpgroup evaluate the condition identically,\notherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples setmaxnreg.dec.sync.aligned.u32 64;\nsetmaxnreg.inc.sync.aligned.u32 192; 10. Special Registers  PTX includes a number of predefined, read-only variables, which are\nvisible as special registers and accessed through mov or cvt instructions. The special registers are: %tid %ntid %laneid %warpid %nwarpid %ctaid %nctaid %smid %nsmid %gridid %is_explicit_cluster %clusterid %nclusterid %cluster_ctaid %cluster_nctaid %cluster_ctarank %cluster_nctarank %lanemask_eq , %lanemask_le , %lanemask_lt , %lanemask_ge , %lanemask_gt %clock , %clock_hi , %clock64 %pm0, ..., %pm7 %pm0_64, ..., %pm7_64 %envreg0, ..., %envreg31 %globaltimer , %globaltimer_lo , %globaltimer_hi %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset<2> %total_smem_size %aggr_smem_size %dynamic_smem_size %current_graph_exec 10.1. Special Registers: %tid  %tid Thread identifier within a CTA. Syntax (predefined) .sreg .v4 .u32 %tid;                  // thread id vector\n.sreg .u32 %tid.x, %tid.y, %tid.z;    // thread id components Description A predefined, read-only, per-thread special register initialized with the thread identifier within\nthe CTA. The %tid special register contains a 1D, 2D, or 3D vector to match the CTA shape; the %tid value in unused dimensions is 0 . The fourth element is unused and always returns\nzero. The number of threads in each dimension are specified by the predefined special register %ntid . Every thread in the CTA has a unique %tid . %tid component values range from 0 through %ntid-1 in each CTA dimension. %tid.y == %tid.z == 0 in 1D CTAs. %tid.z == 0 in 2D CTAs. It is guaranteed that: 0  <=  %tid.x <  %ntid.x\n0  <=  %tid.y <  %ntid.y\n0  <=  %tid.z <  %ntid.z PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %tid . Target ISA Notes Supported on all target architectures. Examples mov.u32      %r1,%tid.x;  // move tid.x to %rh\n\n// legacy code accessing 16-bit components of %tid\nmov.u16      %rh,%tid.x;\ncvt.u32.u16  %r2,%tid.z;  // zero-extend tid.z to %r2 10.2. Special Registers: %ntid  %ntid Number of thread IDs per CTA. Syntax (predefined) .sreg .v4 .u32 %ntid;                   // CTA shape vector\n.sreg .u32 %ntid.x, %ntid.y, %ntid.z;   // CTA dimensions Description A predefined, read-only special register initialized with the number of thread ids in each CTA\ndimension. The %ntid special register contains a 3D CTA shape vector that holds the CTA\ndimensions. CTA dimensions are non-zero; the fourth element is unused and always returns zero. The\ntotal number of threads in a CTA is (%ntid.x * %ntid.y * %ntid.z) . %ntid.y == %ntid.z == 1 in 1D CTAs.\n%ntid.z ==1 in 2D CTAs. Maximum values of %ntid.{x,y,z} are as follows: .target architecture %ntid.x %ntid.y %ntid.z sm_1x 512 512 64 sm_20 , sm_3x , sm_5x , sm_6x , sm_7x , sm_8x , sm_9x 1024 1024 64 PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %ntid . Target ISA Notes Supported on all target architectures. Examples // compute unified thread id for 2D CTA\nmov.u32  %r0,%tid.x;\nmov.u32  %h1,%tid.y;\nmov.u32  %h2,%ntid.x;\nmad.u32  %r0,%h1,%h2,%r0;\n\nmov.u16  %rh,%ntid.x;      // legacy code 10.3. Special Registers: %laneid  %laneid Lane Identifier. Syntax (predefined) .sreg .u32 %laneid; Description A predefined, read-only special register that returns the thread’s lane within the warp. The lane\nidentifier ranges from zero to WARP_SZ-1 . PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples mov.u32  %r, %laneid; 10.4. Special Registers: %warpid  %warpid Warp identifier. Syntax (predefined) .sreg .u32 %warpid; Description A predefined, read-only special register that returns the thread’s warp identifier. The warp\nidentifier provides a unique warp number within a CTA but not across CTAs within a grid. The warp\nidentifier will be the same for all threads within a single warp. Note that %warpid is volatile and returns the location of a thread at the moment when read, but\nits value may change during execution, e.g., due to rescheduling of threads following\npreemption. For this reason, %ctaid and %tid should be used to compute a virtual warp index\nif such a value is needed in kernel code; %warpid is intended mainly to enable profiling and\ndiagnostic code to sample and log information such as work place mapping and load distribution. PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples mov.u32  %r, %warpid; 10.5. Special Registers: %nwarpid  %nwarpid Number of warp identifiers. Syntax (predefined) .sreg .u32 %nwarpid; Description A predefined, read-only special register that returns the maximum number of warp identifiers. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %nwarpid requires sm_20 or higher. Examples mov.u32  %r, %nwarpid; 10.6. Special Registers: %ctaid  %ctaid CTA identifier within a grid. Syntax (predefined) .sreg .v4 .u32 %ctaid;                      // CTA id vector\n.sreg .u32 %ctaid.x, %ctaid.y, %ctaid.z;    // CTA id components Description A predefined, read-only special register initialized with the CTA identifier within the CTA\ngrid. The %ctaid special register contains a 1D, 2D, or 3D vector, depending on the shape and\nrank of the CTA grid. The fourth element is unused and always returns zero. It is guaranteed that: 0  <=  %ctaid.x <  %nctaid.x\n0  <=  %ctaid.y <  %nctaid.y\n0  <=  %ctaid.z <  %nctaid.z PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %ctaid . Target ISA Notes Supported on all target architectures. Examples mov.u32  %r0,%ctaid.x;\nmov.u16  %rh,%ctaid.y;   // legacy code 10.7. Special Registers: %nctaid  %nctaid Number of CTA ids per grid. Syntax (predefined) .sreg .v4 .u32 %nctaid                      // Grid shape vector\n.sreg .u32 %nctaid.x,%nctaid.y,%nctaid.z;   // Grid dimensions Description A predefined, read-only special register initialized with the number of CTAs in each grid\ndimension. The %nctaid special register contains a 3D grid shape vector, with each element\nhaving a value of at least 1 . The fourth element is unused and always returns zero. Maximum values of %nctaid.{x,y,z} are as follows: .target architecture %nctaid.x %nctaid.y %nctaid.z sm_1x , sm_20 65535 65535 65535 sm_3x , sm_5x , sm_6x , sm_7x , sm_8x , sm_9x 2 31 -1 65535 65535 PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %nctaid . Target ISA Notes Supported on all target architectures. Examples mov.u32  %r0,%nctaid.x;\nmov.u16  %rh,%nctaid.x;     // legacy code 10.8. Special Registers: %smid  %smid SM identifier. Syntax (predefined) .sreg .u32 %smid; Description A predefined, read-only special register that returns the processor (SM) identifier on which a\nparticular thread is executing. The SM identifier ranges from 0 to %nsmid-1 . The SM\nidentifier numbering is not guaranteed to be contiguous. Notes Note that %smid is volatile and returns the location of a thread at the moment when read, but\nits value may change during execution, e.g. due to rescheduling of threads following\npreemption. %smid is intended mainly to enable profiling and diagnostic code to sample and log\ninformation such as work place mapping and load distribution. PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples mov.u32  %r, %smid; 10.9. Special Registers: %nsmid  %nsmid Number of SM identifiers. Syntax (predefined) .sreg .u32 %nsmid; Description A predefined, read-only special register that returns the maximum number of SM identifiers. The SM\nidentifier numbering is not guaranteed to be contiguous, so %nsmid may be larger than the\nphysical number of SMs in the device. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %nsmid requires sm_20 or higher. Examples mov.u32  %r, %nsmid; 10.10. Special Registers: %gridid  %gridid Grid identifier. Syntax (predefined) .sreg .u64 %gridid; Description A predefined, read-only special register initialized with the per-grid temporal grid identifier. The %gridid is used by debuggers to distinguish CTAs and clusters within concurrent (small) grids. During execution, repeated launches of programs may occur, where each launch starts a\ngrid-of-CTAs. This variable provides the temporal grid launch number for this context. For sm_1x targets, %gridid is limited to the range [0..2 16 -1]. For sm_20 , %gridid is limited to the range [0..2 32 -1]. sm_30 supports the entire 64-bit range. PTX ISA Notes Introduced in PTX ISA version 1.0 as type .u16 . Redefined as type .u32 in PTX ISA version 1.3. Redefined as type .u64 in PTX ISA version 3.0. For compatibility with legacy PTX code, 16-bit and 32-bit mov and cvt instructions may be\nused to read the lower 16-bits or 32-bits of each component of %gridid . Target ISA Notes Supported on all target architectures. Examples mov.u64  %s, %gridid;  // 64-bit read of %gridid\nmov.u32  %r, %gridid;  // legacy code with 32-bit %gridid 10.11. Special Registers: %is_explicit_cluster  %is_explicit_cluster Checks if user has explicitly specified cluster launch. Syntax (predefined) .sreg .pred %is_explicit_cluster; Description A predefined, read-only special register initialized with the predicate value of whether the cluster\nlaunch is explicitly specified by user. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .pred p;\n\nmov.pred  p, %is_explicit_cluster; 10.12. Special Registers: %clusterid  %clusterid Cluster identifier within a grid. Syntax (predefined) .sreg .v4 .u32 %clusterid;\n.sreg .u32 %clusterid.x, %clusterid.y, %clusterid.z; Description A predefined, read-only special register initialized with the cluster identifier in a grid in each\ndimension. Each cluster in a grid has a unique identifier. The %clusterid special register contains a 1D, 2D, or 3D vector, depending upon the shape and\nrank of the cluster. The fourth element is unused and always returns zero. It is guaranteed that: 0  <=  %clusterid.x <  %nclusterid.x\n0  <=  %clusterid.y <  %nclusterid.y\n0  <=  %clusterid.z <  %nclusterid.z PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %clusterid.x;\nmov.u32     %r1, %clusterid.z;\nmov.v4.u32  %rx, %clusterid; 10.13. Special Registers: %nclusterid  %nclusterid Number of cluster identifiers per grid. Syntax (predefined) .sreg .v4 .u32 %nclusterid;\n.sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z; Description A predefined, read-only special register initialized with the number of clusters in each grid\ndimension. The %nclusterid special register contains a 3D grid shape vector that holds the grid dimensions\nin terms of clusters. The fourth element is unused and always returns zero. Refer to the Cuda Programming Guide for details on the maximum values of %nclusterid.{x,y,z} . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %nclusterid.x;\nmov.u32     %r1, %nclusterid.z;\nmov.v4.u32  %rx, %nclusterid; 10.14. Special Registers: %cluster_ctaid  %cluster_ctaid CTA identifier within a cluster. Syntax (predefined) .sreg .v4 .u32 %cluster_ctaid;\n.sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z; Description A predefined, read-only special register initialized with the CTA identifier in a cluster in each\ndimension. Each CTA in a cluster has a unique CTA identifier. The %cluster_ctaid special register contains a 1D, 2D, or 3D vector, depending upon the shape of\nthe cluster. The fourth element is unused and always returns zero. It is guaranteed that: 0  <=  %cluster_ctaid.x <  %cluster_nctaid.x\n0  <=  %cluster_ctaid.y <  %cluster_nctaid.y\n0  <=  %cluster_ctaid.z <  %cluster_nctaid.z PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_ctaid.x;\nmov.u32     %r1, %cluster_ctaid.z;\nmov.v4.u32  %rx, %cluster_ctaid; 10.15. Special Registers: %cluster_nctaid  %cluster_nctaid Number of CTA identifiers per cluster. Syntax (predefined) .sreg .v4 .u32 %cluster_nctaid;\n.sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z; Description A predefined, read-only special register initialized with the number of CTAs in a cluster in each\ndimension. The %cluster_nctaid special register contains a 3D grid shape vector that holds the cluster\ndimensions in terms of CTAs. The fourth element is unused and always returns zero. Refer to the Cuda Programming Guide for details on the maximum values of %cluster_nctaid.{x,y,z} . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_nctaid.x;\nmov.u32     %r1, %cluster_nctaid.z;\nmov.v4.u32  %rx, %cluster_nctaid; 10.16. Special Registers: %cluster_ctarank  %cluster_ctarank CTA identifier in a cluster across all dimensions. Syntax (predefined) .sreg .u32 %cluster_ctarank; Description A predefined, read-only special register initialized with the CTA rank within a cluster across all\ndimensions. It is guaranteed that: 0  <=  %cluster_ctarank <  %cluster_nctarank PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r;\n\nmov.u32  %r, %cluster_ctarank; 10.17. Special Registers: %cluster_nctarank  %cluster_nctarank Number of CTA identifiers in a cluster across all dimensions. Syntax (predefined) .sreg .u32 %cluster_nctarank; Description A predefined, read-only special register initialized with the nunber of CTAs within a cluster across\nall dimensions. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r;\n\nmov.u32  %r, %cluster_nctarank; 10.18. Special Registers: %lanemask_eq  %lanemask_eq 32-bit mask with bit set in position equal to the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_eq; Description A predefined, read-only special register initialized with a 32-bit mask with a bit set in the\nposition equal to the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_eq requires sm_20 or higher. Examples mov.u32     %r, %lanemask_eq; 10.19. Special Registers: %lanemask_le  %lanemask_le 32-bit mask with bits set in positions less than or equal to the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_le; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than or equal to the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_le requires sm_20 or higher. Examples mov.u32     %r, %lanemask_le 10.20. Special Registers: %lanemask_lt  %lanemask_lt 32-bit mask with bits set in positions less than the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_lt; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_lt requires sm_20 or higher. Examples mov.u32     %r, %lanemask_lt; 10.21. Special Registers: %lanemask_ge  %lanemask_ge 32-bit mask with bits set in positions greater than or equal to the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_ge; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than or equal to the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_ge requires sm_20 or higher. Examples mov.u32     %r, %lanemask_ge; 10.22. Special Registers: %lanemask_gt  %lanemask_gt 32-bit mask with bits set in positions greater than the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_gt; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_gt requires sm_20 or higher. Examples mov.u32     %r, %lanemask_gt; 10.23. Special Registers: %clock, %clock_hi  %clock, %clock_hi %clock A predefined, read-only 32-bit unsigned cycle counter. %clock_hi The upper 32-bits of %clock64 special register. Syntax (predefined) .sreg .u32 %clock;\n.sreg .u32 %clock_hi; Description Special register %clock and %clock_hi are unsigned 32-bit read-only cycle counters that wrap\nsilently. PTX ISA Notes %clock introduced in PTX ISA version 1.0. %clock_hi introduced in PTX ISA version 5.0. Target ISA Notes %clock supported on all target architectures. %clock_hi requires sm_20 or higher. Examples mov.u32 r1,%clock;\nmov.u32 r2, %clock_hi; 10.24. Special Registers: %clock64  %clock64 A predefined, read-only 64-bit unsigned cycle counter. Syntax (predefined) .sreg .u64 %clock64; Description Special register %clock64 is an unsigned 64-bit read-only cycle counter that wraps silently. Notes The lower 32-bits of %clock64 are identical to %clock . The upper 32-bits of %clock64 are identical to %clock_hi . PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %clock64 requires sm_20 or higher. Examples mov.u64  r1,%clock64; 10.25. Special Registers: %pm0..%pm7  %pm0..%pm7 Performance monitoring counters. Syntax (predefined) .sreg .u32 %pm<8>; Description Special registers %pm0..%pm7 are unsigned 32-bit read-only performance monitor counters. Their\nbehavior is currently undefined. PTX ISA Notes %pm0..%pm3 introduced in PTX ISA version 1.3. %pm4..%pm7 introduced in PTX ISA version 3.0. Target ISA Notes %pm0..%pm3 supported on all target architectures. %pm4..%pm7 require sm_20 or higher. Examples mov.u32  r1,%pm0;\nmov.u32  r1,%pm7; 10.26. Special Registers: %pm0_64..%pm7_64  %pm0_64..%pm7_64 64 bit Performance monitoring counters. Syntax (predefined) .sreg .u64 %pm0_64;\n.sreg .u64 %pm1_64;\n.sreg .u64 %pm2_64;\n.sreg .u64 %pm3_64;\n.sreg .u64 %pm4_64;\n.sreg .u64 %pm5_64;\n.sreg .u64 %pm6_64;\n.sreg .u64 %pm7_64; Description Special registers %pm0_64..%pm7_64 are unsigned 64-bit read-only performance monitor\ncounters. Their behavior is currently undefined. Notes The lower 32bits of %pm0_64..%pm7_64 are identical to %pm0..%pm7 . PTX ISA Notes %pm0_64..%pm7_64 introduced in PTX ISA version 4.0. Target ISA Notes %pm0_64..%pm7_64 require sm_50 or higher. Examples mov.u32  r1,%pm0_64;\nmov.u32  r1,%pm7_64; 10.27. Special Registers: %envreg<32>  %envreg<32> Driver-defined read-only registers. Syntax (predefined) .sreg .b32 %envreg<32>; Description A set of 32 pre-defined read-only registers used to capture execution environment of PTX program\noutside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch\nand can contain cta-wide or grid-wide values. Precise semantics of these registers is defined in the driver documentation. PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Supported on all target architectures. Examples mov.b32      %r1,%envreg0;  // move envreg0 to %r1 10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi  %globaltimer, %globaltimer_lo, %globaltimer_hi %globaltimer A predefined, 64-bit global nanosecond timer. %globaltimer_lo The lower 32-bits of %globaltimer. %globaltimer_hi The upper 32-bits of %globaltimer. Syntax (predefined) .sreg .u64 %globaltimer;\n.sreg .u32 %globaltimer_lo, %globaltimer_hi; Description Special registers intended for use by NVIDIA tools. The behavior is target-specific and may change\nor be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is\nunspecified. PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Requires target sm_30 or higher. Examples mov.u64  r1,%globaltimer; 10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2>  %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2> %reserved_smem_offset_begin Start of the reserved shared memory region. %reserved_smem_offset_end End of the reserved shared memory region. %reserved_smem_offset_cap Total size of the reserved shared memory region. %reserved_smem_offset_<2> Offsets in the reserved shared memory region. Syntax (predefined) .sreg .b32 %reserved_smem_offset_begin;\n.sreg .b32 %reserved_smem_offset_end;\n.sreg .b32 %reserved_smem_offset_cap;\n.sreg .b32 %reserved_smem_offset_<2>; Description These are predefined, read-only special registers containing information about the shared memory\nregion which is reserved for the NVIDIA system software use. This region of shared memory is not\navailable to users, and accessing this region from user code results in undefined behavior. Refer to CUDA Programming Guide for details. PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes Require sm_80 or higher. Examples .reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1;\n\nmov.b32 %reg_begin,   %reserved_smem_offset_begin;\nmov.b32 %reg_end,     %reserved_smem_offset_end;\nmov.b32 %reg_cap,     %reserved_smem_offset_cap;\nmov.b32 %reg_offset0, %reserved_smem_offset_0;\nmov.b32 %reg_offset1, %reserved_smem_offset_1; 10.30. Special Registers: %total_smem_size  %total_smem_size Total size of shared memory used by a CTA of a kernel. Syntax (predefined) .sreg .u32 %total_smem_size; Description A predefined, read-only special register initialized with total size of shared memory allocated\n(statically and dynamically, excluding the shared memory reserved for the NVIDIA system software\nuse) for the CTA of a kernel at launch time. Size is returned in multiples of shared memory allocation unit size supported by target\narchitecture. Allocation unit values are as follows: Target architecture Shared memory allocation unit size sm_2x 128 bytes sm_3x , sm_5x , sm_6x , sm_7x 256 bytes sm_8x , sm_9x 128 bytes PTX ISA Notes Introduced in PTX ISA version 4.1. Target ISA Notes Requires sm_20 or higher. Examples mov.u32  %r, %total_smem_size; 10.31. Special Registers: %aggr_smem_size  %aggr_smem_size Total size of shared memory used by a CTA of a kernel. Syntax (predefined) .sreg .u32 %aggr_smem_size; Description A predefined, read-only special register initialized with total aggregated size of shared memory\nconsisting of the size of user shared memory allocated (statically and dynamically) at launch time\nand the size of shared memory region which is reserved for the NVIDIA system software use. PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples mov.u32  %r, %aggr_smem_size; 10.32. Special Registers: %dynamic_smem_size  %dynamic_smem_size Size of shared memory allocated dynamically at kernel launch. Syntax (predefined) .sreg .u32 %dynamic_smem_size; Description Size of shared memory allocated dynamically at kernel launch. A predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time. PTX ISA Notes Introduced in PTX ISA version 4.1. Target ISA Notes Requires sm_20 or higher. Examples mov.u32  %r, %dynamic_smem_size; 10.33. Special Registers: %current_graph_exec  %current_graph_exec An Identifier for currently executing CUDA device graph. Syntax (predefined) .sreg .u64 %current_graph_exec; Description A predefined, read-only special register initialized with the identifier referring to the CUDA\ndevice graph being currently executed. This register is 0 if the executing kernel is not part of a\nCUDA device graph. Refer to the CUDA Programming Guide for more details on CUDA device graphs. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_50 or higher. Examples mov.u64  r1, %current_graph_exec; 11. Directives  11.1. PTX Module Directives  The following directives declare the PTX ISA version of the code in the module, the target\narchitecture for which the code was generated, and the size of addresses within the PTX module. .version .target .address_size 11.1.1. PTX Module Directives: .version  .version PTX ISA version number. Syntax .version  major.minor    // major, minor are integers Description Specifies the PTX language version number. The major number is incremented when there are incompatible changes to the PTX language, such as\nchanges to the syntax or semantics. The version major number is used by the PTX compiler to ensure\ncorrect execution of legacy PTX code. The minor number is incremented when new features are added to PTX. Semantics Indicates that this module must be compiled with tools that support an equal or greater version\nnumber. Each PTX module must begin with a .version directive, and no other .version directive is\nallowed anywhere else within the module. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples .version 3.1\n.version 3.0\n.version 2.3 11.1.2. PTX Module Directives: .target  .target Architecture and Platform target. Syntax .target stringlist         // comma separated list of target specifiers\nstring = { sm_90a, sm_90,               // sm_9x target architectures\n           sm_80, sm_86, sm_87, sm_89,  // sm_8x target architectures\n           sm_70, sm_72, sm_75,         // sm_7x target architectures\n           sm_60, sm_61, sm_62,         // sm_6x target architectures\n           sm_50, sm_52, sm_53,         // sm_5x target architectures\n           sm_30, sm_32, sm_35, sm_37,  // sm_3x target architectures\n           sm_20,                       // sm_2x target architectures\n           sm_10, sm_11, sm_12, sm_13,  // sm_1x target architectures\n           texmode_unified, texmode_independent,   // texturing mode\n           debug,                                  // platform option\n           map_f64_to_f32 };                       // platform option Description Specifies the set of features in the target architecture for which the current PTX code was\ngenerated. In general, generations of SM architectures follow an onion layer model, where each\ngeneration adds new features and retains all features of previous generations. The onion layer model\nallows the PTX code generated for a given target to be run on later generation devices. Target architectures with suffix “ a ”, such as sm_90a , include architecture-accelerated\nfeatures that are supported on the specified architecture only, hence such targets do not follow the\nonion layer model. Therefore, PTX code generated for such targets cannot be run on later generation\ndevices. Architecture-accelerated features can only be used with targets that support these\nfeatures. Semantics Each PTX module must begin with a .version directive, immediately followed by a .target directive containing a target architecture and optional platform options. A .target directive\nspecifies a single target architecture, but subsequent .target directives can be used to change\nthe set of target features allowed during parsing. A program with multiple .target directives\nwill compile and run only on devices that support all features of the highest-numbered architecture\nlisted in the program. PTX features are checked against the specified target architecture, and an error is generated if an\nunsupported feature is used.  The following table summarizes the features in PTX that vary according\nto target architecture. Target Description sm_90 Baseline feature set for sm_90 architecture. sm_90a Adds support for sm_90a accelerated wgmma and setmaxnreg instructions. Target Description sm_80 Baseline feature set for sm_80 architecture. sm_86 Adds support for .xorsign modifier on min and max instructions. sm_87 Baseline feature set for sm_86 architecture. sm_89 Baseline feature set for sm_86 architecture. Target Description sm_70 Baseline feature set for sm_70 architecture. sm_72 Adds support for integer multiplicand and accumulator matrices in wmma instructions. Adds support for cvt.pack instruction. sm_75 Adds support for sub-byte integer and single-bit multiplicant matrices in wmma instructions. Adds support for ldmatrix instruction. Adds support for movmatrix instruction. Adds support for tanh instruction. Target Description sm_60 Baseline feature set for sm_60 architecture. sm_61 Adds support for dp2a and dp4a instructions. sm_62 Baseline feature set for sm_61 architecture. Target Description sm_50 Baseline feature set for sm_50 architecture. sm_52 Baseline feature set for sm_50 architecture. sm_53 Adds support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types. Target Description sm_30 Baseline feature set for sm_30 architecture. sm_32 Adds 64-bit {atom,red}.{and,or,xor,min,max} instructions. Adds shf instruction. Adds ld.global.nc instruction. sm_35 Adds support for CUDA Dynamic Parallelism. sm_37 Baseline feature set for sm_35 architecture. Target Description sm_20 Baseline feature set for sm_20 architecture. Target Description sm_10 Baseline feature set for sm_10 architecture. Requires map_f64_to_f32 if any .f64 instructions used. sm_11 Adds 64-bit {atom,red}.{and,or,xor,min,max} instructions. Requires map_f64_to_f32 if any .f64 instructions used. sm_12 Adds {atom,red}.shared , 64-bit {atom,red}.global , vote instructions. Requires map_f64_to_f32 if any .f64 instructions used. sm_13 Adds double-precision support, including expanded rounding modifiers. Disallows use of map_f64_to_f32 . The texturing mode is specified for an entire module and cannot be changed within the module. The .target debug option declares that the PTX file contains DWARF debug information, and\nsubsequent compilation of PTX will retain information needed for source-level debugging. If the\ndebug option is declared, an error message is generated if no DWARF information is found in the\nfile. The debug option requires PTX ISA version 3.0 or later. map_f64_to_f32 indicates that all double-precision instructions map to single-precision\nregardless of the target architecture. This enables high-level language compilers to compile\nprograms containing type double to target device that do not support double-precision\noperations. Note that .f64 storage remains as 64-bits, with only half being used by instructions\nconverted from .f64 to .f32 . Notes Targets of the form compute_xx are also accepted as synonyms for sm_xx targets. PTX ISA Notes Introduced in PTX ISA version 1.0. Target strings sm_10 and sm_11 introduced in PTX ISA version 1.0. Target strings sm_12 and sm_13 introduced in PTX ISA version 1.2. Texturing mode introduced in PTX ISA version 1.5. Target string sm_20 introduced in PTX ISA version 2.0. Target string sm_30 introduced in PTX ISA version 3.0. Platform option debug introduced in PTX ISA version 3.0. Target string sm_35 introduced in PTX ISA version 3.1. Target strings sm_32 and sm_50 introduced in PTX ISA version 4.0. Target strings sm_37 and sm_52 introduced in PTX ISA version 4.1. Target string sm_53 introduced in PTX ISA version 4.2. Target string sm_60 , sm_61 , sm_62 introduced in PTX ISA version 5.0. Target string sm_70 introduced in PTX ISA version 6.0. Target string sm_72 introduced in PTX ISA version 6.1. Target string sm_75 introduced in PTX ISA version 6.3. Target string sm_80 introduced in PTX ISA version 7.0. Target string sm_86 introduced in PTX ISA version 7.1. Target string sm_87 introduced in PTX ISA version 7.4. Target string sm_89 introduced in PTX ISA version 7.8. Target string sm_90 introduced in PTX ISA version 7.8. Target string sm_90a introduced in PTX ISA version 8.0. Target ISA Notes The .target directive is supported on all target architectures. Examples .target sm_10       // baseline target architecture\n.target sm_13       // supports double-precision\n.target sm_20, texmode_independent\n.target sm_90       // baseline target architecture\n.target sm_90a      // PTX using arch accelerated features 11.1.3. PTX Module Directives: .address_size  .address_size Address size used throughout PTX module. Syntax .address_size  address-size\naddress-size = { 32, 64 }; Description Specifies the address size assumed throughout the module by the PTX code and the binary DWARF\ninformation in PTX. Redefinition of this directive within a module is not allowed. In the presence of separate\ncompilation all modules must specify (or default to) the same address size. The .address_size directive is optional, but it must immediately follow the .target directive if present within a module. Semantics If the .address_size directive is omitted, the address size defaults to 32. PTX ISA Notes Introduced in PTX ISA version 2.3. Target ISA Notes Supported on all target architectures. Examples // example directives\n   .address_size 32       // addresses are 32 bit\n   .address_size 64       // addresses are 64 bit\n\n// example of directive placement within a module\n   .version 2.3\n   .target sm_20\n   .address_size 64\n...\n.entry foo () {\n...\n} 11.2. Specifying Kernel Entry Points and Functions  The following directives specify kernel entry points and functions. .entry .func 11.2.1. Kernel and Function Directives: .entry  .entry Kernel entry point and body, with optional parameters. Syntax .entry kernel-name ( param-list )  kernel-body\n.entry kernel-name  kernel-body Description Defines a kernel entry point name, parameters, and body for the kernel function. Parameters are passed via .param space memory and are listed within an optional parenthesized\nparameter list. Parameters may be referenced by name within the kernel body and loaded into\nregisters using ld.param{::entry} instructions. In addition to normal parameters, opaque .texref , .samplerref , and .surfref variables\nmay be passed as parameters. These parameters can only be referenced by name within texture and\nsurface load, store, and query instructions and cannot be accessed via ld.param instructions. The shape and size of the CTA executing the kernel are available in special registers. Semantics Specify the entry point for a kernel program. At kernel launch, the kernel dimensions and properties are established and made available via\nspecial registers, e.g., %ntid , %nctaid , etc. PTX ISA Notes For PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter\nlist. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body. The maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764\nbytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table\nshows the allowed parameter size for a PTX ISA version: PTX ISA Version Maximum parameter size (In bytes) PTX ISA version 8.1 and above 32764 PTX ISA version 1.5 and above 4352 PTX ISA version 1.4 and above 256 The CUDA and OpenCL drivers support the following limits for parameter memory: Driver Parameter memory size CUDA 256 bytes for sm_1x , 4096 bytes for sm_2x and higher ,\n32764 bytes fo sm_70 and higher OpenCL 32764 bytes for sm_70 and higher, 4352 bytes on sm_6x and lower Target ISA Notes Supported on all target architectures. Examples .entry cta_fft\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r<99>;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n\n.entry prefix_sum ( .param .align 4 .s32 pitch[8000] )\n{\n    .reg .s32 %t;\n    ld.param::entry.s32  %t, [pitch];\n    ...\n} 11.2.2. Kernel and Function Directives: .func  .func Function definition. Syntax .func {.attribute(attr-list)} fname {.noreturn} function-body\n.func {.attribute(attr-list)} fname (param-list) {.noreturn} function-body\n.func {.attribute(attr-list)} (ret-param) fname (param-list) function-body Description Defines a function, including input and return parameters and optional function body. An optional .noreturn directive indicates that the function does not return to the caller\nfunction. .noreturn directive cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in Performance-Tuning Directives: .noreturn . An optional .attribute directive specifies additional information associated with the\nfunction. See the description of Variable and Function Attribute Directive: .attribute for allowed attributes. A .func definition with no body provides a function prototype. The parameter lists define locally-scoped variables in the function body. Parameters must be base\ntypes in either the register or parameter state space. Parameters in register state space may be\nreferenced directly within instructions in the function body. Parameters in .param space are\naccessed using ld.param{::func} and st.param{::func} instructions in the body. Parameter\npassing is call-by-value. The last parameter in the parameter list may be a .param array of type .b8 with no size\nspecified. It is used to pass an arbitrary number of parameters to the function packed into a single\narray object. When calling a function with such an unsized last argument, the last argument may be omitted from\nthe call instruction if no parameter is passed through it. Accesses to this array parameter must\nbe within the bounds of the array. The result of an access is undefined if no array was passed, or\nif the access was outside the bounds of the actual array being passed. Semantics The PTX syntax hides all details of the underlying calling convention and ABI. The implementation of parameter passing is left to the optimizing translator, which may use a\ncombination of registers and stack locations to pass parameters. Release Notes For PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and\nrecursion is illegal. PTX ISA versions 2.0 and later with target sm_20 or higher allow parameters in the .param state space, implements an ABI with stack, and supports recursion. PTX ISA versions 2.0 and later with target sm_20 or higher support at most one return value. PTX ISA Notes Introduced in PTX ISA version 1.0. Support for unsized array parameter introduced in PTX ISA version 6.0. Support for .noreturn directive introduced in PTX ISA version 6.4. Support for .attribute directive introduced in PTX ISA version 8.0. Target ISA Notes Functions without unsized array parameter supported on all target architectures. Unsized array parameter requires sm_30 or higher. .noreturn directive requires sm_30 or higher. .attribute directive requires sm_90 or higher. Examples .func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl)\n{\n.reg .b32 localVar;\n\n... use N, dbl;\nother code;\n\nmov.b32 rval,result;\nret;\n}\n\n...\ncall (fooval), foo, (val0, val1);  // return value in fooval\n...\n\n.func foo (.reg .b32 N, .reg .f64 dbl) .noreturn\n{\n.reg .b32 localVar;\n... use N, dbl;\nother code;\nmov.b32 rval, result;\nret;\n}\n...\ncall foo, (val0, val1);\n...\n\n.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[])\n{\n    .reg .b32 input0, input1;\n    ld.param.b32   input0, [numbers + 0];\n    ld.param.b32   input1, [numbers + 4];\n    ...\n    other code;\n    ret;\n}\n...\n\n.param .u32 N;\n.param .align 4 .b8 numbers[8];\nst.param.u32    [N], 2;\nst.param.b32    [numbers + 0], 5;\nst.param.b32    [numbers + 4], 10;\ncall (rval), bar, (N, numbers);\n... 11.2.3. Kernel and Function Directives: .alias  .alias Define an alias to existing function symbol. Syntax .alias fAlias, fAliasee; Description .alias is a module scope directive that defines identifier fAlias to be an alias to function\nspecified by fAliasee . Both fAlias and fAliasee are non-entry function symbols. Identifier fAlias is a function declaration without body. Identifier fAliasee is a function symbol which must be defined in the same module as .alias declaration. Function fAliasee cannot have .weak linkage. Prototype of fAlias and fAliasee must match. Program can use either fAlias or fAlisee identifiers to reference function defined with fAliasee . PTX ISA Notes .alias directive introduced in PTX ISA 6.3. Target ISA Notes .alias directive requires sm_30 or higher. Examples .visible .func foo(.param .u32 p) {\n   ...\n}\n.visible .func bar(.param .u32 p);\n.alias bar, foo;\n.entry test()\n{\n      .param .u32 p;\n      ...\n      call foo, (p);       // call foo directly\n       ...\n       .param .u32 p;\n       call bar, (p);        // call foo through alias\n}\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r1, %r2, %r3;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n} 11.3. Control Flow Directives  PTX provides directives for specifying potential targets for brx.idx and call instructions. See the descriptions of brx.idx and call for more information. .branchtargets .calltargets .callprototype 11.3.1. Control Flow Directives: .branchtargets  .branchtargets Declare a list of potential branch targets. Syntax Label:   .branchtargets  list-of-labels ; Description Declares a list of potential branch targets for a subsequent brx.idx , and associates the list\nwith the label at the start of the line. All control flow labels in the list must occur within the same function as the declaration. The list of labels may use the compact, shorthand syntax for enumerating a range of labels having a\ncommon prefix, similar to the syntax described in Parameterized Variable Names . PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Requires sm_20 or higher. Examples .function foo () {\n      .reg .u32 %r0;\n      ...\n      L1:\n      ...\n      L2:\n      ...\n      L3:\n      ...\n      ts: .branchtargets L1, L2, L3;\n      @p brx.idx %r0, ts;\n      ...\n\n.function bar() {\n      .reg .u32 %r0;\n      ...\n      N0:\n      ...\n      N1:\n      ...\n      N2:\n      ...\n      N3:\n      ...\n      N4:\n      ...\n      ts: .branchtargets N<5>;\n      @p brx.idx %r0, ts;\n      ... 11.3.2. Control Flow Directives: .calltargets  .calltargets Declare a list of potential call targets. Syntax Label:   .calltargets  list-of-functions ; Description Declares a list of potential call targets for a subsequent indirect call, and associates the list\nwith the label at the start of the line. All functions named in the list must be declared prior to the .calltargets directive, and all\nfunctions must have the same type signature. PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Requires sm_20 or higher. Examples calltgt:  .calltargets  fastsin, fastcos;\n...\n@p   call  (%f1), %r0, (%x), calltgt;\n... 11.3.3. Control Flow Directives: .callprototype  .callprototype Declare a prototype for use in an indirect call. Syntax // no input or return parameters\nlabel: .callprototype _ .noreturn;\n// input params, no return params\nlabel: .callprototype _ (param-list) .noreturn;\n// no input params, // return params\nlabel: .callprototype (ret-param) _ ;\n// input, return parameters\nlabel: .callprototype (ret-param) _ (param-list); Description Defines a prototype with no specific function name, and associates the prototype with a label. The\nprototype may then be used in indirect call instructions where there is incomplete knowledge of the\npossible call targets. Parameters may have either base types in the register or parameter state spaces, or array types in\nparameter state space. The sink symbol '_' may be used to avoid dummy parameter names. An optional .noreturn directive indicates that the function does not return to the caller\nfunction. .noreturn directive cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in Performance-Tuning Directives: .noreturn . PTX ISA Notes Introduced in PTX ISA version 2.1. Support for .noreturn directive introduced in PTX ISA version 6.4. Target ISA Notes Requires sm_20 or higher. .noreturn directive requires sm_30 or higher. Examples Fproto1: .callprototype  _ ;\nFproto2: .callprototype  _ (.param .f32 _);\nFproto3: .callprototype  (.param .u32 _) _ ;\nFproto4: .callprototype  (.param .u32 _) _ (.param .f32 _);\n...\n@p   call  (%val), %r0, (%f1), Fproto4;\n...\n\n// example of array parameter\nFproto5: .callprototype _ (.param .b8 _[12]);\n\nFproto6: .callprototype  _ (.param .f32 _) .noreturn;\n...\n@p   call  %r0, (%f1), Fproto6;\n... 11.4. Performance-Tuning Directives  To provide a mechanism for low-level performance tuning, PTX supports the following directives,\nwhich pass information to the backend optimizing compiler. .maxnreg .maxntid .reqntid .minnctapersm .maxnctapersm (deprecated) .pragma The .maxnreg directive specifies the maximum number of registers to be allocated to a single\nthread; the .maxntid directive specifies the maximum number of threads in a thread block (CTA);\nthe .reqntid directive specifies the required number of threads in a thread block (CTA); and the .minnctapersm directive specifies a minimum number of thread blocks to be scheduled on a single\nmultiprocessor (SM). These can be used, for example, to throttle the resource requirements (e.g.,\nregisters) to increase total thread count and provide a greater opportunity to hide memory\nlatency. The .minnctapersm directive can be used together with either the .maxntid or .reqntid directive to trade-off registers-per-thread against multiprocessor utilization without\nneeded to directly specify a maximum number of registers. This may achieve better performance when\ncompiling PTX for multiple devices having different numbers of registers per SM. Currently, the .maxnreg , .maxntid , .reqntid , and .minnctapersm directives may be\napplied per-entry and must appear between an .entry directive and its body. The directives take\nprecedence over any module-level constraints passed to the optimizing backend. A warning message is\ngenerated if the directives’ constraints are inconsistent or cannot be met for the specified target\ndevice. A general .pragma directive is supported for passing information to the PTX backend. The\ndirective passes a list of strings to the backend, and the strings have no semantics within the PTX\nvirtual machine model. The interpretation of .pragma values is determined by the backend\nimplementation and is beyond the scope of the PTX ISA. Note that .pragma directives may appear\nat module (file) scope, at entry-scope, or as statements within a kernel or device function body. 11.4.1. Performance-Tuning Directives: .maxnreg  .maxnreg Maximum number of registers that can be allocated per thread. Syntax .maxnreg n Description Declare the maximum number of registers per thread in a CTA. Semantics The compiler guarantees that this limit will not be exceeded. The actual number of registers used\nmay be less; for example, the backend may be able to compile to fewer registers, or the maximum\nnumber of registers may be further constrained by .maxntid and .maxctapersm . PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples .entry foo .maxnreg 16 { ... }  // max regs per thread = 16 11.4.2. Performance-Tuning Directives: .maxntid  .maxntid Maximum number of threads in the thread block (CTA). Syntax .maxntid nx\n.maxntid nx, ny\n.maxntid nx, ny, nz Description Declare the maximum number of threads in the thread block (CTA). This maximum is specified by giving\nthe maximum extent of each dimension of the 1D, 2D, or 3D CTA.  The maximum number of threads is the\nproduct of the maximum extent in each dimension. Semantics The maximum number of threads in the thread block, computed as the product of the maximum extent\nspecified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in\nwhich this directive appears. Exceeding the maximum number of threads results in a runtime error or\nkernel launch failure. Note that this directive guarantees that the total number of threads does not exceed the maximum,\nbut does not guarantee that the limit in any particular dimension is not exceeded. PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples .entry foo .maxntid 256       { ... }  // max threads = 256\n.entry bar .maxntid 16,16,4   { ... }  // max threads = 1024 11.4.3. Performance-Tuning Directives: .reqntid  .reqntid Number of threads in the thread block (CTA). Syntax .reqntid nx\n.reqntid nx, ny\n.reqntid nx, ny, nz Description Declare the number of threads in the thread block (CTA) by specifying the extent of each dimension\nof the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in\neach dimension. Semantics The size of each CTA dimension specified in any invocation of the kernel is required to be equal to\nthat specified in this directive. Specifying a different CTA dimension at launch will result in a\nruntime error or kernel launch failure. Notes The .reqntid directive cannot be used in conjunction with the .maxntid directive. PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Supported on all target architectures. Examples .entry foo .reqntid 256       { ... }  // num threads = 256\n.entry bar .reqntid 16,16,4   { ... }  // num threads = 1024 11.4.4. Performance-Tuning Directives: .minnctapersm  .minnctapersm Minimum number of CTAs per SM. Syntax .minnctapersm ncta Description Declare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor\n(SM). Notes Optimizations based on .minnctapersm need either .maxntid or .reqntid to be specified as\nwell. If the total number of threads on a single SM resulting from .minnctapersm and .maxntid / .reqntid exceed maximum number of threads supported by an SM then directive .minnctapersm will be ignored. In PTX ISA version 2.1 or higher, a warning is generated if .minnctapersm is specified without\nspecifying either .maxntid or .reqntid . PTX ISA Notes Introduced in PTX ISA version 2.0 as a replacement for .maxnctapersm . Target ISA Notes Supported on all target architectures. Examples .entry foo .maxntid 256 .minnctapersm 4 { ... } 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated)  .maxnctapersm Maximum number of CTAs per SM. Syntax .maxnctapersm ncta Description Declare the maximum number of CTAs from the kernel’s grid that may be mapped to a single\nmultiprocessor (SM). Notes Optimizations based on .maxnctapersm generally need .maxntid to be specified as well. The\noptimizing backend compiler uses .maxntid and .maxnctapersm to compute an upper-bound on\nper-thread register usage so that the specified number of CTAs can be mapped to a single\nmultiprocessor. However, if the number of registers used by the backend is sufficiently lower than\nthis bound, additional CTAs may be mapped to a single multiprocessor. For this reason, .maxnctapersm has been renamed to .minnctapersm in PTX ISA version 2.0. PTX ISA Notes Introduced in PTX ISA version 1.3. Deprecated in PTX ISA version 2.0. Target ISA Notes Supported on all target architectures. Examples .entry foo .maxntid 256 .maxnctapersm 4 { ... } 11.4.6. Performance-Tuning Directives: .noreturn  .noreturn Indicate that the function does not return to its caller function. Syntax .noreturn Description Indicate that the function does not return to its caller function. Semantics An optional .noreturn directive indicates that the function does not return to caller\nfunction. .noreturn directive can only be specified on device functions and must appear between\na .func directive and its body. The directive cannot be specified on functions which have return parameters. If a function with .noreturn directive returns to the caller function at runtime, then the\nbehavior is undefined. PTX ISA Notes Introduced in PTX ISA version 6.4. Target ISA Notes Requires sm_30 or higher. Examples .func foo .noreturn { ... } 11.4.7. Performance-Tuning Directives: .pragma  .pragma Pass directives to PTX backend compiler. Syntax .pragma list-of-strings ; Description Pass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler. The .pragma directive may occur at module-scope, at entry-scope, or at statement-level. Semantics The interpretation of .pragma directive strings is implementation-specific and has no impact on\nPTX semantics. See Descriptions of .pragma Strings for\ndescriptions of the pragma strings defined in ptxas . PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Supported on all target architectures. Examples .pragma \"nounroll\";    // disable unrolling in backend\n\n// disable unrolling for current kernel\n.entry foo .pragma \"nounroll\"; { ... } 11.5. Debugging Directives  DWARF-format debug information is passed through PTX modules using the following directives: @@DWARF .section .file .loc The .section directive was introduced in PTX ISA version 2.0 and replaces the @@DWARF syntax. The @@DWARF syntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX\nISA version 1.x code. Beginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the .target debug platform option. This forward declaration directs PTX compilation to retain\nmappings for source-level debugging. 11.5.1. Debugging Directives: @@dwarf  @@dwarf DWARF-format information. Syntax @@DWARF dwarf-string\n\ndwarf-string may have one of the\n.byte   byte-list   // comma-separated hexadecimal byte values\n.4byte  int32-list  // comma-separated hexadecimal integers in range [0..2^32-1]\n.quad   int64-list  // comma-separated hexadecimal integers in range [0..2^64-1]\n.4byte  label\n.quad   label PTX ISA Notes Introduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by .section directive. Target ISA Notes Supported on all target architectures. Examples @@DWARF .section .debug_pubnames, \"\", @progbits\n@@DWARF .byte   0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n@@DWARF .4byte  .debug_info\n@@DWARF .4byte  0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n@@DWARF .4byte  0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n@@DWARF .byte   0x00, 0x00, 0x00, 0x00, 0x00 11.5.2. Debugging Directives: .section  .section PTX section definition. Syntax .section section_name { dwarf-lines }\n\ndwarf-lines have the following formats:\n  .b8    byte-list       // comma-separated list of integers\n                         // in range [-128..255]\n  .b16   int16-list      // comma-separated list of integers\n                         // in range [-2^15..2^16-1]\n  .b32   int32-list      // comma-separated list of integers\n                         // in range [-2^31..2^32-1]\n  label:                 // Define label inside the debug section\n  .b64   int64-list      // comma-separated list of integers\n                         // in range [-2^63..2^64-1]\n  .b32   label\n  .b64   label\n  .b32   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 32bit)\n  .b64   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 64bit)\n  .b32   label1-label2   // a difference in label addresses between labels in\n                         // the same dwarf section (32bit)\n  .b64   label3-label4   // a difference in label addresses between labels in\n                         // the same dwarf section (64bit) PTX ISA Notes Introduced in PTX ISA version 2.0, replaces @@DWARF syntax. label+imm expression introduced in PTX ISA version 3.2. Support for .b16 integers in dwarf-lines introduced in PTX ISA version 6.0. Support for defining label inside the DWARF section is introduced in PTX ISA version 7.2. label1-label2 expression introduced in PTX ISA version 7.5. Negative numbers in dwarf lines introduced in PTX ISA version 7.5. Target ISA Notes Supported on all target architectures. Examples .section .debug_pubnames\n{\n    .b32    LpubNames_end0-LpubNames_begin0\n  LpubNames_begin0:\n    .b8     0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n    .b32    .debug_info\n  info_label1:\n    .b32    0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n    .b32    0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n    .b8     0x00, 0x00, 0x00, 0x00, 0x00\n  LpubNames_end0:\n}\n\n.section .debug_info\n{\n    .b32 11430\n    .b8 2, 0\n    .b32 .debug_abbrev\n    .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49\n    .b8 0\n    .b32 3, 37, 176, -99\n    .b32 info_label1\n    .b32 .debug_loc+0x4\n    .b8 -11, 11, 112, 97\n    .b32 info_label1+12\n    .b64 -1\n    .b16 -5, -65535\n} 11.5.3. Debugging Directives: .file  .file Source file name. Syntax .file file_index \"filename\" {, timestamp, file_size} Description Associates a source filename with an integer index. .loc directives reference source files by\nindex. .file directive allows optionally specifying an unsigned number representing time of last\nmodification and an unsigned integer representing size in bytes of source file. timestamp and file_size value can be 0 to indicate this information is not available. timestamp value is in format of C and C++ data type time_t . file_size is an unsigned 64-bit integer. The .file directive is allowed only in the outermost scope, i.e., at the same level as kernel\nand device function declarations. Semantics If timestamp and file size are not specified, they default to 0. PTX ISA Notes Introduced in PTX ISA version 1.0. Timestamp and file size introduced in PTX ISA version 3.2. Target ISA Notes Supported on all target architectures. Examples .file 1 \"example.cu\"\n.file 2 \"kernel.cu\"\n.file 1 “kernel.cu”, 1339013327, 64118 11.5.4. Debugging Directives: .loc  .loc Source file location. Syntax .loc file_index line_number column_position\n.loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2 Description Declares the source file location (source file, line number, and column position) to be associated\nwith lexically subsequent PTX instructions. .loc refers to file_index which is defined by a .file directive. To indicate PTX instructions that are generated from a function that got inlined, additional\nattribute .inlined_at can be specified as part of the .loc directive. .inlined_at attribute specifies source location at which the specified function is inlined. file_index2 , line_number2 , and column_position2 specify the location at which function is inlined. Source\nlocation specified as part of .inlined_at directive must lexically precede as source location in .loc directive. The function_name attribute specifies an offset in the DWARF section named .debug_str . Offset is specified as label expression or label + immediate expression\nwhere label is defined in .debug_str section. DWARF section .debug_str contains ASCII\nnull-terminated strings that specify the name of the function that is inlined. Note that a PTX instruction may have a single associated source location, determined by the nearest\nlexically preceding .loc directive, or no associated source location if there is no preceding .loc\ndirective. Labels in PTX inherit the location of the closest lexically following instruction. A\nlabel with no following PTX instruction has no associated source location. PTX ISA Notes Introduced in PTX ISA version 1.0. function_name and inlined_at attributes are introduced in PTX ISA version 7.2. Target ISA Notes Supported on all target architectures. Examples .loc 2 4237 0\nL1:                        // line 4237, col 0 of file #2,\n                           // inherited from mov\n    mov.u32  %r1,%r2;      // line 4237, col 0 of file #2\n    add.u32  %r2,%r1,%r3;  // line 4237, col 0 of file #2\n...\nL2:                        // line 4239, col 5 of file #2,\n                           // inherited from sub\n    .loc 2 4239 5\n    sub.u32  %r2,%r1,%r3;  // line 4239, col 5 of file #2\n    .loc 1 21 3\n    .loc 1 9 3, function_name info_string0, inlined_at 1 21 3\n    ld.global.u32   %r1, [gg]; // Function at line 9\n    setp.lt.s32 %p1, %r1, 8;   // inlined at line 21\n    .loc 1 27 3\n    .loc 1 10 5, function_name info_string1, inlined_at 1 27 3\n    .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5\n    setp.ne.s32 %p2, %r1, 18;\n    @%p2 bra    BB2_3;\n\n    .section .debug_str {\n    info_string0:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 102 // f\n     .b8 111 // o\n     .b8 111 // o\n     .b8 118 // v\n     .b8 0\n\n    info_string1:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 98  // b\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 99  // c\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n    } 11.6. Linking Directives  .extern .visible .weak 11.6.1. Linking Directives: .extern  .extern External symbol declaration. Syntax .extern identifier Description Declares identifier to be defined external to the current module. The module defining such\nidentifier must define it as .weak or .visible only once in a single object file. Extern\ndeclaration of symbol may appear multiple times and references to that get resolved against the\nsingle definition of that symbol. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples .extern .global .b32 foo;  // foo is defined in another module 11.6.2. Linking Directives: .visible  .visible Visible (externally) symbol declaration. Syntax .visible identifier Description Declares identifier to be globally visible. Unlike C, where identifiers are globally visible unless\ndeclared static, PTX identifiers are visible only within the current module unless declared .visible outside the current. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples .visible .global .b32 foo;  // foo will be externally visible 11.6.3. Linking Directives: .weak  .weak Visible (externally) symbol declaration. Syntax .weak identifier Description Declares identifier to be globally visible but weak . Weak symbols are similar to globally visible\nsymbols, except during linking, weak symbols are only chosen after globally visible symbols during\nsymbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak\nsymbol, and references to a symbol get resolved against a weak symbol only if no global symbols have\nthe same name. PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Supported on all target architectures. Examples .weak .func (.reg .b32 val) foo;  // foo will be externally visible 11.6.4. Linking Directives: .common  .common Visible (externally) symbol declaration. Syntax .common identifier Description Declares identifier to be globally visible but “common”. Common symbols are similar to globally visible symbols. However multiple object files may declare\nthe same common symbol and they may have different types and sizes and references to a symbol get\nresolved against a common symbol with the largest size. Only one object file can initialize a common symbol and that must have the largest size among all\nother definitions of that common symbol from different object files. .common linking directive can be used only on variables with .global storage. It cannot be\nused on function symbols or on symbols with opaque type. PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes .common directive requires sm_20 or higher. Examples .common .global .u32 gbl; 11.7. Cluster Dimension Directives  The following directives specify information about clusters: .reqnctapercluster .explicitcluster .maxclusterrank The .reqnctapercluster directive specifies the number of CTAs in the cluster. The .explicitcluster directive specifies that the kernel should be launched with explicit cluster\ndetails. The .maxclusterrank directive specifies the maximum number of CTAs in the cluster. The cluster dimension directives can be applied only on kernel functions. 11.7.1. Cluster Dimension Directives: .reqnctapercluster  .reqnctapercluster Declare the number of CTAs in the cluster. Syntax .reqnctapercluster nx\n.reqnctapercluster nx, ny\n.reqnctapercluster nx, ny, nz Description Set the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of\nthe 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each\ndimension. For kernels with .reqnctapercluster directive specified, runtime will use the\nspecified values for configuring the launch if the same are not specified at launch time. Semantics If cluster dimension is explicitly specified at launch time, it should be equal to the values\nspecified in this directive. Specifying a different cluster dimension at launch will result in a\nruntime error or kernel launch failure. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .entry foo .reqnctapercluster 2         { . . . }\n.entry bar .reqnctapercluster 2, 2, 1   { . . . }\n.entry ker .reqnctapercluster 3, 2      { . . . } 11.7.2. Cluster Dimension Directives: .explicitcluster  .explicitcluster Declare that Kernel must be launched with cluster dimensions explicitly specified. Syntax .explicitcluster Description Declares that this Kernel should be launched with cluster dimension explicitly specified. Semantics Kernels with .explicitcluster directive must be launched with cluster dimension explicitly\nspecified (either at launch time or via .reqnctapercluster ), otherwise program will fail with\nruntime error or kernel launch failure. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .entry foo .explicitcluster         { . . . } 11.7.3. Cluster Dimension Directives: .maxclusterrank  .maxclusterrank Declare the maximum number of CTAs that can be part of the cluster. Syntax .maxclusterrank n Description Declare the maximum number of thread blocks (CTAs) allowed to be part of the cluster. Semantics Product of the number of CTAs in each cluster dimension specified in any invocation of the kernel is\nrequired to be less or equal to that specified in this directive. Otherwise invocation will result\nin a runtime error or kernel launch failure. The .maxclusterrank directive cannot be used in conjunction with the .reqnctapercluster directive. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .entry foo ..maxclusterrank 8         { . . . } 12. Release Notes  This section describes the history of change in the PTX ISA and implementation. The first section\ndescribes ISA and implementation changes in the current release of PTX ISA version 8.5, and the\nremaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX\nISA version 2.0. Table 32 shows the PTX release history. Table 32 PTX Release History  PTX ISA Version CUDA Release Supported Targets PTX ISA 1.0 CUDA 1.0 sm_{10,11} PTX ISA 1.1 CUDA 1.1 sm_{10,11} PTX ISA 1.2 CUDA 2.0 sm_{10,11,12,13} PTX ISA 1.3 CUDA 2.1 sm_{10,11,12,13} PTX ISA 1.4 CUDA 2.2 sm_{10,11,12,13} PTX ISA 1.5 driver r190 sm_{10,11,12,13} PTX ISA 2.0 CUDA 3.0, driver r195 sm_{10,11,12,13} , sm_20 PTX ISA 2.1 CUDA 3.1, driver r256 sm_{10,11,12,13} , sm_20 PTX ISA 2.2 CUDA 3.2, driver r260 sm_{10,11,12,13} , sm_20 PTX ISA 2.3 CUDA 4.0, driver r270 sm_{10,11,12,13} , sm_20 PTX ISA 3.0 CUDA 4.1, driver r285 sm_{10,11,12,13} , sm_20 CUDA 4.2, driver r295 sm_{10,11,12,13} , sm_20 , sm_30 PTX ISA 3.1 CUDA 5.0, driver r302 sm_{10,11,12,13} , sm_20 , sm_{30,35} PTX ISA 3.2 CUDA 5.5, driver r319 sm_{10,11,12,13} , sm_20 , sm_{30,35} PTX ISA 4.0 CUDA 6.0, driver r331 sm_{10,11,12,13} , sm_20 , sm_{30,32,35} , sm_50 PTX ISA 4.1 CUDA 6.5, driver r340 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52} PTX ISA 4.2 CUDA 7.0, driver r346 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} PTX ISA 4.3 CUDA 7.5, driver r352 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} PTX ISA 5.0 CUDA 8.0, driver r361 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} PTX ISA 6.0 CUDA 9.0, driver r384 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 PTX ISA 6.1 CUDA 9.1, driver r387 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 PTX ISA 6.2 CUDA 9.2, driver r396 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 PTX ISA 6.3 CUDA 10.0, driver r400 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 6.4 CUDA 10.1, driver r418 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 6.5 CUDA 10.2, driver r440 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 7.0 CUDA 11.0, driver r445 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_80 PTX ISA 7.1 CUDA 11.1, driver r455 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.2 CUDA 11.2, driver r460 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.3 CUDA 11.3, driver r465 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.4 CUDA 11.4, driver r470 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.5 CUDA 11.5, driver r495 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.6 CUDA 11.6, driver r510 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.7 CUDA 11.7, driver r515 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.8 CUDA 11.8, driver r520 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_90 PTX ISA 8.0 CUDA 12.0, driver r525 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.1 CUDA 12.1, driver r530 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.2 CUDA 12.2, driver r535 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.3 CUDA 12.3, driver r545 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.4 CUDA 12.4, driver r550 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.5 CUDA 12.5, driver r555 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} 12.1. Changes in PTX ISA Version 8.5  New Features PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction. Semantic Changes and Clarifications Values 0b0000 , 0b0101 , 0b1010 , 0b1111 for sparsity metadata (operand e )\nof instruction mma.sp are invalid and their usage results in undefined behavior. 12.2. Changes in PTX ISA Version 8.4  New Features PTX ISA version 8.4 introduces the following new features: Extends ld , st and atom instructions with .b128 type to support .sys scope. Extends integer wgmma.mma_async instruction to support .u8.s8 and .s8.u8 as .atype and .btype respectively. Extends mma , mma.sp instructions to support FP8 types .e4m3 and .e5m2 . Semantic Changes and Clarifications None. 12.3. Changes in PTX ISA Version 8.3  New Features PTX ISA version 8.3 introduces the following new features: Adds support for pragma used_bytes_mask that is used to specify mask for used bytes for a load operation. Extends isspacep , cvta.to , ld and st instructions to accept ::entry and ::func sub-qualifiers with .param state space qualifier. Adds support for .b128 type on instructions ld , ld.global.nc , ldu , st , mov and atom . Add support for instructions tensormap.replace , tensormap.cp_fenceproxy and support for qualifier .to_proxykind::from_proxykind on instruction fence.proxy to support modifying tensor-map . Semantic Changes and Clarifications None. 12.4. Changes in PTX ISA Version 8.2  New Features PTX ISA version 8.2 introduces the following new features: Adds support for .mmio qualifier on ld and st instructions. Extends lop3 instruction to allow predicate destination. Extends multimem.ld_reduce instruction to support .acc::f32 qualifer to allow .f32 precision of the intermediate accumulation. Extends the asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma.mma_async to support .sp modifier that allows matrix multiply-accumulate operation\nwhen input matrix A is sparse. Semantic Changes and Clarifications The .multicast::cluster qualifier on cp.async.bulk and cp.async.bulk.tensor instructions\nis optimized for target architecture sm_90a and may have substantially reduced performance on\nother targets and hence .multicast::cluster is advised to be used with sm_90a . 12.5. Changes in PTX ISA Version 8.1  New Features PTX ISA version 8.1 introduces the following new features: Adds support for st.async and red.async instructions for asynchronous store and\nasynchronous reduction operations respectively on shared memory. Adds support for .oob modifier on half-precision fma instruction. Adds support for .satfinite saturation modifer on cvt instruction for .f16 , .bf16 and .tf32 formats. Extends support for cvt with .e4m3 / .e5m2 to sm_89 . Extends atom and red instructions to support vector types. Adds support for special register %aggr_smem_size . Extends sured instruction with 64-bit min / max operations. Adds support for increased kernel parameter size of 32764 bytes. Adds support for multimem addresses in memory consistency model. Adds support for multimem.ld_reduce , multimem.st and multimem.red instructions to\nperform memory operations on multimem addresses. Semantic Changes and Clarifications None. 12.6. Changes in PTX ISA Version 8.0  New Features PTX ISA version 8.0 introduces the following new features: Adds support for target sm_90a that supports specialized accelerated features. Adds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma . Extends the asynchronous copy operations with bulk operations that operate on large data,\nincluding tensor data. Introduces packed integer types .u16x2 and .s16x2 . Extends integer arithmetic instruction add to allow packed integer types .u16x2 and .s16x2 . Extends integer arithmetic instructions min and max to allow packed integer types .u16x2 and .s16x2 , as well as saturation modifier .relu on .s16x2 and .s32 types. Adds support for special register %current_graph_exec that identifies the currently executing\nCUDA device graph. Adds support for elect.sync instruction. Adds support for .unified attribute on functions and variables. Adds support for setmaxnreg instruction. Adds support for .sem qualifier on barrier.cluster instruction. Extends the fence instruction to allow opcode-specific synchronizaion using op_restrict qualifier. Adds support for .cluster scope on mbarrier.arrive , mbarrier.arrive_drop , mbarrier.test_wait and mbarrier.try_wait operations. Adds support for transaction count operations on mbarrier objects, specified with .expect_tx and .complete_tx qualifiers. Semantic Changes and Clarifications None. 12.7. Changes in PTX ISA Version 7.8  New Features PTX ISA version 7.8 introduces the following new features: Adds support for sm_89 target architecture. Adds support for sm_90 target architecture. Extends bar and barrier instructions to accept optional scope qualifier .cta . Extends .shared state space qualifier with optional sub-qualifier ::cta . Adds support for movmatrix instruction which transposes a matrix in registers across a warp. Adds support for stmatrix instruction which stores one or more matrices to shared memory. Extends the .f64 floating point type mma operation with shapes .m16n8k4 , .m16n8k8 ,\nand .m16n8k16 . Extends add , sub , mul , set , setp , cvt , tanh , ex2 , atom and red instructions with bf16 alternate floating point data format. Adds support for new alternate floating-point data formats .e4m3 and .e5m2 . Extends cvt instruction to convert .e4m3 and .e5m2 alternate floating point data formats. Adds support for griddepcontrol instruction as a communication mechanism to control the\nexecution of dependent grids. Extends mbarrier instruction to allow a new phase completion check operation try_wait . Adds support for new thread scope .cluster which is a set of Cooperative Thread Arrays (CTAs). Extends fence / membar , ld , st , atom , and red instructions to accept .cluster scope. Adds support for extended visibility of shared state space to all threads within a cluster. Extends .shared state space qualifier with ::cluster sub-qualifier for cluster-level\nvisibility of shared memory. Extends isspacep , cvta , ld , st , atom , and red instructions to accept ::cluster sub-qualifier with .shared state space qualifier. Adds support for mapa instruction to map a shared memory address to the corresponding address\nin a different CTA within the cluster. Adds support for getctarank instruction to query the rank of the CTA that contains a given\naddress. Adds support for new barrier synchronization instruction barrier.cluster . Extends the memory consistency model to include the new cluster scope. Adds support for special registers related to cluster information: %is_explicit_cluster , %clusterid , %nclusterid , %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank . Adds support for cluster dimension directives .reqnctapercluster , .explicitcluster , and .maxclusterrank . Semantic Changes and Clarifications None. 12.8. Changes in PTX ISA Version 7.7  New Features PTX ISA version 7.7 introduces the following new features: Extends isspacep and cvta instructions to include the .param state space for kernel\nfunction parameters. Semantic Changes and Clarifications None. 12.9. Changes in PTX ISA Version 7.6  New Features PTX ISA version 7.6 introduces the following new features: Support for szext instruction which performs sign-extension or zero-extension on a specified\nvalue. Support for bmsk instruction which creates a bitmask of the specified width starting at the\nspecified bit position. Support for special registers %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset<2> . Semantic Changes and Clarifications None. 12.10. Changes in PTX ISA Version 7.5  New Features PTX ISA version 7.5 introduces the following new features: Debug information enhancements to support label difference and negative values in the .section debugging directive. Support for ignore-src operand on cp.async instruction. Extensions to the memory consistency model to introduce the following new concepts: A memory proxy as an abstract label for different methods of memory access. Virtual aliases as distinct memory addresses accessing the same physical memory location. Support for new fence.proxy and membar.proxy instructions to allow synchronization of\nmemory accesses performed via virtual aliases. Semantic Changes and Clarifications None. 12.11. Changes in PTX ISA Version 7.4  New Features PTX ISA version 7.4 introduces the following new features: Support for sm_87 target architecture. Support for .level::eviction_priority qualifier which allows specifying cache eviction\npriority hints on ld , ld.global.nc , st , and prefetch instructions. Support for .level::prefetch_size qualifier which allows specifying data prefetch hints on ld and cp.async instructions. Support for createpolicy instruction which allows construction of different types of cache\neviction policies. Support for .level::cache_hint qualifier which allows the use of cache eviction policies with ld , ld.global.nc , st , atom , red and cp.async instructions. Support for applypriority and discard operations on cached data. Semantic Changes and Clarifications None. 12.12. Changes in PTX ISA Version 7.3  New Features PTX ISA version 7.3 introduces the following new features: Extends mask() operator used in initializers to also support integer constant expression. Adds support for stack manpulation instructions that allow manipulating stack using stacksave and stackrestore instructions and allocation of per-thread stack using alloca instruction. Semantic Changes and Clarifications The unimplemented version of alloca from the older PTX ISA specification has been replaced with\nnew stack manipulation instructions in PTX ISA version 7.3. 12.13. Changes in PTX ISA Version 7.2  New Features PTX ISA version 7.2 introduces the following new features: Enhances .loc directive to represent inline function information. Adds support to define labels inside the debug sections. Extends min and max instructions to support .xorsign and .abs modifiers. Semantic Changes and Clarifications None. 12.14. Changes in PTX ISA Version 7.1  New Features PTX ISA version 7.1 introduces the following new features: Support for sm_86 target architecture. Adds a new operator, mask() , to extract a specific byte from variable’s address used in\ninitializers. Extends tex and tld4 instructions to return an optional predicate that indicates if data\nat specified coordinates is resident in memory. Extends single-bit wmma and mma instructions to support .and operation. Extends mma instruction to support .sp modifier that allows matrix multiply-accumulate\noperation when input matrix A is sparse. Extends mbarrier.test_wait instruction to test the completion of specific phase parity. Semantic Changes and Clarifications None. 12.15. Changes in PTX ISA Version 7.0  New Features PTX ISA version 7.0 introduces the following new features: Support for sm_80 target architecture. Adds support for asynchronous copy instructions that allow copying of data asynchronously from one\nstate space to another. Adds support for mbarrier instructions that allow creation of mbarrier objects in memory and\nuse of these objects to synchronize threads and asynchronous copy operations initiated by threads. Adds support for redux.sync instruction which allows reduction operation across threads in a\nwarp. Adds support for new alternate floating-point data formats .bf16 and .tf32 . Extends wmma instruction to support .f64 type with shape .m8n8k4 . Extends wmma instruction to support .bf16 data format. Extends wmma instruction to support .tf32 data format with shape .m16n16k8 . Extends mma instruction to support .f64 type with shape .m8n8k4 . Extends mma instruction to support .bf16 and .tf32 data formats with shape .m16n8k8 . Extends mma instruction to support new shapes .m8n8k128 , .m16n8k4 , .m16n8k16 , .m16n8k32 , .m16n8k64 , .m16n8k128 and .m16n8k256 . Extends abs and neg instructions to support .bf16 and .bf16x2 data formats. Extends min and max instructions to support .NaN modifier and .f16 , .f16x2 , .bf16 and .bf16x2 data formats. Extends fma instruction to support .relu saturation mode and .bf16 and .bf16x2 data formats. Extends cvt instruction to support .relu saturation mode and .f16 , .f16x2 , .bf16 , .bf16x2 and .tf32 destination formats. Adds support for tanh instruction that computes hyperbolic-tangent. Extends ex2 instruction to support .f16 and .f16x2 types. Semantic Changes and Clarifications None. 12.16. Changes in PTX ISA Version 6.5  New Features PTX ISA version 6.5 introduces the following new features: Adds support for integer destination types for half precision comparison instruction set . Extends abs instruction to support .f16 and .f16x2 types. Adds support for cvt.pack instruction which allows converting two integer values and packing\nthe results together. Adds new shapes .m16n8k8 , .m8n8k16 and .m8n8k32 on the mma instruction. Adds support for ldmatrix instruction which loads one or more matrices from shared memory for mma instruction. Removed Features PTX ISA version 6.5 removes the following features: Support for .satfinite qualifier on floating point wmma.mma instruction has been\nremoved. This support was deprecated since PTX ISA version 6.4. Semantic Changes and Clarifications None. 12.17. Changes in PTX ISA Version 6.4  New Features PTX ISA version 6.4 introduces the following new features: Adds support for .noreturn directive which can be used to indicate a function does not return\nto it’s caller function. Adds support for mma instruction which allows performing matrix multiply-and-accumulate\noperation. Deprecated Features PTX ISA version 6.4 deprecates the following features: Support for .satfinite qualifier on floating point wmma.mma instruction. Removed Features PTX ISA version 6.4 removes the following features: Support for shfl and vote instructions without the .sync qualifier has been removed\nfor .target sm_70 and higher. This support was deprecated since PTX ISA version 6.0 as\ndocumented in PTX ISA version 6.2. Semantic Changes and Clarifications Clarified that resolving references of a .weak symbol considers only .weak or .visible symbols with the same name and does not consider local symbols with the same name. Clarified that in cvt instruction, modifier .ftz can only be specified when either .atype or .dtype is .f32 . 12.18. Changes in PTX ISA Version 6.3  New Features PTX ISA version 6.3 introduces the following new features: Support for sm_75 target architecture. Adds support for a new instruction nanosleep that suspends a thread for a specified duration. Adds support for .alias directive which allows definining alias to function symbol. Extends atom instruction to perform .f16 addition operation and .cas.b16 operation. Extends red instruction to perform .f16 addition operation. The wmma instructions are extended to support multiplicand matrices of type .s8 , .u8 , .s4 , .u4 , .b1 and accumulator matrices of type .s32 . Semantic Changes and Clarifications Introduced the mandatory .aligned qualifier for all wmma instructions. Specified the alignment required for the base address and stride parameters passed to wmma.load and wmma.store . Clarified that layout of fragment returned by wmma operation is architecture dependent and\npassing wmma fragments around functions compiled for different link compatible SM\narchitectures may not work as expected. Clarified that atomicity for {atom/red}.f16x2} operations is guranteed separately for each of\nthe two .f16 elements but not guranteed to be atomic as single 32-bit access. 12.19. Changes in PTX ISA Version 6.2  New Features PTX ISA version 6.2 introduces the following new features: A new instruction activemask for querying active threads in a warp. Extends atomic and reduction instructions to perform .f16x2 addition operation with mandatory .noftz qualifier. Deprecated Features PTX ISA version 6.2 deprecates the following features: The use of shfl and vote instructions without the .sync is deprecated retrospectively\nfrom PTX ISA version 6.0, which introduced the sm_70 architecture that implements Independent Thread Scheduling . Semantic Changes and Clarifications Clarified that wmma instructions can be used in conditionally executed code only if it is\nknown that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined. In the memory consistency model, the definition of morally strong operations was updated to\nexclude fences from the requirement of complete overlap since fences do not access memory. 12.20. Changes in PTX ISA Version 6.1  New Features PTX ISA version 6.1 introduces the following new features: Support for sm_72 target architecture. Support for new matrix shapes 32x8x16 and 8x32x16 in wmma instruction. Semantic Changes and Clarifications None. 12.21. Changes in PTX ISA Version 6.0  New Features PTX ISA version 6.0 introduces the following new features: Support for sm_70 target architecture. Specifies the memory consistency model for programs running on sm_70 and later architectures. Various extensions to memory instructions to specify memory synchronization semantics and scopes\nat which such synchronization can be observed. New instruction wmma for matrix operations which allows loading matrices from memory,\nperforming multiply-and-accumulate on them and storing result in memory. Support for new barrier instruction. Extends neg instruction to support .f16 and .f16x2 types. A new instruction fns which allows finding n-th set bit in integer. A new instruction bar.warp.sync which allows synchronizing threads in warp. Extends vote and shfl instructions with .sync modifier which waits for specified\nthreads before executing the vote and shfl operation respectively. A new instruction match.sync which allows broadcasting and comparing a value across threads in\nwarp. A new instruction brx.idx which allows branching to a label indexed from list of potential\ntargets. Support for unsized array parameter for .func which can be used to implement variadic\nfunctions. Support for .b16 integer type in dwarf-lines. Support for taking address of device function return parameters using mov instruction. Semantic Changes and Clarifications Semantics of bar instruction were updated to indicate that executing thread waits for other\nnon-exited threads from it’s warp. Support for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from\nthe spec. Support for taking address of labels, using labels in initializers which was unimplemented has\nbeen removed from the spec. Support for variadic functions which was unimplemented has been removed from the spec. 12.22. Changes in PTX ISA Version 5.0  New Features PTX ISA version 5.0 introduces the following new features: Support for sm_60 , sm_61 , sm_62 target architecture. Extends atomic and reduction instructions to perform double-precision add operation. Extends atomic and reduction instructions to specify scope modifier. A new .common directive to permit linking multiple object files containing declarations of the\nsame symbol with different size. A new dp4a instruction which allows 4-way dot product with accumulate operation. A new dp2a instruction which allows 2-way dot product with accumulate operation. Support for special register %clock_hi . Semantic Changes and Clarifications Semantics of cache modifiers on ld and st instructions were clarified to reflect cache\noperations are treated as performance hint only and do not change memory consistency behavior of the\nprogram. Semantics of volatile operations on ld and st instructions were clarified to reflect how volatile operations are handled by optimizing compiler. 12.23. Changes in PTX ISA Version 4.3  New Features PTX ISA version 4.3 introduces the following new features: A new lop3 instruction which allows arbitrary logical operation on 3 inputs. Adds support for 64-bit computations in extended precision arithmetic instructions. Extends tex.grad instruction to support cube and acube geometries. Extends tld4 instruction to support a2d , cube and acube geometries. Extends tex and tld4 instructions to support optional operands for offset vector and depth\ncompare. Extends txq instruction to support querying texture fields from specific LOD. Semantic Changes and Clarifications None. 12.24. Changes in PTX ISA Version 4.2  New Features PTX ISA version 4.2 introduces the following new features: Support for sm_53 target architecture. Support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types. Support for memory_layout field for surfaces and suq instruction support for querying this\nfield. Semantic Changes and Clarifications Semantics for parameter passing under ABI were updated to indicate ld.param and st.param instructions used for argument passing cannot be predicated. Semantics of {atom/red}.add.f32 were updated to indicate subnormal inputs and results are\nflushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on\nshared memory preserve subnormal inputs and results and don’t flush them to zero. 12.25. Changes in PTX ISA Version 4.1  New Features PTX ISA version 4.1 introduces the following new features: Support for sm_37 and sm_52 target architectures. Support for new fields array_size , num_mipmap_levels and num_samples for Textures, and\nthe txq instruction support for querying these fields. Support for new field array_size for Surfaces, and the suq instruction support for\nquerying this field. Support for special registers %total_smem_size and %dynamic_smem_size . Semantic Changes and Clarifications None. 12.26. Changes in PTX ISA Version 4.0  New Features PTX ISA version 4.0 introduces the following new features: Support for sm_32 and sm_50 target architectures. Support for 64bit performance counter special registers %pm0_64,..,%pm7_64 . A new istypep instruction. A new instruction, rsqrt.approx.ftz.f64 has been added to compute a fast approximation of the\nsquare root reciprocal of a value. Support for a new directive .attribute for specifying special attributes of a variable. Support for .managed variable attribute. Semantic Changes and Clarifications The vote instruction semantics were updated to clearly indicate that an inactive thread in a\nwarp contributes a 0 for its entry when participating in vote.ballot.b32 . 12.27. Changes in PTX ISA Version 3.2  New Features PTX ISA version 3.2 introduces the following new features: The texture instruction supports reads from multi-sample and multisample array textures. Extends .section debugging directive to include label + immediate expressions. Extends .file directive to include timestamp and file size information. Semantic Changes and Clarifications The vavrg2 and vavrg4 instruction semantics were updated to indicate that instruction adds 1\nonly if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than\nbeing divided by 2). 12.28. Changes in PTX ISA Version 3.1  New Features PTX ISA version 3.1 introduces the following new features: Support for sm_35 target architecture. Support for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work. ld.global.nc for loading read-only global data though the non-coherent texture cache. A new funnel shift instruction, shf . Extends atomic and reduction instructions to perform 64-bit {and, or, xor} operations, and\n64-bit integer {min, max} operations. Adds support for mipmaps . Adds support for indirect access to textures and surfaces. Extends support for generic addressing to include the .const state space, and adds a new\noperator, generic() , to form a generic address for .global or .const variables used in\ninitializers. A new .weak directive to permit linking multiple object files containing declarations of the\nsame symbol. Semantic Changes and Clarifications PTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer. PTX 3.1 code should either\ninclude explicit generic() operators in initializers, use cvta.global to form generic\naddresses at runtime, or load from the non-generic address using ld.global . Instruction mad.f32 requires a rounding modifier for sm_20 and higher targets. However for\nPTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently\ndefaults to mad.rn.f32 . For PTX ISA version 3.1, ptxas generates a warning and defaults to mad.rn.f32 , and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later. 12.29. Changes in PTX ISA Version 3.0  New Features PTX ISA version 3.0 introduces the following new features: Support for sm_30 target architectures. SIMD video instructions. A new warp shuffle instruction. Instructions mad.cc and madc for efficient, extended-precision integer multiplication. Surface instructions with 3D and array geometries. The texture instruction supports reads from cubemap and cubemap array textures. Platform option .target debug to declare that a PTX module contains DWARF debug information. pmevent.mask , for triggering multiple performance monitor events. Performance monitor counter special registers %pm4..%pm7 . Semantic Changes and Clarifications Special register %gridid has been extended from 32-bits to 64-bits. PTX ISA version 3.0 deprecates module-scoped .reg and .local variables when compiling to the\nApplication Binary Interface (ABI). When compiling without use of the ABI, module-scoped .reg and .local variables are supported as before. When compiling legacy PTX code (ISA versions prior\nto 3.0) containing module-scoped .reg or .local variables, the compiler silently disables\nuse of the ABI. The shfl instruction semantics were updated to clearly indicate that value of source operand a is unpredictable for inactive and predicated-off threads within the warp. PTX modules no longer allow duplicate .version directives. This feature was unimplemented, so\nthere is no semantic change. Unimplemented instructions suld.p and sust.p.{u32,s32,f32} have been removed. 12.30. Changes in PTX ISA Version 2.3  New Features PTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D\nor 2D textures, where an integer indexes into the array of textures, and then one or two\nsingle-precision floating point coordinates are used to address within the selected 1D or 2D\ntexture. PTX 2.3 adds a new directive, .address_size , for specifying the size of addresses. Variables in .const and .global state spaces are initialized to zero by default. Semantic Changes and Clarifications The semantics of the .maxntid directive have been updated to match the current\nimplementation. Specifically, .maxntid only guarantees that the total number of threads in a\nthread block does not exceed the maximum. Previously, the semantics indicated that the maximum was\nenforced separately in each dimension, which is not the case. Bit field extract and insert instructions BFE and BFI now indicate that the len and pos operands are restricted to the value range 0..255 . Unimplemented instructions {atom,red}.{min,max}.f32 have been removed. 12.31. Changes in PTX ISA Version 2.2  New Features PTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a\nnew directives for specifying that a kernel parameter is a pointer, for specifying to which state\nspace the parameter points, and for optionally specifying the alignment of the memory to which the\nparameter points. PTX 2.2 adds a new field named force_unnormalized_coords to the .samplerref opaque\ntype. This field is used in the independent texturing mode to override the normalized_coords field in the texture header. This field is needed to support languages such as OpenCL, which\nrepresent the property of normalized/unnormalized coordinates in the sampler header rather than in\nthe texture header. PTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the .const state space. Legacy PTX that uses explicit constant banks is still supported. PTX 2.2 adds a new tld4 instruction for loading a component ( r , g , b , or a ) from\nthe four texels compising the bilinear interpolation footprint of a given texture location. This\ninstruction may be used to compute higher-precision bilerp results in software, or for performing\nhigher-bandwidth texture loads. Semantic Changes and Clarifications None. 12.32. Changes in PTX ISA Version 2.1  New Features The underlying, stack-based ABI is supported in PTX ISA version 2.1 for sm_2x targets. Support for indirect calls has been implemented for sm_2x targets. New directives, .branchtargets and .calltargets , have been added for specifying potential\ntargets for indirect branches and indirect function calls. A .callprototype directive has been\nadded for declaring the type signatures for indirect function calls. The names of .global and .const variables can now be specified in variable initializers to\nrepresent their addresses. A set of thirty-two driver-specific execution environment special registers has been added. These\nare named %envreg0..%envreg31 . Textures and surfaces have new fields for channel data type and channel order, and the txq and suq instructions support queries for these fields. Directive .minnctapersm has replaced the .maxnctapersm directive. Directive .reqntid has been added to allow specification of exact CTA dimensions. A new instruction, rcp.approx.ftz.f64 , has been added to compute a fast, gross approximate\nreciprocal. Semantic Changes and Clarifications A warning is emitted if .minnctapersm is specified without also specifying .maxntid . 12.33. Changes in PTX ISA Version 2.0  New Features Floating Point Extensions This section describes the floating-point changes in PTX ISA version 2.0 for sm_20 targets. The\ngoal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility\nwith legacy PTX ISA version 1.x code and sm_1x targets. The changes from PTX ISA version 1.x are as follows: Single-precision instructions support subnormal numbers by default for sm_20 targets. The .ftz modifier may be used to enforce backward compatibility with sm_1x . Single-precision add , sub , and mul now support .rm and .rp rounding modifiers\nfor sm_20 targets. A single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754\ncompliant rounding modifiers and support for subnormal numbers. The fma.f32 instruction also\nsupports .ftz and .sat modifiers. fma.f32 requires sm_20 . The mad.f32 instruction has been extended with rounding modifiers so that it’s synonymous with fma.f32 for sm_20 targets. Both fma.f32 and mad.f32 require a rounding modifier for sm_20 targets. The mad.f32 instruction without rounding is retained so that compilers can generate code for sm_1x targets. When code compiled for sm_1x is executed on sm_20 devices, mad.f32 maps to fma.rn.f32 . Single- and double-precision div , rcp , and sqrt with IEEE 754 compliant rounding have\nbeen added. These are indicated by the use of a rounding modifier and require sm_20 . Instructions testp and copysign have been added. New Instructions A load uniform instruction, ldu , has been added. Surface instructions support additional .clamp modifiers, .clamp and .zero . Instruction sust now supports formatted surface stores. A count leading zeros instruction, clz , has been added. A find leading non-sign bit instruction , bfind , has been added. A bit reversal instruction, brev , has been added. Bit field extract and insert instructions, bfe and bfi , have been added. A population count instruction, popc , has been added. A vote ballot instruction, vote.ballot.b32 , has been added. Instructions {atom,red}.add.f32 have been implemented. Instructions {atom,red} .shared have been extended to handle 64-bit data types for sm_20 targets. A system-level membar instruction, membar.sys , has been added. The bar instruction has been extended as follows: A bar.arrive instruction has been added. Instructions bar.red.popc.u32 and bar.red.{and,or}.pred have been added. bar now supports optional thread count and register operands. Scalar video instructions (includes prmt ) have been added. Instruction isspacep for querying whether a generic address falls within a specified state space\nwindow has been added. Instruction cvta for converting global, local, and shared addresses to generic address and\nvice-versa has been added. Other New Features Instructions ld , ldu , st , prefetch , prefetchu , isspacep , cvta , atom ,\nand red now support generic addressing. New special registers %nwarpid , %nsmid , %clock64 , %lanemask_{eq,le,lt,ge,gt} have\nbeen added. Cache operations have been added to instructions ld , st , suld , and sust , e.g., for prefetching to specified level of memory hierarchy. Instructions prefetch and prefetchu have also been added. The .maxnctapersm directive was deprecated and replaced with .minnctapersm to better match\nits behavior and usage. A new directive, .section , has been added to replace the @@DWARF syntax for passing\nDWARF-format debugging information through PTX. A new directive, .pragma nounroll , has been added to allow users to disable loop unrolling. Semantic Changes and Clarifications The errata in cvt.ftz for PTX ISA versions 1.4 and earlier, where single-precision subnormal\ninputs and results were not flushed to zero if either source or destination type size was 64-bits,\nhas been fixed. In PTX ISA version 1.5 and later, cvt.ftz (and cvt for .target sm_1x ,\nwhere .ftz is implied) instructions flush single-precision subnormal inputs and results to\nsign-preserving zero for all combinations of floating-point instruction types. To maintain\ncompatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs\nand results are flushed to sign-preserving zero only when neither source nor destination type size\nis 64-bits. Components of special registers %tid , %ntid , %ctaid , and %nctaid have been extended\nfrom 16-bits to 32-bits. These registers now have type .v4.u32 . The number of samplers available in independent texturing mode was incorrectly listed as thirty-two\nin PTX ISA version 1.5; the correct number is sixteen. 14. Descriptions of .pragma Strings  This section describes the .pragma strings defined by ptxas. 14.1. Pragma Strings: “nounroll”  “nounroll” Disable loop unrolling in optimizing the backend compiler. Syntax .pragma \"nounroll\"; Description The \"nounroll\" pragma is a directive to disable loop unrolling in the optimizing backend\ncompiler. The \"nounroll\" pragma is allowed at module, entry-function, and statement levels, with the\nfollowing meanings: module scope disables unrolling for all loops in module, including loops preceding the .pragma . entry-function scope disables unrolling for all loops in the entry function body. statement-level pragma disables unrolling of the loop for which the current block is the loop header. Note that in order to have the desired effect at statement level, the \"nounroll\" directive must\nappear before any instruction statements in the loop header basic block for the desired loop. The\nloop header block is defined as the block that dominates all blocks in the loop body and is the\ntarget of the loop backedge. Statement-level \"nounroll\" directives appearing outside of loop\nheader blocks are silently ignored. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Ignored for sm_1x targets. Examples .entry foo (...)\n.pragma \"nounroll\";  // do not unroll any loop in this function\n{\n...\n}\n\n.func bar (...)\n{\n...\nL1_head:\n     .pragma \"nounroll\";  // do not unroll this loop\n     ...\n@p   bra L1_end;\nL1_body:\n     ...\nL1_continue:\n     bra L1_head;\nL1_end:\n     ...\n} 14.2. Pragma Strings: “used_bytes_mask”  “used_bytes_mask” Mask for indicating used bytes in data of ld operation. Syntax .pragma \"used_bytes_mask mask\"; Description The \"used_bytes_mask\" pragma is a directive that specifies used bytes in a load\noperation based on the mask provided. \"used_bytes_mask\" pragma needs to be specified prior to a load instruction for which\ninformation about bytes used from the load operation is needed.\nPragma is ignored if instruction following it is not a load instruction. For a load instruction without this pragma, all bytes from the load operation are assumed\nto be used. Operand mask is a 32-bit integer with set bits indicating the used bytes in data of\nload operation. Semantics Each bit in mask operand corresponds to a byte data where each set bit represents the used byte.\nMost-significant bit corresponds to most-significant byte of data.\n\n// For 4 bytes load with only lower 3 bytes used\n.pragma \"used_bytes_mask 0x7\";\nld.global.u32 %r0, [gbl];     // Higher 1 byte from %r0 is unused\n\n// For vector load of 16 bytes with lower 12 bytes used\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];  // %r3 unused PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_50 or higher. Examples .pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used 15. Notices  15.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 15.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 15.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html", "content_type": "text/html", "text": "CUDA for Tegra 1. CUDA for Tegra 2. Overview 3. Memory Management 3.1. I/O Coherency 3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device 4. Porting Considerations 4.1. Memory Selection 4.2. Pinned Memory 4.3. Effective Usage of Unified Memory on Tegra 4.4. GPU Selection 4.5. Synchronization Mechanism Selection 4.6. CUDA Features Not Supported on Tegra 5. EGL Interoperability 5.1. EGLStream 5.1.1. EGLStream Flow 5.1.2. CUDA as Producer 5.1.3. CUDA as Consumer 5.1.4. Implicit Synchronization 5.1.5. Data Transfer Between Producer and Consumer 5.1.6. EGLStream Pipeline 5.2. EGLImage 5.2.1. CUDA interop with EGLImage 5.3. EGLSync 5.3.1. CUDA Interop with EGLSync 5.3.2. Creating EGLSync from a CUDA Event 5.3.3. Creating a CUDA Event from EGLSync 6. CUDA Upgradable Package for Jetson 6.1. Installing the CUDA Upgrade Package 6.1.1. Prerequisite 6.1.2. From Network Repositories or Local Installers 6.2. Deployment Considerations for CUDA Upgrade Package 6.2.1. Use the Right Upgrade Package 6.2.2. Feature Exceptions 6.2.3. Check for Compatibility Support 7. cuDLA 7.1. Developer Guide 7.1.1. Device Model 7.1.2. Loading and Querying Modules 7.1.3. Memory Model 7.1.4. Task Execution and Synchronization Model 7.1.4.1. Task Execution 7.1.4.1.1. Multithreaded User Submission 7.1.4.2. Synchronization 7.1.4.2.1. Registering an external semaphore: 7.1.4.2.2. Events setup for cudlaSubmitTask() 7.1.4.2.3. Waiting on the signal event 7.1.4.2.4. Supported Synchronization Primitives in cuDLA 7.1.4.2.5. Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList 7.1.4.2.6. Timestamp Support for NvSciFence 7.1.4.2.7. Requesting Timestamp Support for NvSciSync Object 7.1.4.2.8. Extracting Timestamp Value from Fence 7.1.4.3. Fault Diagnostics 7.1.4.4. NOOP Submission 7.1.5. Error Reporting Model 7.2. Migrating from NvMediaDla to cuDLA 7.3. Profiling a cuDLA App 7.4. cuDLA Release Notes 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks CUDA for Tegra » 1. CUDA for Tegra v12.5 | PDF | Archive 1. CUDA for Tegra  This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability. 2. Overview  This document provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability. This guide is for developers who are already familiar with programming in CUDA®, and C/C++, and who want to develop applications for the Tegra® SoC. Performance guidelines, best practices, terminology, and general information provided in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide are applicable to all CUDA-capable GPU architectures, including Tegra® devices. The CUDA C++ Programming Guide and the CUDA C Best Practices Guide are available at the following web sites: CUDA C++ Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA C++ Best Practices Guide: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html 3. Memory Management  In Tegra® devices, both the CPU (Host) and the iGPU share SoC DRAM memory. A dGPU with separate DRAM memory can be connected to the Tegra device over PCIe or NVLink. It is currently supported only on the NVIDIA DRIVE platform. An overview of a dGPU-connected Tegra® memory system is shown in Figure 1 . dGPU-connected Tegra Memory System  In Tegra, device memory, host memory, and unified memory are allocated on the same physical SoC DRAM. On a dGPU, device memory is allocated on the dGPU DRAM. The caching behavior in a Tegra system is different from that of an x86 system with a dGPU. The caching and accessing behavior of different memory types in a Tegra system is shown in Table 1 . Table 1. Characteristics of Different Memory Types in a Tegra System  Memory Type CPU iGPU Tegra-connected dGPU Device memory Not directly accessible Cached Cached Pageable host memory Cached Not directly accessible Not directly accessible Pinned host memory Uncached where compute capability is less than 7.2. Cached where compute capability is greater than or equal to 7.2. Uncached Uncached Unified memory Cached Cached Not supported On Tegra, because device memory, host memory, and unified memory are allocated on the same physical SoC DRAM, duplicate memory allocations and data transfers can be avoided. 3.1. I/O Coherency  I/O coherency (also known as one-way coherency) is a feature with which an I/O device such as a GPU can read the latest updates in CPU caches. It removes the need to perform CPU cache management operations when the same physical memory is shared between CPU and GPU. The GPU cache management operations still need to be performed because the coherency is one way. Please note that the CUDA driver internally performs the GPU cache management operations when managed memory or interop memory is used. I/O coherency is supported on Tegra devices starting with Xavier SOC. Applications should realize benefits from this HW feature without needing to make changes to the application’s code (see point 2 below). The following functionalities depend on I/O coherency support: cudaHostRegister() / cuMemHostRegister() is supported only on platforms which are I/O coherent. The host register support can be queried using the device attribute cudaDevAttrHostRegisterSupported / CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED. CPU cache for pinned memory allocated using cudaMallocHost() / cuMemHostAlloc() / cuMemAllocHost() is enabled only on platforms which are I/O coherent. 3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device  The cudaMemGetInfo() API returns the snapshot of free and total amount of memory available for allocation for the GPU. The free memory could change if any other client allocate memory. The discrete GPU has the dedicated DRAM called VIDMEM which is separate from CPU memory. The snapshot of free memory in discrete GPU is returned by the cudaMemGetInfo API. The integrated GPU, on Tegra SoC, shares the DRAM with CPU and other the Tegra engines. The CPU can control the contents of DRAM and free DRAM memory by moving the contents of DMAR to SWAP area or vice versa. The cudaMemGetInfo API currently does not account for SWAP memory area. The cudaMemGetInfo API may return a smaller size than the actually allocatable memory since the CPU may be able to free up some DRAM region by moving pages to the SWAP area. In order to estimate the amount of allocatable device memory, CUDA application developers should consider following: On Linux and Android platforms: Device allocatable memory on Linux and Android depends mainly on the total and free sizes of swap space and main memory. The following points can help users to estimate the total amount of device allocatable memory in various situations: Host allocated memory = Total used physical memory – Device allocated memory If (Host allocated memory < Free Swap Space) then Device allocatable memory = Total Physical Memory – already allocated device memory If (Host allocated memory > Free Swap Space) then Device allocatable memory = Total Physical Memory – (Host allocated memory - Free swap space) Here, Device allocated memory is memory already allocated on the device. It can be obtained from the NvMapMemUsed field in /proc/meminfo or from the total field of /sys/kernel/debug/nvmap/iovmm/clients . Total used physical memory can be obtained using the free -m command. The used field in row Mem represents this information. Total Physical memory is obtained from the MemTotal field in /proc/meminfo . Free swap space can be find by using the free -m command. The free field in the Swap row represents this information. If the free command is not available, the same information can be obtained from /proc/meminfo as: Total Used physical memory = MemTotal – MemFree Free swap space = SwapFree On QNX platforms: QNX does not use swap space, hence, cudaMemGetInfo.free will be a fair estimate of allocatable device memory as there is no swap space to move memory pages to swap area. 4. Porting Considerations  CUDA applications originally developed for dGPUs attached to x86 systems may require modifications to perform efficiently on Tegra systems. This section describes the considerations for porting such applications to a Tegra system, such as selecting an appropriate memory buffer type (pinned memory, unified memory, and others) and selecting between iGPU and dGPU, to achieve efficient performance for the application. 4.1. Memory Selection  CUDA applications can use various kinds of memory buffers, such as device memory, pageable host memory, pinned memory, and unified memory. Even though these memory buffer types are allocated on the same physical device, each has different accessing and caching behaviors, as shown in Table 1 . It is important to select the most appropriate memory buffer type for efficient application execution. Device Memory Use device memory for buffers whose accessibility is limited to the iGPU. For example, in an application with multiple kernels, there may be buffers that are used only by the intermediate kernels of the application as input or output. These buffers are accessed only by the iGPU. Such buffers should be allocated with device memory. Pageable Host Memory Use pageable host memory for buffers whose accessibility is limited to the CPU. Pinned Memory Tegra® systems with different compute capabilities exhibit different behavior in terms of I/O coherency. For example, Tegra® systems with compute capability greater than or equal to 7.2 are I/O coherent and others are not I/O coherent. On Tegra® systems with I/O coherency, the CPU access time of pinned memory is as good as pageable host memory because it is cached on the CPU. However, on Tegra® systems without I/O coherency, the CPU access time of pinned memory is higher, because it is not cached on the CPU. Pinned memory is recommended for small buffers because the caching effect is negligible for such buffers and also because pinned memory does not involve any additional overhead, unlike Unified Memory. With no additional overhead, pinned memory is also preferable for large buffers if the access pattern is not cache friendly on iGPU. For large buffers, when the buffer is accessed only once on iGPU in a coalescing manner, performance on iGPU can be as good as unified memory on iGPU. Unified Memory Unified memory is cached on the iGPU and the CPU. On Tegra®, using unified memory in applications requires additional coherency and cache maintenance operations during the kernel launch, synchronization and prefetching hint calls. This coherency maintenance overhead is slightly higher on a Tegra® system with compute capability less than 7.2 as they lack I/O coherency. On Tegra® devices with I/O coherency (with a compute capability of 7.2 or greater) where unified memory is cached on both CPU and iGPU, for large buffers which are frequently accessed by the iGPU and the CPU and the accesses on iGPU are repetitive , unified memory is preferable since repetitive accesses can offset the cache maintenance cost. On Tegra® devices without I/O coherency (with a compute capability of less than 7.2), for large buffers which are frequently accessed by the CPU and the iGPU and the accesses on iGPU are not repetitive , unified memory is still preferable over pinned memory because pinned memory is not cached on both CPU and iGPU. That way, the application can take advantage of unified memory caching on the CPU. Pinned memory or unified memory can be used to reduce the data transfer overhead between CPU and iGPU as both memories are directly accessible from the CPU and the iGPU. In an application, input and output buffers that must be accessible on both the host and the iGPU can be allocated using either unified memory or pinned memory. Note The unified memory model requires the driver and system software to manage coherence on the current Tegra SOC. Software managed coherence is by nature non-deterministic and not recommended in a safe context. Zero-copy memory (pinned memory) is preferable in these applications. Evaluate the impact of unified memory overheads, pinned memory cache misses, and device memory data transfers in applications to determine the correct memory selection. 4.2. Pinned Memory  This section provides guidelines for porting applications that use pinned memory allocations in x86 systems with dGPUs to Tegra®. CUDA applications developed for a dGPU attached to x86 system use pinned memory to reduce data transfer time and to overlap data transfers with kernel execution time. For specific information on this topic, see “Data Transfer Between Host and Device” and “Asynchronous and Overlapping Transfers with Computation” at the following websites. “Data Transfer Between Host and Device”: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device “Asynchronous and Overlapping Transfers with Computation”: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation On Tegra® systems with no I/O coherency, repetitive access of pinned memory degrades application performance, because pinned memory is not cached on the CPU in such systems. A sample application is shown below in which a set of filters and operations (k1, k2, and k3) are applied to an image. Pinned memory is allocated to reduce data transfer time on an x86 system with a dGPU, increasing the overall application speed. However, targeting a Tegra® device with the same code causes a drastic increase in the execution time of the readImage() function because it repeatedly accesses an uncached buffer. This increases the overall application time. If the time taken by readImage() is significantly higher compared to kernels execution time, it is recommended to use unified memory to reduce the readImage() time. Otherwise, evaluate the application with pinned memory and unified memory by removing unnecessary data transfer calls to decide best suited memory. // Sample code for an x86 system with a discrete GPU int main () { int * h_a , * d_a , * d_b , * d_c , * d_d , * h_d ; int height = 1024 ; int width = 1024 ; size_t sizeOfImage = width * height * sizeof ( int ); // 4MB image //Pinned memory allocated to reduce data transfer time cudaMallocHost ( h_a , sizeOfImage ); cudaMallocHost ( h_d , sizeOfImage ); //Allocate buffers on GPU cudaMalloc ( & d_a , sizeOfImage ); cudaMalloc ( & d_b , sizeOfImage ); cudaMalloc ( & d_c , sizeOfImage ); cudaMalloc ( & d_d , sizeOfImage ); //CPU reads Image; readImage ( h_a ); // Intialize the h_a buffer // Transfer image to GPU cudaMemcpy ( d_a , h_a , sizeOfImage , cudaMemcpyHostToDevice ); // Data transfer is fast as we used pinned memory // ----- CUDA Application pipeline start ---- k1 <<< .. >>> ( d_a , d_b ) // Apply filter 1 k2 <<< .. >>> ( d_b , d_c ) // Apply filter 2 k3 <<< .. >>> ( d_c , d_d ) // Some operation on image data // ----- CUDA Application pipeline end ---- // Transfer processed image to CPU cudaMemcpy ( h_d , d_d , sizeOfImage , cudaMemcpyDeviceToHost ); // Data transfer is fast as we used pinned memory // Use processed Image i.e h_d in later computations on CPU. UseImageonCPU ( h_d ); } // Porting the code on Tegra int main () { int * h_a , * d_b , * d_c , * h_d ; int height = 1024 ; int width = 1024 ; size_t sizeOfImage = width * height * sizeof ( int ); // 4MB image //Unified memory allocated for input and output //buffer of application pipeline cudaMallocManaged ( h_a , sizeOfImage , cudaMemAttachHost ); cudaMallocManaged ( h_d , sizeOfImage ); //Intermediate buffers not needed on CPU side. //So allocate them on device memory cudaMalloc ( & d_b , sizeOfImage ); cudaMalloc ( & d_c , sizeOfImage ); //CPU reads Image; readImage ( h_a ); // Intialize the h_a buffer // ----- CUDA Application pipeline start ---- // Prefetch input image data to GPU cudaStreamAttachMemAsync ( NULL , h_a , 0 , cudaMemAttachGlobal ); k1 <<< .. >>> ( h_a , d_b ) k2 <<< .. >>> ( d_b , d_c ) k3 <<< .. >>> ( d_c , h_d ) // Prefetch output image data to CPU cudaStreamAttachMemAsync ( NULL , h_d , 0 , cudaMemAttachHost ); cudaStreamSynchronize ( NULL ); // ----- CUDA Application pipeline end ---- // Use processed Image i.e h_d on CPU side. UseImageonCPU ( h_d ); } The cudaHostRegister() function The cudaHostRegister() function is not supported on Tegra® devices with compute capability less than 7.2, because those devices do not have I/O coherency. Use other pinned memory allocation functions such as cudaMallocHost() and cudaHostAlloc() if cudaHostRegister() is not supported on the device. GNU Atomic operations on pinned memory The GNU atomic operations on uncached memory is not supported on Tegra® CPU. As pinned memory is not cached on Tegra® devices with compute capability less than 7.2, GNU atomic operations is not supported on pinned memory. 4.3. Effective Usage of Unified Memory on Tegra  Using unified memory in applications requires additional coherency and cache maintenance operations at kernel launch, synchronization, and prefetching hint calls. These operations are performed synchronously with other GPU work which can cause unpredictable latencies in the application. The performance of unified memory on Tegra® can be improved by providing data prefetching hints. The driver can use these prefetching hints to optimize the coherence operations. To prefetch the data, the cudaStreamAttachMemAsync() function can be used, in addition to the techniques described in the “Coherency and Concurrency” section of the CUDA C Programming Guide at the following link: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd to prefetch the data. The prefetching behavior of unified memory, as triggered by the changing states of the attachment flag, is shown in Table 2 . Table 2. Unified Memory Prefetching Behavior per Changing Attachment Flag States  Previous Flag Current Flag Prefetching Behavior cudaMemAttachGlobal/cudaMemAttachSingle cudaMemAttachHost Causes prefetch to CPU cudaMemAttachHost cudaMemAttachGlobal/ cudaMemAttachSingle Causes prefetch to GPU cudaMemAttachGlobal cudaMemAttachSingle No prefetch to GPU cudaMemAttachSingle cudaMemAttachGlobal No prefetch to GPU The following example shows usage of cudaStreamAttachMemAsync() to prefetch data. Note However, not supported on Tegra® devices are the data prefetching techniques that use cudaMemPrefetchAsync() as described in the “Performance Tuning” section of the CUDA C++ Programming Guide at the following web site: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning Note There are limitations in QNX system software which prevent implementation of all UVM optimizations. Because of this, using cudaStreamAttachMemAsync() to prefetch hints on QNX does not benefit performance. __global__ void matrixMul ( int * p , int * q , int * r , int hp , int hq , int wp , int wq ) { // Matrix multiplication kernel code } void MatrixMul ( int hp , int hq , int wp , int wq ) { int * p , * q , * r ; int i ; size_t sizeP = hp * wp * sizeof ( int ); size_t sizeQ = hq * wq * sizeof ( int ); size_t sizeR = hp * wq * sizeof ( int ); //Attach buffers 'p' and 'q' to CPU and buffer 'r' to GPU cudaMallocManaged ( & p , sizeP , cudaMemAttachHost ); cudaMallocManaged ( & q , sizeQ , cudaMemAttachHost ); cudaMallocManaged ( & r , sizeR ); //Intialize with random values randFill ( p , q , hp , wp , hq , wq ); // Prefetch p,q to GPU as they are needed in computation cudaStreamAttachMemAsync ( NULL , p , 0 , cudaMemAttachGlobal ); cudaStreamAttachMemAsync ( NULL , q , 0 , cudaMemAttachGlobal ); matrixMul <<< .... >>> ( p , q , r , hp , hq , wp , wq ); // Prefetch 'r' to CPU as only 'r' is needed cudaStreamAttachMemAsync ( NULL , r , 0 , cudaMemAttachHost ); cudaStreamSynchronize ( NULL ); // Print buffer 'r' values for ( i = 0 ; i < hp * wq ; i ++ ) printf ( \"%d \" , r [ i ]); } Note An additional cudaStreamSynchronize(NULL) call can be added after the matrixMul kernel code to avoid callback threads that cause unpredictability in a cudaStreamAttachMemAsync() call. 4.4. GPU Selection  On a Tegra system with a dGPU, deciding whether a CUDA application runs on the iGPU or the dGPU can have implications for the performance of the application. Some of the factors that need to be considered while making such a decision are kernel execution time, data transfer time, data locality, and latency. For example, to run an application on a dGPU, data must be transferred between the SoC and the dGPU. This data transfer can be avoided if the application runs on an iGPU. 4.5. Synchronization Mechanism Selection  The cudaSetDeviceFlags API is used to control the synchronization behaviour of CPU thread. Until CUDA 10.1, by default, the synchronization mechanism on iGPU uses cudaDeviceBlockingSync flag, which blocks the CPU thread on a synchronization primitive when waiting for the device to finish work. The cudaDeviceBlockingSync flag is suited for platforms with power constraints. But on platforms which requires low latency, cudaDeviceScheduleSpin flag needs to set manually. Since CUDA 10.1, for each platform, the default synchronization flag is determined based on what is optimized for that platform. More information about the synchronization flags is given at cudaSetDeviceFlags API documentation. 4.6. CUDA Features Not Supported on Tegra  All core features of CUDA are supported on Tegra platforms. The exceptions are listed below. The cudaHostRegister() function is not supported on QNX systems. This is due to the limitations on QNX OS. It is supported in Linux systems with compute capability greater than or equal to 7.2. System wide atomics are not supported on Tegra devices with compute capability less than 7.2. Unified memory is not supported on dGPU attached to Tegra. cudaMemPrefetchAsync() function is not supported since unified memory with concurrent access is not yet supported on iGPU. NVIDIA management library (NVML) library is not supported on Tegra. However, as an alternative to monitor the resource utilization, tegrastats can be used. Since CUDA 11.5, only events-sharing IPC APIs are supported on L4T and embedded Linux Tegra devices with compute capability 7.x and higher. The memory-sharing IPC APIs are still not supported on Tegra platforms. EGLStream, NvSci, or the cuMemExportToShareableHandle() / cuMemImportFromShareableHandle() APIs can be used to communicate between CUDA contexts in two processes. Remote direct memory access (RDMA) is supported only on Tegra devices running L4T or embedded-linux. JIT compilation might require a considerable amount of CPU and bandwidth resources, potentially interfering with other workloads in the system. Thus, JIT compilations such as PTX-JIT and NVRTC JIT are not recommended for deterministic embedded applications and can be bypassed completely by compiling for specific GPU targets. For example: If you are compiling for SM version 87, use this nvcc flag --generate-code arch=compute_87,code=sm_87 to create a CUDA binary for this device. This avoids JIT compilation during the first run and improves the runtime performance.\nJIT compilation is not supported on Tegra devices in the safe context. Multi process service (MPS) is not supported on Tegra. Peer to peer (P2P) communication calls are not supported on Tegra. The cuSOLVER library is not supported on Tegra systems running QNX. The nvGRAPH library is not supported. CUB is experimental on Tegra products. More information on some of these features can be found at the following web sites: IPC: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#interprocess-communication NVSCI: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#nvidia-softwarcommunication-interface-interoperability-nvsci RDMA: https://docs.nvidia.com/cuda/gpudirect-rdma/index.html MPS: https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf P2P: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#peer-to-peer-memory-access 5. EGL Interoperability  An interop is an efficient mechanism to share resources between two APIs. To share data with multiple APIs, an API must implement an individual interop for each. EGL provides interop extensions that allow it to function as a hub connecting APIs, removing the need for multiple interops, and encapsulating the shared resource. An API must implement these extensions to interoperate with any other API via EGL. The CUDA supported EGL interops are EGLStream, EGLImage, and EGLSync. EGL interop extensions allow applications to switch between APIs without the need to rewrite code. For example, an EGLStream-based application in which NvMedia is the producer and CUDA is the consumer can be modified to use OpenGL as the consumer without modifying the producer code. Note On the DRIVE OS platform, NVSCI is provided as an alternative to EGL interoperability for safety critical applications. Please see NVSCI for more details. 5.1. EGLStream  EGLStream interoperability facilitates efficient transfer of a sequence of frames from one API to another API, allowing use of multiple Tegra® engines such as CPU, GPU, ISP, and others. Consider an application where a camera captures images continuously, shares them with CUDA for processing, and then later renders those images using OpenGL. In this application, the image frames are shared across NvMedia, CUDA and OpenGL. The absence of EGLStream interoperability would require the application to include multiple interops and redundant data transfers between APIs. EGLStream has one producer and one consumer. EGLStream offers the following benefits: Efficient transfer of frames between a producer and a consumer. Implicit synchronization handling. Cross-process support. dGPU and iGPU support. Linux, QNX, and Android operating system support. 5.1.1. EGLStream Flow  The EGLStream flow has the following steps: Initialize producer and consumer APIs Create an EGLStream and connect the consumer and the producer. Note EGLStream is created using eglCreateStreamKHR() and destroyed using eglDestroyStreamKHR() . The consumer should always connect to EGLStream before the producer. For more information see the EGLStream specification at the following web site: https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_stream.txt Allocate memory used for EGL frames. The producer populates an EGL frame and presents it to EGLStream. The consumer acquires the frame from EGLStream and releases it back to EGLStream after processing. The producer collects the consumer-released frame from EGLStream. The producer presents the same frame, or a new frame to EGLStream. Steps 4-7 are repeated until completion of the task, with an old frame or a new frame. The consumer and the producer disconnect from EGLStream. Deallocate the memory used for EGL frames. De-initialize the producer and consumer APIs. EGLStream application flow is shown in Figure 2 . EGLStream Flow  CUDA producer and consumer functions are listed in Table 3 . Table 3. CUDA Producer and Consumer Functions  Role Functionality API Producer To connect a producer to EGLStream cuEGLStreamProducerConnect () cudaEGLStreamProducerConnect () To present frame to EGLStream cuEGLStreamProducerPresentFrame() cudaEGLStreamProducerPresentFrame() Obtain released frames cuEGLStreamProducerReturnFrame() cudaEGLStreamProducerReturnFrame() To disconnect from EGLStream cuEGLStreamProducerDisconnect () cudaEGLStreamProducerDisconnect () Consumer To connect a consumer to EGLStream cuEGLStreamConsumerConnect() cuEGLStreamConsumeConnectWithFlags() cudaEGLStreamConsumerConnect () cudaEGLStreamConsumerConnectWithFlags() To acquire frame from EGLStream cuEGLStreamConsumerAcquireFrame() cudaEGLStreamConsumerAcquireFrame () To release the consumed frame cuEGLStreamConsumerReleaseFrame() cudaEGLStreamConsumerReleaseFrame() To disconnect from EGLStream cuEGLStreamConsumerDisconnect () cudaEGLStreamConsumerDisconnect () 5.1.2. CUDA as Producer  When CUDA is the producer, the supported consumers are CUDA, NvMedia and OpenGL. API functions to be used when CUDA is the producer are listed in Table 3 . Except for connecting and disconnecting from EGLStream, all API calls are non-blocking. The following producer side steps are shown in the example code that follows: Prepare a frame (lines 3-19). Connect the producer to EGLStream (line 21). Populate the frame and present to EGLStream (lines 23-25). Get the released frame back from EGLStream (Line 27). Disconnect the consumer after completion of the task. (Line 31). void ProducerThread(EGLStreamKHR eglStream) {\n //Prepares frame\n cudaEglFrame* cudaEgl = (cudaEglFrame *)malloc(sizeof(cudaEglFrame));\n cudaEgl->planeDesc[0].width = WIDTH;\n cudaEgl->planeDesc[0].depth = 0;\n cudaEgl->planeDesc[0].height = HEIGHT;\n cudaEgl->planeDesc[0].numChannels = 4;\n cudaEgl->planeDesc[0].pitch = WIDTH * cudaEgl->planeDesc[0].numChannels;\n cudaEgl->frameType = cudaEglFrameTypePitch;\n cudaEgl->planeCount = 1;\n cudaEgl->eglColorFormat = cudaEglColorFormatARGB;\n cudaEgl->planeDesc[0].channelDesc.f=cudaChannelFormatKindUnsigned\n cudaEgl->planeDesc[0].channelDesc.w = 8;\n cudaEgl->planeDesc[0].channelDesc.x = 8;\n cudaEgl->planeDesc[0].channelDesc.y = 8;\n cudaEgl->planeDesc[0].channelDesc.z = 8;\n size_t numElem = cudaEgl->planeDesc[0].pitch * cudaEgl->planeDesc[0].height;\n // Buffer allocated by producer\n cudaMalloc(&(cudaEgl->pPitch[0].ptr), numElem);\n //CUDA producer connects to EGLStream\n cudaEGLStreamProducerConnect(&conn, eglStream, WIDTH, HEIGHT))\n // Sets all elements in the buffer to 1\n K1<<<...>>>(cudaEgl->pPitch[0].ptr, 1, numElem);\n // Present frame to EGLStream\n cudaEGLStreamProducerPresentFrame(&conn, *cudaEgl, NULL);\n\n cudaEGLStreamProducerReturnFrame(&conn, cudaEgl, eglStream);\n .\n .\n //clean up\n cudaEGLStreamProducerDisconnect(&conn);\n\n .\n} A frame is represented as a cudaEglFramestructure . The frameType parameter in cudaEglFrame indicates the memory layout of the frame. The supported memory layouts are CUDA Array and device pointer. Any mismatch in the width and height values of frame with the values specified in cudaEGLStreamProducerConnect() leads to undefined behavior. In the sample, the CUDA producer is sending a single frame, but it can send multiple frames over a loop. CUDA cannot present more than 64 active frames to EGLStream. The cudaEGLStreamProducerReturnFrame() call waits until it receives the released frame from the consumer. Once the CUDA producer presents the first frame to EGLstream, at least one frame is always available for consumer acquisition until the producer disconnects. This prevents the removal of the last frame from EGLStream, which would block cudaEGLStreamProducerReturnFrame (). Use the EGL_NV_stream_reset extension to set EGLStream attribute EGL_SUPPORT_REUSE_NV to false to allow the last frame to be removed from EGLStream. This allows removing or returning the last frame from EGLStream. 5.1.3. CUDA as Consumer  When CUDA is the consumer, the supported producers are CUDA, OpenGL, NvMedia, Argus, and Camera. API functions to be used when CUDA is the consumer are listed in Table 3. Except for connecting and disconnecting from EGLStream, all API calls are non-blocking. The following consumer side steps are shown in the sample code that follows: Connect consumer to EGLStream (line 5). Acquire frame from EGLStream (lines 8-10). Process the frame on consumer (line 16). Release frame back to EGLStream (line 19). Disconnect the consumer after completion of the task (line 22). void ConsumerThread(EGLStreamKHR eglStream) {\n.\n.\n//Connect consumer to EGLStream\ncudaEGLStreamConsumerConnect(&conn, eglStream);\n// consumer acquires a frame\nunsigned int timeout = 16000;\ncudaEGLStreamConsumerAcquireFrame(& conn, &cudaResource, eglStream, timeout);\n//consumer gets a cuda object pointer\ncudaGraphicsResourceGetMappedEglFrame(&cudaEgl, cudaResource, 0, 0);\nsize_t numElem = cudaEgl->planeDesc[0].pitch * cudaEgl->planeDesc[0].height;\n.\n.\nint checkIfOne = 1;\n// Checks if each value in the buffer is 1, if any value is not 1, it sets checkIfOne = 0.\nK2<<<...>>>(cudaEgl->pPitch[0].ptr, 1, numElem, checkIfOne);\n.\n.\ncudaEGLStreamConsumerReleaseFrame(&conn, cudaResource, &eglStream);\n.\n.\ncudaEGLStreamConsumerDisconnect(&conn);\n.\n} In the sample code, the CUDA consumer receives a single frame, but it can also receive multiple frames over a loop. If a CUDA consumer fails to receive a new frame in the specified time limit using cudaEGLStreamConsumerAcquireFrame() , it reacquires the previous frame from EGLStream. The time limit is indicated by the timeout parameter. The application can use eglQueryStreamKHR() to query for the availability of new frames using. If the consumer uses already released frames, it results in undefined behavior. The consumer behavior is defined only for read operations. Behavior is undefined when the consumer writes to a frame. If the CUDA context is destroyed while connected to EGLStream, the stream is placed in the EGL_STREAM_STATE_DISCONNECTED_KHR state and the connection handle is invalidated. 5.1.4. Implicit Synchronization  EGLStream provides implicit synchronization in an application. For example, in the previous code samples, both the producer and consumer threads are running in parallel and the K1 and K2 kernel processes access the same frame, but K2 execution in the consumer thread is guaranteed to occur only after kernel K1 in the producer thread finishes. The cudaEGLStreamConsumerAcquireFrame() function waits on the GPU side until K1 finishes and ensures synchronization between producer and consumer. The variable checkIfOne is never set to 0 inside the K2 kernel in the consumer thread. Similarly, cudaEGLStreamProducerReturnFrame() in the producer thread is guaranteed to get the frame only after K2 finishes and the consumer releases the frame. These non-blocking calls allow the CPU to do other computation in between, as synchronization is taken care of on the GPU side. The EGLStreams_CUDA_Interop CUDA sample code shows the usage of EGLStream in detail. 5.1.5. Data Transfer Between Producer and Consumer  Data transfer between producer and consumer is avoided when they are present on the same device. In a Tegra® platform that includes a dGPU however, such as is in NVIDIA DRIVE™ PX 2, the producer and consumer can be present on different devices. In that case, an additional memory copy is required internally to move the frame between Tegra® SoC DRAM and dGPU DRAM. EGLStream allows producer and consumer to run on any GPU without code modification. Note On systems where a Tegra® device is connected to a dGPU, if a producer frame uses CUDA array, both producer and consumer should be on the same GPU. But if a producer frame uses CUDA device pointers, the consumer can be present on any GPU. 5.1.6. EGLStream Pipeline  An application can use multiple EGL streams in a pipeline to pass the frames from one API to another. For an application where NvMedia sends a frame to CUDA for computation, CUDA sends the same frame to OpenGL for rendering after the computation. The EGLStream pipeline is illustrated in Figure 3 . EGLStream Pipeline  NvMedia and CUDA connect as producer and consumer respectively to one EGLStream. CUDA and OpenGL connect as producer and consumer respectively to another EGLStream. Using multiple EGLStreams in pipeline fashion gives the flexibility to send frames across multiple APIs without allocating additional memory or requiring explicit data transfers. Sending a frame across the above EGLStream pipeline involves the following steps. NvMedia sends a frame to CUDA for processing. CUDA uses the frame for computation and sends to OpenGL for rendering. OpenGL consumes the frame and releases it back to CUDA. CUDA releases the frame back to NvMedia. The above steps can be performed in a loop to facilitate the transfer of multiple frames in the EGLStream pipeline. 5.2. EGLImage  An EGLImage interop allows an EGL client API to share image data with other EGL client APIs. For example, an application can use an EGLImage interop to share an OpenGL texture with CUDA without allocating any additional memory. A single EGLImage object can be shared across multiple client APIs for modification. An EGLImage interop does not provide implicit synchronization. Applications must maintain synchronization to avoid race conditions. Note An EGLImage is created using eglCreateImageKHR() and destroyed using eglDestroyImageKHR() . For more information see the EGLImage specification at the following web site: https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_image_base.txt 5.2.1. CUDA interop with EGLImage  CUDA supports interoperation with EGLImage, allowing CUDA to read or modify the data of an EGLImage. An EGLImage can be a single or multi-planar resource. In CUDA, a single-planar EGLImage object is represented as a CUDA array or device pointer. Similarly, a multi-planar EGLImage object is represented as an array of device pointers or CUDA arrays. EGLImage is supported on Tegra® devices running the Linux, QNX, or Android operating systems. Use the cudaGraphicsEGLRegisterImage() API to register an EGLImage object with CUDA. Registering an EGLImage with CUDA creates a graphics resource object. An application can use cudaGraphicsResourceGetMappedEglFrame() to get a frame from the graphics resource object. In CUDA, a frame is represented as a cudaEglFrame structure. The frameType parameter in cudaEglFrame indicates if the frame is a CUDA device pointer or a CUDA array. For a single planar graphics resource, an application can directly obtain a device pointer or CUDA array using cudaGraphicsResourceGetMappedPointer() or cudaGraphicsSubResourceGetMappedArray() respectively. A CUDA array can be bound to a texture or surface reference to access inside a kernel. Also, a multi-dimensional CUDA array can be read and written via cudaMemcpy3D() . Note An EGLImage cannot be created from a CUDA object. The cudaGraphicsEGLRegisterImage() function is only supported on Tegra® devices. Also, cudaGraphicsEGLRegisterImage() expects only the ‘0’ flag as other API flags are for future use. The following sample code shows EGLImage interoperability. In the code, an EGLImage object eglImage is created using OpenGL texture. The eglImage object is mapped as a CUDA array pArray in CUDA. The pArray array is bound to a surface object to allow modification of the OpenGL texture in the changeTexture. The function checkBuf() checks if the texture is updated with new values. int width = 256;\nint height = 256;\nint main()\n{\n .\n .\n unsigned char *hostSurf;\n unsigned char *pSurf;\n CUarray pArray;\n unsigned int bufferSize = WIDTH * HEIGHT * 4;\n pSurf= (unsigned char *)malloc(bufferSize); hostSurf = (unsigned char *)malloc(bufferSize);\n // Initialize the buffer\n for(int y = 0; y < HEIGHT; y++)\n {\n    for(int x = 0; x < WIDTH; x++)\n    {\n    pSurf[(y*WIDTH + x) * 4 ] = 0; pSurf[(y*WIDTH + x) * 4 + 1] = 0;\n    pSurf[(y*WIDTH + x) * 4 + 2] = 0; pSurf[(y*WIDTH + x) * 4 + 3] = 0;\n    }\n }\n\n // NOP call to error-check the above glut calls\n GL_SAFE_CALL({});\n\n //Init texture\n GL_SAFE_CALL(glGenTextures(1, &tex));\n GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, tex));\n GL_SAFE_CALL(glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, WIDTH, HEIGHT, 0, GL_RGBA, GL_UNSIGNED_BYTE, pSurf));\n\n EGLDisplay eglDisplayHandle = eglGetCurrentDisplay();\n EGLContext eglCtx = eglGetCurrentContext();\n\n // Create the EGL_Image\n EGLint eglImgAttrs[] = { EGL_IMAGE_PRESERVED_KHR, EGL_FALSE, EGL_NONE, EGL_NONE };\n EGLImageKHR eglImage = eglCreateImageKHR(eglDisplayHandle, eglCtx, EGL_GL_TEXTURE_2D_KHR, (EGLClientBuffer)(intptr_t)tex, eglImgAttrs);\n glFinish();\n glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, WIDTH, HEIGHT, GL_RGBA, GL_UNSIGNED_BYTE, pSurf);\n glFinish();\n\n // Register buffer with CUDA\ncuGraphicsEGLRegisterImage(&pResource, eglImage,0);\n\n //Get CUDA array from graphics resource object\n cuGraphicsSubResourceGetMappedArray( &pArray, pResource, 0, 0);\n\n cuCtxSynchronize();\n\n //Create a CUDA surface object from pArray\n CUresult status = CUDA_SUCCESS;\n CUDA_RESOURCE_DESC wdsc;\n memset(&wdsc, 0, sizeof(wdsc));\n wdsc.resType = CU_RESOURCE_TYPE_ARRAY; wdsc.res.array.hArray = pArray;\n CUsurfObject writeSurface;\n cuSurfObjectCreate(&writeSurface, &wdsc);\n\n dim3 blockSize(32,32);\n dim3 gridSize(width/blockSize.x,height/blockSize.y);\n // Modifies the OpenGL texture using CUDA surface object\n changeTexture<<<gridSize, blockSize>>>(writeSurface, width, height);\n cuCtxSynchronize();\n\n CUDA_MEMCPY3D cpdesc;\n memset(&cpdesc, 0, sizeof(cpdesc));\n cpdesc.srcXInBytes = cpdesc.srcY = cpdesc.srcZ = cpdesc.srcLOD = 0;\n cpdesc.dstXInBytes = cpdesc.dstY = cpdesc.dstZ = cpdesc.dstLOD = 0;\n cpdesc.srcMemoryType = CU_MEMORYTYPE_ARRAY; cpdesc.dstMemoryType = CU_MEMORYTYPE_HOST;\n cpdesc.srcArray = pArray; cpdesc.dstHost = (void *)hostSurf;\n cpdesc.WidthInBytes = WIDTH * 4; cpdesc.Height = HEIGHT; cpdesc.Depth = 1;\n\n //Copy CUDA surface object values to hostSurf\n cuMemcpy3D(&cpdesc);\n\n cuCtxSynchronize();\n\n unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char)));\n // Get the modified texture values as\n GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp));\n glFinish();\n // Check if the OpenGL texture got modified values\n checkbuf(temp,hostSurf);\n\n // Clean up CUDA\n cuGraphicsUnregisterResource(pResource);\n cuSurfObjectDestroy(writeSurface);\n .\n .\n}\n__global__ void changeTexture(cudaSurfaceObject_t arr, unsigned int width, unsigned int height){\n unsigned int x = threadIdx.x + blockIdx.x * blockDim.x;\n unsigned int y = threadIdx.y + blockIdx.y * blockDim.y;\n uchar4 data = make_uchar4(1, 2, 3, 4);\n surf2Dwrite(data, arr, x * 4, y);\n}\nvoid checkbuf(unsigned char *ref, unsigned char *hostSurf) {\n for(int y = 0; y < height*width*4; y++){\n if (ref[y] != hostSurf[y])\n printf(\"mis match at %d\\n\",y);\n }\n} Because EGLImage does not provide implicit synchronization, the above sample application uses glFinish() and cudaThreadSynchronize() calls to achieve synchronization. Both calls block the CPU thread. To avoid blocking the CPU thread, use EGLSync to provide synchronization. An example using EGLImage and EGLSync is shown in the following section. 5.3. EGLSync  EGLSync is a cross-API synchronization primitive. It allows an EGL client API to share its synchronization object with other EGL client APIs. For example, applications can use an EGLSync interop to share the OpenGL synchronization object with CUDA. Note An EGLSync object is created using eglCreateSyncKHR() and destroyed using eglDestroySyncKHR() . For more information see the EGLSync specification at the following web site: https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_fence_sync.txt 5.3.1. CUDA Interop with EGLSync  In an imaging application, where two clients run on a GPU and share a resource, the absence of a cross-API GPU synchronization object forces the clients to use CPU-side synchronization to avoid race conditions. The CUDA interop with EGLSync allows the application to exchange synchronization objects between CUDA and other client APIs directly. This avoids the need for CPU-side synchronization and allows CPU to complete other tasks. In CUDA, an EGLSync object is mapped as a CUDA event. Note Currently CUDA interop with EGLSync is supported only on Tegra® devices. 5.3.2. Creating EGLSync from a CUDA Event  Creating an EGLSync object from a CUDA event is shown in the following sample code. Note that EGLSync object creation from a CUDA event should happen immediately after the CUDA event is recorded. EGLDisplay dpy = eglGetCurrentDisplay (); // Create CUDA event cudaEvent_t event ; cudaStream_t * stream ; cudaEventCreate ( & event ); cudaStreamCreate ( & stream ); // Record the event with cuda event cudaEventRecord ( event , stream ); const EGLAttrib attribs [] = { EGL_CUDA_EVENT_HANDLE_NV , ( EGLAttrib ) event , EGL_NONE }; //Create EGLSync from the cuda event eglsync = eglCreateSync ( dpy , EGL_NV_CUDA_EVENT_NV , attribs ); //Wait on the sync eglWaitSyncKHR (...); Note Initialize a CUDA event before creating an EGLSync object from it to avoid undefined behavior. 5.3.3. Creating a CUDA Event from EGLSync  Creating a CUDA event from an EGLSync object is shown in the following sample code. EGLSync eglsync ; EGLDisplay dpy = eglGetCurrentDisplay (); // Create an eglSync object from openGL fense sync object eglsync = eglCreateSyncKHR ( dpy , EGL_SYNC_FENCE_KHR , NULL ); cudaEvent_t event ; cudaStream_t * stream ; cudaStreamCreate ( & stream ); // Create CUDA event from eglSync cudaEventCreateFromEGLSync ( & event , eglSync , cudaEventDefault ); // Wait on the cuda event. It waits on GPU till OpenGL finishes its // task cudaStreamWaitEvent ( stream , event , 0 ); Note The cudaEventRecord() and cudaEventElapsedTime() functions are not supported for events created from an EGLSync object. The same example given in the EGLImage section is re-written below to illustrate the usage of an EGLSync interop. In the sample code, the CPU blocking calls such as glFinish() and cudaThreadSynchronize() are replaced with EGLSync interop calls. int width = 256;\nint height = 256;\nint main()\n{\n .\n .\n unsigned char *hostSurf;\n unsigned char *pSurf;\n cudaArray_t pArray;\n unsigned int bufferSize = WIDTH * HEIGHT * 4;\n pSurf= (unsigned char *)malloc(bufferSize); hostSurf = (unsigned char *)malloc(bufferSize);\n // Intialize the buffer\n for(int y = 0; y < bufferSize; y++)\n pSurf[y] = 0;\n\n //Init texture\n GL_SAFE_CALL(glGenTextures(1, &tex));\n GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, tex));\n GL_SAFE_CALL(glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, WIDTH, HEIGHT, 0, GL_RGBA, GL_UNSIGNED_BYTE, pSurf));\n EGLDisplay eglDisplayHandle = eglGetCurrentDisplay();\n EGLContext eglCtx = eglGetCurrentContext();\n\n cudaEvent_t cuda_event;\n cudaEventCreateWithFlags(cuda_event, cudaEventDisableTiming);\n EGLAttribKHR eglattrib[] = { EGL_CUDA_EVENT_HANDLE_NV, (EGLAttrib) cuda_event, EGL_NONE};\n cudaStream_t* stream;\n cudaStreamCreateWithFlags(&stream,cudaStreamDefault);\n\n EGLSyncKHR eglsync1, eglsync2;\n cudaEvent_t egl_event;\n\n // Create the EGL_Image\n EGLint eglImgAttrs[] = { EGL_IMAGE_PRESERVED_KHR, EGL_FALSE, EGL_NONE, EGL_NONE };\n EGLImageKHR eglImage = eglCreateImageKHR(eglDisplayHandle, eglCtx, EGL_GL_TEXTURE_2D_KHR, (EGLClientBuffer)(intptr_t)tex, eglImgAttrs);\n\n glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, WIDTH, HEIGHT, GL_RGBA, GL_UNSIGNED_BYTE, pSurf);\n //Creates an EGLSync object from GL Sync object to track\n //finishing of copy.\n eglsync1 = eglCreateSyncKHR(eglDisplayHandle, EGL_SYNC_FENCE_KHR, NULL);\n\n //Create CUDA event object from EGLSync obejct\n cuEventCreateFromEGLSync(&egl_event, eglsync1, cudaEventDefault);\n\n //Waiting on GPU to finish GL copy\n cuStreamWaitEvent(stream, egl_event, 0);\n\n // Register buffer with CUDA\n cudaGraphicsEGLRegisterImage(&pResource, eglImage, cudaGraphicsRegisterFlagsNone);\n //Get CUDA array from graphics resource object\n cudaGraphicsSubResourceGetMappedArray( &pArray, pResource, 0, 0);\n .\n .\n //Create a CUDA surface object from pArray\n struct cudaResourceDesc resDesc;\n memset(&resDesc, 0, sizeof(resDesc));\n resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = pArray;\n cudaSurfaceObject_t inputSurfObj = 0;\n cudaCreateSurfaceObject(&inputSurfObj, &resDesc);\n\n dim3 blockSize(32,32);\n dim3 gridSize(width/blockSize.x,height/blockSize.y);\n // Modifies the CUDA array using CUDA surface object\n changeTexture<<<gridSize, blockSize>>>(inputSurfObj, width, height);\n cuEventRecord(cuda_event, stream);\n //Create EGLsync object from CUDA event cuda_event\n eglsync2 = eglCreateSync64KHR(dpy, EGL_SYNC_CUDA_EVENT_NV, eglattrib);\n //waits till kernel to finish\n eglWaitSyncKHR(eglDisplayHandle, eglsync2, 0);\n .\n //Copy modified pArray values to hostSurf\n .\n unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char)));\n // Get the modified texture values\n GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp));\n .\n .\n // This function check if the OpenGL texture got modified values\n checkbuf(temp,hostSurf);\n\n // Clean up CUDA\n cudaGraphicsUnregisterResource(pResource);\n cudaDestroySurfaceObject(inputSurfObj);\n eglDestroySyncKHR(eglDisplayHandle, eglsync1);\n eglDestroySyncKHR(eglDisplayHandle, eglsync2);\n cudaEventDestroy(egl_event);\n cudaEventDestroy(cuda_event);\n .\n .\n} 6. CUDA Upgradable Package for Jetson  CUDA introduced an upgrade path starting with JetPack SDK 5.0 which provides an option to update the CUDA driver and the CUDA toolkit to the latest version. 6.1. Installing the CUDA Upgrade Package  6.1.1. Prerequisite  The Jetson device must be installed with a compatible NVIDIA JetPack version. Refer to Use the Right Upgrade Package for more info. 6.1.2. From Network Repositories or Local Installers  The CUDA Downloads page provides step-by-step instructions on how to download and use the local installer or CUDA network repositories to install the latest Toolkit. The CUDA upgrade package gets downloaded and installed along with the corresponding CUDA toolkit for Linux-aarch64-jetson devices. For use cases where applications are built on the host and require just the CUDA upgrade package to be installed independently on the target, the corresponding Debians can be found in CUDA Repos . Taking 11.8 for example, this can be installed by running the command: $ sudo apt-get install -y cuda-compat-11-8 Note This is the recommended path for CUDA upgrade for devices that have disk space (secondary storage) limitations. The installed upgrade package is available in the versioned toolkit location. For example, for 11.8 it is located in /usr/local/cuda-11.8/ . The upgrade package consists of the following files: libcuda.so.* - the CUDA Driver libnvidia-nvvm.so.* - Just In Time - Link Time Optimization (CUDA 11.8 and later only) libnvidia-ptxjitcompiler.so.* - the JIT (just-in-time) compiler for PTX files nvidia-cuda-mps-control - CUDA MPS control executable nvidia-cuda-mps-server - CUDA MPS server executable These files together implement the CUDA 11.8 driver interfaces. Note This package only provides the files, and does not configure the system. Example The following commands show how the CUDA Upgrade package can be installed and used to run the applications. $ sudo apt-get -y install cuda\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following additional packages will be installed:\n  cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8\n  ...<snip>...\nThe following NEW packages will be installed:\n  cuda cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8\n  ...<snip>...\n0 upgraded, 48 newly installed, 0 to remove and 38 not upgraded.\nNeed to get 15.7 MB/1,294 MB of archives.\nAfter this operation, 4,375 MB of additional disk space will be used.\nGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/arm64  cuda-compat-11-8 11.8.31339915-1 [15.8 MB]\nFetched 15.7 MB in 12s (1,338 kB/s)\nSelecting previously unselected package cuda-compat-11-8.\n(Reading database ...\n  ...<snip>...\n(Reading database ... 100%\n(Reading database ... 148682 files and directories currently installed.)\nPreparing to unpack .../00-cuda-compat-11-8_11.8.30682616-1_arm64.deb ...\nUnpacking cuda-compat-11-8 (11.8.30682616-1) ...\n  ...<snip>...\nUnpacking cuda-11-8 (11.8.0-1) ...\nSelecting previously unselected package cuda.\nPreparing to unpack .../47-cuda_11.8.0-1_arm64.deb ...\nUnpacking cuda (11.8.0-1) ...\nSetting up cuda-toolkit-config-common (11.8.56-1) ...\nSetting up cuda-nvml-dev-11-8 (11.8.56-1) ...\nSetting up cuda-compat-11-8 (11.8.30682616-1) ...\n  ...<snip>...\n\n\n$ ls -l /usr/local/cuda-11.8/compat\ntotal 55300\nlrwxrwxrwx 1 root root       12 Jan  6 19:14 libcuda.so -> libcuda.so.1\nlrwxrwxrwx 1 root root       14 Jan  6 19:14 libcuda.so.1 -> libcuda.so.1.1\n-rw-r--r-- 1 root root 21702832 Jan  6 19:14 libcuda.so.1.1\nlrwxrwxrwx 1 root root       19 Jan  6 19:14 libnvidia-nvvm.so -> libnvidia-nvvm.so.4\nlrwxrwxrwx 1 root root       23 Jan  6 19:14 libnvidia-nvvm.so.4 -> libnvidia-nvvm.so.4.0.0\n-rw-r--r-- 1 root root 24255256 Jan  6 19:14 libnvidia-nvvm.so.4.0.0\n-rw-r--r-- 1 root root 10665608 Jan  6 19:14 libnvidia-ptxjitcompiler.so\nlrwxrwxrwx 1 root root       27 Jan  6 19:14 libnvidia-ptxjitcompiler.so.1 -> libnvidia-ptxjitcompiler.so\n\n$ export PATH=/usr/local/cuda-11.8/bin:$PATH\n$ export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH The user can set LD_LIBRARY_PATH to include the libraries installed by the upgrade package before running the CUDA 11.8 application: $ LD_LIBRARY_PATH=/usr/local/cuda-11.8/compat:$LD_LIBRARY_PATH ~/Samples/1_Utilities/deviceQuery\n\nCUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"Orin\"\n  CUDA Driver Version / Runtime Version          11.8 / 11.8\n  CUDA Capability Major/Minor version number:    8.7\n      ...<snip>...\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1\nResult = PASS Only a single CUDA upgrade package can be installed at any point in time on a given system. While installing a new CUDA upgrade package, the previous version of the installed upgrade package will be removed and replaced with the new one. The default drivers (originally installed with the NVIDIA JetPack and part of the L4T BSP) will be retained by the installer. The application has the ability to use either the default version of CUDA (originally installed with NVIDIA JetPack) or the one installed by the upgrade package. The LD_LIBRARY_PATH variable can be used to choose the required version. In addition to LD_LIBRARY_PATH , CUDA MPS users must set the PATH variable in order to use the nvidia-cuda-mps-* executables installed by the upgrade package before starting MPS and running the CUDA applications that use MPS. The MPS executables installed with the upgrade package are only compatible with the CUDA driver installed with the same upgrade package, and vice versa, which can be checked with the version info. Installation of the upgrade package will fail if it is not compatible with the NVIDIA JetPack version. 6.2. Deployment Considerations for CUDA Upgrade Package  6.2.1. Use the Right Upgrade Package  The CUDA upgrade package is named after the highest toolkit that it can support. For example, if you are on the NVIDIA JetPack SDK 5.0 (11.4) driver but require 11.8 application support, install the CUDA upgrade package for 11.8. Each CUDA release will support upgrades only for a specific set of NVIDIA JetPack releases. The table below shows the NVIDIA JetPack SDK version supported by each CUDA release. JetPack SDK CUDA 11.4 CUDA 11.8 CUDA 12.0 CUDA 12.1 CUDA 12.2 CUDA 12.3 onwards 5.x default C C C C X JetPack SDK CUDA 12.2 CUDA 12.3 CUDA 12.4 CUDA 12.5 6.x default X C C The following table shows the CUDA UMD and CUDA Toolkit version compatibility on NVIDIA JetPack 5.x release: CUDA UMD CUDA Toolkit 11.4 (default - part of NVIDIA JetPack) 11.8 12.0 12.1 12.2 11.4 (default – part of NVIDIA JetPack) C C ( Minor Version Compatibility ) X X X 11.8 (through Upgrade Package) C ( Binary Compatibility ) C X X X 12.0 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C C ( Minor Version Compatibility ) C ( Minor Version Compatibility ) 12.1 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C ( Binary Compatibility ) C C ( Minor Version Compatibility ) 12.2 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C ( Binary Compatibility ) C ( Binary Compatibility ) C The following table shows the CUDA UMD and CUDA Toolkit version compatibility on NVIDIA JetPack 6.x release: CUDA UMD CUDA Toolkit 12.2 (default - part of NVIDIA JetPack) 12.4 12.5 12.2 (default – part of NVIDIA JetPack) C C ( Minor Version Compatibility ) C ( Minor Version Compatibility ) 12.4 (through Upgrade Package) C ( Binary Compatibility ) C C ( Minor Version Compatibility ) 12.5 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C C - Compatible X – Not compatible Note CUDA upgrade packages on NVIDIA JetPack SDK 5.x are available from CUDA 11.8 onwards. 6.2.2. Feature Exceptions  CUDA upgrade package only updates the CUDA driver interfaces while leaving the rest of the NVIDIA JetPack SDK components as is. If a new feature in the latest CUDA driver needs an updated NVIDIA JetPack SDK component/interface, it might not work and error out when used. 6.2.3. Check for Compatibility Support  In addition to the CUDA driver and certain compiler components, there are other drivers in the NVIDIA JetPack that remain at the default version. The CUDA upgrade path is for CUDA only. A well-written application should use following error codes to determine if CUDA Upgrade is supported. System administrators should be aware of these error codes to determine if there are errors in the deployment. CUDA_ERROR_SYSTEM_DRIVER_MISMATCH = 803 . This error indicates that there is a mismatch between the versions of the upgraded CUDA driver and the already installed drivers on the system. CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE = 804 . This error indicates that the system was updated to run with the CUDA upgrade package but the visible hardware detected by CUDA does not support this configuration. 7. cuDLA  DLA (Deep Learning Accelerator) is a fixed function accelerator present on the NVIDIA Tegra SoC and is used for inference applications. The DLA HW has superior performance/W and can natively run many of the layers in modern neural networks, thus making it an attractive value proposition for embedded AI applications. Programming the DLA typically consists of an offline and online step: in the offline step, an input network is parsed and compiled by the DLA compiler into a loadable and in the online step, that loadable is executed by the DLA HW to generate an inference result. The SW stack that is currently provided by NVIDIA to perform the online or execution step consists of NvMediaDla and the DLA runtime/KMD. Together, these APIs enable the user to submit a DLA task to the DLA HW for inferencing purposes. The main functional paths are illustrated in the figure below. DLA SW stack  It follows from the model above that users wishing to use GPU and DLA together in an application would have to use interop mechanisms such as EGLStreams/NvSci to share buffers as well as synchronization primitives between the GPU and DLA. These interop mechanisms usually involve many steps for each buffer that is being shared and have limited ability to fine-tune the scheduling of tasks between the GPU and DLA. cuDLA is an extension of the CUDA programming model that integrates DLA (Deep Learning Accelerator) with CUDA thereby making it possible to submit DLA tasks using CUDA programming constructs such as streams and graphs. Managing shared buffers as well as synchronizing the tasks between GPU and DLA is transparently handled by cuDLA, freeing up the programmer to focus on the high-level usecase. 7.1. Developer Guide  This section describes the key principles involved in programming the DLA HW using cuDLA APIs. The cuDLA interfaces expose mechanisms to initialize devices, manage memory and submit DLA tasks. As such, this section discusses how the cuDLA APIs can be used for these usecases. The detailed specification of these APIs is described in the API specification and should be referred while writing a cuDLA application. Since cuDLA is an extension of CUDA, it is designed to work in conjunction with CUDA APIs that perform CUDA functions such as GPU management, context management etc. Therefore, the current state of the application in terms of which GPU is selected and the current active context (and its lifecycle) are all important considerations while evaluating the behavior of a cuDLA API. 7.1.1. Device Model  To perform any DLA operation, it is necessary that an application first create a cuDLA device handle. The cudlaCreateDevice() API creates a logical instance of a cuDLA device wherein the selected DLA HW instance is coupled with the current active GPU selected via CUDA. For example, the following code snippet would create a logical instance consisting of the current GPU (set via cudaSetDevice() ) and DLA HW 0. Currently, cuDLA supports only iGPU on Tegra and an attempt to create a device handle by setting the current GPU as a dGPU would result in a device creation error during cudlaCreateDevice() . cudlaDevHandle devHandle ; cudlaStatus ret ; ret = cudlaCreateDevice ( 0 , & devHandle , CUDLA_CUDA_DLA ); Device model  The user can create any number of such logical instances using cudlaCreateDevice() using any combination of GPU and DLA HW instances (subject to system resource availability): Device model - multiple instances  In addition, cudlaCreateDevice() supports an alternative flag during device creation - CUDLA_STANDALONE. This flag can be used by applications when they wish to create a cuDLA device in standalone mode i.e without coupling it with a GPU device. All device submissions can be accomplished using cuDLA in standalone mode as well but in this mode there is no support for CUDA interactions. Consequently, in what follows, two modes of execution are considered while describing a particular API or a particular usecase: the hybrid mode and the standalone mode. The API spec has complete details about which API is supported in which mode. 7.1.2. Loading and Querying Modules  The cuDLA device handle needs an appropriate loadable to be associated with it before any DLA task submission occurs. The loadable is usually created offline using TensorRT. The loadable has information about the number of input and output tensors as well as their respective metadata and can be queried by the application to retrieve this information. A typical application flow after a successful cuDLA device initialization would look like this (interspersed with some debug logs): DPRINTF ( \"Device created successfully \\n \" ); // Load the loadable from 'loadableData' in which the loadable binary has // been copied from the location of the loadable - disk or otherwise. err = cudlaModuleLoadFromMemory ( devHandle , loadableData , file_size , & amp ; moduleHandle , 0 ); if ( err != cudlaSuccess ) { // handle error } // Get tensor attributes. uint32_t numInputTensors = 0 ; uint32_t numOutputTensors = 0 ; cudlaModuleAttribute attribute ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_INPUT_TENSORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } numInputTensors = attribute . numInputTensors ; DPRINTF ( \"numInputTensors = %d \\n \" , numInputTensors ); err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_OUTPUT_TENSORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } numOutputTensors = attribute . numOutputTensors ; DPRINTF ( \"numOutputTensors = %d \\n \" , numOutputTensors ); cudlaModuleTensorDescriptor * inputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numInputTensors ); cudlaModuleTensorDescriptor * outputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numOutputTensors ); if (( inputTensorDesc == NULL ) || ( outputTensorDesc == NULL )) { // handle error } attribute . inputTensorDesc = inputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_INPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } attribute . outputTensorDesc = outputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_OUTPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } Applications can use the retrieved tensor descriptors to setup their data buffers in terms of size and formats. Detailed information about the contents of the tensor descriptors is present in the API specification section under cudlaModuleGetAttributes() . 7.1.3. Memory Model  The GPU and DLA have different MMUs that manage the VA to PA conversion while performing their respective functions. The figure below shows an example where the GMMU performs a translation for GPU VAs and the SMMU performs a similar function for the VAs arriving from the DLA. Virtual address to physical address conversion  In hybrid mode, before a CUDA pointer can be accessed by the DLA, it is necessary that the CUDA pointer be registered with the DLA. This registration step creates an entry in the SMMU and returns the corresponding VA for use in task submissions. The following code snippet shows an example registration for a device handle created with the flag CUDLA_CUDA_DLA : // Allocate memory on GPU. void * buffer ; uint32_t size = 100 ; result = cudaMalloc ( & inputBufferGPU , size ); if ( result != cudaSuccess ) { // handle error } // Register the CUDA-allocated buffers. uint64_t * bufferRegisteredPtr = NULL ; err = cudlaMemRegister ( devHandle , ( uint64_t * ) inputBufferGPU , size , & bufferRegisteredPtr , 0 ); if ( err != cudlaSuccess ) { // handle error } In standalone mode, cuDLA functions without the underlying CUDA device. Consequently, in this mode, the memory allocations performed by the application (which need to be subsequently registered) need to come from outside CUDA. On Tegra systems, cuDLA supports registration of NvSciBuf allocations via the cudlaImportExternalMemory() API as the following code snippet shows: // Allocate the NvSciBuf object. NvSciBufObj inputBufObj ; sciError = NvSciBufObjAlloc ( reconciledInputAttrList , & inputBufObj ); if ( sciError != NvSciError_Success ) { // handle error } uint64_t * inputBufObjRegPtr = NULL ; // importing external memory cudlaExternalMemoryHandleDesc memDesc = { 0 }; memset ( & memDesc , 0 , sizeof ( memDesc )); memDesc . extBufObject = ( void * ) inputBufObj ; memDesc . size = size ; err = cudlaImportExternalMemory ( devHandle , & memDesc , & inputBufObjRegPtr , 0 ); if ( err != cudlaSuccess ) { // handle error } 7.1.4. Task Execution and Synchronization Model  7.1.4.1. Task Execution  Submitting a DLA task for execution is similar to submitting a CUDA kernel to the GPU. cuDLA natively supports CUDA streams and works seamlessly with the stream semantics to ensure that all tasks intended for the DLA are executed by the DLA HW only after the previous tasks on the stream have completed execution. This enables applications to setup complex processing workflows between the GPU and the DLA using familiar stream semantics without having to manage memory coherency and execution dependencies between GPU and DLA. A visual illustration of the execution model is shown in the following figure. DLA tasks can be interspersed with GPU tasks in a given stream or multiple streams and cudlaSubmitTask() handles all the memory/execution dependencies. cuDLA task execution model  The submit task API needs the input and output tensors in the form of the addresses registered with the DLA (using cudlaMemRegister() ). An application can pre-register all the required pointers with cuDLA and then use the registered pointers during cudlaSubmitTask() . This API, in turn, ensures that the results of the previous operations on the underlying memory corresponding to the registered pointers is visible to the DLA before it begins execution of the current task. A typical application code consisting of CUDA and cuDLA operations is shown in the snippet below: DPRINTF ( \"ALL MEMORY REGISTERED SUCCESSFULLY \\n \" ); // Copy data from CPU buffers to GPU buffers. result = cudaMemcpyAsync ( inputBufferGPU , inputBuffer , inputTensorDesc [ 0 ]. size , cudaMemcpyHostToDevice , stream ); if ( result != cudaSuccess ) { // handle error } result = cudaMemsetAsync ( outputBufferGPU , 0 , outputTensorDesc [ 0 ]. size , stream ); if ( result != cudaSuccess ) { // handle error } // Enqueue a cuDLA task. cudlaTask task ; task . moduleHandle = moduleHandle ; task . outputTensor = & outputBufferRegisteredPtr ; task . numOutputTensors = 1 ; task . numInputTensors = 1 ; task . inputTensor = & inputBufferRegisteredPtr ; task . waitEvents = NULL ; task . signalEvents = NULL ; err = cudlaSubmitTask ( devHandle , & task , 1 , stream , 0 ); if ( err != cudlaSuccess ) { // handle error } DPRINTF ( \"SUBMIT IS DONE !!! \\n \" ); result = cudaMemcpyAsync ( outputBuffer , outputBufferGPU , outputTensorDesc [ 0 ]. size , cudaMemcpyDeviceToHost , stream ); if ( result != cudaSuccess ) { // handle error } In standalone mode, the stream parameter in cudlaSubmitTask() must be specified as NULL as cuDLA is operating independently of CUDA. In this case, the tasks submitted to the DLA are executed in FIFO order. 7.1.4.1.1. Multithreaded User Submission  Users can specify the CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE flag during submission to a particular device handle if they are sure that submission to this particular device handle occurs only in this thread and that there is no shared data at the application level between this device handle and any other device handle which might be used in a parallel thread for submission. This flag facilitates some optimizations in the submission path which might lead to better submission times from the application point of view. 7.1.4.2. Synchronization  Synchronization of tasks in hybrid mode does not need a different API. Since DLA tasks are submitted to CUDA streams, it is sufficient to wait on the stream to complete its work in order to ensure that all DLA tasks submitted on that stream are completed. In this regard DLA task synchronization is compatible with any of the different synchronization mechanisms available in CUDA – Event, Stream, Device – and the entire CUDA machinery is available for applications to setup different flows and usecases. In standalone mode, however, the synchronization mechanisms are different given that cuDLA operates independently of CUDA. In this mode, the cudlaTask structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively as part of cudlaSubmitTask() . Each submitted task will wait for all its wait events to be signaled before beginning execution and will provide a signal event (if one is requested for during cudlaSubmitTask() ) that the application (or any other entity) can wait on to ensure that the submitted task has completed execution. In cuDLA 1.0, only NvSciSync fences are supported as part of wait events. Furthermore, only NvSciSync objects can be registered and signaled as part of signal events and the fence corresponding to the signaled event is returned as part of cudlaSubmitTask() . Like all memory operations, the underlying backing store for the events (in this case the NvSciSync object) must be registered with cuDLA before using it in a task submission. The code snippet below shows an example flow where the application creates an input and output NvSciSync object and registers them, creates fences corresponding to them, marks the corresponding fences as wait/signal as part of cudlaSubmitTask() and then signals the input fence and waits on the output fence. 7.1.4.2.1. Registering an external semaphore:  sciError = NvSciSyncObjAlloc ( nvSciSyncReconciledListObj1 , & syncObj1 ); if ( sciError != NvSciError_Success ) { // handle error } sciError = NvSciSyncObjAlloc ( nvSciSyncReconciledListObj2 , & syncObj2 ); if ( sciError != NvSciError_Success ) { // handle error } // importing external semaphore uint64_t * nvSciSyncObjRegPtr1 = NULL ; uint64_t * nvSciSyncObjRegPtr2 = NULL ; cudlaExternalSemaphoreHandleDesc semaMemDesc = { 0 }; memset ( & semaMemDesc , 0 , sizeof ( semaMemDesc )); semaMemDesc . extSyncObject = syncObj1 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr1 , 0 ); if ( err != cudlaSuccess ) { // handle error } memset ( & semaMemDesc , 0 , sizeof ( semaMemDesc )); semaMemDesc . extSyncObject = syncObj2 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr2 , 0 ); if ( err != cudlaSuccess ) { // handle error } DPRINTF ( \"ALL EXTERNAL SEMAPHORES REGISTERED SUCCESSFULLY \\n \" ); 7.1.4.2.2. Events setup for cudlaSubmitTask()  // Wait events NvSciSyncFence preFence = NvSciSyncFenceInitializer ; sciError = NvSciSyncObjGenerateFence ( syncObj1 , & preFence ); if ( sciError != NvSciError_Success ) { // handle error } cudlaWaitEvents * waitEvents ; waitEvents = ( cudlaWaitEvents * ) malloc ( sizeof ( cudlaWaitEvents )); if ( waitEvents == NULL ) { // handle error } waitEvents -> numEvents = 1 ; CudlaFence * preFences = ( CudlaFence * ) malloc ( waitEvents -> numEvents * sizeof ( CudlaFence )); if ( preFences == NULL ) { // handle error } preFences [ 0 ]. fence = & preFence ; preFences [ 0 ]. type = CUDLA_NVSCISYNC_FENCE ; waitEvents -> preFences = preFences ; // Signal Events cudlaSignalEvents * signalEvents ; signalEvents = ( cudlaSignalEvents * ) malloc ( sizeof ( cudlaSignalEvents )); if ( signalEvents == NULL ) { // handle error } signalEvents -> numEvents = 1 ; uint64_t ** devPtrs = ( uint64_t ** ) malloc ( signalEvents -> numEvents * sizeof ( uint64_t * )); if ( devPtrs == NULL ) { // handle error } devPtrs [ 0 ] = nvSciSyncObjRegPtr2 ; signalEvents -> devPtrs = devPtrs ; signalEvents -> eofFences = ( CudlaFence * ) malloc ( signalEvents -> numEvents * sizeof ( CudlaFence )); if ( signalEvents -> eofFences == NULL ) { // handle error } NvSciSyncFence eofFence = NvSciSyncFenceInitializer ; signalEvents -> eofFences [ 0 ]. fence = & eofFence ; signalEvents -> eofFences [ 0 ]. type = CUDLA_NVSCISYNC_FENCE ; // Enqueue a cuDLA task. cudlaTask task ; task . moduleHandle = moduleHandle ; task . outputTensor = & outputBufObjRegPtr ; task . numOutputTensors = 1 ; task . numInputTensors = 1 ; task . inputTensor = & inputBufObjRegPtr ; task . waitEvents = waitEvents ; task . signalEvents = signalEvents ; err = cudlaSubmitTask ( devHandle , & task , 1 , NULL , 0 ); if ( err != cudlaSuccess ) { // handle error } DPRINTF ( \"SUBMIT IS DONE !!! \\n \" ); 7.1.4.2.3. Waiting on the signal event  // Signal wait events. // For illustration purposes only. In practice, this signal will be done by another // entity or driver that provides the data input for this particular submitted task. NvSciSyncObjSignal ( syncObj1 ); // Wait for operations to finish. // For illustration purposes only. In practice, this wait will be done by // another entity or driver that is waiting for the output of the submitted task. sciError = NvSciSyncFenceWait ( reinterpret_cast < NvSciSyncFence *> ( signalEvents -> eofFences [ 0 ]. fence ), nvSciCtx , -1 ); if ( sciError != NvSciError_Success ) { // handle error } 7.1.4.2.4. Supported Synchronization Primitives in cuDLA  cuDLA supports two types of NvSciSync object primitives. These are sync point and deterministic semaphores. By default, cuDLA prioritizes sync point primitive over deterministic semaphore primitive and sets these priorities in the NvSciSync attribute list when requested by the application using cudlaGetNvSciSyncAttributes() . For Deterministic semaphore, the NvSciSync attribute list used to create the NvSciSync object must have the value of NvSciSyncAttrKey_RequireDeterministicFences key set to true. Deterministic fences allow users to enqueue a wait over the semaphore object even before corresponding signal is enqueued. For such semaphore object, cuDLA guarantees that each signal operation will increment the fence value by ‘1’. Users are expected to keep track of signals enqueued on the semaphore object and insert waits accordingly. 7.1.4.2.5. Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList  // Set NvSciSyncAttrKey_RequireDeterministicFences key to true in // NvScisyncAtrrList that is used to create NvSciSync object with // Deterministic Semaphore primitive. NvSciSyncAccessPerm cpuPerm = NvSciSyncAccessPerm_SignalOnly ; keyValue [ 0 ]. attrKey = NvSciSyncAttrKey_RequiredPerm ; keyValue [ 0 ]. value = ( void * ) & cpuPerm ; keyValue [ 0 ]. len = sizeof ( cpuPerm ); bool detFenceReq = true ; keyValue [ 1 ]. attrKey = NvSciSyncAttrKey_RequireDeterministicFences ; keyValue [ 1 ]. value = ( const void * ) & detFenceReq ; keyValue [ 1 ]. len = sizeof ( detFenceReq ); return NvSciSyncAttrListSetAttrs ( list , keyValue , 2 ); 7.1.4.2.6. Timestamp Support for NvSciFence  cuDLA supports the timestamp feature of NvSci in cuDLA standalone mode. Timestamp support enables users to get the time at which a particular fence has been signaled. This time value is the snapshot of the DLA clock in microseconds. cuDLA users can request timestamp support by setting the value of the NvSciSyncAttrKey_WaiterRequireTimestamps key as true while filling up the NvSci waiter attribute list. The users can use this timestamp along with SOF(Start Of Frame) fence and EOF(End OF Frame) fence to get a snapshot of DLA clock just before start of task & after task completion respectively. This enables users to calculate time taken by DLA to execute the submitted task. 7.1.4.2.7. Requesting Timestamp Support for NvSciSync Object  sciError fillCpuWaiterAttrList ( NvSciSyncAttrList list ) { bool cpuWaiter = true ; NvSciSyncAttrKeyValuePair keyValue [ 3 ]; memset ( keyValue , 0 , sizeof ( keyValue )); keyValue [ 0 ]. attrKey = NvSciSyncAttrKey_NeedCpuAccess ; keyValue [ 0 ]. value = ( void * ) & cpuWaiter ; keyValue [ 0 ]. len = sizeof ( cpuWaiter ); NvSciSyncAccessPerm cpuPerm = NvSciSyncAccessPerm_WaitOnly ; keyValue [ 1 ]. attrKey = NvSciSyncAttrKey_RequiredPerm ; keyValue [ 1 ]. value = ( void * ) & cpuPerm ; keyValue [ 1 ]. len = sizeof ( cpuPerm ); bool cpuRequiresTimeStamp = true ; keyValue [ 2 ]. attrKey = NvSciSyncAttrKey_WaiterRequireTimestamps ; keyValue [ 2 ]. value = ( void * ) & cpuRequiresTimeStamp ; keyValue [ 2 ]. len = sizeof ( cpuRequiresTimeStamp ); return NvSciSyncAttrListSetAttrs ( list , keyValue , 3 ); } NvSciSyncCpuWaitContext nvSciCtx ; NvSciSyncModule syncModule ; NvSciSyncAttrList waiterAttrListObj = nullptr ; NvSciSyncAttrList signalerAttrListObj = nullptr ; NvSciSyncAttrList syncAttrListObj [ 2 ]; NvSciSyncAttrList nvSciSyncConflictListObj ; NvSciSyncAttrList nvSciSyncReconciledListObj ; sciError = NvSciSyncModuleOpen ( & syncModule ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncAttrListCreate ( syncModule , & signalerAttrListObj ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncAttrListCreate ( syncModule , & waiterAttrListObj ); if ( sciError != NvSciError_Success ) { //handle error } err = cudlaGetNvSciSyncAttributes ( reinterpret_cast < uint64_t *> ( signalerAttrListObj ), CUDLA_NVSCISYNC_ATTR_SIGNAL ); if ( err != cudlaSuccess ) { //handle error } sciError = fillCpuWaiterAttrList ( waiterAttrListObj ); if ( sciError != NvSciError_Success ) { //handle error } syncAttrListObj [ 0 ] = signalerAttrListObj ; syncAttrListObj [ 1 ] = waiterAttrListObj ; sciError = NvSciSyncAttrListReconcile ( syncAttrListObj , 2 , & nvSciSyncReconciledListObj , & nvSciSyncConflictListObj3 ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncObjAlloc ( nvSciSyncReconciledListObj , & syncObj ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncCpuWaitContextAlloc ( syncModule , & nvSciCtx ); if ( sciError != NvSciError_Success ) { //handle error } 7.1.4.2.8. Extracting Timestamp Value from Fence  Refer to these sections for more information: Registering an external semaphore: Events setup for cudlaSubmitTask() Waiting on the signal event // To extract Timestamp of the fence // Timestamp will be valid only after fence is signaled // hence Fence must be waited up on before extracting timestamp value uint64_t eofTimestampUS = 0UL ; sciError = NvSciSyncFenceGetTimestamp ( reinterpret_cast < NvSciSyncFence *> ( signalEvents -> eofFences . fence ), & ( eofTimestampUS )); if (( sciError != NvSciError_Success ) || ( eofTimestampUS == 0UL )) { //handle error } 7.1.4.3. Fault Diagnostics  To perform fault diagnostics for DLA HW, users should specify the CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS flag to load the module and CUDLA_SUBMIT_DIAGNOSTICS_TASK during task submission. This task can be used to probe the state of DLA HW. With this flag set, in standalone mode the user is not allowed to do event only submissions, where tensor information is NULL and only events (wait/signal or both) are present in task. This is because the task always runs on an internally loaded diagnostic module. This diagnostic module does not expect any input tensors and so does not require input tensor memory. However the user is expected to query the number of output tensors, allocate the output tensor memory, and pass the same while using the submit task. 7.1.4.4. NOOP Submission  Users can mark certain tasks as noop tasks while calling cudlaSubmitTask() . This is done by passing CUDLA_SUBMIT_NOOP in the flags parameter of cudlaSubmitTask() . A noop submission implies that all the other submission semantics are maintained. Specifically, the task is submitted to DLA, wait/signal events are considered before and after and stream semantics are respected. The key difference is that the task is skipped by the DLA for execution. This is supported in both hybrid and standalone modes. 7.1.5. Error Reporting Model  The asynchronous nature of task execution results in two kinds of errors that can get reported via cuDLA APIs: Synchronous errors Asynchronous errors Synchronous errors are those that are reported by the cuDLA APIs as part of their return code when they are invoked in an application. Asynchronous errors are those that are detected later compared to sequential program execution. The typical scenario here is that each task submitted to the DLA HW executes after a particular duration of time. As a result, if there are errors in the task execution, they cannot be reported as part of the task submission APIs. Depending on the timing of the errors, they are reported during a subsequent cuDLA API call or after a synchronization operation. HW execution errors reported as part of cuDLA APIs are straightforward to handle at the application level. However, if there is a no cuDLA API call currently executing or about to execute in the application, then the application needs to perform extra steps to handle asynchronous errors. In hybrid mode, DLA HW errors can get reported via CUDA synchronization operations. As mentioned in the device model section, cuDLA logically associates DLA with a GPU for the purposes of execution. Therefore, any DLA HW errors are propagated via CUDA to the user. The user needs to check for DLA-specific errors from CUDA synchronization operations and then check the cuDLA device handle for the exact error using cudlaGetLastError() . If there are multiple cuDLA device handles in the application and each of them have submitted some tasks to cuDLA in hybrid mode, then each and every device handle much be checked for errors. The underlying model here is to use CUDA to detect DLA HW errors and then use cudlaGetLastError() on the relevant handle to report the exact error. The code snippet below shows an example: result = cudaStreamSynchronize ( stream ); if ( result != cudaSuccess ) { DPRINTF ( \"Error in synchronizing stream = %s \\n \" , cudaGetErrorName ( result )); if ( result == cudaErrorExternalDevice ) { cudlaStatus hwStatus = cudlaGetLastError ( devHandle ); if ( hwStatus != cudlaSuccess ) { DPRINTF ( \"Asynchronous error in HW = %u \\n \" , hwStatus ); } } } This error reporting model is compatible with CUDA Driver APIs as well and therefore if the application uses CUDA Driver APIs for synchronization, similar error codes and error handling flow is applicable. In standalone mode, the model is similar with the exception that there is no corresponding mechanism to detect errors as part of synchronization operations. In this mode, the only option that an application has to wait on the submitted tasks is to wait on the NvSciSync fence returned by the latest submission. As of this writing, NvSciSync does not support reporting DLA HW errors and therefore an application is expected to wait for the fence and then query cudlaGetLastError() for any errors during execution. 7.2. Migrating from NvMediaDla to cuDLA  NvMediaDla and cuDLA have different programming models with some degree of overlap in the functionality exposed by the respective APIs. The following table provides a mapping from the NvMediaDla API to the equivalent cuDLA API or functionality. This is intended to be used as a reference when migrating an NvMediaDla app to a cuDLA app. NvMediaDla cuDLA NvMediaDlaGetVersion() cudlaGetVersion() NvMediaDlaPingById() Not required as ping is done inside cudlaCreateDevice and only upon successful ping does device handle creation succeed. NvMediaDlaCreate() cudlaCreateDevice() NvMediaDlaDestroy() cudlaDestroyDevice() NvMediaDlaGetUMDVersion() Not available NvMediaDlaGetNumEngines() cudlaDeviceGetCount() NvMediaDlaGetMaxOutstandingTasks() Not available NvMediaDlaInit() cudlaCreateDevice (but specifying number of input tasks is not available) NvMediaDlaGetInstanceId() Not available NvMediaDlaGetNumTasks() Not available NvMediaDlaLoadableCreate() Not required as declaring a variable of type cudlaModule is sufficient alongwith cudlaModuleLoadFromMemory() . NvMediaDlaLoadableDestroy() Not required as cuDLA modules are declared as variables of type cudlaModule . NvMediaDlaAppendLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() . NvMediaDlaSetCurrentLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() . NvMediaDlaGetNumOfInputTensors() cudlaModuleGetAttributes() NvMediaDlaGetInputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaGetNumOfOutputTensors() cudlaModuleGetAttributes() NvMediaDlaGetOutputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaDataRegister() cudlaMemRegister() NvMediaDlaDataUnregister() cudlaMemUnregister() NvMediaDlaLoadLoadable() cudlaModuleLoadFromMemory() NvMediaDlaRemoveLoadable() cudlaModuleUnload() NvMediaDlaSubmit() cudlaSubmitTask() NvMediaDlaNvSciSyncGetVersion() Not available NvMediaDlaFillNvSciSyncAttrList() cudlaGetNvSciSyncAttributes() NvMediaDlaRegisterNvSciSyncObj() cudlaImportExternalSemaphore() NvMediaDlaUnregisterNvSciSyncObj() cudlaMemUnregister() NvMediaDlaSetNvSciSyncObjforEOF() Not required as cudlaTask structure has the required capability to specify this. NvMediaDlaInsertPreNvSciSyncFence() Not required as cudlaTask structure has the required capability to specify this. NvMediaDlaGetEOFNvSciSyncFence() Not required as cudlaTask structure has the required capability to retrieve this. 7.3. Profiling a cuDLA App  cuDLA APIs can be profiled using NVIDIA Nsight Systems. The following command can be used to generate traces for cuDLA APIs. These traces can be viewed in Nsight. $ nsys profile --trace nvtx -e CUDLA_NVTX_LEVEL=1 --output <file> <cudla_App> 7.4. cuDLA Release Notes  Known Issues in cuDLA 1.2.1: In hybrid mode, cuDLA internally allocates memory with CUDA using the primary context. As a result, before destroying/resetting a CUDA primary context, it is mandatory that all cuDLA device initializations are destroyed. Before destroying a cuDLA device handle, it is important to ensure that all tasks submitted previously to the device are completed. Failure to do so can lead to application crashes as the internal memory allocations would still be in use. NvSciBuf buffer allocations made by the application must adhere to DLA alignment constraints. It is the application’s responsibility to ensure that there are no duplicate fences specified as part of wait events while submitting tasks. In general, any synchronous or asynchronous error returned by cuDLA APIs must be treated as a non-recoverable error. In this case, the application is expected to restart and initialize cuDLA again in order to submit DLA tasks. The exception to this rule is cudlaErrorMemoryRegistered which is returned by cuDLA when the application tries to register a particular memory again without unregistering. cuDLA does not support UVM between CUDA and DLA. cuDLA does not support CUDA Graph. cuDLA does not support per-thread default stream. cuDLA does not support CNP (DLA functions cannot be used with CNP). cuDLA does not support block linear memory. cuDLA does not support CUDA VMM APIs at the present moment. cuDLA does not support dGPU. Under certain conditions, DLA FW can hang for certain tasks. This can result in the application hanging in both hybrid as well as standalone mode. Applications are expected to detect these scenarios and respond accordingly. 8. Notices  8.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/incomplete-lu-cholesky/index.html", "parent_url": "https://docs.nvidia.com/cuda/incomplete-lu-cholesky/index.html", "content_type": "text/html", "text": "Incomplete-LU and Cholesky Preconditioned Iterative Methods 1. Introduction 2. Preconditioned Iterative Methods 2.1. Algorithm 1 Conjugate Gradient (CG) 2.2. Algorithm 2 Bi-Conjugate Gradient Stabilized (BiCGStab) 3. Numerical Experiments 4. Conclusion 5. Acknowledgements 6. References 7. Notices 7.1. Notice 7.2. OpenCL 7.3. Trademarks Incomplete-LU and Cholesky Preconditioned Iterative Methods » 1. Introduction v12.5 | PDF | Archive Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS White paper describing how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. 1. Introduction  The solution of large sparse linear systems is an important problem in computational mechanics, atmospheric modeling, geophysics, biology, circuit simulation and many other applications in the field of computational science and engineering. In general, these linear systems can be solved using direct or preconditioned iterative methods. Although the direct methods are often more reliable, they usually have large memory requirements and do not scale well on massively parallel computer platforms. The iterative methods are more amenable to parallelism and therefore can be used to solve larger problems. Currently, the most popular iterative schemes belong to the Krylov subspace family of methods. They include Bi-Conjugate Gradient Stabilized (BiCGStab) and Conjugate Gradient (CG) iterative methods for nonsymmetric and symmetric positive definite (s.p.d.) linear systems, respectively [2] , [11] . We describe these methods in more detail in the next section. In practice, we often use a variety of preconditioning techniques to improve the convergence of the iterative methods. In this white paper we focus on the incomplete-LU and Cholesky preconditioning [11] , which is one of the most popular of these preconditioning techniques. It computes an incomplete factorization of the coefficient matrix and requires a solution of lower and upper triangular linear systems in every iteration of the iterative method. In order to implement the preconditioned BiCGStab and CG we use the sparse matrix-vector multiplication [3] , [15] and the sparse triangular solve [8] , [16] implemented in the cuSPARSE library. We point out that the underlying implementation of these algorithms takes advantage of the CUDA parallel programming paradigm [5] , [9] , [13] , which allows us to explore the computational resources of the graphical processing unit (GPU). In our numerical experiments the incomplete factorization is performed on the CPU (host) and the resulting lower and upper triangular factors are then transferred to the GPU (device) memory before starting the iterative method. However, the computation of the incomplete factorization could also be accelerated on the GPU. We point out that the parallelism available in these iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand. In our numerical experiments the incomplete-LU and Cholesky preconditioned iterative methods achieve on average more than 2x speedup using the cuSPARSE and cuBLAS libraries on the GPU over the MKL [17] implementation on the CPU. For example, the speedup for the preconditioned iterative methods with the incomplete-LU and Cholesky factorization with 0 fill-in (ilu0) is shown in Figure 1 for matrices resulting from a variety of applications. It will be described in more detail in the last section. Speedup of the Incomplete-LU Cholesky (with 0 fill-in) Prec. Iterative Methods  In the next sections we briefly describe the methods of interest and comment on the role played in them by the parallel sparse matrix-vector multiplication and triangular solve algorithms. 2. Preconditioned Iterative Methods  Let us consider the linear system \\(A\\mathbf{x} = \\mathbf{f}\\) where \\(A \\in \\mathbb{R}^{n \\times n}\\) is a nonsingular coefficient matrix and \\(\\mathbf{x},\\mathbf{f} \\in \\mathbb{R}^{n}\\) are the solution and right-hand-side vectors. In general, the iterative methods start with an initial guess and perform a series of steps that find more accurate approximations to the solution. There are two types of iterative methods: (i) the stationary iterative methods, such as the splitting-based Jacobi and Gauss-Seidel (GS), and (ii) the nonstationary iterative methods, such as the Krylov subspace family of methods, which includes CG and BiCGStab . As we mentioned earlier we focus on the latter in this white paper. The convergence of the iterative methods depends highly on the spectrum of the coefficient matrix and can be significantly improved using preconditioning. The preconditioning modifies the spectrum of the coefficient matrix of the linear system in order to reduce the number of iterative steps required for convergence. It often involves finding a preconditioning matrix \\(M\\) , such that \\(M^{- 1}\\) is a good approximation of \\(A^{- 1}\\) and the systems with \\(M\\) are relatively easy to solve. For the s.p.d. matrix \\(A\\) we can let \\(M\\) be its incomplete-Cholesky factorization, so that \\(A \\approx M = {\\widetilde{R}}^{T}\\widetilde{R}\\) , where \\(\\widetilde{R}\\) is an upper triangular matrix. Let us assume that \\(M\\) is nonsingular, then \\({\\widetilde{R}}^{- T}A{\\widetilde{R}}^{- 1}\\) is s.p.d. and instead of solving the linear system (1) , we can solve the preconditioned linear system \\(\\left( {{\\widetilde{R}}^{- T}A{\\widetilde{R}}^{- 1}} \\right)\\left( {\\widetilde{R}\\mathbf{x}} \\right) = {\\widetilde{R}}^{- T}\\mathbf{f}\\) The pseudocode for the preconditioned CG iterative method is shown in Algorithm 1 . 2.1. Algorithm 1 Conjugate Gradient (CG)  1: \\(\\text{Letting initial guess be }\\mathbf{x}_{0}\\text{, compute }\\mathbf{r}\\leftarrow\\mathbf{f} - A\\mathbf{x}_{0}\\) 2: \\(\\textbf{for }i\\leftarrow 1,2,...\\text{ until convergence }\\textbf{do}\\) 3: \\(\\quad\\quad\\text{Solve }M\\mathbf{z}\\leftarrow\\mathbf{r}\\) \\(\\vartriangleright \\text{Sparse lower and upper triangular solves}\\) 4: \\(\\quad\\quad\\rho_{i}\\leftarrow\\mathbf{r}^{T}\\mathbf{z}\\) 5: \\(\\quad\\quad\\textbf{if }i==1\\textbf{ then}\\) 6: \\(\\quad\\quad\\quad\\quad\\mathbf{p}\\leftarrow\\mathbf{z}\\) 7: \\(\\quad\\quad\\textbf{else}\\) 8: \\(\\quad\\quad\\quad\\quad\\beta\\leftarrow\\frac{\\rho_{i}}{\\rho_{i - 1}}\\) 9: \\(\\quad\\quad\\quad\\quad\\mathbf{p}\\leftarrow\\mathbf{z} + \\beta\\mathbf{p}\\) 10: \\(\\quad\\quad\\textbf{end if}\\) 11: \\(\\quad\\quad\\text{Compute }\\mathbf{q}\\leftarrow A\\mathbf{p}\\) \\(\\vartriangleright \\text{Sparse matrix-vector multiplication}\\) 12: \\(\\quad\\quad\\alpha\\leftarrow\\frac{\\rho_{i}}{\\mathbf{p}^{T}\\mathbf{q}}\\) 13: \\(\\quad\\quad\\mathbf{x}\\leftarrow\\mathbf{x} + \\alpha\\mathbf{p}\\) 14: \\(\\quad\\quad\\mathbf{r}\\leftarrow\\mathbf{r} - \\alpha\\mathbf{q}\\) 15: \\(\\textbf{end for}\\) Notice that in every iteration of the incomplete-Cholesky preconditioned CG iterative method we need to perform one sparse matrix-vector multiplication and two triangular solves. The corresponding CG code using the cuSPARSE and cuBLAS libraries in C programming language is shown below. /***** CG Code *****/ /* ASSUMPTIONS: 1. The cuSPARSE and cuBLAS libraries have been initialized. 2. The appropriate memory has been allocated and set to zero. 3. The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- Cholesky upper triangular factor R (valR, csrRowPtrR, csrColIndR) have been computed and are present in the device (GPU) memory. */ //create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo ( & inforRt ); cusparseCreateSolveAnalysisInfo ( & inforR ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforRt ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforR ); //1: compute initial residual r = f -  A x0 (using initial guess in x) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , x , 0.0 , r ); cublasDscal ( n , -1.0 , r , 1 ); cublasDaxpy ( n , 1.0 , f , 1 , r , 1 ); nrmr0 = cublasDnrm2 ( n , r , 1 ); //2: repeat until convergence (based on max. it. and relative residual) for ( i = 0 ; i < maxit ; i ++ ){ //3: Solve M z = r (sparse lower and upper triangular solves) cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_TRANSPOSE , n , 1.0 , descrpR , valR , csrRowPtrR , csrColIndR , inforRt , r , t ); cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrpR , valR , csrRowPtrR , csrColIndR , inforR , t , z ); //4: \\rho = r^{T} z rhop = rho ; rho = cublasDdot ( n , r , 1 , z , 1 ); if ( i == 0 ){ //6: p = z cublasDcopy ( n , z , 1 , p , 1 ); } else { //8: \\beta = rho_{i} / \\rho_{i-1} beta = rho / rhop ; //9: p = z + \\beta p cublasDaxpy ( n , beta , p , 1 , z , 1 ); cublasDcopy ( n , z , 1 , p , 1 ); } //11: Compute q = A p (sparse matrix-vector multiplication) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , p , 0.0 , q ); //12: \\alpha = \\rho_{i} / (p^{T} q) temp = cublasDdot ( n , p , 1 , q , 1 ); alpha = rho / temp ; //13: x = x + \\alpha p cublasDaxpy ( n , alpha , p , 1 , x , 1 ); //14: r = r - \\alpha q cublasDaxpy ( n , - alpha , q , 1 , r , 1 ); //check for convergence nrmr = cublasDnrm2 ( n , r , 1 ); if ( nrmr / nrmr0 < tol ){ break ; } } //destroy the analysis info (for lower and upper triangular factors) cusparseDestroySolveAnalysisInfo ( inforRt ); cusparseDestroySolveAnalysisInfo ( inforR ); For the nonsymmetric matrix \\(A\\) we can let \\(M\\) be its incomplete-LU factorization, so that \\(A \\nvdash M = \\widetilde{L}\\widetilde{U}\\) , where \\(\\widetilde{L}\\) and \\(\\widetilde{U}\\) are lower and upper triangular matrices, respectively. Let us assume that \\(M\\) is nonsingular, then \\(M^{- 1}A\\) is nonsingular and instead of solving the linear system (1) , we can solve the preconditioned linear system \\(\\left( {M^{- 1}A} \\right)\\mathbf{x} = M^{- 1}\\mathbf{f}\\) The pseudocode for the preconditioned BiCGStab iterative method is shown in Algorithm 2 . 2.2. Algorithm 2 Bi-Conjugate Gradient Stabilized (BiCGStab)  1: \\(\\text{Letting initial guess be }\\mathbf{x}_{0}\\text{, compute }\\mathbf{r}\\leftarrow\\mathbf{f} - A\\mathbf{x}_{0}\\) 2: \\(\\text{Set }\\mathbf{p}\\leftarrow\\mathbf{r}\\text{ and choose }\\widetilde{\\mathbf{r}}\\text{, for example you can set }\\widetilde{\\mathbf{r}}\\leftarrow\\mathbf{r}\\) 3: \\(\\textbf{for }i\\leftarrow 1,2,...\\text{ until convergence }\\textbf{do}\\) 4: \\(\\quad\\quad\\rho_{i}\\leftarrow{\\widetilde{\\mathbf{r}}}^{T}\\mathbf{r}\\) 5: \\(\\quad\\quad\\textbf{if }\\rho_{i}==0.0\\textbf{ then}\\) 6: \\(\\quad\\quad\\quad\\quad\\text{method failed}\\) 7: \\(\\quad\\quad\\textbf{end if}\\) 8: \\(\\quad\\quad\\textbf{if }i > 1\\textbf{ then}\\) 9: \\(\\quad\\quad\\quad\\quad\\textbf{if }\\omega==0.0\\textbf{ then}\\) 10: \\(\\quad\\quad\\quad\\quad\\quad\\quad\\text{method failed}\\) 11: \\(\\quad\\quad\\textbf{end if}\\) 12: \\(\\quad\\quad\\quad\\quad\\beta\\leftarrow\\frac{\\rho_{i}}{\\rho_{i - 1}} times \\frac{\\alpha}{\\omega}\\) 13: \\(\\quad\\quad\\quad\\quad\\mathbf{p}\\leftarrow\\mathbf{r} + \\beta\\left( {\\mathbf{p} - \\omega\\mathbf{v}} \\right)\\) 14: \\(\\quad\\quad\\textbf{end if}\\) 15: \\(\\quad\\quad\\text{Solve }M\\hat{\\mathbf{p}}\\leftarrow\\mathbf{p}\\) \\(\\vartriangleright \\text{Sparse lower and upper triangular solves}\\) 16: \\(\\quad\\quad\\text{Compute }\\mathbf{q}\\leftarrow A\\hat{\\mathbf{p}}\\) \\(\\vartriangleright \\text{Sparse matrix-vector multiplication}\\) 17: \\(\\quad\\quad\\alpha\\leftarrow\\frac{\\rho_{i}}{{\\widetilde{\\mathbf{r}}}^{T}\\mathbf{q}}\\) 18: \\(\\quad\\quad\\mathbf{s}\\leftarrow\\mathbf{r} - \\alpha\\mathbf{q}\\) 19: \\(\\quad\\quad\\mathbf{x}\\leftarrow\\mathbf{x} + \\alpha\\hat{\\mathbf{p}}\\) 20: \\(\\quad\\quad\\textbf{if }\\left\\| s \\right\\|_{2} \\leq \\mathit{tol}\\textbf{ then}\\) 21: \\(\\quad\\quad\\quad\\quad\\text{method converged}\\) 22: \\(\\quad\\quad\\textbf{end if}\\) 23: \\(\\quad\\quad\\text{Solve }M\\hat{\\mathbf{s}}\\leftarrow\\mathbf{s}\\) \\(\\vartriangleright \\text{Sparse lower and upper triangular solves}\\) 24: \\(\\quad\\quad\\text{Compute }\\mathbf{t}\\leftarrow A\\hat{\\mathbf{s}}\\) \\(\\vartriangleright \\text{Sparse matrix-vector multiplication}\\) 25: \\(\\quad\\quad\\omega\\leftarrow\\frac{\\mathbf{t}^{T}\\mathbf{s}}{\\mathbf{t}^{T}\\mathbf{t}}\\) 26: \\(\\quad\\quad\\mathbf{x}\\leftarrow\\mathbf{x} + \\omega\\hat{\\mathbf{s}}\\) 27: \\(\\quad\\quad\\mathbf{r}\\leftarrow\\mathbf{s} - \\omega\\mathbf{t}\\) 28: \\(\\textbf{end for}\\) Notice that in every iteration of the incomplete-LU preconditioned BiCGStab iterative method we need to perform two sparse matrix-vector multiplications and four triangular solves. The corresponding BiCGStab code using the cuSPARSE and cuBLAS libraries in C programming language is shown below. /***** BiCGStab Code *****/ /* ASSUMPTIONS: 1. The cuSPARSE and cuBLAS libraries have been initialized. 2. The appropriate memory has been allocated and set to zero. 3. The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- LU lower L (valL, csrRowPtrL, csrColIndL)  and upper U (valU, csrRowPtrU, csrColIndU) triangular factors have been computed and are present in the device (GPU) memory. */ //create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo ( & infoL ); cusparseCreateSolveAnalysisInfo ( & infoU ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrL , valL , csrRowPtrL , csrColIndL , infoL ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrU , valU , csrRowPtrU , csrColIndU , infoU ); //1: compute initial residual r = b - A x0 (using initial guess in x) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , x , 0.0 , r ); cublasDscal ( n , -1.0 , r , 1 ); cublasDaxpy ( n , 1.0 , f , 1 , r , 1 ); //2: Set p=r and \\tilde{r}=r cublasDcopy ( n , r , 1 , p , 1 ); cublasDcopy ( n , r , 1 , rw , 1 ); nrmr0 = cublasDnrm2 ( n , r , 1 ); //3: repeat until convergence (based on max. it. and relative residual) for ( i = 0 ; i < maxit ; i ++ ){ //4: \\rho = \\tilde{r}^{T} r rhop = rho ; rho = cublasDdot ( n , rw , 1 , r , 1 ); if ( i > 0 ){ //12: \\beta = (\\rho_{i} / \\rho_{i-1}) ( \\alpha / \\omega ) beta = ( rho / rhop ) * ( alpha / omega ); //13: p = r + \\beta (p - \\omega v) cublasDaxpy ( n , - omega , q , 1 , p , 1 ); cublasDscal ( n , beta , p , 1 ); cublasDaxpy ( n , 1.0 , r , 1 , p , 1 ); } //15: M \\hat{p} = p (sparse lower and upper triangular solves) cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrL , valL , csrRowPtrL , csrColIndL , infoL , p , t ); cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrU , valU , csrRowPtrU , csrColIndU , infoU , t , ph ); //16: q = A \\hat{p} (sparse matrix-vector multiplication) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , ph , 0.0 , q ); //17: \\alpha = \\rho_{i} / (\\tilde{r}^{T} q) temp = cublasDdot ( n , rw , 1 , q , 1 ); alpha = rho / temp ; //18: s = r - \\alpha q cublasDaxpy ( n , - alpha , q , 1 , r , 1 ); //19: x = x + \\alpha \\hat{p} cublasDaxpy ( n , alpha , ph , 1 , x , 1 ); //20: check for convergence nrmr = cublasDnrm2 ( n , r , 1 ); if ( nrmr / nrmr0 < tol ){ break ; } //23: M \\hat{s} = r (sparse lower and upper triangular solves) cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrL , valL , csrRowPtrL , csrColIndL , infoL , r , t ); cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrU , valU , csrRowPtrU , csrColIndU , infoU , t , s ); //24: t = A \\hat{s} (sparse matrix-vector multiplication) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , s , 0.0 , t ); //25: \\omega = (t^{T} s) / (t^{T} t) temp = cublasDdot ( n , t , 1 , r , 1 ); temp2 = cublasDdot ( n , t , 1 , t , 1 ); omega = temp / temp2 ; //26: x = x + \\omega \\hat{s} cublasDaxpy ( n , omega , s , 1 , x , 1 ); //27: r = s - \\omega t cublasDaxpy ( n , - omega , t , 1 , r , 1 ); //check for convergence nrmr = cublasDnrm2 ( n , r , 1 ); if ( nrmr / nrmr0 < tol ){ break ; } } //destroy the analysis info (for lower and upper triangular factors) cusparseDestroySolveAnalysisInfo ( infoL ); cusparseDestroySolveAnalysisInfo ( infoU ); As shown in Figure 2 the majority of time in each iteration of the incomplete-LU and Cholesky preconditioned iterative methods is spent in the sparse matrix-vector multiplication and triangular solve. The sparse matrix-vector multiplication has already been extensively studied in the following references [3] , [15] . The sparse triangular solve is not as well known, so we briefly point out the strategy used to explore parallelism in it and refer the reader to the NVIDIA technical report [8] for further details. The Splitting of Total Time Taken on the GPU by the Preconditioned Iterative Method  To understand the main ideas behind the sparse triangular solve, notice that although the forward and back substitution is an inherently sequential algorithm for dense triangular systems, the dependencies on the previously obtained elements of the solution do not necessarily exist for the sparse triangular systems. We pursue the strategy that takes advantage of the lack of these dependencies and split the solution process into two phases as mentioned in [1] , [4] , [6] , [7] , [8] , [10] , [12] , [14] . The analysis phase builds the data dependency graph that groups independent rows into levels based on the matrix sparsity pattern. The solve phase iterates across the constructed levels one-by-one and computes all elements of the solution corresponding to the rows at a single level in parallel. Notice that by construction the rows within each level are independent of each other, but are dependent on at least one row from the previous level. The analysis phase needs to be performed only once and is usually significantly slower than the solve phase, which can be performed multiple times. This arrangement is ideally suited for the incomplete-LU and Cholesky preconditioned iterative methods. 3. Numerical Experiments  In this section we study the performance of the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods. We use twelve matrices selected from The University of Florida Sparse Matrix Collection [18] in our numerical experiments. The seven s.p.d. and five nonsymmetric matrices with the respective number of rows (m), columns (n=m) and non-zero elements (nnz) are grouped and shown according to their increasing order in Table 1 . Table 1. Symmetric Positive Definite (s.p.d.) and Nonsymmetric Test Matrices  # Matrix m,n nnz s.p.d. Application 1 offshore 259,789 4,242,673 yes Geophysics 2 af_shell3 504,855 17,562,051 yes Mechanics 3 parabolic_fem 525,825 3,674,625 yes General 4 apache2 715,176 4,817,870 yes Mechanics 5 ecology2 999,999 4,995,991 yes Biology 6 thermal2 1,228,045 8,580,313 yes Thermal Simulation 7 G3_circuit 1,585,478 7,660,826 yes Circuit Simulation 8 FEM_3D_thermal2 147,900 3,489,300 no Mechanics 9 thermomech_dK 204,316 2,846,228 no Mechanics 10 ASIC_320ks 321,671 1,316,08511 no Circuit Simulation 11 cage13 445,315 7,479,343 no Biology 12 atmosmodd 1,270,432 8,814,880 no Atmospheric Model In the following experiments we use the hardware system with NVIDIA C2050 (ECC on) GPU and Intel Core i7 CPU 950 @ 3.07GHz, using the 64-bit Linux operating system Ubuntu 10.04 LTS, cuSPARSE library 4.0 and MKL 10.2.3.029. The MKL_NUM_THREADS and MKL_DYNAMIC environment variables are left unset to allow MKL to use the optimal number of threads. We compute the incomplete-LU and Cholesky factorizations using the MKL routines csrilu0 and csrilut with 0 and threshold fill-in, respectively. In the csrilut routine we allow three different levels of fill-in denoted by (5,10 -3 ), (10,10 -5 ) and (20,10 -7 ). In general, the \\(\\left( k,\\mathit{tol} \\right)\\) fill-in is based on \\(nnz/n + k\\) maximum allowed number of elements per row and the dropping of elements with magnitude \\(\\left| l_{ij} \\middle| , \\middle| u_{ij} \\middle| < \\mathit{tol} \\times \\left\\| \\mathbf{a}_{i}^{T} \\right\\|_{2} \\right.\\) , where \\(l_{ij}\\) , \\(u_{ij}\\) and \\(\\mathbf{a}_{i}^{T}\\) are the elements of the lower \\(L\\) , upper \\(U\\) triangular factors and the i -th row of the coefficient matrix \\(A\\) , respectively. We compare the implementation of the BiCGStab and CG iterative methods using the cuSPARSE and cuBLAS libraries on the GPU and MKL on the CPU. In our experiments we let the initial guess be zero, the right-hand-side \\(\\mathbf{f} = A\\mathbf{e}\\) where \\(\\mathbf{e}^{T}{= (1,\\ldots,1)}^{T}\\) , and the stopping criteria be the maximum number of iterations 2000 or relative residual \\(\\left\\| \\mathbf{r}_{i} \\right\\|_{2}/\\left\\| \\mathbf{r}_{0} \\right\\|_{2} < 10^{- 7}\\) , where \\(\\mathbf{r}_{i} = \\mathbf{f} - A\\mathbf{x}_{i}\\) is the residual at i -th iteration. Table 2. csrilu0 Preconditioned CG and BiCGStab Methods  ilu0 CPU GPU Speedup # fact. time(s) copy time(s) solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. vs. ilu0 1 0.38 0.02 0.72 8.83E-08 25 1.52 8.83E-08 25 0.57 2 1.62 0.04 38.5 1.00E-07 569 33.9 9.69E-08 571 1.13 3 0.13 0.01 39.2 9.84E-08 1044 6.91 9.84E-08 1044 5.59 4 0.12 0.01 35.0 9.97E-08 713 12.8 9.97E-08 713 2.72 5 0.09 0.01 107 9.98E-08 1746 55.3 9.98E-08 1746 1.92 6 0.40 0.02 155 9.96E-08 1656 54.4 9.79E-08 1656 2.83 7 0.16 0.02 20.2 8.70E-08 183 8.61 8.22E-08 183 2.32 8 0.32 0.02 0.13 5.25E-08 4 0.52 5.25E-08 4 0.53 9 0.20 0.01 72.7 1.96E-04 2000 40.0 2.08E-04 2000 1.80 10 0.11 0.01 0.27 6.33E-08 6 0.12 6.33E-08 6 1.59 11 0.70 0.03 0.28 2.52E-08 2.5 0.15 2.52E-08 2.5 1.10 12 0.25 0.04 12.5 7.33E-08 76.5 4.30 9.69E-08 74.5 2.79 Table 3. csrilut (5,10 -3 ) Preconditioned CG and BiCGStab Methods  ilut(5,10 -3 ) CPU GPU Speedup # fact. time(s) copy time(s) solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. vs. ilut (5,10 -3 ) vs. ilu0 1 0.14 0.01 1.17 9.70E-08 32 1.82 9.70E-08 32 0.67 0.69 2 0.51 0.03 49.1 9.89E-08 748 33.6 9.89E-08 748 1.45 1.39 3 1.47 0.02 11.7 9.72E-08 216 6.93 9.72E-08 216 1.56 1.86 4 0.17 0.01 67.9 9.96E-08 1495 26.5 9.96E-08 1495 2.56 5.27 5 0.55 0.04 59.5 9.22E-08 653 71.6 9.22E-08 653 0.83 1.08 6 3.59 0.05 47.0 9.50E-08 401 90.1 9.64E-08 401 0.54 0.92 7 1.24 0.05 23.1 8.08E-08 153 24.8 8.08E-08 153 0.93 2.77 8 0.82 0.03 0.12 3.97E-08 2 1.12 3.97E-08 2 0.48 1.10 9 0.10 0.01 54.3 5.68E-04 2000 24.5 1.58E-04 2000 2.21 1.34 10 0.12 0.01 0.16 4.89E-08 4 0.08 6.45E-08 4 1.37 1.15 11 4.99 0.07 0.36 1.40E-08 2.5 0.37 1.40E-08 2.5 0.99 6.05 12 0.32 0.03 39.2 7.05E-08 278.5 10.6 8.82E-08 270.5 3.60 8.60 The results of the numerical experiments are shown in Table 2 through Table 5 , where we state the speedup obtained by the iterative method on the GPU over CPU (speedup), number of iterations required for convergence (# it.), achieved relative residual ( \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) ) and time in seconds taken by the factorization (fact.), iterative solution of the linear system (solve), and cudaMemcpy of the lower and upper triangular factors to the GPU (copy). We include the time taken to compute the incomplete-LU and Cholesky factorization as well as to transfer the triangular factors from the CPU to the GPU memory in the computed speedup. Table 4. csrilut (10,10 -5 ) Preconditioned CG and BiCGStab Methods  ilut(10,10 -5 ) CPU GPU Speedup # fact. time(s) copy time(s) solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. vs. ilut (10,10 -5 ) vs. ilu0 1 0.15 0.01 1.06 8.79E-08 34 1.96 8.79E-08 34 0.57 0.63 2 0.52 0.03 60.0 9.86E-08 748 38.7 9.86E-08 748 1.54 1.70 3 3.89 0.03 9.02 9.79E-08 147 5.42 9.78E-08 147 1.38 1.83 4 1.09 0.03 34.5 9.83E-08 454 38.2 9.83E-08 454 0.91 2.76 5 3.25 0.06 26.3 9.71E-08 272 55.2 9.71E-08 272 0.51 0.53 6 11.0 0.07 44.7 9.42E-08 263 84.0 9.44E-08 263 0.59 1.02 7 5.95 0.09 8.84 8.53E-08 43 17.0 8.53E-08 43 0.64 1.68 8 2.94 0.04 0.09 2.10E-08 1.5 1.75 2.10E-08 1.5 0.64 3.54 9 0.11 0.01 53.2 4.24E-03 2000 24.4 4.92E-03 2000 2.18 1.31 10 0.12 0.01 0.16 4.89E-11 4 0.08 6.45E-11 4 1.36 1.18 11 2.89 0.09 0.44 6.10E-09 2.5 0.48 6.10E-09 2.5 1.00 33.2 12 0.36 0.03 36.6 7.05E-08 278.5 10.6 8.82E-08 270.5 3.35 8.04 Table 5. csrilut (20,10 -7 ) Preconditioned CG and BiCGStab Methods  ilut(20,10 -7 ) CPU GPU Speedup # fact. time(s) copy time(s) solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} \\right\\|_{2}}{\\left\\| \\mathbf{r}_{0} \\right\\|_{2}}\\) # it. vs. ilut (20,10 -7 ) vs. ilu0 1 0.82 0.02 47.6 9.90E-08 1297 159 9.86E-08 1292 0.30 25.2 2 9.21 0.11 32.1 8.69E-08 193 84.6 8.67E-08 193 0.44 1.16 3 10.04 0.04 6.26 9.64E-08 90 4.75 9.64E-08 90 1.10 2.36 4 8.12 0.10 15.7 9.02E-08 148 22.5 9.02E-08 148 0.78 1.84 5 8.60 0.10 21.2 9.52E-08 158 53.6 9.52E-08 158 0.48 0.54 6 35.2 0.11 29.2 9.88E-08 162 80.5 9.88E-08 162 0.56 1.18 7 23.1 0.14 3.79 7.50E-08 14 12.1 7.50E-08 14 0.76 3.06 8 5.23 0.05 0.14 1.19E-09 1.5 2.37 1.19E-09 1.5 0.70 6.28 9 0.12 0.01 55.1 3.91E-03 2000 24.4 2.27E-03 2000 2.25 1.36 10 0.14 0.01 0.14 9.35E-08 3.5 0.07 7.19E-08 3.5 1.28 1.18 11 218 0.12 0.43 9.80E-08 2 0.66 9.80E-08 2 1.00 12 15.0 0.21 12.2 3.45E-08 31 4.95 3.45E-08 31 1.35 5.93 The summary of performance of BiCGStab and CG iterative methods preconditioned with different incomplete factorizations on the GPU is shown in Figure 3 , where “*” indicates that the method did not converge to the required tolerance. Notice that in general in our numerical experiments the performance for the incomplete factorizations decreases as the threshold parameters are relaxed and the factorization becomes more dense, thus inhibiting parallelism due to data dependencies between rows in the sparse triangular solve. For this reason, the best performance on the GPU is obtained for the incomplete-LU and Cholesky factorization with 0 fill-in, which will be our point of reference. Performance of BiCGStab and CG with Incomplete-LU Cholesky Preconditioning  Although the incomplete factorizations with a more relaxed threshold are often closer to the exact factorization and thus result in fewer iterative steps, they are also much more expensive to compute. Moreover, notice that even though the number of iterative steps decreases, each step is more computationally expensive. As a result of these tradeoffs the total time, the sum of the time taken by the factorization and the iterative solve, for the iterative method does not necessarily decrease with a more relaxed threshold in our numerical experiments. The speedup based on the total time taken by the preconditioned iterative method on the GPU with csrilu0 preconditioner and CPU with all four preconditioners is shown in Figure 4 . Notice that for majority of matrices in our numerical experiments the implementation of the iterative method using the cuSPARSE and cuBLAS libraries does indeed outperform the MKL. Speedup of prec. BiCGStab and CG on GPU (with csrilu0 ) vs. CPU (with all)  Finally, the average of the obtained speedups is shown in Figure 5 , where we have excluded the runs with cage13 matrix for ilut (10,10 -5 ) and runs with offshore and cage13 matrices for ilut (20,10 -7 ) incomplete factorizations because of their disproportional speedup. However, the speedup including these runs is shown in parenthesis on the same plot. Consequently, we can conclude that the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods obtain on average more than 2x speedup on the GPU over their CPU implementation. Average Speedup of BiCGStab and CG on GPU (with csrilu0 ) and CPU (with all)  4. Conclusion  The performance of the iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand. In our numerical experiments the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods implemented on the GPU using the cuSPARSE and cuBLAS libraries achieved an average of 2x speedup over their MKL implementation. The sparse matrix-vector multiplication and triangular solve, which is split into a slower analysis phase that needs to be performed only once and a faster solve phase that can be performed multiple times, were the essential building blocks of these iterative methods. In fact the obtained speedup was usually mostly influenced by the time taken by the solve phase of the algorithm. Finally, we point out that the use of multiple-right-hand-sides would increase the available parallelism and can result in a significant relative performance improvement in the preconditioned iterative methods. Also, the development of incomplete-LU and Cholesky factorizations using CUDA parallel programming paradigm can further improve the obtained speedup. 5. Acknowledgements  This white paper was authored by Maxim Naumov for NVIDIA Corporation. Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page. 6. References  [1] E. Anderson and Y. Saad Solving Sparse Triangular Linear Systems on Parallel Computers, Int. J. High Speed Comput., pp. 73-95, 1989. [2] R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, H. van der Vorst, Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, SIAM, Philadelphia, PA, 1994. [3] N. Bell and M. Garland, Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors, Proc. Conf. HPC Networking, Storage and Analysis (SC09), ACM, pp. 1-11, 2009. [4] A. Greenbaum, Solving Sparse Triangular Linear Systems using Fortran with Parallel Extensions on the NYU Ultracomputer Prototype, Report 99, NYU Ultracomputer Note, New York University, NY, April, 1986. [5] D. B. Kirk and W. W. Hwu, Programming Massively Parallel Processors: A Hands-on Approach, Elsevier, 2010. [6] J. Mayer, Parallel Algorithms for Solving Linear Systems with Sparse Triangular Matrices, Computing, pp. 291-312 (86), 2009. [7] R. Mirchandaney, J. H. Saltz and D. Baxter, Run-Time Parallelization and Scheduling of Loops, IEEE Transactions on Computers, pp. (40), 1991. [8] M. Naumov, Parallel Solution of Sparse Triangular Linear Systems in the Preconditioned Iterative Methods on the GPU, NVIDIA Technical Report, NVR-2011-001, 2011. [9] J. Nickolls, I. Buck, M. Garland and K. Skadron, Scalable Parallel Programming with CUDA, Queue, pp. 40-53 (6-2), 2008. [10] E. Rothberg and A. Gupta, Parallel ICCG on a Hierarchical Memory Multiprocessor - Addressing the Triangular Solve Bottleneck, Parallel Comput., pp. 719-741 (18), 1992. [11] Y. Saad, Iterative Methods for Sparse Linear Systems, SIAM, Philadelphia, PA, 2nd Ed., 2003. [12] J. H. Saltz, Aggregation Methods for Solving Sparse Triangular Systems on Multiprocessors, SIAM J. Sci. Statist. Comput., pp. 123-144 (11), 1990. [13] J. Sanders and E. Kandrot, CUDA by Example: An Introduction to General-Purpose GPU Programming, Addison-Wesley, 2010. [14] M. Wolf, M. Heroux and E. Boman, Factors Impacting Performance of Multithreaded Sparse Triangular Solve, 9th Int. Meet. HPC Comput. Sci. (VECPAR), 2010. [15] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick and J. Demmel, Optimization of Sparse Matrix-Vector Multiplication on Emerging Multicore Platforms, Parallel Comput., pp. 178-194 (35-3), 2009. [16] NVIDIA cuSPARSE and cuBLAS Libraries, http://www.nvidia.com/object/cuda_develop.html [17] Intel Math Kernel Library, http://software.intel.com/en-us/articles/intel-mkl [18] The University of Florida Sparse Matrix Collection, http://www.cise.ufl.edu/research/sparse/matrices/ . 7. Notices  7.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2011-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/floating-point/index.html", "parent_url": "https://docs.nvidia.com/cuda/floating-point/index.html", "content_type": "text/html", "text": "Floating Point and IEEE 754 1. Introduction 2. Floating Point 2.1. Formats 2.2. Operations and Accuracy 2.3. The Fused Multiply-Add (FMA) 3. Dot Product: An Accuracy Example 3.1. Example Algorithms 3.2. Comparison 4. CUDA and Floating Point 4.1. Compute Capability 2.0 and Above 4.2. Rounding Modes 4.3. Controlling Fused Multiply-add 4.4. Compiler Flags 4.5. Differences from x86 5. Considerations for a Heterogeneous World 5.1. Mathematical Function Accuracy 5.2. x87 and SSE 5.3. Core Counts 5.4. Verifying GPU Results 6. Concrete Recommendations 7. Acknowledgements 8. References 9. Notices 9.1. Notice 9.2. OpenCL 9.3. Trademarks Floating Point and IEEE 754 » 1. Introduction v12.5 | PDF | Archive Floating Point and IEEE 754 Compliance for NVIDIA GPUs White paper covering the most common issues related to NVIDIA GPUs. A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide. 1. Introduction  Since the widespread adoption in 1985 of the IEEE Standard for Binary Floating-Point Arithmetic (IEEE 754-1985 [1] ) virtually all mainstream computing systems have implemented the standard, including NVIDIA with the CUDA architecture. IEEE 754 standardizes how arithmetic results should be approximated in floating point. Whenever working with inexact results, programming decisions can affect accuracy. It is important to consider many aspects of floating point behavior in order to achieve the highest performance with the precision required for any specific application. This is especially true in a heterogeneous computing environment where operations will be performed on different types of hardware. Understanding some of the intricacies of floating point and the specifics of how NVIDIA hardware handles floating point is obviously important to CUDA programmers striving to implement correct numerical algorithms. In addition, users of libraries such as cuBLAS and cuFFT will also find it informative to learn how NVIDIA handles floating point under the hood. We review some of the basic properties of floating point calculations in Chapter 2 . We also discuss the fused multiply-add operator, which was added to the IEEE 754 standard in 2008 [2] and is built into the hardware of NVIDIA GPUs. In Chapter 3 we work through an example of computing the dot product of two short vectors to illustrate how different choices of implementation affect the accuracy of the final result. In Chapter 4 we describe NVIDIA hardware versions and NVCC compiler options that affect floating point calculations. In Chapter 5 we consider some issues regarding the comparison of CPU and GPU results. Finally, in Chapter 6 we conclude with concrete recommendations to programmers that deal with numeric issues relating to floating point on the GPU. 2. Floating Point  2.1. Formats  Floating point encodings and functionality are defined in the IEEE 754 Standard [2] last revised in 2008. Goldberg [5] gives a good introduction to floating point and many of the issues that arise. The standard mandates binary floating point data be encoded on three fields: a one bit sign field, followed by exponent bits encoding the exponent offset by a numeric bias specific to each format, and bits encoding the significand (or fraction). In order to ensure consistent computations across platforms and to exchange floating point data, IEEE 754 defines basic and interchange formats. The 32 and 64 bit basic binary floating point formats correspond to the C data types float and double . Their corresponding representations have the following bit lengths: For numerical data representing finite values, the sign is either negative or positive, the exponent field encodes the exponent in base 2, and the fraction field encodes the significand without the most significant non-zero bit. For example, the value -192 equals (-1) 1 x 2 7 x 1.5, and can be represented as having a negative sign, an exponent of 7, and a fractional part .5. The exponents are biased by 127 and 1023, respectively, to allow exponents to extend from negative to positive. Hence the exponent 7 is represented by bit strings with values 134 for float and 1030 for double. The integral part of 1. is implicit in the fraction. Also, encodings to represent infinity and not-a-number (NaN) data are reserved. The IEEE 754 Standard [2] describes floating point encodings in full. Given that the fraction field uses a limited number of bits, not all real numbers can be represented exactly. For example the mathematical value of the fraction 2/3 represented in binary is 0.10101010… which has an infinite number of bits after the binary point. The value 2/3 must be rounded first in order to be represented as a floating point number with limited precision. The rules for rounding and the rounding modes are specified in IEEE 754. The most frequently used is the round-to-nearest-or-even mode (abbreviated as round-to-nearest). The value 2/3 rounded in this mode is represented in binary as: The sign is positive and the stored exponent value represents an exponent of -1. 2.2. Operations and Accuracy  The IEEE 754 standard requires support for a handful of operations. These include the arithmetic operations add, subtract, multiply, divide, square root, fused-multiply-add, remainder, conversion operations, scaling, sign operations, and comparisons. The results of these operations are guaranteed to be the same for all implementations of the standard, for a given format and rounding mode. The rules and properties of mathematical arithmetic do not hold directly for floating point arithmetic because of floating point’s limited precision. For example, the table below shows single precision values A , B , and C , and the mathematical exact value of their sum computed using different associativity. \\(\\begin{matrix}\nA & = & {2^{1} \\times 1.00000000000000000000001} \\\\\nB & = & {2^{0} \\times 1.00000000000000000000001} \\\\\nC & = & {2^{3} \\times 1.00000000000000000000001} \\\\\n{(A + B) + C} & = & {2^{3} \\times 1.01100000000000000000001011} \\\\\n{A + (B + C)} & = & {2^{3} \\times 1.01100000000000000000001011} \\\\\n\\end{matrix}\\) Mathematically, ( A + B ) + C does equal A + ( B + C ). Let rn( x ) denote one rounding step on x . Performing these same computations in single precision floating point arithmetic in round-to-nearest mode according to IEEE 754, we obtain: \\(\\begin{matrix}\n{A + B} & = & {2^{1} \\times 1.1000000000000000000000110000...} \\\\\n{\\text{rn}(A + B)} & = & {2^{1} \\times 1.10000000000000000000010} \\\\\n{B + C} & = & {2^{3} \\times 1.0010000000000000000000100100...} \\\\\n{\\text{rn}(B + C)} & = & {2^{3} \\times 1.00100000000000000000001} \\\\\n{A + B + C} & = & {2^{3} \\times 1.0110000000000000000000101100...} \\\\\n{\\text{rn}\\left( \\text{rn}(A + B) + C \\right)} & = & {2^{3} \\times 1.01100000000000000000010} \\\\\n{\\text{rn}\\left( A + \\text{rn}(B + C) \\right)} & = & {2^{3} \\times 1.01100000000000000000001} \\\\\n\\end{matrix}\\) For reference, the exact, mathematical results are computed as well in the table above. Not only are the results computed according to IEEE 754 different from the exact mathematical results, but also the results corresponding to the sum rn(rn(A + B) + C) and the sum rn(A + rn(B + C)) are different from each other. In this case, rn(A + rn(B + C)) is closer to the correct mathematical result than rn(rn(A + B) + C). This example highlights that seemingly identical computations can produce different results even if all basic operations are computed in compliance with IEEE 754. Here, the order in which operations are executed affects the accuracy of the result. The results are independent of the host system. These same results would be obtained using any microprocessor, CPU or GPU, which supports single precision floating point. 2.3. The Fused Multiply-Add (FMA)  In 2008 the IEEE 754 standard was revised to include the fused multiply-add operation ( FMA ). The FMA operation computes \\(\\text{rn}(X \\times Y + Z)\\) with only one rounding step. Without the FMA operation the result would have to be computed as \\(\\text{rn}\\left( \\text{rn}(X \\times Y) + Z \\right)\\) with two rounding steps, one for multiply and one for add. Because the FMA uses only a single rounding step the result is computed more accurately. Let’s consider an example to illustrate how the FMA operation works using decimal arithmetic first for clarity. Let’s compute \\(x^{2} - 1\\) with four digits of precision after the decimal point, or a total of five digits of precision including the leading digit before the decimal point. For \\(x = 1.0008\\) , the correct mathematical result is \\(x^{2} - 1 = 1.60064 \\times 10^{- 4}\\) . The closest number using only four digits after the decimal point is \\(1.6006 \\times 10^{- 4}\\) . In this case \\(\\text{rn}\\left( x^{2} - 1 \\right) = 1.6006 \\times 10^{- 4}\\) which corresponds to the fused multiply-add operation \\(\\text{rn}\\left( x \\times x + ( - 1) \\right)\\) . The alternative is to compute separate multiply and add steps. For the multiply, \\(x^{2} = 1.00160064\\) , so \\(\\text{rn}\\left( x^{2} \\right) = 1.0016\\) . The final result is \\(\\text{rn}\\left( \\text{rn}\\left( x^{2} \\right) - 1 \\right) = 1.6000 \\times 10^{- 4}\\) . Rounding the multiply and add separately yields a result that is off by 0.00064. The corresponding FMA computation is wrong by only 0.00004, and its result is closest to the correct mathematical answer. The results are summarized below: \\(\\begin{matrix}\nx & = & 1.0008 & \\\\\nx^{2} & = & 1.00160064 & \\\\\n{x^{2} - 1} & = & {1.60064 \\times 10^{- 4}\\text{~~}} & \\text{true\\ value} \\\\\n{\\text{rn}\\left( x^{2} - 1 \\right)} & = & {1.6006 \\times 10^{- 4}} & \\text{fused\\ multiply-add} \\\\\n{\\text{rn}\\left( x^{2} \\right)} & = & {1.0016 \\times 10^{- 4}} & \\\\\n{\\text{rn}\\left( \\text{rn}\\left( x^{2} \\right) - 1 \\right)} & = & {1.6000 \\times 10^{- 4}} & \\text{multiply,\\ then\\ add} \\\\\n\\end{matrix}\\) Below is another example, using binary single precision values: \\(\\begin{matrix}\nA & = & & 2^{0} & {\\times 1.00000000000000000000001} \\\\\nB & = & - & 2^{0} & {\\times 1.00000000000000000000010} \\\\\n{\\text{rn}(A \\times A + B)} & = & & 2^{- 46} & {\\times 1.00000000000000000000000} \\\\\n{\\text{rn}\\left( \\text{rn}(A \\times A) + B \\right)} & = & & 0 & \\\\\n\\end{matrix}\\) In this particular case, computing \\(\\text{rn}\\left( \\text{rn}(A \\times A) + B \\right)\\) as an IEEE 754 multiply followed by an IEEE 754 add loses all bits of precision, and the computed result is 0. The alternative of computing the FMA \\(\\text{rn}(A \\times A + B)\\) provides a result equal to the mathematical value. In general, the fused-multiply-add operation generates more accurate results than computing one multiply followed by one add. The choice of whether or not to use the fused operation depends on whether the platform provides the operation and also on how the code is compiled. Figure 1 shows CUDA C++ code and output corresponding to inputs A and B and operations from the example above. The code is executed on two different hardware platforms: an x86-class CPU using SSE in single precision, and an NVIDIA GPU with compute capability 2.0. At the time this paper is written (Spring 2011) there are no commercially available x86 CPUs which offer hardware FMA. Because of this, the computed result in single precision in SSE would be 0. NVIDIA GPUs with compute capability 2.0 do offer hardware FMAs, so the result of executing this code will be the more accurate one by default. However, both results are correct according to the IEEE 754 standard. The code fragment was compiled without any special intrinsics or compiler options for either platform. The fused multiply-add helps avoid loss of precision during subtractive cancellation. Subtractive cancellation occurs during the addition of quantities of similar magnitude with opposite signs. In this case many of the leading bits cancel, leaving fewer meaningful bits of precision in the result. The fused multiply-add computes a double-width product during the multiplication. Thus even if subtractive cancellation occurs during the addition there are still enough valid bits remaining in the product to get a precise result with no loss of precision. 3. Dot Product: An Accuracy Example  Consider the problem of finding the dot product of two short vectors \\(\\overset{\\rightarrow}{a}\\) and \\(\\overset{\\rightarrow}{b}\\) , both with four elements. \\(\\overset{\\rightharpoonup}{a} = \\begin{bmatrix}\na_{1} \\\\\na_{2} \\\\\na_{3} \\\\\na_{4} \\\\\n\\end{bmatrix}\\mspace{2mu}\\quad\\overset{\\rightharpoonup}{b} = \\begin{bmatrix}\nb_{1} \\\\\nb_{2} \\\\\nb_{3} \\\\\nb_{4} \\\\\n\\end{bmatrix}\\quad\\overset{\\rightharpoonup}{a} \\cdot \\overset{\\rightharpoonup}{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + a_{4}b_{4}\\) This operation is easy to write mathematically, but its implementation in software involves several choices. All of the strategies we will discuss use purely IEEE 754 compliant operations. 3.1. Example Algorithms  We present three algorithms which differ in how the multiplications, additions, and possibly fused multiply-adds are organized. These algorithms are presented in Figure 2 , Figure 3 , and Figure 4 . Each of the three algorithms is represented graphically. Individual operation are shown as a circle with arrows pointing from arguments to operations. The simplest way to compute the dot product is using a short loop as shown in Figure 2 . The multiplications and additions are done separately. Serial Method to Compute Vectors Dot Product.  The serial method uses a simple loop with separate multiplies and adds to compute the do t product of the vectors. The final result can be represented as ((((a 1 x b 1 ) + (a 2 x b 2 )) + (a 3 x b 3 )) + (a 4 x b 4 )). FMA Method to Compute Vector Dot Product.  The FMA method uses a simple loop with fused multiply-adds to compute the dot product of the vectors. The final result can be represented as a 4 x b 4 = (a 3 x b 3 + (a 2 x b 2 + (a 1 x b 1 + 0))). A simple improvement to the algorithm is to use the fused multiply-add to do the multiply and addition in one step to improve accuracy. Figure 3 shows this version. Yet another way to compute the dot product is to use a divide-and-conquer strategy in which we first find the dot products of the first half and the second half of the vectors, then combine these results using addition. This is a recursive strategy; the base case is the dot product of vectors of length 1 which is a single multiply. Figure 4 graphically illustrates this approach. We call this algorithm the parallel algorithm because the two sub-problems can be computed in parallel as they have no dependencies. The algorithm does not require a parallel implementation, however; it can still be implemented with a single thread. 3.2. Comparison  All three algorithms for computing a dot product use IEEE 754 arithmetic and can be implemented on any system that supports the IEEE standard. In fact, an implementation of the serial algorithm on multiple systems will give exactly the same result. So will implementations of the FMA or parallel algorithms. However, results computed by an implementation of the serial algorithm may differ from those computed by an implementation of the other two algorithms. The Parallel Method to Reduce Individual Elements Products into a Final Sum.  The parallel method uses a tree to reduce all the products of individual elements into a final sum. The final result can be represented as ((a 1 x b 1 ) + (a 2 x b 2 )) + ((a 3 x b 3 ) + (a 4 x b 4 )). 4. CUDA and Floating Point  NVIDIA has extended the capabilities of GPUs with each successive hardware generation. Current generations of the NVIDIA architecture such as Tesla Kxx , GTX 8xx , and GTX 9xx , support both single and double precision with IEEE 754 precision and include hardware support for fused multiply-add in both single and double precision. In CUDA, the features supported by the GPU are encoded in the compute capability number. The runtime library supports a function call to determine the compute capability of a GPU at runtime; the CUDA C++ Programming Guide also includes a table of compute capabilities for many different devices [7] . 4.1. Compute Capability 2.0 and Above  Devices with compute capability 2.0 and above support both single and double precision IEEE 754 including fused multiply-add in both single and double precision. Operations such as square root and division will result in the floating point value closest to the correct mathematical result in both single and double precision, by default. 4.2. Rounding Modes  The IEEE 754 standard defines four rounding modes: round-to-nearest, round towards positive, round towards negative, and round towards zero. CUDA supports all four modes. By default, operations use round-to-nearest. Compiler intrinsics like the ones listed in the tables below can be used to select other rounding modes for individual operations. mode interpretation rn round to nearest, ties to even rz round towards zero ru round towards \\(+ \\text{∞}\\) rd round towards \\(- \\text{∞}\\) x + y __fadd_[rn | rz | ru | rd] (x, y) addition x * y __fmul_[rn | rz | ru | rd] (x, y) multiplication fmaf (x, y, z) __fmaf_[rn | rz | ru | rd] (x, y, z) FMA 1.0f / x __frcp_[rn | rz | ru | rd] (x) reciprocal x / y __fdiv_[rn | rz | ru | rd] (x, y) division sqrtf(x) __fsqrt_[rn | rz | ru | rd] (x) square root x + y __dadd_[rn | rz | ru | rd] (x, y) addition x * y __dmul_[rn | rz | ru | rd] (x, y) multiplication fma (x, y, z) __fma_[rn | rz | ru | rd] (x, y, z) FMA 1.0 / x __drcp_[rn | rz | ru | rd] (x) reciprocal x / y __ddiv_[rn | rz | ru | rd] (x, y) division sqrtf(x) __dsqrt_[rn | rz | ru | rd] (x) square root 4.3. Controlling Fused Multiply-add  In general, the fused multiply-add operation is faster and more accurate than performing separate multiply and add operations. However, on occasion you may wish to disable the merging of multiplies and adds into fused multiply-add instructions. To inhibit this optimization one can write the multiplies and additions using intrinsics with explicit rounding mode as shown in the previous tables. Operations written directly as intrinsics are guaranteed to remain independent and will not be merged into fused multiply-add instructions. It is also possible to disable FMA merging via a compiler flag. 4.4. Compiler Flags  Compiler flags relevant to IEEE 754 operations are -ftz={true|false} , -prec-div={true|false} , and -prec-sqrt={true|false} . These flags control single precision operations on devices of compute capability of 2.0 or later. mode flags IEEE 754 mode (default) -ftz=false -prec-div=true -prec-sqrt=true fast mode -ftz=true -prec-div=false -prec-sqrt=false The default IEEE 754 mode means that single precision operations are correctly rounded and support denormals, as per the IEEE 754 standard. In the fast mode denormal numbers are flushed to zero, and the operations division and square root are not computed to the nearest floating point value. The flags have no effect on double precision or on devices of compute capability below 2.0. 4.5. Differences from x86  NVIDIA GPUs differ from the x86 architecture in that rounding modes are encoded within each floating point instruction instead of dynamically using a floating point control word. Trap handlers for floating point exceptions are not supported. On the GPU there is no status flag to indicate when calculations have overflowed, underflowed, or have involved inexact arithmetic. Like SSE , the precision of each GPU operation is encoded in the instruction (for x87 the precision is controlled dynamically by the floating point control word). 5. Considerations for a Heterogeneous World  5.1. Mathematical Function Accuracy  So far we have only considered simple math operations such as addition, multiplication, division, and square root. These operations are simple enough that computing the best floating point result (e.g., the closest in round-to-nearest) is reasonable. For other mathematical operations computing the best floating point result is harder. The problem is called the table maker’s dilemma . To guarantee the correctly rounded result, it is not generally enough to compute the function to a fixed high accuracy. There might still be rare cases where the error in the high accuracy result affects the rounding step at the lower accuracy. It is possible to solve the dilemma for particular functions by doing mathematical analysis and formal proofs [4] , but most math libraries choose instead to give up the guarantee of correct rounding. Instead they provide implementations of math functions and document bounds on the relative error of the functions over the input range. For example, the double precision sin function in CUDA is guaranteed to be accurate to within 2 units in the last place (ulp) of the correctly rounded result. In other words, the difference between the computed result and the mathematical result is at most ±2 with respect to the least significant bit position of the fraction part of the floating point result. For most inputs the sin function produces the correctly rounded result. Take for example the C code sequence shown in Figure 6 . We compiled the code sequence on a 64-bit x86 platform using gcc version 4.4.3 (Ubuntu 4.3.3-4ubuntu5). This shows that the result of computing cos(5992555.0) using a common library differs depending on whether the code is compiled in 32-bit mode or 64-bit mode. The consequence is that different math libraries cannot be expected to compute exactly the same result for a given input. This applies to GPU programming as well. Functions compiled for the GPU will use the NVIDIA CUDA math library implementation while functions compiled for the CPU will use the host compiler math library implementation (e.g., glibc on Linux). Because these implementations are independent and neither is guaranteed to be correctly rounded, the results will often differ slightly. 5.2. x87 and SSE  One of the unfortunate realities of C compilers is that they are often poor at preserving IEEE 754 semantics of floating point operations [6] . This can be particularly confusing on platforms that support x87 and SSE operations. Just like CUDA operations, SSE operations are performed on single or double precision values, while x87 operations often use an additional internal 80-bit precision format. Sometimes the results of a computation using x87 can depend on whether an intermediate result was allocated to a register or stored to memory. Values stored to memory are rounded to the declared precision (e.g., single precision for float and double precision for double ). Values kept in registers can remain in extended precision. Also, x87 instructions will often be used by default for 32-bit compiles but SSE instructions will be used by default for 64-bit compiles. Because of these issues, guaranteeing a specific precision level on the CPU can sometimes be tricky. When comparing CPU results to results computed on the GPU, it is generally best to compare using SSE instructions. SSE instructions follow IEEE 754 for single and doubleprecision. On 32-bit x86 targets without SSE it can be helpful to declare variables using volatile and force floating point values to be stored to memory ( /Op in Visual Studio and -ffloat-store in gcc ). This moves results from extended precision registers into memory, where the precision is precisely single or double precision. Alternately, the x87 control word can be updated to set the precision to 24 or 53 bits using the assembly instruction fldcw or a compiler option such as -mpc32 or -mpc64 in gcc . 5.3. Core Counts  As we have shown in Section 3 , the final values computed using IEEE 754 arithmetic can depend on implementation choices such as whether to use fused multiply-add or whether additions are organized in series or parallel. These differences affect computation on the CPU and on the GPU. One way such differences can arise is from differences between the number of concurrent threads involved in a computation. On the GPU, a common design pattern is to have all threads in a block coordinate to do a parallel reduction on data within the block, followed by a serial reduction of the results from each block. Changing the number of threads per block reorganizes the reduction; if the reduction is addition, then the change rearranges parentheses in the long string of additions. Even if the same general strategy such as parallel reduction is used on the CPU and GPU, it is common to have widely different numbers of threads on the GPU compared to the CPU. For example, the GPU implementation might launch blocks with 128 threads per block, while the CPU implementation might use 4 threads in total. 5.4. Verifying GPU Results  The same inputs will give the same results for individual IEEE 754 operations to a given precision on the CPU and GPU. As we have explained, there are many reasons why the same sequence of operations may not be performed on the CPU and GPU. The GPU has fused multiply-add while the CPU does not. Parallelizing algorithms may rearrange operations, yielding different numeric results. The CPU may be computing results in a precision higher than expected. Finally, many common mathematical functions are not required by the IEEE 754 standard to be correctly rounded so should not be expected to yield identical results between implementations. When porting numeric code from the CPU to the GPU of course it makes sense to use the x86 CPU results as a reference. But differences between the CPU result and GPU result must be interpreted carefully. Differences are not automatically evidence that the result computed by the GPU is wrong or that there is a problem on the GPU. Computing results in a high precision and then comparing to results computed in a lower precision can be helpful to see if the lower precision is adequate for a particular application. However, rounding high precision results to a lower precision is not equivalent to performing the entire computation in lower precision. This can sometimes be a problem when using x87 and comparing results against the GPU. The results of the CPU may be computed to an unexpectedly high extended precision for some or all of the operations. The GPU result will be computed using single or double precision only. 6. Concrete Recommendations  The key points we have covered are the following: Use the fused multiply-add operator. The fused multiply-add operator on the GPU has high performance and increases the accuracy of computations. No special flags or function calls are needed to gain this benefit in CUDA programs. Understand that a hardware fused multiply-add operation is not yet available on the CPU, which can cause differences in numerical results. Compare results carefully. Even in the strict world of IEEE 754 operations, minor details such as organization of parentheses or thread counts can affect the final result. Take this into account when doing comparisons between implementations. Know the capabilities of your GPU. The numerical capabilities are encoded in the compute capability number of your GPU. Devices of compute capability 2.0 and later are capable of single and double precision arithmetic following the IEEE 754 standard, and have hardware units for performing fused multiply-add in both single and double precision. Take advantage of the CUDA math library functions. These functions are documented in the CUDA C++ Programming Guide [7] . The math library includes all the math functions listed in the C99 standard [3] plus some additional useful functions. These functions have been tuned for a reasonable compromise between performance and accuracy.\nWe constantly strive to improve the quality of our math library functionality. Please let us know about any functions that you require that we do not provide, or if the accuracy or performance of any of our functions does not meet your needs. Leave comments in the NVIDIA CUDA forum 1 or join the Registered Developer Program 2 and file a bug with your feedback. 7. Acknowledgements  This paper was authored by Nathan Whitehead and Alex Fit-Florea for NVIDIA Corporation. Thanks to Ujval Kapasi, Kurt Wall, Paul Sidenblad, Massimiliano Fatica, Everett Phillips, Norbert Juffa, and Will Ramey for their helpful comments and suggestions. Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page. 8. References  [1] ANSI/IEEE 754-1985. American National Standard - IEEE Standard for Binary Floating-Point Arithmetic. American National Standards Institute, Inc., New York, 1985. [2] IEEE 754-2008. IEEE 754–2008 Standard for Floating-Point Arithmetic. August 2008. [3] ISO/IEC 9899:1999(E). Programming languages - C. American National Standards Institute, Inc., New York, 1999. [4] Catherine Daramy-Loirat, David Defour, Florent de Dinechin, Matthieu Gallet, Nicolas Gast, and Jean-Michel Muller. CR-LIBM: A library of correctly rounded elementary functions in double-precision, February 2005. [5] David Goldberg. What every computer scientist should know about floating-point arithmetic. ACM Computing Surveys, March 1991. Edited reprint available at: http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html . [6] David Monniaux. The pitfalls of verifying floating-point computations. ACM Transactions on Programming Languages and Systems, May 2008. [7] NVIDIA. CUDA C++ Programming Guide Version 10.2, 2019. 9. Notices  9.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 9.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 9.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 https://forums.nvidia.com/index.php?showforum=62 2 https://developer.nvidia.com/ join-nvidia-registered-developer-program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2011-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html", "content_type": "text/html", "text": "CUDA Binary Utilities 1. Overview 1.1. What is a CUDA Binary? 1.2. Differences between cuobjdump and nvdisasm 1.3. Command Option Types and Notation 2. cuobjdump 2.1. Usage 2.2. Command-line Options 3. nvdisasm 3.1. Usage 3.2. Command-line Options 4. Instruction Set Reference 4.1. Maxwell and Pascal Instruction Set 4.2. Volta Instruction Set 4.3. Turing Instruction Set 4.4. NVIDIA Ampere GPU and Ada Instruction Set 4.5. Hopper Instruction Set 5. cu++filt 5.1. Usage 5.2. Command-line Options 5.3. Library Availability 6. nvprune 6.1. Usage 6.2. Command-line Options 7. Notices 7.1. Notice 7.2. OpenCL 7.3. Trademarks cuda-binary-utilities » 1. Overview v12.5 | PDF | Archive CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, cu++filt, and nvprune. 1. Overview  This document introduces cuobjdump , nvdisasm , cu++filt and nvprune , four CUDA binary tools for Linux (x86, ARM and P9), Windows, Mac OS and Android. 1.1. What is a CUDA Binary?  A CUDA binary (also referred to as cubin) file is an ELF-formatted file which consists of CUDA executable code sections as well as other sections containing symbols, relocators, debug info, etc. By default, the CUDA compiler driver nvcc embeds cubin files into the host executable file. But they can also be generated separately by using the “ -cubin ” option of nvcc . cubin files are loaded at run time by the CUDA driver API. Note For more details on cubin files or the CUDA compilation trajectory, refer to NVIDIA CUDA Compiler Driver NVCC . 1.2. Differences between cuobjdump and nvdisasm  CUDA provides two binary utilities for examining and disassembling cubin files and host executables: cuobjdump and nvdisasm . Basically, cuobjdump accepts both cubin files and host binaries while nvdisasm only accepts cubin files; but nvdisasm provides richer output options. Here’s a quick comparison of the two tools: Table 1. Comparison of cuobjdump and nvdisasm  cuobjdump nvdisasm Disassemble cubin Yes Yes Extract ptx and extract and disassemble cubin from the following input files: Host binaries Executables Object files Static libraries External fatbinary files Yes No Control flow analysis and output No Yes Advanced display options No Yes 1.3. Command Option Types and Notation  This section of the document provides common details about the command line options for the following tools: cuobjdump nvdisasm nvprune Each command-line option has a long name and a short name, which are interchangeable with each other. These two variants are distinguished by the number of hyphens that must precede the option name, i.e. long names must be preceded by two hyphens and short names must be preceded by a single hyphen. For example, -I is the short name of --include-path . Long options are intended for use in build scripts, where size of the option is less important than descriptive value and short options are intended for interactive use. The tools mentioned above recognize three types of command options: boolean options, single value options and list options. Boolean options do not have an argument, they are either specified on a command line or not. Single value options must be specified at most once and list options may be repeated. Examples of each of these option types are, respectively: Boolean option : nvdisams --print-raw <file>\nSingle value   : nvdisasm --binary SM70 <file>\nList options   : cuobjdump --function \"foo,bar,foobar\" <file> Single value options and list options must have arguments, which must follow the name of the option by either one or more spaces or an equals character. When a one-character short name such as -I , -l , and -L is used, the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character. The individual values of list options may be separated by commas in a single instance of the option or the option may be repeated, or any combination of these two cases. Hence, for the two sample options mentioned above that may take values, the following notations are legal: -o file\n-o=file\n-Idir1,dir2 -I=dir3 -I dir4,dir5 For options taking a single value, if specified multiple times, the rightmost value in the command line will be considered for that option. In the below example, test.bin binary will be disassembled assuming SM75 as the architecture. nvdisasm.exe -b SM70 -b SM75 test.bin\nnvdisasm warning : incompatible redefinition for option 'binary', the last value of this option was used For options taking a list of values, if specified multiple times, the values get appended to the list. If there are duplicate values specified, they are ignored. In the below example, functions foo and bar are considered as valid values for option --function and the duplicate value foo is ignored. cuobjdump --function \"foo\" --function \"bar\" --function \"foo\" -sass  test.cubin 2. cuobjdump  cuobjdump extracts information from CUDA binary files (both standalone and those embedded in host binaries) and presents them in human readable format. The output of cuobjdump includes CUDA assembly code for each kernel, CUDA ELF section headers, string tables, relocators and other CUDA specific sections. It also extracts embedded ptx text from host binaries. For a list of CUDA assembly instruction set of each GPU architecture, see Instruction Set Reference . 2.1. Usage  cuobjdump accepts a single input file each time it’s run. The basic usage is as following: cuobjdump [options] <file> To disassemble a standalone cubin or cubins embedded in a host executable and show CUDA assembly of the kernels, use the following command: cuobjdump -sass <input file> To dump cuda elf sections in human readable format from a cubin file, use the following command: cuobjdump -elf <cubin file> To extract ptx text from a host binary, use the following command: cuobjdump -ptx <host binary> Here’s a sample output of cuobjdump : $ cuobjdump a.out -sass -ptx\nFatbin elf code:\n================\narch = sm_70\ncode version = [1,7]\nproducer = cuda\nhost = linux\ncompile_size = 64bit\nidentifier = add.cu\n\ncode for sm_70\n        Function : _Z3addPiS_S_\n.headerflags    @\"EF_CUDA_SM70 EF_CUDA_PTX_SM(EF_CUDA_SM70)\"\n/*0000*/      IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;  /* 0x00000a00ff017624 */\n                                                       /* 0x000fd000078e00ff */\n/*0010*/ @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;            /* 0x000000fffffff389 */\n                                                       /* 0x000fe200000e00ff */\n/*0020*/      IMAD.MOV.U32 R2, RZ, RZ, c[0x0][0x160] ; /* 0x00005800ff027624 */\n                                                       /* 0x000fe200078e00ff */\n/*0030*/      MOV R3, c[0x0][0x164] ;                  /* 0x0000590000037a02 */\n                                                       /* 0x000fe20000000f00 */\n/*0040*/      IMAD.MOV.U32 R4, RZ, RZ, c[0x0][0x168] ; /* 0x00005a00ff047624 */\n                                                       /* 0x000fe200078e00ff */\n/*0050*/      MOV R5, c[0x0][0x16c] ;                  /* 0x00005b0000057a02 */\n                                                       /* 0x000fcc0000000f00 */\n/*0060*/      LDG.E.SYS R2, [R2] ;                     /* 0x0000000002027381 */\n                                                       /* 0x000ea800001ee900 */\n/*0070*/      LDG.E.SYS R5, [R4] ;                     /* 0x0000000004057381 */\n                                                       /* 0x000ea200001ee900 */\n/*0080*/      IMAD.MOV.U32 R6, RZ, RZ, c[0x0][0x170] ; /* 0x00005c00ff067624 */\n                                                       /* 0x000fe200078e00ff */\n/*0090*/      MOV R7, c[0x0][0x174] ;                  /* 0x00005d0000077a02 */\n                                                       /* 0x000fe40000000f00 */\n/*00a0*/      IADD3 R9, R2, R5, RZ ;                   /* 0x0000000502097210 */\n                                                       /* 0x004fd00007ffe0ff */\n/*00b0*/      STG.E.SYS [R6], R9 ;                     /* 0x0000000906007386 */\n                                                       /* 0x000fe2000010e900 */\n/*00c0*/      EXIT ;                                   /* 0x000000000000794d */\n                                                       /* 0x000fea0003800000 */\n/*00d0*/      BRA 0xd0;                                /* 0xfffffff000007947 */\n                                                       /* 0x000fc0000383ffff */\n/*00e0*/      NOP;                                     /* 0x0000000000007918 */\n                                                       /* 0x000fc00000000000 */\n/*00f0*/      NOP;                                     /* 0x0000000000007918 */\n                                                       /* 0x000fc00000000000 */\n        ....................... Fatbin ptx code:\n================\narch = sm_70\ncode version = [7,0]\nproducer = cuda\nhost = linux\ncompile_size = 64bit\ncompressed\nidentifier = add.cu\n\n.version 7.0\n.target sm_70\n.address_size 64\n\n.visible .entry _Z3addPiS_S_(\n.param .u64 _Z3addPiS_S__param_0,\n.param .u64 _Z3addPiS_S__param_1,\n.param .u64 _Z3addPiS_S__param_2\n)\n{\n.reg .s32 %r<4>;\n.reg .s64 %rd<7>;\n\nld.param.u64 %rd1, [_Z3addPiS_S__param_0];\nld.param.u64 %rd2, [_Z3addPiS_S__param_1];\nld.param.u64 %rd3, [_Z3addPiS_S__param_2];\ncvta.to.global.u64 %rd4, %rd3;\ncvta.to.global.u64 %rd5, %rd2;\ncvta.to.global.u64 %rd6, %rd1;\nld.global.u32 %r1, [%rd6];\nld.global.u32 %r2, [%rd5];\nadd.s32 %r3, %r2, %r1;\nst.global.u32 [%rd4], %r3;\nret;\n} As shown in the output, the a.out host binary contains cubin and ptx code for sm_70. To list cubin files in the host binary use -lelf option: $ cuobjdump a.out -lelf\nELF file    1: add_new.sm_70.cubin\nELF file    2: add_new.sm_75.cubin\nELF file    3: add_old.sm_70.cubin\nELF file    4: add_old.sm_75.cubin To extract all the cubins as files from the host binary use -xelf all option: $ cuobjdump a.out -xelf all\nExtracting ELF file    1: add_new.sm_70.cubin\nExtracting ELF file    2: add_new.sm_75.cubin\nExtracting ELF file    3: add_old.sm_70.cubin\nExtracting ELF file    4: add_old.sm_75.cubin To extract the cubin named add_new.sm_70.cubin : $ cuobjdump a.out -xelf add_new.sm_70.cubin\nExtracting ELF file    1: add_new.sm_70.cubin To extract only the cubins containing _old in their names: $ cuobjdump a.out -xelf _old\nExtracting ELF file    1: add_old.sm_70.cubin\nExtracting ELF file    2: add_old.sm_75.cubin You can pass any substring to -xelf and -xptx options. Only the files having the substring in the name will be extracted from the input binary. To dump common and per function resource usage information: $ cuobjdump test.cubin -res-usage\n\nResource usage:\n Common:\n  GLOBAL:56 CONSTANT[3]:28\n Function calculate:\n  REG:24 STACK:8 SHARED:0 LOCAL:0 CONSTANT[0]:472 CONSTANT[2]:24 TEXTURE:0 SURFACE:0 SAMPLER:0\n Function mysurf_func:\n  REG:38 STACK:8 SHARED:4 LOCAL:0 CONSTANT[0]:532 TEXTURE:8 SURFACE:7 SAMPLER:0\n Function mytexsampler_func:\n  REG:42 STACK:0 SHARED:0 LOCAL:0 CONSTANT[0]:472 TEXTURE:4 SURFACE:0 SAMPLER:1 Note that value for REG, TEXTURE, SURFACE and SAMPLER denotes the count and for other resources it denotes no. of byte(s) used. 2.2. Command-line Options  Table 2 contains supported command-line options of cuobjdump , along with a description of what each option does. Each option has a long name and a short name, which can be used interchangeably. Table 2. cuobjdump Command-line Options  Option (long) Option (short) Description --all-fatbin -all Dump all fatbin sections. By default will only dump contents of executable fatbin (if exists), else relocatable fatbin if no executable fatbin. --dump-elf -elf Dump ELF Object sections. --dump-elf-symbols -symbols Dump ELF symbol names. --dump-ptx -ptx Dump PTX for all listed device functions. --dump-sass -sass Dump CUDA assembly for a single cubin file or all cubin files embedded in the binary. --dump-resource-usage -res-usage Dump resource usage for each ELF. Useful in getting all the resource usage information at one place. --extract-elf <partial file name>,... -xelf Extract ELF file(s) name containing <partial file name> and save as file(s). Use all to extract all files. To get the list of ELF files use -lelf option. Works with host executable/object/library and external fatbin. All dump and list options are ignored with this option. --extract-ptx <partial file name>,... -xptx Extract PTX file(s) name containing <partial file name> and save as file(s). Use all to extract all files. To get the list of PTX files use -lptx option. Works with host executable/object/library and external fatbin. All dump and list options are ignored with this option. --extract-text <partial file name>,... -xtext Extract text binary encoding file(s) name containing <partial file name> and save as file(s). Use ‘all’ to extract all files. To get the list of text binary encoding use -ltext option. All ‘dump’ and ‘list’ options are ignored with this option. --function <function name>,... -fun Specify names of device functions whose fat binary structures must be dumped. --function-index <function index>,... -findex Specify symbol table index of the function whose fat binary structures must be dumped. --gpu-architecture <gpu architecture name> -arch Specify GPU Architecture for which information should be dumped. Allowed values for this option: sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 , sm_90a . --help -h Print this help information on this tool. --list-elf -lelf List all the ELF files available in the fatbin. Works with host executable/object/library and external fatbin. All other options are ignored with this flag. This can be used to select particular ELF with -xelf option later. --list-ptx -lptx List all the PTX files available in the fatbin. Works with host executable/object/library and external fatbin. All other options are ignored with this flag. This can be used to select particular PTX with -xptx option later. --list-text -ltext List all the text binary function names available in the fatbin. All other options are ignored with the flag. This can be used to select particular function with -xtext option later. --options-file <file>,... -optf Include command line options from specified file. --sort-functions -sort Sort functions when dumping sass. --version -V Print version information on this tool. 3. nvdisasm  nvdisasm extracts information from standalone cubin files and presents them in human readable format. The output of nvdisasm includes CUDA assembly code for each kernel, listing of ELF data sections and other CUDA specific sections. Output style and options are controlled through nvdisasm command-line options. nvdisasm also does control flow analysis to annotate jump/branch targets and makes the output easier to read. Note nvdisasm requires complete relocation information to do control flow analysis. If this information is missing from the CUDA binary, either use the nvdisasm option -ndf to turn off control flow analysis, or use the ptxas and nvlink option -preserve-relocs to re-generate the cubin file. For a list of CUDA assembly instruction set of each GPU architecture, see Instruction Set Reference . 3.1. Usage  nvdisasm accepts a single input file each time it’s run. The basic usage is as following: nvdisasm [options] <input cubin file> Here’s a sample output of nvdisasm : .headerflags    @\"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM70\n                      EF_CUDA_VIRTUAL_SM(EF_CUDA_SM70)\"\n    .elftype        @\"ET_EXEC\"\n\n//--------------------- .nv.info                  --------------------------\n    .section        .nv.info,\"\",@\"SHT_CUDA_INFO\"\n    .align  4\n\n......\n\n//--------------------- .text._Z9acos_main10acosParams --------------------------\n    .section    .text._Z9acos_main10acosParams,\"ax\",@progbits\n    .sectioninfo    @\"SHI_REGISTERS=14\"\n    .align    128\n        .global     _Z9acos_main10acosParams\n        .type       _Z9acos_main10acosParams,@function\n        .size       _Z9acos_main10acosParams,(.L_21 - _Z9acos_main10acosParams)\n        .other      _Z9acos_main10acosParams,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n_Z9acos_main10acosParams:\n.text._Z9acos_main10acosParams:\n        /*0000*/               MOV R1, c[0x0][0x28] ;\n        /*0010*/               NOP;\n        /*0020*/               S2R R0, SR_CTAID.X ;\n        /*0030*/               S2R R3, SR_TID.X ;\n        /*0040*/               IMAD R0, R0, c[0x0][0x0], R3 ;\n        /*0050*/               ISETP.GE.AND P0, PT, R0, c[0x0][0x170], PT ;\n        /*0060*/           @P0 EXIT ;\n.L_1:\n        /*0070*/               MOV R11, 0x4 ;\n        /*0080*/               IMAD.WIDE R2, R0, R11, c[0x0][0x160] ;\n        /*0090*/               LDG.E.SYS R2, [R2] ;\n        /*00a0*/               MOV R7, 0x3d53f941 ;\n        /*00b0*/               FADD.FTZ R4, |R2|.reuse, -RZ ;\n        /*00c0*/               FSETP.GT.FTZ.AND P0, PT, |R2|.reuse, 0.5699, PT ;\n        /*00d0*/               FSETP.GEU.FTZ.AND P1, PT, R2, RZ, PT ;\n        /*00e0*/               FADD.FTZ R5, -R4, 1 ;\n        /*00f0*/               IMAD.WIDE R2, R0, R11, c[0x0][0x168] ;\n        /*0100*/               FMUL.FTZ R5, R5, 0.5 ;\n        /*0110*/           @P0 MUFU.SQRT R4, R5 ;\n        /*0120*/               MOV R5, c[0x0][0x0] ;\n        /*0130*/               IMAD R0, R5, c[0x0][0xc], R0 ;\n        /*0140*/               FMUL.FTZ R6, R4, R4 ;\n        /*0150*/               FFMA.FTZ R7, R6, R7, 0.018166976049542427063 ;\n        /*0160*/               FFMA.FTZ R7, R6, R7, 0.046756859868764877319 ;\n        /*0170*/               FFMA.FTZ R7, R6, R7, 0.074846573173999786377 ;\n        /*0180*/               FFMA.FTZ R7, R6, R7, 0.16667014360427856445 ;\n        /*0190*/               FMUL.FTZ R7, R6, R7 ;\n        /*01a0*/               FFMA.FTZ R7, R4, R7, R4 ;\n        /*01b0*/               FADD.FTZ R9, R7, R7 ;\n        /*01c0*/          @!P0 FADD.FTZ R9, -R7, 1.5707963705062866211 ;\n        /*01d0*/               ISETP.GE.AND P0, PT, R0, c[0x0][0x170], PT ;\n        /*01e0*/          @!P1 FADD.FTZ R9, -R9, 3.1415927410125732422 ;\n        /*01f0*/               STG.E.SYS [R2], R9 ;\n        /*0200*/          @!P0 BRA `(.L_1) ;\n        /*0210*/               EXIT ;\n.L_2:\n        /*0220*/               BRA `(.L_2);\n.L_21: To get the control flow graph of a kernel, use the following: nvdisasm -cfg <input cubin file> nvdisasm is capable of generating control flow of CUDA assembly in the format of DOT graph description language. The output of the control flow from nvdisasm can be directly imported to a DOT graph visualization tool such as Graphviz . Here’s how you can generate a PNG image ( cfg.png ) of the control flow of the above cubin ( a.cubin ) with nvdisasm and Graphviz: nvdisasm -cfg a.cubin | dot -ocfg.png -Tpng Here’s the generated graph: Control Flow Graph  To generate a PNG image ( bbcfg.png ) of the basic block control flow of the above cubin ( a.cubin ) with nvdisasm and Graphviz: nvdisasm -bbcfg a.cubin | dot -obbcfg.png -Tpng Here’s the generated graph: Basic Block Control Flow Graph  nvdisasm is capable of showing the register (general and predicate) liveness range information. For each line of CUDA assembly, nvdisasm displays whether a given device register was assigned, accessed, live or re-assigned. It also shows the total number of registers used. This is useful if the user is interested in the life range of any particular register, or register usage in general. Here’s a sample output (output is pruned for brevity): // +-----------------+------+\n                                                      // |      GPR        | PRED |\n                                                      // |                 |      |\n                                                      // |                 |      |\n                                                      // |    000000000011 |      |\n                                                      // |  # 012345678901 | # 01 |\n                                                      // +-----------------+------+\n    .global acos                                      // |                 |      |\n    .type   acos,@function                            // |                 |      |\n    .size   acos,(.L_21 - acos)                       // |                 |      |\n    .other  acos,@\"STO_CUDA_ENTRY STV_DEFAULT\"        // |                 |      |\nacos:                                                 // |                 |      |\n.text.acos:                                           // |                 |      |\n    MOV R1, c[0x0][0x28] ;                            // |  1  ^           |      |\n    NOP;                                              // |  1  ^           |      |\n    S2R R0, SR_CTAID.X ;                              // |  2 ^:           |      |\n    S2R R3, SR_TID.X ;                                // |  3 :: ^         |      |\n    IMAD R0, R0, c[0x0][0x0], R3 ;                    // |  3 x: v         |      |\n    ISETP.GE.AND P0, PT, R0, c[0x0][0x170], PT ;      // |  2 v:           | 1 ^  |\n@P0 EXIT ;                                            // |  2 ::           | 1 v  |\n.L_1:                                                 // |  2 ::           |      |\n     MOV R11, 0x4 ;                                   // |  3 ::         ^ |      |\n     IMAD.WIDE R2, R0, R11, c[0x0][0x160] ;           // |  5 v:^^       v |      |\n     LDG.E.SYS R2, [R2] ;                             // |  4 ::^        : |      |\n     MOV R7, 0x3d53f941 ;                             // |  5 :::    ^   : |      |\n     FADD.FTZ R4, |R2|.reuse, -RZ ;                   // |  6 ::v ^  :   : |      |\n     FSETP.GT.FTZ.AND P0, PT, |R2|.reuse, 0.5699, PT; // |  6 ::v :  :   : | 1 ^  |\n     FSETP.GEU.FTZ.AND P1, PT, R2, RZ, PT ;           // |  6 ::v :  :   : | 2 :^ |\n     FADD.FTZ R5, -R4, 1 ;                            // |  6 ::  v^ :   : | 2 :: |\n     IMAD.WIDE R2, R0, R11, c[0x0][0x168] ;           // |  8 v:^^:: :   v | 2 :: |\n     FMUL.FTZ R5, R5, 0.5 ;                           // |  5 ::  :x :     | 2 :: |\n @P0 MUFU.SQRT R4, R5 ;                               // |  5 ::  ^v :     | 2 v: |\n     MOV R5, c[0x0][0x0] ;                            // |  5 ::  :^ :     | 2 :: |\n     IMAD R0, R5, c[0x0][0xc], R0 ;                   // |  5 x:  :v :     | 2 :: |\n     FMUL.FTZ R6, R4, R4 ;                            // |  5 ::  v ^:     | 2 :: |\n     FFMA.FTZ R7, R6, R7, 0.018166976049542427063 ;   // |  5 ::  : vx     | 2 :: |\n     FFMA.FTZ R7, R6, R7, 0.046756859868764877319 ;   // |  5 ::  : vx     | 2 :: |\n     FFMA.FTZ R7, R6, R7, 0.074846573173999786377 ;   // |  5 ::  : vx     | 2 :: |\n     FFMA.FTZ R7, R6, R7, 0.16667014360427856445 ;    // |  5 ::  : vx     | 2 :: |\n     FMUL.FTZ R7, R6, R7 ;                            // |  5 ::  : vx     | 2 :: |\n     FFMA.FTZ R7, R4, R7, R4 ;                        // |  4 ::  v  x     | 2 :: |\n     FADD.FTZ R9, R7, R7 ;                            // |  4 ::     v ^   | 2 :: |\n@!P0 FADD.FTZ R9, -R7, 1.5707963705062866211 ;        // |  4 ::     v ^   | 2 v: |\n     ISETP.GE.AND P0, PT, R0, c[0x0][0x170], PT ;     // |  3 v:       :   | 2 ^: |\n@!P1 FADD.FTZ R9, -R9, 3.1415927410125732422 ;        // |  3 ::       x   | 2 :v |\n     STG.E.SYS [R2], R9 ;                             // |  3 ::       v   | 1 :  |\n@!P0 BRA `(.L_1) ;                                    // |  2 ::           | 1 v  |\n     EXIT ;                                           // |  1  :           |      |\n.L_2:                                                 // +.................+......+\n     BRA `(.L_2);                                     // |                 |      |\n.L_21:                                                // +-----------------+------+\n                                                      // Legend:\n                                                      //     ^       : Register assignment\n                                                      //     v       : Register usage\n                                                      //     x       : Register usage and reassignment\n                                                      //     :       : Register in use\n                                                      //     <space> : Register not in use\n                                                      //     #       : Number of occupied registers nvdisasm is capable of showing line number information of the CUDA source file which can be useful for debugging. To get the line-info of a kernel, use the following: nvdisasm -g <input cubin file> Here’s a sample output of a kernel using nvdisasm -g command: //--------------------- .text._Z6kernali          --------------------------\n        .section        .text._Z6kernali,\"ax\",@progbits\n        .sectioninfo    @\"SHI_REGISTERS=24\"\n        .align  128\n        .global         _Z6kernali\n        .type           _Z6kernali,@function\n        .size           _Z6kernali,(.L_4 - _Z6kernali)\n        .other          _Z6kernali,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n_Z6kernali:\n.text._Z6kernali:\n        /*0000*/                   MOV R1, c[0x0][0x28] ;\n        /*0010*/                   NOP;\n    //## File \"/home/user/cuda/sample/sample.cu\", line 25\n        /*0020*/                   MOV R0, 0x160 ;\n        /*0030*/                   LDC R0, c[0x0][R0] ;\n        /*0040*/                   MOV R0, R0 ;\n        /*0050*/                   MOV R2, R0 ;\n    //## File \"/home/user/cuda/sample/sample.cu\", line 26\n        /*0060*/                   MOV R4, R2 ;\n        /*0070*/                   MOV R20, 32@lo((_Z6kernali + .L_1@srel)) ;\n        /*0080*/                   MOV R21, 32@hi((_Z6kernali + .L_1@srel)) ;\n        /*0090*/                   CALL.ABS.NOINC `(_Z3fooi) ;\n.L_1:\n        /*00a0*/                   MOV R0, R4 ;\n        /*00b0*/                   MOV R4, R2 ;\n        /*00c0*/                   MOV R2, R0 ;\n        /*00d0*/                   MOV R20, 32@lo((_Z6kernali + .L_2@srel)) ;\n        /*00e0*/                   MOV R21, 32@hi((_Z6kernali + .L_2@srel)) ;\n        /*00f0*/                   CALL.ABS.NOINC `(_Z3bari) ;\n.L_2:\n        /*0100*/                   MOV R4, R4 ;\n        /*0110*/                   IADD3 R4, R2, R4, RZ ;\n        /*0120*/                   MOV R2, 32@lo(arr) ;\n        /*0130*/                   MOV R3, 32@hi(arr) ;\n        /*0140*/                   MOV R2, R2 ;\n        /*0150*/                   MOV R3, R3 ;\n        /*0160*/                   ST.E.SYS [R2], R4 ;\n    //## File \"/home/user/cuda/sample/sample.cu\", line 27\n        /*0170*/                   ERRBAR ;\n        /*0180*/                   EXIT ;\n.L_3:\n        /*0190*/                   BRA `(.L_3);\n.L_4: nvdisasm is capable of showing line number information with additional function inlining info (if any). In absence of any function inlining the output is same as the one with nvdisasm -g command. Here’s a sample output of a kernel using nvdisasm -gi command: //--------------------- .text._Z6kernali          --------------------------\n    .section    .text._Z6kernali,\"ax\",@progbits\n    .sectioninfo    @\"SHI_REGISTERS=16\"\n    .align    128\n        .global         _Z6kernali\n        .type           _Z6kernali,@function\n        .size           _Z6kernali,(.L_18 - _Z6kernali)\n        .other          _Z6kernali,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n_Z6kernali:\n.text._Z6kernali:\n        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0010*/                   UMOV UR4, 32@lo(arr) ;\n        /*0020*/                   UMOV UR5, 32@hi(arr) ;\n        /*0030*/                   IMAD.U32 R2, RZ, RZ, UR4 ;\n        /*0040*/                   MOV R3, UR5 ;\n        /*0050*/                   ULDC.64 UR4, c[0x0][0x118] ;\n    //## File \"/home/user/cuda/inline.cu\", line 10 inlined at \"/home/user/cuda/inline.cu\", line 17\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0060*/                   LDG.E R4, [R2.64] ;\n        /*0070*/                   LDG.E R5, [R2.64+0x4] ;\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0080*/                   LDG.E R0, [R2.64+0x8] ;\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0090*/                   UMOV UR6, 32@lo(ans) ;\n        /*00a0*/                   UMOV UR7, 32@hi(ans) ;\n    //## File \"/home/user/cuda/inline.cu\", line 10 inlined at \"/home/user/cuda/inline.cu\", line 17\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*00b0*/                   IADD3 R7, R4, c[0x0][0x160], RZ ;\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*00c0*/                   IMAD.U32 R4, RZ, RZ, UR6 ;\n    //## File \"/home/user/cuda/inline.cu\", line 10 inlined at \"/home/user/cuda/inline.cu\", line 17\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*00d0*/                   IADD3 R9, R5, c[0x0][0x160], RZ ;\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*00e0*/                   MOV R5, UR7 ;\n    //## File \"/home/user/cuda/inline.cu\", line 10 inlined at \"/home/user/cuda/inline.cu\", line 17\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*00f0*/                   IADD3 R11, R0.reuse, c[0x0][0x160], RZ ;\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0100*/                   IMAD.IADD R13, R0, 0x1, R7 ;\n    //## File \"/home/user/cuda/inline.cu\", line 10 inlined at \"/home/user/cuda/inline.cu\", line 17\n    //## File \"/home/user/cuda/inline.cu\", line 17 inlined at \"/home/user/cuda/inline.cu\", line 23\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0110*/                   STG.E [R2.64+0x4], R9 ;\n        /*0120*/                   STG.E [R2.64], R7 ;\n        /*0130*/                   STG.E [R2.64+0x8], R11 ;\n    //## File \"/home/user/cuda/inline.cu\", line 23\n        /*0140*/                   STG.E [R4.64], R13 ;\n    //## File \"/home/user/cuda/inline.cu\", line 24\n        /*0150*/                   EXIT ;\n.L_3:\n        /*0160*/                   BRA (.L_3);\n.L_18: 3.2. Command-line Options  Table 3 contains the supported command-line options of nvdisasm , along with a description of what each option does. Each option has a long name and a short name, which can be used interchangeably. Table 3. nvdisasm Command-line Options  Option (long) Option (short) Description --base-address <value> -base Specify the logical base address of the image to disassemble. This option is only valid when disassembling a raw instruction binary (see option --binary ), and is ignored when disassembling an Elf file. Default value: 0. --binary <SMxy> -b When this option is specified, the input file is assumed to contain a raw instruction binary, that is, a sequence of binary instruction encodings as they occur in instruction memory.\nThe value of this option must be the asserted architecture of the raw binary. Allowed values for this option: SM50 , SM52 , SM53 , SM60 , SM61 , SM62 , SM70 , SM72 , SM75 , SM80 , SM86 , SM87 , SM89 , SM90 , SM90a . --cuda-function-index <symbol index>,... -fun Restrict the output to the CUDA functions represented by symbols with the given indices. The CUDA function for a given symbol is the enclosing section. This only restricts executable sections; all other sections will still be printed. --help -h Print this help information on this tool. --life-range-mode -lrm This option implies option --print-life-ranges , and determines how register live range info should be printed. count : Not at all, leaving only the # column (number of live registers); wide : Columns spaced out for readability (default); narrow : A one-character column for each register, economizing on table width Allowed values for this option: count , narrow , wide . --no-dataflow -ndf Disable dataflow analyzer after disassembly. Dataflow analysis is normally enabled to perform branch stack analysis and annotate all instructions that jump via the GPU branch stack with inferred branch target labels. However, it may occasionally fail when certain restrictions on the input nvelf/cubin are not met. --no-vliw -novliw Conventional mode; disassemble paired instructions in normal syntax, instead of VLIW syntax. --options-file <file>,... -optf Include command line options from specified file. --output-control-flow-graph -cfg When specified output the control flow graph, where each node is a hyperblock, in a format consumable by graphviz tools (such as dot). --output-control-flow-graph-with-basic-blocks -bbcfg When specified output the control flow graph, where each node is a basicblock, in a format consumable by graphviz tools (such as dot). --print-code -c Only print code sections. --print-instr-offsets-cfg -poff When specified, print instruction offsets in the control flow graph. This should be used along with the option –output-control-flow-graph or –output-control-flow-graph-with-basic-blocks. --print-instruction-encoding -hex When specified, print the encoding bytes after each disassembled operation. --print-life-ranges -plr Print register life range information in a trailing column in the produced disassembly. --print-line-info -g Annotate disassembly with source line information obtained from .debug_line section, if present. --print-line-info-inline -gi Annotate disassembly with source line information obtained from .debug_line section along with function inlining info, if present. --print-line-info-ptx -gp Annotate disassembly with source line information obtained from .nv_debug_line_sass section, if present. --print-raw -raw Print the disassembly without any attempt to beautify it. --separate-functions -sf Separate the code corresponding with function symbols by some new lines to let them stand out in the printed disassembly. --version -V Print version information on this tool. 4. Instruction Set Reference  This section contains instruction set reference for NVIDIA NVIDIA ® GPU architectures. 4.1. Maxwell and Pascal Instruction Set  The Maxwell (Compute Capability 5.x) and the Pascal (Compute Capability 6.x) architectures have the following instruction set format: (instruction) (destination) (source1), (source2) ... Valid destination and source locations include: RX for registers SRX for special system-controlled registers PX for condition registers c[X][Y] for constant memory Table 4 lists valid instructions for the Maxwell and Pascal GPUs. Table 4. Maxwell and Pascal Instruction Set  Opcode Description Floating Point Instructions FADD FP32 Add FCHK Single Precision FP Divide Range Check FCMP FP32 Compare to Zero and Select Source FFMA FP32 Fused Multiply and Add FMNMX FP32 Minimum/Maximum FMUL FP32 Multiply FSET FP32 Compare And Set FSETP FP32 Compare And Set Predicate FSWZADD FP32 Add used for FSWZ emulation MUFU Multi Function Operation RRO Range Reduction Operator FP DADD FP64 Add DFMA FP64 Fused Mutiply Add DMNMX FP64 Minimum/Maximum DMUL FP64 Multiply DSET FP64 Compare And Set DSETP FP64 Compare And Set Predicate HADD2 FP16 Add HFMA2 FP16 Fused Mutiply Add HMUL2 FP16 Multiply HSET2 FP16 Compare And Set HSETP2 FP16 Compare And Set Predicate Integer Instructions BFE Bit Field Extract BFI Bit Field Insert FLO Find Leading One IADD Integer Addition IADD3 3-input Integer Addition ICMP Integer Compare to Zero and Select Source IMAD Integer Multiply And Add IMADSP Extracted Integer Multiply And Add. IMNMX Integer Minimum/Maximum IMUL Integer Multiply ISCADD Scaled Integer Addition ISET Integer Compare And Set ISETP Integer Compare And Set Predicate LEA Compute Effective Address LOP Logic Operation LOP3 3-input Logic Operation POPC Population count SHF Funnel Shift SHL Shift Left SHR Shift Right XMAD Integer Short Multiply Add Conversion Instructions F2F Floating Point To Floating Point Conversion F2I Floating Point To Integer Conversion I2F Integer To Floating Point Conversion I2I Integer To Integer Conversion Movement Instructions MOV Move PRMT Permute Register Pair SEL Select Source with Predicate SHFL Warp Wide Register Shuffle Predicate/CC Instructions CSET Test Condition Code And Set CSETP Test Condition Code and Set Predicate PSET Combine Predicates and Set PSETP Combine Predicates and Set Predicate P2R Move Predicate Register To Register R2P Move Register To Predicate/CC Register Texture Instructions TEX Texture Fetch TLD Texture Load TLD4 Texture Load 4 TXQ Texture Query TEXS Texture Fetch with scalar/non-vec4 source/destinations TLD4S Texture Load 4 with scalar/non-vec4 source/destinations TLDS Texture Load with scalar/non-vec4 source/destinations Compute Load/Store Instructions LD Load from generic Memory LDC Load Constant LDG Load from Global Memory LDL Load within Local Memory Window LDS Local within Shared Memory Window ST Store to generic Memory STG Store to global Memory STL Store to Local Memory STS Store to Shared Memory ATOM Atomic Operation on generic Memory ATOMS Atomic Operation on Shared Memory RED Reduction Operation on generic Memory CCTL Cache Control CCTLL Cache Control MEMBAR Memory Barrier CCTLT Texture Cache Control Surface Memory Instructions SUATOM Atomic Op on Surface Memory SULD Surface Load SURED Reduction Op on Surface Memory SUST Surface Store Control Instructions BRA Relative Branch BRX Relative Branch Indirect JMP Absolute Jump JMX Absolute Jump Indirect SSY Set Synchronization Point SYNC Converge threads after conditional branch CAL Relative Call JCAL Absolute Call PRET Pre-Return From Subroutine RET Return From Subroutine BRK Break PBK Pre-Break CONT Continue PCNT Pre-continue EXIT Exit Program PEXIT Pre-Exit BPT BreakPoint/Trap Miscellaneous Instructions NOP No Operation CS2R Move Special Register to Register S2R Move Special Register to Register B2R Move Barrier To Register BAR Barrier Synchronization R2B Move Register to Barrier VOTE Vote Across SIMD Thread Group 4.2. Volta Instruction Set  The Volta architecture (Compute Capability 7.x) has the following instruction set format: (instruction) (destination) (source1), (source2) ... Valid destination and source locations include: RX for registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory Table 5 lists valid instructions for the Volta GPUs. Table 5. Volta Instruction Set  Opcode Description Floating Point Instructions FADD FP32 Add FADD32I FP32 Add FCHK Floating-point Range Check FFMA32I FP32 Fused Multiply and Add FFMA FP32 Fused Multiply and Add FMNMX FP32 Minimum/Maximum FMUL FP32 Multiply FMUL32I FP32 Multiply FSEL Floating Point Select FSET FP32 Compare And Set FSETP FP32 Compare And Set Predicate FSWZADD FP32 Swizzle Add MUFU FP32 Multi Function Operation HADD2 FP16 Add HADD2_32I FP16 Add HFMA2 FP16 Fused Mutiply Add HFMA2_32I FP16 Fused Mutiply Add HMMA Matrix Multiply and Accumulate HMUL2 FP16 Multiply HMUL2_32I FP16 Multiply HSET2 FP16 Compare And Set HSETP2 FP16 Compare And Set Predicate DADD FP64 Add DFMA FP64 Fused Mutiply Add DMUL FP64 Multiply DSETP FP64 Compare And Set Predicate Integer Instructions BMSK Bitfield Mask BREV Bit Reverse FLO Find Leading One IABS Integer Absolute Value IADD Integer Addition IADD3 3-input Integer Addition IADD32I Integer Addition IDP Integer Dot Product and Accumulate IDP4A Integer Dot Product and Accumulate IMAD Integer Multiply And Add IMMA Integer Matrix Multiply and Accumulate IMNMX Integer Minimum/Maximum IMUL Integer Multiply IMUL32I Integer Multiply ISCADD Scaled Integer Addition ISCADD32I Scaled Integer Addition ISETP Integer Compare And Set Predicate LEA LOAD Effective Address LOP Logic Operation LOP3 Logic Operation LOP32I Logic Operation POPC Population count SHF Funnel Shift SHL Shift Left SHR Shift Right VABSDIFF Absolute Difference VABSDIFF4 Absolute Difference Conversion Instructions F2F Floating Point To Floating Point Conversion F2I Floating Point To Integer Conversion I2F Integer To Floating Point Conversion I2I Integer To Integer Conversion I2IP Integer To Integer Conversion and Packing FRND Round To Integer Movement Instructions MOV Move MOV32I Move PRMT Permute Register Pair SEL Select Source with Predicate SGXT Sign Extend SHFL Warp Wide Register Shuffle Predicate Instructions PLOP3 Predicate Logic Operation PSETP Combine Predicates and Set Predicate P2R Move Predicate Register To Register R2P Move Register To Predicate Register Load/Store Instructions LD Load from generic Memory LDC Load Constant LDG Load from Global Memory LDL Load within Local Memory Window LDS Load within Shared Memory Window ST Store to Generic Memory STG Store to Global Memory STL Store to Local Memory STS Store to Shared Memory MATCH Match Register Values Across Thread Group QSPC Query Space ATOM Atomic Operation on Generic Memory ATOMS Atomic Operation on Shared Memory ATOMG Atomic Operation on Global Memory RED Reduction Operation on Generic Memory CCTL Cache Control CCTLL Cache Control ERRBAR Error Barrier MEMBAR Memory Barrier CCTLT Texture Cache Control Texture Instructions TEX Texture Fetch TLD Texture Load TLD4 Texture Load 4 TMML Texture MipMap Level TXD Texture Fetch With Derivatives TXQ Texture Query Surface Instructions SUATOM Atomic Op on Surface Memory SULD Surface Load SURED Reduction Op on Surface Memory SUST Surface Store Control Instructions BMOV Move Convergence Barrier State BPT BreakPoint/Trap BRA Relative Branch BREAK Break out of the Specified Convergence Barrier BRX Relative Branch Indirect BSSY Barrier Set Convergence Synchronization Point BSYNC Synchronize Threads on a Convergence Barrier CALL Call Function EXIT Exit Program JMP Absolute Jump JMX Absolute Jump Indirect KILL Kill Thread NANOSLEEP Suspend Execution RET Return From Subroutine RPCMOV PC Register Move RTT Return From Trap WARPSYNC Synchronize Threads in Warp YIELD Yield Control Miscellaneous Instructions B2R Move Barrier To Register BAR Barrier Synchronization CS2R Move Special Register to Register DEPBAR Dependency Barrier GETLMEMBASE Get Local Memory Base Address LEPC Load Effective PC NOP No Operation PMTRIG Performance Monitor Trigger R2B Move Register to Barrier S2R Move Special Register to Register SETCTAID Set CTA ID SETLMEMBASE Set Local Memory Base Address VOTE Vote Across SIMD Thread Group 4.3. Turing Instruction Set  The Turing architecture (Compute Capability 7.3 and 7.5) have the following instruction set format: (instruction) (destination) (source1), (source2) ... Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory Table 6 lists valid instructions for the Turing GPUs. Table 6. Turing Instruction Set  Opcode Description Floating Point Instructions FADD FP32 Add FADD32I FP32 Add FCHK Floating-point Range Check FFMA32I FP32 Fused Multiply and Add FFMA FP32 Fused Multiply and Add FMNMX FP32 Minimum/Maximum FMUL FP32 Multiply FMUL32I FP32 Multiply FSEL Floating Point Select FSET FP32 Compare And Set FSETP FP32 Compare And Set Predicate FSWZADD FP32 Swizzle Add MUFU FP32 Multi Function Operation HADD2 FP16 Add HADD2_32I FP16 Add HFMA2 FP16 Fused Mutiply Add HFMA2_32I FP16 Fused Mutiply Add HMMA Matrix Multiply and Accumulate HMUL2 FP16 Multiply HMUL2_32I FP16 Multiply HSET2 FP16 Compare And Set HSETP2 FP16 Compare And Set Predicate DADD FP64 Add DFMA FP64 Fused Mutiply Add DMUL FP64 Multiply DSETP FP64 Compare And Set Predicate Integer Instructions BMMA Bit Matrix Multiply and Accumulate BMSK Bitfield Mask BREV Bit Reverse FLO Find Leading One IABS Integer Absolute Value IADD Integer Addition IADD3 3-input Integer Addition IADD32I Integer Addition IDP Integer Dot Product and Accumulate IDP4A Integer Dot Product and Accumulate IMAD Integer Multiply And Add IMMA Integer Matrix Multiply and Accumulate IMNMX Integer Minimum/Maximum IMUL Integer Multiply IMUL32I Integer Multiply ISCADD Scaled Integer Addition ISCADD32I Scaled Integer Addition ISETP Integer Compare And Set Predicate LEA LOAD Effective Address LOP Logic Operation LOP3 Logic Operation LOP32I Logic Operation POPC Population count SHF Funnel Shift SHL Shift Left SHR Shift Right VABSDIFF Absolute Difference VABSDIFF4 Absolute Difference Conversion Instructions F2F Floating Point To Floating Point Conversion F2I Floating Point To Integer Conversion I2F Integer To Floating Point Conversion I2I Integer To Integer Conversion I2IP Integer To Integer Conversion and Packing FRND Round To Integer Movement Instructions MOV Move MOV32I Move MOVM Move Matrix with Transposition or Expansion PRMT Permute Register Pair SEL Select Source with Predicate SGXT Sign Extend SHFL Warp Wide Register Shuffle Predicate Instructions PLOP3 Predicate Logic Operation PSETP Combine Predicates and Set Predicate P2R Move Predicate Register To Register R2P Move Register To Predicate Register Load/Store Instructions LD Load from generic Memory LDC Load Constant LDG Load from Global Memory LDL Load within Local Memory Window LDS Load within Shared Memory Window LDSM Load Matrix from Shared Memory with Element Size Expansion ST Store to Generic Memory STG Store to Global Memory STL Store to Local Memory STS Store to Shared Memory MATCH Match Register Values Across Thread Group QSPC Query Space ATOM Atomic Operation on Generic Memory ATOMS Atomic Operation on Shared Memory ATOMG Atomic Operation on Global Memory RED Reduction Operation on Generic Memory CCTL Cache Control CCTLL Cache Control ERRBAR Error Barrier MEMBAR Memory Barrier CCTLT Texture Cache Control Uniform Datapath Instructions R2UR Move from Vector Register to a Uniform Register S2UR Move Special Register to Uniform Register UBMSK Uniform Bitfield Mask UBREV Uniform Bit Reverse UCLEA Load Effective Address for a Constant UFLO Uniform Find Leading One UIADD3 Uniform Integer Addition UIADD3.64 Uniform Integer Addition UIMAD Uniform Integer Multiplication UISETP Integer Compare and Set Uniform Predicate ULDC Load from Constant Memory into a Uniform Register ULEA Uniform Load Effective Address ULOP Logic Operation ULOP3 Logic Operation ULOP32I Logic Operation UMOV Uniform Move UP2UR Uniform Predicate to Uniform Register UPLOP3 Uniform Predicate Logic Operation UPOPC Uniform Population Count UPRMT Uniform Byte Permute UPSETP Uniform Predicate Logic Operation UR2UP Uniform Register to Uniform Predicate USEL Uniform Select USGXT Uniform Sign Extend USHF Uniform Funnel Shift USHL Uniform Left Shift USHR Uniform Right Shift VOTEU Voting across SIMD Thread Group with Results in Uniform Destination Texture Instructions TEX Texture Fetch TLD Texture Load TLD4 Texture Load 4 TMML Texture MipMap Level TXD Texture Fetch With Derivatives TXQ Texture Query Surface Instructions SUATOM Atomic Op on Surface Memory SULD Surface Load SURED Reduction Op on Surface Memory SUST Surface Store Control Instructions BMOV Move Convergence Barrier State BPT BreakPoint/Trap BRA Relative Branch BREAK Break out of the Specified Convergence Barrier BRX Relative Branch Indirect BRXU Relative Branch with Uniform Register Based Offset BSSY Barrier Set Convergence Synchronization Point BSYNC Synchronize Threads on a Convergence Barrier CALL Call Function EXIT Exit Program JMP Absolute Jump JMX Absolute Jump Indirect JMXU Absolute Jump with Uniform Register Based Offset KILL Kill Thread NANOSLEEP Suspend Execution RET Return From Subroutine RPCMOV PC Register Move RTT Return From Trap WARPSYNC Synchronize Threads in Warp YIELD Yield Control Miscellaneous Instructions B2R Move Barrier To Register BAR Barrier Synchronization CS2R Move Special Register to Register DEPBAR Dependency Barrier GETLMEMBASE Get Local Memory Base Address LEPC Load Effective PC NOP No Operation PMTRIG Performance Monitor Trigger R2B Move Register to Barrier S2R Move Special Register to Register SETCTAID Set CTA ID SETLMEMBASE Set Local Memory Base Address VOTE Vote Across SIMD Thread Group 4.4. NVIDIA Ampere GPU and Ada Instruction Set  The NVIDIA Ampere GPU and Ada architectures (Compute Capability 8.0 and 8.6) have the following instruction set format: (instruction) (destination) (source1), (source2) ... Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers UPX for uniform predicate registers c[X][Y] for constant memory Table 7 lists valid instructions for the NVIDIA Ampere architecrture and Ada GPUs. Table 7. NVIDIA Ampere GPU and Ada Instruction Set  Opcode Description Floating Point Instructions FADD FP32 Add FADD32I FP32 Add FCHK Floating-point Range Check FFMA32I FP32 Fused Multiply and Add FFMA FP32 Fused Multiply and Add FMNMX FP32 Minimum/Maximum FMUL FP32 Multiply FMUL32I FP32 Multiply FSEL Floating Point Select FSET FP32 Compare And Set FSETP FP32 Compare And Set Predicate FSWZADD FP32 Swizzle Add MUFU FP32 Multi Function Operation HADD2 FP16 Add HADD2_32I FP16 Add HFMA2 FP16 Fused Mutiply Add HFMA2_32I FP16 Fused Mutiply Add HMMA Matrix Multiply and Accumulate HMNMX2 FP16 Minimum / Maximum HMUL2 FP16 Multiply HMUL2_32I FP16 Multiply HSET2 FP16 Compare And Set HSETP2 FP16 Compare And Set Predicate DADD FP64 Add DFMA FP64 Fused Mutiply Add DMMA Matrix Multiply and Accumulate DMUL FP64 Multiply DSETP FP64 Compare And Set Predicate Integer Instructions BMMA Bit Matrix Multiply and Accumulate BMSK Bitfield Mask BREV Bit Reverse FLO Find Leading One IABS Integer Absolute Value IADD Integer Addition IADD3 3-input Integer Addition IADD32I Integer Addition IDP Integer Dot Product and Accumulate IDP4A Integer Dot Product and Accumulate IMAD Integer Multiply And Add IMMA Integer Matrix Multiply and Accumulate IMNMX Integer Minimum/Maximum IMUL Integer Multiply IMUL32I Integer Multiply ISCADD Scaled Integer Addition ISCADD32I Scaled Integer Addition ISETP Integer Compare And Set Predicate LEA LOAD Effective Address LOP Logic Operation LOP3 Logic Operation LOP32I Logic Operation POPC Population count SHF Funnel Shift SHL Shift Left SHR Shift Right VABSDIFF Absolute Difference VABSDIFF4 Absolute Difference Conversion Instructions F2F Floating Point To Floating Point Conversion F2I Floating Point To Integer Conversion I2F Integer To Floating Point Conversion I2I Integer To Integer Conversion I2IP Integer To Integer Conversion and Packing I2FP Integer to FP32 Convert and Pack F2IP FP32 Down-Convert to Integer and Pack FRND Round To Integer Movement Instructions MOV Move MOV32I Move MOVM Move Matrix with Transposition or Expansion PRMT Permute Register Pair SEL Select Source with Predicate SGXT Sign Extend SHFL Warp Wide Register Shuffle Predicate Instructions PLOP3 Predicate Logic Operation PSETP Combine Predicates and Set Predicate P2R Move Predicate Register To Register R2P Move Register To Predicate Register Load/Store Instructions LD Load from generic Memory LDC Load Constant LDG Load from Global Memory LDGDEPBAR Global Load Dependency Barrier LDGSTS Asynchronous Global to Shared Memcopy LDL Load within Local Memory Window LDS Load within Shared Memory Window LDSM Load Matrix from Shared Memory with Element Size Expansion ST Store to Generic Memory STG Store to Global Memory STL Store to Local Memory STS Store to Shared Memory MATCH Match Register Values Across Thread Group QSPC Query Space ATOM Atomic Operation on Generic Memory ATOMS Atomic Operation on Shared Memory ATOMG Atomic Operation on Global Memory RED Reduction Operation on Generic Memory CCTL Cache Control CCTLL Cache Control ERRBAR Error Barrier MEMBAR Memory Barrier CCTLT Texture Cache Control Uniform Datapath Instructions R2UR Move from Vector Register to a Uniform Register REDUX Reduction of a Vector Register into a Uniform Register S2UR Move Special Register to Uniform Register UBMSK Uniform Bitfield Mask UBREV Uniform Bit Reverse UCLEA Load Effective Address for a Constant UF2FP Uniform FP32 Down-convert and Pack UFLO Uniform Find Leading One UIADD3 Uniform Integer Addition UIADD3.64 Uniform Integer Addition UIMAD Uniform Integer Multiplication UISETP Integer Compare and Set Uniform Predicate ULDC Load from Constant Memory into a Uniform Register ULEA Uniform Load Effective Address ULOP Logic Operation ULOP3 Logic Operation ULOP32I Logic Operation UMOV Uniform Move UP2UR Uniform Predicate to Uniform Register UPLOP3 Uniform Predicate Logic Operation UPOPC Uniform Population Count UPRMT Uniform Byte Permute UPSETP Uniform Predicate Logic Operation UR2UP Uniform Register to Uniform Predicate USEL Uniform Select USGXT Uniform Sign Extend USHF Uniform Funnel Shift USHL Uniform Left Shift USHR Uniform Right Shift VOTEU Voting across SIMD Thread Group with Results in Uniform Destination Texture Instructions TEX Texture Fetch TLD Texture Load TLD4 Texture Load 4 TMML Texture MipMap Level TXD Texture Fetch With Derivatives TXQ Texture Query Surface Instructions SUATOM Atomic Op on Surface Memory SULD Surface Load SURED Reduction Op on Surface Memory SUST Surface Store Control Instructions BMOV Move Convergence Barrier State BPT BreakPoint/Trap BRA Relative Branch BREAK Break out of the Specified Convergence Barrier BRX Relative Branch Indirect BRXU Relative Branch with Uniform Register Based Offset BSSY Barrier Set Convergence Synchronization Point BSYNC Synchronize Threads on a Convergence Barrier CALL Call Function EXIT Exit Program JMP Absolute Jump JMX Absolute Jump Indirect JMXU Absolute Jump with Uniform Register Based Offset KILL Kill Thread NANOSLEEP Suspend Execution RET Return From Subroutine RPCMOV PC Register Move WARPSYNC Synchronize Threads in Warp YIELD Yield Control Miscellaneous Instructions B2R Move Barrier To Register BAR Barrier Synchronization CS2R Move Special Register to Register DEPBAR Dependency Barrier GETLMEMBASE Get Local Memory Base Address LEPC Load Effective PC NOP No Operation PMTRIG Performance Monitor Trigger S2R Move Special Register to Register SETCTAID Set CTA ID SETLMEMBASE Set Local Memory Base Address VOTE Vote Across SIMD Thread Group 4.5. Hopper Instruction Set  The Hopper architecture (Compute Capability 9.0) has the following instruction set format: (instruction) (destination) (source1), (source2) ... Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers UPX for uniform predicate registers c[X][Y] for constant memory desc[URX][RY] for memory descriptors Table 8 lists valid instructions for the Hopper GPUs. Table 8. Hopper Instruction Set  Opcode Description Floating Point Instructions FADD FP32 Add FADD32I FP32 Add FCHK Floating-point Range Check FFMA32I FP32 Fused Multiply and Add FFMA FP32 Fused Multiply and Add FMNMX FP32 Minimum/Maximum FMUL FP32 Multiply FMUL32I FP32 Multiply FSEL Floating Point Select FSET FP32 Compare And Set FSETP FP32 Compare And Set Predicate FSWZADD FP32 Swizzle Add MUFU FP32 Multi Function Operation HADD2 FP16 Add HADD2_32I FP16 Add HFMA2 FP16 Fused Mutiply Add HFMA2_32I FP16 Fused Mutiply Add HMMA Matrix Multiply and Accumulate HMNMX2 FP16 Minimum / Maximum HMUL2 FP16 Multiply HMUL2_32I FP16 Multiply HSET2 FP16 Compare And Set HSETP2 FP16 Compare And Set Predicate DADD FP64 Add DFMA FP64 Fused Mutiply Add DMMA Matrix Multiply and Accumulate DMUL FP64 Multiply DSETP FP64 Compare And Set Predicate Integer Instructions BMMA Bit Matrix Multiply and Accumulate BMSK Bitfield Mask BREV Bit Reverse FLO Find Leading One IABS Integer Absolute Value IADD Integer Addition IADD3 3-input Integer Addition IADD32I Integer Addition IDP Integer Dot Product and Accumulate IDP4A Integer Dot Product and Accumulate IMAD Integer Multiply And Add IMMA Integer Matrix Multiply and Accumulate IMNMX Integer Minimum/Maximum IMUL Integer Multiply IMUL32I Integer Multiply ISCADD Scaled Integer Addition ISCADD32I Scaled Integer Addition ISETP Integer Compare And Set Predicate LEA LOAD Effective Address LOP Logic Operation LOP3 Logic Operation LOP32I Logic Operation POPC Population count SHF Funnel Shift SHL Shift Left SHR Shift Right VABSDIFF Absolute Difference VABSDIFF4 Absolute Difference VHMNMX SIMD FP16 3-Input Minimum / Maximum VIADD SIMD Integer Addition VIADDMNMX SIMD Integer Addition and Fused Min/Max Comparison VIMNMX SIMD Integer Minimum / Maximum VIMNMX3 SIMD Integer 3-Input Minimum / Maximum Conversion Instructions F2F Floating Point To Floating Point Conversion F2I Floating Point To Integer Conversion I2F Integer To Floating Point Conversion I2I Integer To Integer Conversion I2IP Integer To Integer Conversion and Packing I2FP Integer to FP32 Convert and Pack F2IP FP32 Down-Convert to Integer and Pack FRND Round To Integer Movement Instructions MOV Move MOV32I Move MOVM Move Matrix with Transposition or Expansion PRMT Permute Register Pair SEL Select Source with Predicate SGXT Sign Extend SHFL Warp Wide Register Shuffle Predicate Instructions PLOP3 Predicate Logic Operation PSETP Combine Predicates and Set Predicate P2R Move Predicate Register To Register R2P Move Register To Predicate Register Load/Store Instructions FENCE Memory Visibility Guarantee for Shared or Global Memory LD Load from generic Memory LDC Load Constant LDG Load from Global Memory LDGDEPBAR Global Load Dependency Barrier LDGMC Reducing Load LDGSTS Asynchronous Global to Shared Memcopy LDL Load within Local Memory Window LDS Load within Shared Memory Window LDSM Load Matrix from Shared Memory with Element Size Expansion STSM Store Matrix to Shared Memory ST Store to Generic Memory STG Store to Global Memory STL Store to Local Memory STS Store to Shared Memory STAS Asynchronous Store to Distributed Shared Memory With Explicit Synchronization SYNCS Sync Unit MATCH Match Register Values Across Thread Group QSPC Query Space ATOM Atomic Operation on Generic Memory ATOMS Atomic Operation on Shared Memory ATOMG Atomic Operation on Global Memory REDAS Asynchronous Reduction on Distributed Shared Memory With Explicit Synchronization REDG Reduction Operation on Generic Memory CCTL Cache Control CCTLL Cache Control ERRBAR Error Barrier MEMBAR Memory Barrier CCTLT Texture Cache Control Uniform Datapath Instructions R2UR Move from Vector Register to a Uniform Register REDUX Reduction of a Vector Register into a Uniform Register S2UR Move Special Register to Uniform Register UBMSK Uniform Bitfield Mask UBREV Uniform Bit Reverse UCGABAR_ARV CGA Barrier Synchronization UCGABAR_WAIT CGA Barrier Synchronization UCLEA Load Effective Address for a Constant UF2FP Uniform FP32 Down-convert and Pack UFLO Uniform Find Leading One UIADD3 Uniform Integer Addition UIADD3.64 Uniform Integer Addition UIMAD Uniform Integer Multiplication UISETP Integer Compare and Set Uniform Predicate ULDC Load from Constant Memory into a Uniform Register ULEA Uniform Load Effective Address ULEPC Uniform Load Effective PC ULOP Logic Operation ULOP3 Logic Operation ULOP32I Logic Operation UMOV Uniform Move UP2UR Uniform Predicate to Uniform Register UPLOP3 Uniform Predicate Logic Operation UPOPC Uniform Population Count UPRMT Uniform Byte Permute UPSETP Uniform Predicate Logic Operation UR2UP Uniform Register to Uniform Predicate USEL Uniform Select USETMAXREG Release, Deallocate and Allocate Registers USGXT Uniform Sign Extend USHF Uniform Funnel Shift USHL Uniform Left Shift USHR Uniform Right Shift VOTEU Voting across SIMD Thread Group with Results in Uniform Destination Warpgroup Instructions BGMMA Bit Matrix Multiply and Accumulate Across Warps HGMMA Matrix Multiply and Accumulate Across a Warpgroup IGMMA Integer Matrix Multiply and Accumulate Across a Warpgroup QGMMA FP8 Matrix Multiply and Accumulate Across a Warpgroup WARPGROUP Warpgroup Synchronization WARPGROUPSET Set Warpgroup Counters Tensor Memory Access Instructions UBLKCP Bulk Data Copy UBLKPF Bulk Data Prefetch UBLKRED Bulk Data Copy from Shared Memory with Reduction UTMACCTL TMA Cache Control UTMACMDFLUSH TMA Command Flush UTMALDG Tensor Load from Global to Shared Memory UTMAPF Tensor Prefetch UTMAREDG Tensor Store from Shared to Global Memory with Reduction UTMASTG Tensor Store from Shared to Global Memory Texture Instructions TEX Texture Fetch TLD Texture Load TLD4 Texture Load 4 TMML Texture MipMap Level TXD Texture Fetch With Derivatives TXQ Texture Query Surface Instructions SUATOM Atomic Op on Surface Memory SULD Surface Load SURED Reduction Op on Surface Memory SUST Surface Store Control Instructions ACQBULK Wait for Bulk Release Status Warp State BMOV Move Convergence Barrier State BPT BreakPoint/Trap BRA Relative Branch BREAK Break out of the Specified Convergence Barrier BRX Relative Branch Indirect BRXU Relative Branch with Uniform Register Based Offset BSSY Barrier Set Convergence Synchronization Point BSYNC Synchronize Threads on a Convergence Barrier CALL Call Function CGAERRBAR CGA Error Barrier ELECT Elect a Leader Thread ENDCOLLECTIVE Reset the MCOLLECTIVE mask EXIT Exit Program JMP Absolute Jump JMX Absolute Jump Indirect JMXU Absolute Jump with Uniform Register Based Offset KILL Kill Thread NANOSLEEP Suspend Execution PREEXIT Dependent Task Launch Hint RET Return From Subroutine RPCMOV PC Register Move WARPSYNC Synchronize Threads in Warp YIELD Yield Control Miscellaneous Instructions B2R Move Barrier To Register BAR Barrier Synchronization CS2R Move Special Register to Register DEPBAR Dependency Barrier GETLMEMBASE Get Local Memory Base Address LEPC Load Effective PC NOP No Operation PMTRIG Performance Monitor Trigger S2R Move Special Register to Register SETCTAID Set CTA ID SETLMEMBASE Set Local Memory Base Address VOTE Vote Across SIMT Thread Group 5. cu++filt  cu++filt decodes (demangles) low-level identifiers that have been mangled by CUDA C++ into user readable names. For every input alphanumeric word, the output of cu++filt is either the demangled name if the name decodes to a CUDA C++ name, or the original name itself. 5.1. Usage  cu++filt accepts one or more alphanumeric words (consisting of letters, digits, underscores, dollars, or periods) and attepts to decipher them. The basic usage is as following: cu++filt [options] <symbol(s)> To demangle an entire file, like a binary, pipe the contents of the file to cu++filt, such as in the following command: nm <input file> | cu++filt To demangle function names without printing their parameter types, use the following command : cu++filt -p <symbol(s)> To skip a leading underscore from mangled symbols, use the following command: cu++filt -_ <symbol(s)> Here’s a sample output of cu++filt : $ cu++filt _Z1fIiEbl\nbool f<int>(long) As shown in the output, the symbol _Z1fIiEbl was successfully demangled. To strip all types in the function signature and parameters, use the -p option: $ cu++filt -p _Z1fIiEbl\nf<int> To skip a leading underscore from a mangled symbol, use the -_ option: $ cu++filt -_ __Z1fIiEbl\nbool f<int>(long) To demangle an entire file, pipe the contents of the file to cu++filt: $ nm test.sm_70.cubin | cu++filt\n0000000000000000 t hello(char *)\n0000000000000070 t hello(char *)::display()\n0000000000000000 T hello(int *) Symbols that cannot be demangled are printed back to stdout as is: $ cu++filt _ZD2\n_ZD2 Multiple symbols can be demangled from the command line: $ cu++filt _ZN6Scope15Func1Enez _Z3fooIiPFYneEiEvv _ZD2\nScope1::Func1(__int128, long double, ...)\nvoid foo<int, __int128 (*)(long double), int>()\n_ZD2 5.2. Command-line Options  Table 9 contains supported command-line options of cu++filt , along with a description of what each option does. Table 9. cu++filt Command-line Options  Option Description -_ Strip underscore. On some systems, the CUDA compiler puts an underscore in front of every name. This option removes the initial underscore. Whether cu++filt removes the underscore by default is target dependent. -p When demangling the name of a function, do not display the types of the function’s parameters. -h Print a summary of the options to cu++filt and exit. -v Print the version information of this tool. 5.3. Library Availability  cu++filt is also available as a static library (libcufilt) that can be linked against an existing project. The following interface describes it’s usage: char* __cu_demangle(const char *id, char *output_buffer, size_t *length, int *status) This interface can be found in the file “nv_decode.h” located in the SDK. Input Parameters id Input mangled string. output_buffer Pointer to where the demangled buffer will be stored. This memory must be allocated with malloc. If output-buffer is NULL, memory will be malloc’d to store the demangled name and returned through the function return value. If the output-buffer is too small, it is expanded using realloc. length It is necessary to provide the size of the output buffer if the user is providing pre-allocated memory. This is needed by the demangler in case the size needs to be reallocated. If the length is non-null, the length of the demangled buffer is placed in length. status *status is set to one of the following values: 0 - The demangling operation succeeded -1 - A memory allocation failure occurred -2 - Not a valid mangled id -3 - An input validation failure has occurred (one or more arguments are invalid) Return Value A pointer to the start of the NUL-terminated demangled name, or NULL if the demangling fails. The caller is responsible for deallocating this memory using free. Note : This function is thread-safe. Example Usage #include <stdio.h>\n#include <stdlib.h>\n#include \"nv_decode.h\"\n\nint main()\n{\n  int     status;\n  const char *real_mangled_name=\"_ZN8clstmp01I5cls01E13clstmp01_mf01Ev\";\n  const char *fake_mangled_name=\"B@d_iDentiFier\";\n\n  char* realname = __cu_demangle(fake_mangled_name, 0, 0, &status);\n  printf(\"fake_mangled_name:\\t result => %s\\t status => %d\\n\", realname, status);\n  free(realname);\n\n  size_t size = sizeof(char)*1000;\n  realname = (char*)malloc(size);\n  __cu_demangle(real_mangled_name, realname, &size, &status);\n  printf(\"real_mangled_name:\\t result => %s\\t status => %d\\n\", realname, status);\n  free(realname);\n\n  return 0;\n} This prints: fake_mangled_name:   result => (null)     status => -2\nreal_mangled_name:   result => clstmp01<cls01>::clstmp01_mf01()   status => 0 6. nvprune  nvprune prunes host object files and libraries to only contain device code for the specified targets. 6.1. Usage  nvprune accepts a single input file each time it’s run, emitting a new output file. The basic usage is as following: nvprune [options] -o <outfile> <infile> The input file must be either a relocatable host object or static library (not a host executable), and the output file will be the same format. Either the –arch or –generate-code option must be used to specify the target(s) to keep. All other device code is discarded from the file. The targets can be either a sm_NN arch (cubin) or compute_NN arch (ptx). For example, the following will prune libcublas_static.a to only contain sm_70 cubin rather than all the targets which normally exist: nvprune -arch sm_70 libcublas_static.a -o libcublas_static70.a Note that this means that libcublas_static70.a will not run on any other architecture, so should only be used when you are building for a single architecture. 6.2. Command-line Options  Table 10 contains supported command-line options of nvprune , along with a description of what each option does. Each option has a long name and a short name, which can be used interchangeably. Table 10. nvprune Command-line Options  Option (long) Option (short) Description --arch <gpu architecture name>,... -arch Specify the name of the NVIDIA GPU architecture which will remain in the object or library. --generate-code -gencode This option is same format as nvcc –generate-code option, and provides a way to specify multiple architectures which should remain in the object or library. Only the ‘code’ values are used as targets to match. Allowed keywords for this option: ‘arch’,’code’. --no-relocatable-elf -no-relocatable-elf Don’t keep any relocatable ELF. --output-file -o Specify name and location of the output file. --help -h Print this help information on this tool. --options-file <file>,... -optf Include command line options from specified file. --version -V Print version information on this tool. 7. Notices  7.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2013-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/nsight-visual-studio-edition/index.html", "parent_url": "https://docs.nvidia.com/nsight-visual-studio-edition/index.html", "content_type": "text/html", "text": "NVIDIA Nsight Visual Studio Edition — nsight-visual-studio-edition 12.5 documentation Introduction Release Notes Installation and Setup CUDA Debugger Getting Started with the CUDA Debugger Build and Run Control GPU Execution Inspect State Advanced Topics Reference Reference Release Information Archives Copyright and License Notices EULA nsight-visual-studio-edition » NVIDIA Nsight Visual Studio Edition v2024.2.1 | Archive NVIDIA Nsight Visual Studio Edition  Introduction NVIDIA® Nsight™ Visual Studio Edition is an application development environment which brings GPU computing into Microsoft Visual Studio. allows you to build and debug integrated GPU kernels and native CPU code as well as inspect the state of the CPU, GPU, and memory. Release Notes See the latest features and updates for this version of NVIDIA Nsight Visual Studio Edition. Installation and Setup This chapter walks you through the system requirements for NVIDIA Nsight Visual Studio Edition, and the steps you’ll need to install and get started using the software. CUDA Debugger  Getting Started with the CUDA Debugger This section provides a walkthrough and tutorial for using the CUDA Debugger with NVIDIA Nsight Visual Studio Edition. Build and Run This section details how to configure the properties of a CUDA project, launching the CUDA Debugger, and how to attach debugging to a running CUDA Process. Control GPU Execution In this section, learn more about how to control GPU execution, set GPU breakpoints, and use global freeze. Inspect State In this section, learn more about how to use various state inspection features of the CUDA Debugger, such as specifying the debugger context, viewing memory and variables, using the CUDA Info View, and using the CUDA Warp Watch. Advanced Topics In this section, learn more about advanced CUDA topics, such as PTX and SASS assembly debugging, as well as how to use the CUDA Memory Checker. Reference  Reference Additional resources for learning more about working with NVIDIA Nsight Visual Studio Edition. Release Information  Archives Find documentation for previous versions of NVIDIA Nsight Visual Studio Edition. Copyright And License Notices  EULA This document is the End User License Agreement (EULA) for NVIDIA Nsight Visual Studio Edition. This document contains specific license terms and conditions for NVIDIA Nsight Visual Studio Edition. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the specific product(s) included herein. © Copyright 2018-2024, NVIDIA Corporation & Affiliates. All rights reserved. Last updated on Jun 03, 2024."}, {"url": "https://docs.nvidia.com/nsight-compute/index.html", "parent_url": "https://docs.nvidia.com/nsight-compute/index.html", "content_type": "text/html", "text": "Nsight Compute Documentation — NsightCompute 12.5 documentation Nsight Compute 1. Release Notes 2. Kernel Profiling Guide 3. Nsight Compute 4. Nsight Compute CLI Developer Interfaces 1. Customization Guide 2. NvRules API Training Training Release Information Archives Copyright and Licenses Copyright and Licenses NsightCompute » Nsight Compute Documentation v2024.2.1 | Archive Nsight Compute Documentation  Nsight Compute  Release Notes Release notes, including new features and important bug fixes. Supported platforms and GPUs. List of known issues for the current release. Kernel Profiling Guide Kernel Profiling Guide with metric types and meaning, data collection modes and FAQ for common problems. Nsight Compute NVIDIA Nsight Compute User Interface (UI) manual. Information on all views, controls and workflows within the tool UI. Transitions guide for Visual Profiler. Nsight Compute CLI NVIDIA Nsight Compute Command Line Interface (CLI) manual. Information on workflows and options for the command line, including multi-process profiling and NVTX filtering. Transitions guide for Nvprof. Developer Interfaces  Customization Guide User manual on customizing NVIDIA Nsight Compute tools or integrating them with custom workflows. Information on writing section files, rules for automatic result analysis and scripting access to report files. Training  Training NVIDIA Nsight Compute Training resources. Release Information  Archives Find documentation for previous versions of NVIDIA Nsight Compute. Copyright And Licenses  Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Nsight Compute. © Copyright 2018-2024, NVIDIA Corporation & Affiliates. All rights reserved. Last updated on Jun 03, 2024."}, {"url": "https://docs.nvidia.com/cuda/profiler-users-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/profiler-users-guide/index.html", "content_type": "text/html", "text": "Profiler 1. Preparing An Application For Profiling 1.1. Focused Profiling 1.2. Marking Regions of CPU Activity 1.3. Naming CPU and CUDA Resources 1.4. Flush Profile Data 1.5. Profiling CUDA Fortran Applications 2. ​Visual Profiler 2.1. Getting Started 2.1.1. Setting up Java Runtime Environment 2.1.2. Installing JRE 2.1.3. Modify Your Application For Profiling 2.1.4. Creating a Session 2.1.5. Analyzing Your Application 2.1.6. Exploring the Timeline 2.1.7. Looking at the Details 2.1.8. Improve Loading of Large Profiles 2.2. Sessions 2.2.1. Executable Session 2.2.2. Import Session 2.2.2.1. Import Single-Process nvprof Session 2.2.2.2. Import Multi-Process nvprof Session 2.2.2.3. Import Command-Line Profiler Session 2.3. Application Requirements 2.4. Visual Profiler Views 2.4.1. Timeline View 2.4.1.1. Timeline Controls 2.4.1.2. Navigating the Timeline 2.4.1.3. Timeline Refreshing 2.4.1.4. Dependency Analysis Controls 2.4.2. Analysis View 2.4.2.1. Guided Application Analysis 2.4.2.2. Unguided Application Analysis 2.4.2.3. PC Sampling View 2.4.2.4. Memory Statistics 2.4.2.5. NVLink view 2.4.3. Source-Disassembly View 2.4.4. GPU Details View 2.4.5. CPU Details View 2.4.6. OpenACC Details View 2.4.7. OpenMP Details View 2.4.8. Properties View 2.4.9. Console View 2.4.10. Settings View 2.4.11. CPU Source View 2.5. Customizing the Profiler 2.5.1. Resizing a View 2.5.2. Reordering a View 2.5.3. Moving a View 2.5.4. Undocking a View 2.5.5. Opening and Closing a View 2.6. Command Line Arguments 3. ​nvprof 3.1. Command Line Options 3.1.1. CUDA Profiling Options 3.1.2. CPU Profiling Options 3.1.3. Print Options 3.1.4. IO Options 3.2. Profiling Modes 3.2.1. Summary Mode 3.2.2. GPU-Trace and API-Trace Modes 3.2.3. Event/metric Summary Mode 3.2.4. Event/metric Trace Mode 3.3. Profiling Controls 3.3.1. Timeout 3.3.2. Concurrent Kernels 3.3.3. Profiling Scope 3.3.4. Multiprocess Profiling 3.3.5. System Profiling 3.3.6. Unified Memory Profiling 3.3.7. CPU Thread Tracing 3.4. Output 3.4.1. Adjust Units 3.4.2. CSV 3.4.3. Export/Import 3.4.4. Demangling 3.4.5. Redirecting Output 3.4.6. Dependency Analysis 3.5. CPU Sampling 3.5.1. CPU Sampling Limitations 3.6. OpenACC 3.6.1. OpenACC Options 3.6.2. OpenACC Summary Modes 3.7. OpenMP 3.7.1. OpenMP Options 4. Remote Profiling 4.1. Remote Profiling With Visual Profiler 4.1.1. One-hop remote profiling 4.2. Remote Profiling With nvprof 4.2.1. Collect Data On Remote System 4.2.2. View And Analyze Data 5. NVIDIA Tools Extension 5.1. NVTX API Overview 5.2. NVTX API Events 5.2.1. NVTX Markers 5.2.2. NVTX Range Start/Stop 5.2.3. NVTX Range Push/Pop 5.2.4. Event Attributes Structure 5.2.5. NVTX Synchronization Markers 5.3. NVTX Domains 5.4. NVTX Resource Naming 5.5. NVTX String Registration 6. MPI Profiling 6.1. Automatic MPI Annotation with NVTX 6.2. Manual MPI Profiling 6.3. Further Reading 7. MPS Profiling 7.1. MPS profiling with Visual Profiler 7.2. MPS profiling with nvprof 7.3. Viewing nvprof MPS timeline in Visual Profiler 8. Dependency Analysis 8.1. Background 8.2. Metrics 8.3. Support 8.4. Limitations 9. Metrics Reference 9.1. Metrics for Capability 5.x 9.2. Metrics for Capability 6.x 9.3. Metrics for Capability 7.x 10. Warp State 11. Migrating to Nsight Tools from Visual Profiler and nvprof 12. Profiler Known Issues 13. Changelog 14. Notices 14.1. Notice 14.2. OpenCL 14.3. Trademarks Profiler » 1. Preparing An Application For Profiling v12.5 | PDF | Archive Profiler User’s Guide The user manual for NVIDIA profiling tools for optimizing performance of CUDA applications. Profiling Overview This document describes NVIDIA profiling tools that enable you to understand and optimize the performance of your CUDA, OpenACC or OpenMP applications. The Visual Profiler is a graphical profiling tool that displays a timeline of your application’s CPU and GPU activity, and that includes an automated analysis engine to identify optimization opportunities. The nvprof profiling tool enables you to collect and view profiling data from the command-line. Note that Visual Profiler and nvprof will be deprecated in a future CUDA release. The NVIDIA Volta platform is the last architecture on which these tools are fully supported. It is recommended to use next-generation tools NVIDIA Nsight Systems for GPU and CPU sampling and tracing and NVIDIA Nsight Compute for GPU kernel profiling. Refer the Migrating to Nsight Tools from Visual Profiler and nvprof section for more details. Terminology An event is a countable activity, action, or occurrence on a device. It corresponds to a single hardware counter value which is collected during kernel execution. To see a list of all available events on a particular NVIDIA GPU, type nvprof --query-events . A metric is a characteristic of an application that is calculated from one or more event values. To see a list of all available metrics on a particular NVIDIA GPU, type nvprof --query-metrics . You can also refer to the metrics reference . 1. Preparing An Application For Profiling  The CUDA profiling tools do not require any application changes to enable profiling; however, by making some simple modifications and additions, you can greatly increase the usability and effectiveness profiling. This section describes these modifications and how they can improve your profiling results. 1.1. Focused Profiling  By default, the profiling tools collect profile data over the entire run of your application. But, as explained below, you typically only want to profile the region(s) of your application containing some or all of the performance-critical code. Limiting profiling to performance-critical regions reduces the amount of profile data that both you and the tools must process, and focuses attention on the code where optimization will result in the greatest performance gains. There are several common situations where profiling a region of the application is helpful. The application is a test harness that contains a CUDA implementation of all or part of your algorithm. The test harness initializes the data, invokes the CUDA functions to perform the algorithm, and then checks the results for correctness. Using a test harness is a common and productive way to quickly iterate and test algorithm changes. When profiling, you want to collect profile data for the CUDA functions implementing the algorithm, but not for the test harness code that initializes the data or checks the results. The application operates in phases, where a different set of algorithms is active in each phase. When the performance of each phase of the application can be optimized independently of the others, you want to profile each phase separately to focus your optimization efforts. The application contains algorithms that operate over a large number of iterations, but the performance of the algorithm does not vary significantly across those iterations. In this case you can collect profile data from a subset of the iterations. To limit profiling to a region of your application, CUDA provides functions to start and stop profile data collection. cudaProfilerStart() is used to start profiling and cudaProfilerStop() is used to stop profiling (using the CUDA driver API, you get the same functionality with cuProfilerStart() and cuProfilerStop() ). To use these functions you must include cuda_profiler_api.h (or cudaProfiler.h for the driver API). When using the start and stop functions, you also need to instruct the profiling tool to disable profiling at the start of the application. For nvprof you do this with the --profile-from-start off flag. For the Visual Profiler you use the Start execution with profiling enabled checkbox in the Settings View . 1.2. Marking Regions of CPU Activity  The Visual Profiler can collect a trace of the CUDA function calls made by your application. The Visual Profiler shows these calls in the Timeline View , allowing you to see where each CPU thread in the application is invoking CUDA functions. To understand what the application’s CPU threads are doing outside of CUDA function calls, you can use the NVIDIA Tools Extension API (NVTX). When you add NVTX markers and ranges to your application, the Timeline View shows when your CPU threads are executing within those regions. nvprof also supports NVTX markers and ranges. Markers and ranges are shown in the API trace output in the timeline. In summary mode, each range is shown with CUDA activities associated with that range. 1.3. Naming CPU and CUDA Resources  The Visual Profiler Timeline View shows default naming for CPU thread and GPU devices, context and streams. Using custom names for these resources can improve understanding of the application behavior, especially for CUDA applications that have many host threads, devices, contexts, or streams. You can use the NVIDIA Tools Extension API to assign custom names for your CPU and GPU resources. Your custom names will then be displayed in the Timeline View . nvprof also supports NVTX naming. Names of CUDA devices, contexts and streams are displayed in summary and trace mode. Thread names are displayed in summary mode. 1.4. Flush Profile Data  To reduce profiling overhead, the profiling tools collect and record profile information into internal buffers. These buffers are then flushed asynchronously to disk with low priority to avoid perturbing application behavior. To avoid losing profile information that has not yet been flushed, the application being profiled should make sure, before exiting, that all GPU work is done (using CUDA synchronization calls), and then call cudaProfilerStop() or cuProfilerStop() . Doing so forces buffered profile information on corresponding context(s) to be flushed. If your CUDA application includes graphics that operate using a display or main loop, care must be taken to call cudaProfilerStop() or cuProfilerStop() before the thread executing that loop calls exit() . Failure to call one of these APIs may result in the loss of some or all of the collected profile data. For some graphics applications like the ones use OpenGL, the application exits when the escape key is pressed. In those cases where calling the above functions before exit is not feasible, use nvprof option --timeout or set the “Execution timeout” in the Visual Profiler. The profiler will force a data flush just before the timeout. 1.5. Profiling CUDA Fortran Applications  CUDA Fortran applications compiled with the PGI CUDA Fortran compiler can be profiled by nvprof and the Visual Profiler. In cases where the profiler needs source file and line information (kernel profile analysis, global memory access pattern analysis, divergent execution analysis, etc.), use the “-Mcuda=lineinfo” option when compiling. This option is supported on Linux 64-bit targets in PGI 2019 version 19.1 or later. 2. ​Visual Profiler  The NVIDIA Visual Profiler allows you to visualize and optimize the performance of your application. The Visual Profiler displays a timeline of your application’s activity on both the CPU and GPU so that you can identify opportunities for performance improvement. In addition, the Visual Profiler will analyze your application to detect potential performance bottlenecks and direct you on how to take action to eliminate or reduce those bottlenecks. The Visual Profiler is available as both a standalone application and as part of Nsight Eclipse Edition. The standalone version of the Visual Profiler, nvvp , is included in the CUDA Toolkit for all supported OSes except for macOS. Starting with the CUDA 11.0, Visual Profiler and nvprof don’t support macOS as the target platform. However Visual Profiler supports remote profiling from the macOS host. This support is deprecated in the CUDA 12.5 release. Visual Profiler is provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on macOS. See Developer Tools for macOS for download instructions. Within Nsight Eclipse Edition, the Visual Profiler is located in the Profile Perspective and is activated when an application is run in profile mode. 2.1. Getting Started  This section describes steps you might take as you begin profiling. 2.1.1. Setting up Java Runtime Environment  Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system. However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install the required version of JRE 1.8 in order to use Visual Profiler. See Installing JRE . To run Visual Profiler on OpenSUSE15 or SLES15: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/lib64/jvm/jre-1.8.0/bin/java Note The -vm option is only required when JRE 1.8 is not in the default path. To run Visual Profiler on Ubuntu 18.04 or Ubuntu 18.10: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java Note The -vm option is only required when JRE 1.8 is not in the default path. On Ubuntu 18.10, if you get error “ no swt-pi-gtk in java.library.path ” when running Visual Profiler, then you need to install GTK2. Type the below command to install the required GTK2. apt-get install libgtk2.0-0 To run Visual Profiler on Fedora 29: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/bin/java Note The -vm option is only required when JRE 1.8 is not in the default path. To run Visual Profiler on macOS: Visual Profiler requires Java Runtime Environment (JRE) 1.8 update 151. Visual Profiler does not work with newer versions i.e. JRE 1.8 update 152 or later. Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/bin/java Note The -vm option is only required when JRE 1.8 update 151 is not in the default path. To run Visual Profiler on Windows: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm \"C:\\Program Files\\Java\\jdk1.8.0_77\\jre\\bin\\java\" Note The -vm option is only required when JRE 1.8 is not in the default path. 2.1.2. Installing JRE  Visual Profiler require Java Runtime Environment (JRE) 1.8 to be available on the local system. However, as of CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install JRE 1.8 in order to use Visual Profiler. See below for available options. Also see Java Platform, Standard Edition 8 Names and Versions. Windows Oracle JRE 1.8 (may require paid updates) OpenJDK JRE 1.8 Linux Oracle JRE 1.8 (may require paid updates) OpenJDK JRE 1.8 Mac Oracle JRE 1.8 (may require paid updates) Note JRE 1.8u152 or later is not supported for Visual Profiler. You can find the JRE update 151 on the Oracle Download Archive site here: https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html?printOnly=1 . 2.1.3. Modify Your Application For Profiling  The Visual Profiler does not require any application changes; however, by making some simple modifications and additions, you can greatly increase its usability and effectiveness. Section Preparing An Application For Profiling describes how you can focus your profiling efforts and add extra annotations to your application that will greatly improve your profiling experience. 2.1.4. Creating a Session  The first step in using the Visual Profiler to profile your application is to create a new profiling session. A session contains the settings, data, and results associated with your application. The Sessions section gives more information on working with sessions. You can create a new session by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu. In the Create New Session dialog enter the executable for your application. Optionally, you can also specify the working directory, arguments, multi-process profiling option and environment. The muti-process profiling options are: Profile child processes - If selected, profile all processes launched by the specified application. Profile all processes - If selected, profile every CUDA process launched on the same system by the same user who launched nvprof. In this mode the Visual Profiler will launch nvprof and user needs to run his application in another terminal outside the Visual Profiler. User can exit this mode by pressing “Cancel” button on progress dialog in Visual Profiler to load the profile data Profile current process only - If selected, only profile specified application. Press Next to choose some additional profiling options. CUDA options: Start execution with profiling enabled - If selected profile data is collected from the start of application execution. If not selected profile data is not collected until cudaProfilerStart() is called in the application. See Focused Profiling for more information about cudaProfilerStart() . Enable concurrent kernel profiling - This option should be selected for an application that uses CUDA streams to launch kernels that can execute concurrently. If the application uses only a single stream (and therefore cannot have concurrent kernel execution), deselecting this option may decrease profiling overhead. Enable CUDA API tracing in the timeline - If selected, the CUDA driver and runtime API call trace is collected and displayed on timeline. Enable power, clock, and thermal profiling - If selected, power, clock, and thermal conditions on the GPUs will be sampled and displayed on the timeline. Collection of this data is not supported on all GPUs. See the description of the Device timeline in Timeline View for more information. Enable unified memory profiling - If selected for the GPU that supports Unified Memory, the Unified Memory related memory traffic to and from each GPU is collected on your system and displayed on timeline. Replay application to collect events and metrics - If selected, the whole application is re-run instead of replaying each kernel, in order to collect all events/metrics. Run guided analysis - If selected, the guided analysis is run immediately after the creation of a new session. Uncheck this option to disable this behavior. CPU (host) options: Profile execution on the CPU - If selected the CPU threads are sampled and data collected about the CPU performance is shown in the CPU Details View . Enable OpenACC profiling - If selected and an OpenACC application is profiled, OpenACC activities will be recorded and displayed on a new OpenACC timeline. Collection of this data is only supported on Linux and PGI 19.1 or later. See the description of the OpenACC timeline in Timeline View for more information. Enable CPU thread tracing - If enabled, selected CPU thread API calls will be recorded and displayed on a new thread API timeline. This currently includes the Pthread API, mutexes and condition variables. For performance reasons, only those API calls that influence concurrent execution are recorded and collection of this data is not supported on Windows. See the description of the thread timeline in Timeline View for more information. This option should be selected for dependency analysis of applications with multiple CPU threads using CUDA. Timeline Options: Load data for time range - If selected the start and end time stamps for the range of data to be loaded can be specified. This option is useful to select a subset of a large data. Enable timelines in the session - By default all timelines are enabled. If a timeline is un-checked, the data associated with that timeline will not be loaded and it will not be displayed. Note If some timelines are disabled by un-checking the option the analyses results which use this timeline data will be incorrect. Press Finish. 2.1.5. Analyzing Your Application  If the Don’t run guided analysis option was not selected when you created your session, the Visual Profiler will immediately run your application to collect the data needed for the first stage of guided analysis. As described in the Analysis View section, you can use the guided analysis system to get recommendations on performance limiting behavior in your application. 2.1.6. Exploring the Timeline  In addition to the guided analysis results, you will see a timeline for your application showing the CPU and GPU activity that occurred as your application executed. Read Timeline View and Properties View to learn how to explore the profiling information that is available in the timeline. Navigating the Timeline describes how you can zoom and scroll the timeline to focus on specific areas of your application. 2.1.7. Looking at the Details  In addition to the results provided in the Analysis View , you can also look at the specific metric and event values collected as part of the analysis. Metric and event values are displayed in the GPU Details View . You can collect specific metric and event values that reveal how the kernels in your application are behaving. You collect metrics and events as described in the GPU Details View section. 2.1.8. Improve Loading of Large Profiles  Some applications launch many tiny kernels, making them prone to very large (100s of megabytes or larger) output, even for application runs of only a few seconds. The Visual Profiler needs roughly the same amount of memory as the size of the profile it is opening/importing. The Java virtual machine may use a fraction of the main memory if no “max heap size” setting is specified. So depending on the size of main memory, the Visual Profiler may fail to load some large files. If the Visual Profiler fails to load a large profile, try setting the max heap size that JVM is allowed to use according to main memory size. You can modify the config file libnvvp/nvvp.ini in the toolkit installation directory. On macOS the nvvp.ini file is present in folder /Developer/{cuda_install_dir}/libnvvp/nvvp.app/Contents/MacOS/ . The nvvp.ini configuration file looks like this: -startup\nplugins/org.eclipse.equinox.launcher_1.3.0.v20140415-2008.jar\n--launcher.library\nplugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.200.v20140603-1326\n-data\n@user.home/nvvp_workspace\n-vm\n../jre/bin/java\n-vmargs\n-Dorg.eclipse.swt.browser.DefaultType=mozilla To force the JVM to use 3 gigabytes of memory, for example, add a new line with ‑Xmx3G after ‑vmargs . The -Xmx setting should be tailored to the available system memory and input size. For example, if your system has 24GB of system memory, and you happen to know that you won’t need to run any other memory-intensive applications at the same time as the Visual Profiler, so it’s okay for the profiler to take up the vast majority of that space. So you might pick, say, 22GB as the maximum heap size, leaving a few gigabytes for the OS, GUI, and any other programs that might be running. Some other nvvp.ini configuration settings can also be modified: Increase the default heap size (the one Java automatically starts up with) to, say, 2GB. ( -Xms ) Tell Java to run in 64-bit mode instead of the default 32-bit mode (only works on 64-bit systems); this is required if you want heap sizes >4GB. ( -d64 ) Enable Javas parallel garbage collection system, which helps both to decrease the required memory space for a given input size as well as to catch out of memory errors more gracefully. ( -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode ) Note: most installations require administrator/root-level access to modify this file. The modified nvvp.ini file as per examples given above is as follows: -data\n@user.home/nvvp_workspace\n-vm\n../jre/bin/java\n-d64\n-vmargs\n-Xms2g\n-Xmx22g\n-XX:+UseConcMarkSweepGC\n-XX:+CMSIncrementalMode\n-Dorg.eclipse.swt.browser.DefaultType=Mozilla For more details on JVM settings, consult the Java virtual machine manual. In addition to this you can use timeline options Load data for time range and Enable timelines in the session mentioned in the Creating a Session section to limit the data which is loaded and displayed. 2.2. Sessions  A session contains the settings, data, and profiling results associated with your application. Each session is saved in a separate file; so you can delete, move, copy, or share a session by simply deleting, moving, copying, or sharing the session file. By convention, the file extension .nvvp is used for Visual Profiler session files. There are two types of sessions: an executable session that is associated with an application that is executed and profiled from within the Visual Profiler, and an import session that is created by importing data generated by nvprof . 2.2.1. Executable Session  You can create a new executable session for your application by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu. Once a session is created, you can edit the session’s settings as described in the Settings View . You can open and save existing sessions using the open and save options in the File menu. To analyze your application and to collect metric and event values, the Visual Profiler will execute your application multiple times. To get accurate profiling results, it is important that your application conform to the requirements detailed in Application Requirements . 2.2.2. Import Session  You create an import session from the output of nvprof by using the Import… option in the File menu. Selecting this option opens the import dialog which guides you through the import process. Because an executable application is not associated with an import session, the Visual Profiler cannot execute the application to collect additional profile data. As a result, analysis can only be performed with the data that is imported. Also, the GPU Details View will show any imported event and metrics values but new metrics and events cannot be selected and collected for the import session. 2.2.2.1. Import Single-Process nvprof Session  Using the import dialog you can select one or more nvprof data files for import into the new session. You must have one nvprof data file that contains the timeline information for the session. This data file should be collected by running nvprof with the --export-profile option. You can optionally enable other options such as --system-profiling on , but you should not collect any events or metrics as that will distort the timeline so that it is not representative of the applications true behavior. You may optionally specify one or more event/metric data files that contain event and metric values for the application. These data files should be collected by running nvprof with one or both of the --events and --metrics options. To collect all the events and metrics that are needed for the analysis system, you can simply use the --analysis-metrics option along with the --kernels option to select the kernel(s) to collect events and metrics for. See Remote Profiling for more information. If you are importing multiple nvprof output files into the session, it is important that your application conform to the requirements detailed in Application Requirements . 2.2.2.2. Import Multi-Process nvprof Session  Using the import wizard you can select multiple nvprof data files for import into the new multi-process session. Each nvprof data file must contain the timeline information for one of the processes. This data file should be collected by running nvprof with the --export-profile option. You can optionally enable other options such as --system-profiling on , but you should not collect any events or metrics as that will distort the timeline so that it is not representative of the applications true behavior. Select the Multiple Processes option in the Import nvprof Data dialog as shown in the figure below. When importing timeline data from multiple processes you may not specify any event/metric data files for those processes. Multi-processes profiling is only supported for timeline data. 2.2.2.3. Import Command-Line Profiler Session  Support for command-line profiler (using the environment variable COMPUTE_PROFILE) has been dropped, but CSV files generated using earlier versions can still be imported. Using the import wizard you can select one or more command-line profiler generated CSV files for import into the new session. When you import multiple CSV files, their contents are combined and displayed in a single timeline. The command-line profiler CSV file must be generated with the gpustarttimestamp and streamid configuration parameters. It is fine to include other configuration parameters, including events. 2.3. Application Requirements  To collect performance data about your application, the Visual Profiler must be able to execute your application repeatedly in a deterministic manner. Due to software and hardware limitations, it is not possible to collect all the necessary profile data in a single execution of your application. Each time your application is run, it must operate on the same data and perform the same kernel and memory copy invocations in the same order. Specifically, For a device, the order of context creation must be the same each time the application executes. For a multi-threaded application where each thread creates its own context(s), care must be taken to ensure that the order of those context creations is consistent across multiple runs. For example, it may be necessary to create the contexts on a single thread and then pass the contexts to the other threads. Alternatively, the NVIDIA Tools Extension API can be used to provide a custom name for each context. As long as the same custom name is applied to the same context on each execution of the application, the Visual Profiler will be able to correctly associate those contexts across multiple runs. For a context, the order of stream creation must be the same each time the application executes. Alternatively, the NVIDIA Tools Extension API can be used to provide a custom name for each stream. As long as the same custom name is applied to the same stream on each execution of the application, the Visual Profiler will be able to correctly associate those streams across multiple runs. Within a stream, the order of kernel and memcpy invocations must be the same each time the application executes. 2.4. Visual Profiler Views  The Visual Profiler is organized into views. Together, the views allow you to analyze and visualize the performance of your application. This section describes each view and how you use it while profiling your application. 2.4.1. Timeline View  The Timeline View shows CPU and GPU activity that occurred while your application was being profiled. Multiple timelines can be opened in the Visual Profiler at the same time in different tabs. The following figure shows a Timeline View for a CUDA application. Along the top of the view is a horizontal ruler that shows elapsed time from the start of application profiling. Along the left of the view is a vertical ruler that describes what is being shown for each horizontal row of the timeline, and that contains various controls for the timeline. These controls are described in Timeline Controls The timeline view is composed of timeline rows. Each row shows intervals that represent the start and end times of the activities that correspond to the type of the row. For example, timeline rows representing kernels have intervals representing the start and end times of executions of that kernel. In some cases (as noted below) a timeline row can display multiple sub-rows of activity. Sub-rows are used when there is overlapping activity. These sub-rows are created dynamically as necessary depending on how much activity overlap there is. The placement of intervals within certain sub-rows does not convey any particular meaning. Intervals are just packed into sub-rows using a heuristic that attempts to minimize the number of needed sub-rows. The height of the sub-rows is scaled to keep vertical space reasonable. The types of timeline rows that are displayed in the Timeline View are: Process A timeline will contain a Process row for each application profiled. The process identifier represents the pid of the process. The timeline row for a process does not contain any intervals of activity. Threads within the process are shown as children of the process. Thread A timeline will contain a Thread row for each CPU thread in the profiled application that performed either a CUDA driver or CUDA runtime API call. The thread identifier is a unique id for that CPU thread. The timeline row for a thread is does not contain any intervals of activity. Runtime API A timeline will contain a Runtime API row for each CPU thread that performs a CUDA Runtime API call. Each interval in the row represents the duration of the call on the corresponding thread. Driver API A timeline will contain a Driver API row for each CPU thread that performs a CUDA Driver API call. Each interval in the row represents the duration of the call on the corresponding thread. OpenACC A timeline will contain one or multiple OpenACC rows for each CPU thread that calls OpenACC directives. Each interval in the row represents the duration of the call on the corresponding thread. Each OpenACC timeline may consist of multiple rows. Within one timeline, OpenACC activities on rows further down are called from within activities on the rows above. OpenMP A timeline will contain one OpenMP row for each CPU thread that calls OpenMP. Each interval in the row represents how long the application spends in a given OpenMP region or state. The application may be in multiple states at the same time, this is shown by drawing multiple rows where some intervals overlap. Pthread A timeline will contain one Pthread row for each CPU thread that performs Pthread API calls, given that host thread API calls have been recorded during measurement. Each interval in the row represents the duration of the call. Note that for performance reasons, only selected Pthread API calls may have been recorded. Markers and Ranges A timeline will contain a single Markers and Ranges row for each CPU thread that uses the NVIDIA Tools Extension API to annotate a time range or marker. Each interval in the row represents the duration of a time range, or the instantaneous point of a marker. This row will have sub-rows if there are overlapping ranges. Profiling Overhead A timeline will contain a single Profiling Overhead row for each process. Each interval in the row represents the duration of execution of some activity required for profiling. These intervals represent activity that does not occur when the application is not being profiled. Device A timeline will contain a Device row for each GPU device utilized by the application being profiled. The name of the timeline row indicates the device ID in square brackets followed by the name of the device. After running the Compute Utilization analysis, the row will contain an estimate of the compute utilization of the device over time. If power, clock, and thermal profiling are enabled, the row will also contain points representing those readings. Unified Memory A timeline will contain a Unified Memory row for each CPU thread and device that uses unified memory. The Unified memory may contain CPU Page Faults, GPU Page Faults, Data Migration (DtoH) and Data Migration (HtoD) rows. When creating a session user can select segment mode or non-segment mode for Unified Memory timelines. In the segment mode the timeline is split into equal width segments and only aggregated data values for each time segment are shown. The number of segments can be changed. In non-segment mode each interval on the timeline will represent the actual data collected and the properties for each interval can be viewed. The segments are colored using a heat-map color scheme. Under properties for the timeline the property which is used for selecting the color is given and also a legend displays the mapping of colors to different range of property values. CPU Page Faults This will contain a CPU Page Faults row for each CPU thread. In the non-segment mode each interval on the timeline corresponds to one CPU page fault. Data Migration (DtoH) A timeline will contain Data Migration (DtoH) row for each device. In the non-segment mode each interval on the timeline corresponds to one data migration from device to host. GPU Page Faults A timeline will contain GPU Page Faults. row for each CPU thread. In the non-segment mode each interval on the timeline corresponds to one GPU page fault group. Data Migration (DtoH) A timeline will contain Data Migration (HtoD) row for each device. In the non-segment mode each interval on the timeline corresponds to one data migration from host to device. Context A timeline will contains a Context row for each CUDA context on a GPU device. The name of the timeline row indicates the context ID or the custom context name if the NVIDIA Tools Extension API was used to name the context. The row for a context does not contain any intervals of activity. Memcpy A timeline will contain memory copy row(s) for each context that performs memcpys. A context may contain up to four memcpy rows for device-to-host, host-to-device, device-to-device, and peer-to-peer memory copies. Each interval in a row represents the duration of a memcpy executing on the GPU. Compute A timeline will contain a Compute row for each context that performs computation on the GPU. Each interval in a row represents the duration of a kernel on the GPU device. The Compute row indicates all the compute activity for the context. Sub-rows are used when concurrent kernels are executed on the context. All kernel activity, including kernels launched using CUDA Dynamic Parallelism, is shown on the Compute row. The Kernel rows following the Compute row show activity of each individual application kernel. Kernel A timeline will contain a Kernel row for each kernel executed by the application. Each interval in a row represents the duration of execution of an instance of that kernel in the containing context. Each row is labeled with a percentage that indicates the total execution time of all instances of that kernel compared to the total execution time of all kernels. For each context, the kernels are ordered top to bottom by this execution time percentage. Sub-rows are used to show concurrent kernel execution. For CUDA Dynamic Parallelism applications, the kernels are organized in a hierarchy that represents the parent/child relationship between the kernels. Host-launched kernels are shown as direct children of the Context row. Kernels that use CUDA Dynamic Parallelism to launch other kernels can be expanded using the ‘+’ icon to show the kernel rows representing those child kernels. For kernels that don’t launch child kernels, the kernel execution is represented by a solid interval, showing the time that that instance of the kernel was executing on the GPU. For kernels that launch child kernels, the interval can also include a hollow part at the end. The hollow part represents the time after the kernel has finished executing where it is waiting for child kernels to finish executing. The CUDA Dynamic Parallelism execution model requires that a parent kernel not complete until all child kernels complete and this is what the hollow part is showing. The Focus control described in Timeline Controls can be used to control display of the parent/child timelines. Stream A timeline will contain a Stream row for each stream used by the application (including both the default stream and any application created streams). Each interval in a Stream row represents the duration of a memcpy or kernel execution performed on that stream. 2.4.1.1. Timeline Controls  The Timeline View has several controls that you use to control how the timeline is displayed. Some of these controls also influence the presentation of data in the GPU Details View and the Analysis View . Resizing the Vertical Timeline Ruler The width of the vertical ruler can be adjusted by placing the mouse pointer over the right edge of the ruler. When the double arrow pointer appears, click and hold the left mouse button while dragging. The vertical ruler width is saved with your session. Reordering Timelines The Kernel and Stream timeline rows can be reordered. You may want to reorder these rows to aid in visualizing related kernels and streams, or to move unimportant kernels and streams to the bottom of the timeline. To reorder a row, left-click and hold onto the row label. When the double arrow pointer appears, drag up or down to position the row. The timeline ordering is saved with your session. Filtering Timelines Memcpy and Kernel rows can be filtered to exclude their activities from presentation in the GPU Details View and the Analysis View . To filter out a row, left-click on the filter icon just to the left of the row label. To filter all Kernel or Memcpy rows, Shift-left-click one of the rows. When a row is filtered, any intervals on that row are dimmed to indicate their filtered status. Expanding and Collapsing Timelines Groups of timeline rows can be expanded and collapsed using the [+] and [-] controls just to the left of the row labels. There are three expand/collapse states: Collapsed No timeline rows contained in the collapsed row are shown. Expanded All non-filtered timeline rows are shown. All-Expanded All timeline rows, filtered and non-filtered, are shown. Intervals associated with collapsed rows may not be shown in the GPU Details View and the Analysis View , depending on the filtering mode set for those views (see view documentation for more information). For example, if you collapse a device row, then all memcpys, memsets, and kernels associated with that device are excluded from the results shown in those views. Coloring Timelines There are three modes for timeline coloring. The coloring mode can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar. In kernel coloring mode, each type of kernel is assigned a unique color (that is, all activity intervals in a kernel row have the same color). In stream coloring mode, each stream is assigned a unique color (that is, all memcpy and kernel activity occurring on a stream are assigned the same color). In process coloring mode, each process is assigned a unique color (that is, all memcpy and kernel activity occurring in a process are assigned the same color). Focusing Kernel Timelines For applications using CUDA Dynamic Parallelism, the Timeline View displays a hierarchy of kernel activity that shows the parent/child relationship between kernels. By default all parent/child relationships are shown simultaneously. The focus timeline control can be used to focus the displayed parent/child relationships to a specific, limited set of “family trees”. The focus timeline mode can be selected and deselected in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar. To see the “family tree” of a particular kernel, select a kernel and then enable Focus mode. All kernels except those that are ancestors or descendants of the selected kernel will be hidden. Ctrl-select can be used to select multiple kernels before enabling Focus mode. Use the “Don’t Focus” option to disable focus mode and restore all kernels to the Timeline view. Dependency Analysis Controls There are two modes for visualizing dependency analysis results in the timeline: Focus Critical Path and Highlight Execution Dependencies. These modes can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the Visual Profiler toolbar. These options become available after the Dependency Analysis application analysis stage has been run (see Unguided Application Analysis ). A detailed explanation of these modes is given in Dependency Analysis Controls 2.4.1.2. Navigating the Timeline  The timeline can be scrolled, zoomed, and focused in several ways to help you better understand and visualize your application’s performance. Zooming The zoom controls are available in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar. Zoom-in reduces the timespan displayed in the view, zoom-out increases the timespan displayed in the view, and zoom-to-fit scales the view so that the entire timeline is visible. You can also zoom-in and zoom-out with the mouse wheel while holding the Ctrl key (for macOS use the Command key). Another useful zoom mode is zoom-to-region. Select a region of the timeline by holding Ctrl (for macOS use the Command key) while left-clicking and dragging the mouse. The highlighted region will be expanded to occupy the entire view when the mouse button is released. Scrolling The timeline can be scrolled vertically with the scrollbar of the mouse wheel. The timeline can be scrolled horizontally with the scrollbar or by using the mouse wheel while holding the Shift key. Highlighting/Correlation When you move the mouse pointer over an activity interval on the timeline, that interval is highlighted in all places where the corresponding activity is shown. For example, if you move the mouse pointer over an interval representing a kernel execution, that kernel execution is also highlighted in the Stream and in the Compute timeline row. When a kernel or memcpy interval is highlighted, the corresponding driver or runtime API interval will also highlight. This allows you to see the correlation between the invocation of a driver or runtime API or OpenACC directive on the CPU and the corresponding activity on the GPU. Information about the highlighted interval is shown in the Properties View . Selecting You can left-click on a timeline interval or row to select it. Multi-select is done using Ctrl-left-click. To unselect an interval or row simply Ctrl-left-click on it again. When a single interval or row is selected, the information about that interval or row is pinned in the Properties View . In the GPU Details View , the detailed information for the selected interval is shown in the table. Measuring Time Deltas Measurement rulers can be created by left-click dragging in the horizontal ruler at the top of the timeline. Once a ruler is created it can be activated and deactivated by left-clicking. Multiple rulers can be activated by Ctrl-left-click. Any number of rulers can be created. Active rulers are deleted with the Delete or Backspace keys. After a ruler is created, it can be resized by dragging the vertical guide lines that appear over the timeline. If the mouse is dragged over a timeline interval, the guideline will snap to the nearest edge of that interval. 2.4.1.3. Timeline Refreshing  The profiler loads the timeline gradually as it reads the data. This is more apparent if the data file being loaded is big, or the application has generated a lot of data. In such cases, the timeline may be partially rendered. At the same time, a spinning circle replaces the icon of the current session tab, indicating the timeline is not fully loaded. Loading is finished when the icon changes back. To reduce its memory footprint, the profiler may skip loading some timeline contents if they are not visible at the current zoom level. These contents will be automatically loaded when they become visible on a new zoom level. 2.4.1.4. Dependency Analysis Controls  The profiler allows the visualization of dependency analysis results in the timeline once the respective analysis stage has been run. For a detailed description on how dependency analysis works, see Dependency Analysis . Focus Critical Path visualizes the critical path through the application by focusing on all intervals on the critical path and fading others. When the mode is enabled and any timeline interval is selected (by left-clicking it), the selected interval will have focus. However, the critical path will still be visible as hollow intervals. This allows you to “follow” the critical path through the execution and to inspect individual intervals. Highlight Execution Dependencies allows you to analyze the execution dependencies for each interval (Note that for certain intervals, no dependency information is collected). When this mode is enabled, the highlighting color changes from yellow (representing correlated intervals) to red (representing dependencies). Both the selected interval as well as all incoming and outgoing dependencies are highlighted. 2.4.2. Analysis View  The Analysis View is used to control application analysis and to display the analysis results. There are two analysis modes: guided and unguided. In guided mode the analysis system will guide you through multiple analysis stages to help you understand the likely performance limiters and optimization opportunities in your application. In unguided mode you can manually explore all the analysis results collected for your application. The following figure shows the analysis view in guided analysis mode. The left part of the view provides step-by-step directions to help you analyze and optimize your application. The right part of the view shows detailed analysis results appropriate for each part of the analysis. 2.4.2.1. Guided Application Analysis  In guided mode, the analysis view will guide you step-by-step through analysis of your entire application with specific analysis guidance provided for each kernel within your application. Guided analysis starts with CUDA Application Analysis and from there will guide you to optimization opportunities within your application. 2.4.2.2. Unguided Application Analysis  In unguided analysis mode each application analysis stage has a Run analysis button that can be used to generate the analysis results for that stage. When the Run analysis button is selected, the profiler will execute the application to collect the profiling data needed to perform the analysis. The green check-mark next to an analysis stage indicates that the analysis results for that stage are available. Each analysis result contains a brief description of the analysis and a More… link to detailed documentation on the analysis. When you select an analysis result, the timeline rows or intervals associated with that result are highlighted in the Timeline View . When a single kernel instance is selected in the timeline, additional kernel-specific analysis stages are available. Each kernel-specific analysis stage has a Run analysis button that operates in the same manner as for the application analysis stages. The following figure shows the analysis results for the Divergent Execution analysis stage. Some kernel instance analysis results, like Divergent Execution, are associated with specific source-lines within the kernel. To see the source associated with each result, select an entry from the table. The source file associated with that entry will open. 2.4.2.3. PC Sampling View  Devices with compute capability 5.2 and higher, excluding mobile devices, have a feature for PC sampling. In this feature PC and state of warp are sampled at regular interval for one of the active warps per SM. The warp state indicates if that warp issued an instruction in a cycle or why it was stalled and could not issue an instruction. When a warp that is sampled is stalled, there is a possibility that in the same cycle some other warp is issuing an instruction. Hence the stall for the sampled warp need not necessarily indicate that there is a hole in the instruction issue pipeline. Refer to the Warp State section for a description of different states. Devices with compute capability 6.0 and higher have a new feature that gives latency reasons. The latency samples indicate the reasons for holes in the issue pipeline. While collecting these samples, there is no instruction issued in the respective warp scheduler and hence these give the latency reasons. The latency reasons will be one of the stall reasons in Warp State section except ‘not selected’ stall reason. The profiler collects this information and presents it in the Kernel Profile - PC Sampling view. In this view, the sample distribution for all functions and kernels is given in a table. A pie chart shows the distribution of stall reasons collected for each kernel. After clicking on the source file or device function the Kernel Profile - PC Sampling view is opened. The hotspots shown next to the vertical scroll bar are determined by the number of samples collected for each source and assembly line. The distribution of the stall reasons is shown as a stacked bar for each source and assembly line. This helps in pinpointing the latency reasons at the source code level. For devices with compute capability 6.0 and higher, Visual Profiler show two views: ‘Kernel Profile - PC Sampling’ which gives the warp state view and ‘Kernel Profile - PC Sampling - Latency’ which gives the latency reasons. Hotspots can be seleted to point to hotspot of ‘Warp State’ or ‘Latency Reasons’. The tables in result section give percentage distribution for total latency samples, issue pipeline busy samples and instruction issued samples. The blog post Pinpoint Performance Problems with Instruction-Level Profiling shows how PC Sampling can be used to optimize a CUDA kernel. 2.4.2.4. Memory Statistics  Devices with compute capability 5.0 and higher have a feature to show usage of the memory sub-system during kernel execution. The chart shows a summary view of the memory hierarchy of the CUDA programming model. The green nodes in the diagram depict logical memory space whereas blue nodes depicts actual hardware unit on the chip. For the various caches the reported percentage number states the cache hit rate; that is the ratio of requests that could be served with data locally available to the cache over all requests made. The links between the nodes in the diagram depict the data paths between the SMs to the memory spaces into the memory system. Different metrics are shown per data path. The data paths from the SMs to the memory spaces (Global, Local, Texture, Surface and Shared) report the total number of memory instructions executed, it includes both read and write operations. The data path between memory spaces and “Unified Cache” or “Shared Memory” reports the total amount of memory requests made. All other data paths report the total amount of transferred memory in bytes. The arrow pointing to right direction indicates WRITE operation whereas the arrow pointing to left direction indicates the READ operations. 2.4.2.5. NVLink view  NVIDIA NVLink is a high-bandwidth, energy-efficient interconnect that enables fast communication between the CPU and GPU, and between GPUs. Visual Profiler collects NVLink topology and NVLink transmit/receive throughput metrics and maps the metrics on to the topology. The topology is collected by default along with the timeline. Throughput/ utilization metrics are generated only when NVLink option is chosen. NVLink information is presented in the Results section of Examine GPU Usage in CUDA Application Analysis in Guided Analysis. NVLink Analysis shows topology that shows the logical NVLink connections between different devices. A logical link comprises of 1 to 4 physical NVLinks of same properties connected between two devices. Visual profiler lists the properties and achieved utilization for logical NVLinks in ‘Logical NVLink Properties’ table. It also lists the transmit and receive throughputs for logical NVLink in ‘Logical NVLink Throughput’ table. 2.4.3. Source-Disassembly View  The Source-Disassembly View is used to display the analysis results for a kernel at the source and assembly instruction level. To be able to view the kernel source you need to compile the code using the -lineinfo option. If this compiler option is not used, only the disassembly view will be shown. This view is displayed for the following types of analysis: Global Memory Access Pattern Analysis Shared Memory Access Pattern Analysis Divergent Execution Analysis Kernel Profile - Instruction Execution Analysis Kernel Profile - PC Sampling Analysis As part of the Guided Analysis or Unguided Analysis for a kernel the analysis results are displayed under the Analysis view. After clicking on the source file or device function the Source-Disassembly view is opened. If the source file is not found a dialog is opened to select and point to the new location of the source file. This can happen for example when the profiling is done on a different system. The Source-Disassembly view contains: High level source Assembly instructions Hotspots at the source level Hotspots at the assembly instruction level Columns for profiling data aggregated to the source level Columns for profiling data collected at the assembly instruction level The information shown in the Source-Disassembly view can be customized by the following toolbar options: View menu - Select one or more out of the available profiler data columns to display. This is chosen by default based on the analysis type. Hot Spot menu - Select which profiler data to use for hot spots. This is chosen by default based on the analysis type. Show the source and disassembly views side by side. Show the source and disassembly views top to bottom. Maximize the source view Maximize the disassembly view Hotspots are colored based on level of importance - low, medium or high. Hovering the mouse over the hotspot displays the value of the profiler data, the level of importance and the source or disassembly line. You can click on a hotspot at the source level or assembly instruction level to view the source or disassembly line corresponding to the hotspot. In the disassembly view the assembly instructions corresponding to the selected source line are highlighted. You can click on the up and down arrow buttons displayed at the right of the disassembly column header to navigate to the next or previous instruction block. 2.4.4. GPU Details View  The GPU Details View displays a table of information for each memory copy and kernel execution in the profiled application. The following figure shows the table containing several memcpy and kernel executions. Each row of the table contains general information for a kernel execution or memory copy. For kernels, the table will also contain a column for each metric or event value collected for that kernel. In the figure, the Achieved Occupancy column shows the value of that metric for each of the kernel executions. You can sort the data by column by left clicking on the column header, and you can rearrange the columns by left clicking on a column header and dragging it to its new location. If you select a row in the table, the corresponding interval will be selected in the Timeline View . Similarly, if you select a kernel or memcpy interval in the Timeline View the table will be scrolled to show the corresponding data. If you hover the mouse over a column header, a tooltip will display the data shown in that column. For a column containing event or metric data, the tooltip will describe the corresponding event or metric. The Metrics Reference section contains more detailed information about each metric. The information shown in the GPU Details View can be filtered in various ways using the menu accessible from the Details View toolbar. The following modes are available: Filter By Selection - If selected, the GPU Details View shows data only for the selected kernel and memcpy intervals. Show Hidden Timeline Data - If not selected, data is shown only for kernels and memcpys that are visible in the timeline. Kernels and memcpys that are not visible because they are inside collapsed parts of the timeline are not shown. Show Filtered Timeline Data - If not selected, data is shown only for kernels and memcpys that are in timeline rows that are not filtered. Collecting Events and Metrics Specific event and metric values can be collected for each kernel and displayed in the details table. Use the toolbar icon in the upper right corner of the view to configure the events and metrics to collect for each device, and to run the application to collect those events and metrics. Show Summary Data By default the table shows one row for each memcpy and kernel invocation. Alternatively, the table can show summary results for each kernel function. Use the toolbar icon in the upper right corner of the view to select or deselect summary format. Formatting Table Contents The numbers in the table can be displayed either with or without grouping separators. Use the toolbar icon in the upper right corner of the view to select or deselect grouping separators. Exporting Details The contents of the table can be exported in CSV format using the toolbar icon in the upper right corner of the view. 2.4.5. CPU Details View  CPU Details view This view details the amount of time your application spends executing functions on the CPU. Each thread is sampled periodically to capture its callstack and the summary of these measurements are displayed in this view. You can manipulate the view by selecting different orientations for organizing the callstack: Top-down, Bottom-up, Code Structure (3), choosing which thread to view (1), and by sorting or highlighting a specific thread (7, 8). All the threads profiled are shown in one view when the ‘all threads’ option is selected (default). You can use this drop-down menu to instead select an individual thread. This column displays a tree of events representing the structure of the application’s execution on the CPU. Each of the remaining columns show the measurements collected for this event. The events shown here are determined by which tree orientation mode is selected (3). The tree is organized to show the calling hierarchy among functions. The following modes are available: Top-down (callers first) call tree view - The CPU details tree is organized as a call tree with each function shown as a child of its caller. In this mode you can see the callstack starting at the ‘main’ function. Bottom-up (callees first) call tree view - The CPU details tree is organized in such a way that each function is shown as a child of any functions it calls. In this mode you can quickly identify the call path that contributes the most time to the application’s execution. Code structure (file and line) tree view - The CPU details tree shows which functions belong to each source file and library as well as how much of the application’s execution is attributed to a given line of source code. In every mode the time listed for each function is ‘inclusive’ and includes time spent both in this function and any functions that it calls. For the code structure view the region of code is inclusive (i.e. the file entry lists the time spent in every function contained within a file). This column displays the total amount of time spent by all threads in this event as a percentage of the total amount of time spent in all events. This column displays a bar denoting a range where the amount of time spent in an event by any thread is always within this this range. On the left the minimum value is written, and on the right the maximum value is written. Also, if there is space, a small ‘diamond’ is drawn in the middle of the bar where the mean time is spent in this event across all threads. These columns display a distinct chart for each event. On the left is a vertical scale showing the same minimum and maximum values as shown on the range chart. The following columns each show the amount of time spent in this event by thread. If the cell for the given event / thread combination is greyed out then no time was spent by this thread in this event (for this example both threads 1 and 2 spent no time in the event ‘x_solve’). Furthermore, the thread(s) with the minimum or maximum amount of time spent in the event across all threads are annotated with the ‘triangle / line’. In this example thread 3 spent the most and thread 6 the least amount of time in the event ‘x_solve’. To reorder the rows by the time spent on a given thread click on the thread column header. To highlight a given thread click on one of its bars in this chart. This change to the view is the result of sorting by thread 3 (7) and highlighting it (8). Having highlighted thread 3 we now see a vertical line on the range chart showing the amount of time this thread spent in this event compared to the range across all thread. This thread is also highlighted on each row. CPU Threads CPU Source Code You can open the CPU Source View for any function by double-clicking on it in the tree. To be displayed the source files must be on the local file system. By default the directory containing the executable or profile file is searched. If the source file cannot be found a prompt will appear asking for its location. Sometimes a file within a specific directory is being sought, in this case you should give the path to where this directory resides. Tip The CPU profile is gathered by periodically sampling the state of the running application. For this reason a function will only appear in this view if it was sampled during execution. Short-running or very infrequently called functions are less likely to be sampled. If a function was not sampled the time it was running is accounted to the function that called it. In order to gather a CPU profile that is representative of the application’s performance the code of interest must execute for enough to gather enough samples. Usually a minute of runtime is sufficient. Tip The file and line information is gathered from the application’s debug information obtained by the compiler. To ensure that this information is available it is recommended that you compile with ‘-g’ or a similar option. 2.4.6. OpenACC Details View  OpenACC table view The OpenACC Details View displays each OpenACC runtime activity executed by the profiled application. Each activity is grouped by source location: each activity which occurs at the same file and line number in the application’s source code is placed under a node labeled with the source location. Each activity shows the amount of time spent by the profiled application as both a unit of time and as a percentage of the total time this application was executing any OpenACC activity. Also the number of times this activity was called is shown. There are two ways to count how much time is spent in a particular OpenACC activity: Show the Inclusive durations (counting any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the total time spent in each activity including any activities that were executed as the result of this activity. In this case the amount of time spent in each activity occurring at a given application source location is totaled and displayed on the row displaying the source location. Show the Exclusive durations (excluding any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the time spent only in a given activity. In this case the amount of time spent at a given source location is always zero—time is attributed solely to each activity occurring at this source location. 2.4.7. OpenMP Details View  OpenMP table view The OpenMP Details view displays the activity of the OpenMP runtime on the CPU. The time your application spends in a parallel region or idling is shown both on the timeline and is summarized in this view. The reference for the percentage of time spent in each type of activity is the time from the start of the first parallel region to the end of the last parallel region. The sum of the percentages of each activity type often exceeds 100% because the OpenMP runtime can be in multiple states at the same time. 2.4.8. Properties View  The Properties View shows information about the row or interval highlighted or selected in the Timeline View . If a row or interval is not selected, the displayed information tracks the motion of the mouse pointer. If a row or interval is selected, the displayed information is pinned to that row or interval. When an OpenACC interval with an associated source file is selected, this filename is shown in the Source File table entry. Double-clicking on the filename opens the respective source file if it is available on the file-system. 2.4.9. Console View  The Console View shows stdout and stderr output of the application each time it executes. If you need to provide stdin input to your application, do so by typing into the console view. 2.4.10. Settings View  The Settings View allows you to specify execution settings for the application being profiled. As shown in the following figure, the Executable settings tab allows you to specify the executable file, the working directory, the command-line arguments, and the environment for the application. Only the executable file is required, all other fields are optional. Exection Timeout The Executable settings tab also allows you to specify an optional execution timeout. If the execution timeout is specified, the application execution will be terminated after that number of seconds. If the execution timeout is not specified, the application will be allowed to continue execution until it terminates normally. Note The timer starts counting from the moment the CUDA driver is initialized. If the application doesn’t call any CUDA APIs, a timeout won’t be triggered. Start execution with profiling enabled The Start execution with profiling enabled checkbox is set by default to indicate that application profiling begins at the start of application execution. If you are using cudaProfilerStart() and cudaProfilerStop() to control profiling within your application as described in Focused Profiling , then you should uncheck this box. Enable concurrent kernel profiling The Enable concurrent kernel profiling checkbox is set by default to enable profiling of applications that exploit concurrent kernel execution. If this checkbox is unset, the profiler will disable concurrent kernel execution. Disabling concurrent kernel execution can reduce profiling overhead in some cases and so may be appropriate for applications that do not exploit concurrent kernels. Enable power, clock, and thermal profiling The Enable power, clock, and thermal profiling checkbox can be set to enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application. 2.4.11. CPU Source View  The CPU source code view allows you to inspect the files that comprise the profiled application’s CPU source. This view can be opened in the CPU Details View by double-clicking on a function in the tree–the source file that corresponds to this function is then opened. Line numbers can be enabled by right-clicking left side ruler. When compiling using the PGI® compilers annotations can be added to this view (see Common Compiler Feedback Format for more information). These annotation are notes about how a given line of code is compiled. PGI compilers save information about how your program was optimized, or why a particular optimization was not made. This can be combined with the CPU Details View to help identify why certain lines of code performed the way they did. For example, the message may tell you about the following: vector instructions generated by the compiler. compute-intensity of a loop, a ratio computation to memory operations–higher numbers mean that there is more computation than memory loads and stores. information about parallelization, with a hint for how it might be possible to make the loop run in parallel if the compiler could not auto-parallelize it. 2.5. Customizing the Profiler  When you first start the Visual Profiler , and after closing the Welcome page, you will be presented with a default placement of the views. By moving and resizing the views, you can customize the profiler to meet your development needs. Any changes you make are restored the next time you start the profiler. 2.5.1. Resizing a View  To resize a view, simply left click and drag on the dividing area between the views. All views stacked together in one area are resized at the same time. 2.5.2. Reordering a View  To reorder a view in a stacked set of views, left click and drag the view tab to the new location within the view stack. 2.5.3. Moving a View  to move a view, left click the view tab and drag it to its new location. As you drag the view, an outline will show the target location for the view. You can place the view in a new location, or stack it in the same location as other views. 2.5.4. Undocking a View  You can undock a view from the profiler window so that the view occupies its own stand-alone window. You may want to do this to take advantage of multiple monitors or to maximum the size of an individual view. To undock a view, left click the view tab and drag it outside of the profiler window. To dock a view, left click the view tab (not the window decoration) and drag it into the profiler window. 2.5.5. Opening and Closing a View  Use the X icon on a view tab to close a view. To open a view, use the View menu. 2.6. Command Line Arguments  When the Visual Profiler is started from the command line, it is possible, using command line arguments, to specify executable to start new session with or import profile files exported from nvprof using one of the following patterns: Start new executable session by launching nvvp with name of executable followed, optionally, by its arguments: nvvp executableName [[ executableArguments ]...] Import single-process nvprof session by launching nvvp with single .nvprof file as argument(see nvprof’s export/import options section for more details): nvvp data . nvprof Import multi-process nvprof session, by launching nvvp with multiple .nvprof files as arguments: nvvp data1 . nvprof data2 . nvprof ... 3. ​nvprof  The nvprof profiling tool enables you to collect and view profiling data from the command-line. nvprof enables the collection of a timeline of CUDA-related activities on both CPU and GPU, including kernel execution, memory transfers, memory set and CUDA API calls and events or metrics for CUDA kernels. Profiling options are provided to nvprof through command-line options. Profiling results are displayed in the console after the profiling data is collected, and may also be saved for laterviewing by either nvprof or the Visual Profiler. Note The textual output of the profiler is redirected to stderr by default. Use --log-file to redirect the output to another file. See Redirecting Output . To profile an application from the command-line: nvprof [ options ] [ application ] [ application - arguments ] To view the full help page, type nvprof --help . 3.1. Command Line Options  3.1.1. CUDA Profiling Options  Option Values Default Description aggregate-mode on, off on Turn on/off aggregate mode for events and metrics specified by subsequent --events and --metrics options. Those event/metric values will be collected for each domain instance, instead of the whole device. See Event/metric Trace Mode for more information. analysis-metrics N/A N/A Collect profiling data that can be imported to Visual Profiler’s “analysis” mode. Note: Use --export-profile to specify an export file. annotate-mpi off, openmpi, mpich off Automatically annotate MPI calls with NVTX markers. Specify the MPI implementation installed on your machine. Currently, Open MPI and MPICH implementations are supported. See Automatic MPI Annotation with NVTX for more information. concurrent-kernels on, off on Turn on/off concurrent kernel execution. If concurrent kernel execution is off, all kernels running on one device will be serialized. continuous-sampling-interval {interval in milliseconds} 2 milliseconds Set the continuous mode sampling interval in milliseconds. Minimum is 1 ms. cpu-thread-tracing on, off off Collect information about CPU thread API activity. See CPU Thread Tracing for more information. dependency-analysis N/A N/A Generate event dependency graph for host and device activities and run dependency analysis. See Dependency Analysis for more information. device-buffer-size {size in MBs} 8 MB Set the device memory size (in MBs) reserved for storing profiling data for non-CDP operations, especially for concurrent kernel tracing, for each buffer on a context. The size should be a positive integer. device-cdp-buffer-size {size in MBs} 8 MB Set the device memory size (in MBs) reserved for storing profiling data for CDP operations for each buffer on a context. The size should be a positive integer. devices {comma-separated device IDs}, all N/A Change the scope of subsequent --events , --metrics , --query-events and --query-metrics options. See Profiling Scope for more information. event-collection-mode kernel, continuous kernel Choose event collection mode for all events/metrics. kernel: Events/metrics are collected only for durations of kernel executions continuous: Events/metrics are collected for duration of application. This is not applicable for non-Tesla devices. This mode is compatible only with NVLink events/metrics. This mode is incompatible with --profile-all-processes or --profile-child-processes or --replay-mode kernel or --replay-mode application . events (e) {comma-separated event names}, all N/A Specify the events to be profiled on certain device(s). Multiple event names separated by comma can be specified. Which device(s) are profiled is controlled by the --devices option. Otherwise events will be collected on all devices. For a list of available events, use --query-events . Use --events all to profile all events available for each device. Use --devices and --kernels to select a specific kernel invocation. kernel-latency-timestamps on, off off Turn on/off collection of kernel latency timestamps, namely queued and submitted. The queued timestamp is captured when a kernel launch command was queued into the CPU command buffer. The submitted timestamp denotes when the CPU command buffer containing this kernel launch was submitted to the GPU. Turning this option on may incur an overhead during profiling. kernels {kernel name}, {[context id/name]:[stream id/name]:[kernel name]:[invocation]} N/A Change the scope of subsequent --events , --metrics options. The syntax is as follows: {kernel name}: Limit scope to given kernel name. {[context id/name]:[stream id/name]:[kernel name]:[invocation]}: The context/stream IDs, names, kernel name and invocation can be regular expressions. Empty string matches any number or characters. If [context id/name] or [stream id/name] is a positive number, it’s strictly matched against the CUDA context/stream ID. Otherwise it’s treated as a regular expression and matched against the context/stream name specified by the NVTX library. If the invocation count is a positive number, it’s strictly matched against the invocation of the kernel. Otherwise it’s treated as a regular expression. Example: --kernels \"1:foo:bar:2\" will profile any kernel whose name contains “bar” and is the 2nd instance on context 1 and on stream named “foo”. See Profiling Scope for more information. metrics (m) {comma-separated metric names}, all N/A Specify the metrics to be profiled on certain device(s). Multiple metric names separated by comma can be specified. Which device(s) are profiled is controlled by the --devices option. Otherwise metrics will be collected on all devices. For a list of available metrics, use --query-metrics . Use --metrics all to profile all metrics available for each device. Use --devices and --kernels to select a specific kernel invocation. Note: --metrics all does not include some metrics which are needed for Visual Profiler’s source level analysis. For that, use --analysis-metrics . pc-sampling-period {period in cycles} Between 5 and 12 based on the setup Specify PC Sampling period in cycles, at which the sampling records will be dumped. Allowed values for the period are integers between 5 to 31 both inclusive. This will set the sampling period to (2^period) cycles Note: Only available for GM20X+. profile-all-processes N/A N/A Profile all processes launched by the same user who launched this nvprof instance. Note: Only one instance of nvprof can run with this option at the same time. Under this mode, there’s no need to specify an application to run. See Multiprocess Profiling for more information. profile-api-trace none, runtime, driver, all all Turn on/off CUDA runtime/driver API tracing. none: turn off API tracing runtime: only turn on CUDA runtime API tracing driver: only turn on CUDA driver API tracing all: turn on all API tracing profile-child-processes N/A N/A Profile the application and all child processes launched by it. See Multiprocess Profiling for more information. profile-from-start on, off on Enable/disable profiling from the start of the application. If it’s disabled, the application can use {cu,cuda}Profiler{Start,Stop} to turn on/off profiling. See Focused Profiling for more information. profiling-semaphore-pool-size {count} 65536 Set the profiling semaphore pool size reserved for storing profiling data for serialized kernels and memory operations for each context. The size should be a positive integer. query-events N/A N/A List all the events available on the device(s). Device(s) queried can be controlled by the --devices option. query-metrics N/A N/A List all the metrics available on the device(s). Device(s) queried can be controlled by the --devices option. replay-mode disabled, kernel, application kernel Choose replay mode used when not all events/metrics can be collected in a single run. disabled: replay is disabled, events/metrics couldn’t be profiled will be dropped kernel: each kernel invocation is replayed application: the entire application is replayed. This mode is incompatible with --profile-all-processes or profile-child-processes . skip-kernel-replay-save-restore on, off off If enabled, this option can vastly improve kernel replay speed, as save and restore of the mutable state for each kernel pass will be skipped. Skipping of save/restore of input/output buffers allows you to specify that all profiled kernels on the context do not change the contents of their input buffers during execution, or call device malloc/free or new/delete, that leave the device heap in a different state. Specifically, a kernel can malloc and free a buffer in the same launch, but it cannot call an unmatched malloc or an unmatched free. Note: incorrectly using this mode while one of the kernels does modify the input buffer or uses unmatched malloc/free will result in undefined behavior, including kernel execution failure and/or corrupted device data. on: skip save/restore of the input/output buffers off: save/restore input/output buffers for each kernel replay pass source-level-analysis (a) global_access, shared_access, branch, instruction_execution, pc_sampling N/A Specify the source level metrics to be profiled on a certain kernel invocation. Use --devices and --kernels to select a specific kernel invocation. One or more of these may be specified, separated by commas global_access: global access shared_access: shared access branch: divergent branch instruction_execution: instruction execution pc_sampling: pc sampling, available only for GM20X+ Note: Use --export-profile to specify an export file. See Source-Disassembly View for more information. system-profiling on, off off Turn on/off power, clock, and thermal profiling. See System Profiling for more information. timeout (t) {seconds} N/A Set an execution timeout (in seconds) for the CUDA application. Note: Timeout starts counting from the moment the CUDA driver is initialized. If the application doesn’t call any CUDA APIs, timeout won’t be triggered. See Timeout and Flush Profile Data for more information. track-memory-allocations on, off off Turn on/off tracking of memory operations, which involves recording timestamps, memory size, memory type and program counters of the memory allocations and frees. Turning this option on may incur an overhead during profiling. unified-memory-profiling per-process-device, off per-process-device Configure unified memory profiling. per-process-device: collect counts for each process and each device off: turn off unified memory profiling See Unified Memory Profiling for more information. 3.1.2. CPU Profiling Options  Option Values Default Description cpu-profiling on, off off Turn on CPU profiling. Note: CPU profiling is not supported in multi-process mode. cpu-profiling-explain-ccff {filename} N/A Set the path to a PGI pgexplain.xml file that should be used to interpret Common Compiler Feedback Format (CCFF) messages. cpu-profiling-frequency {frequency} 100Hz Set the CPU profiling frequency in samples per second. Maximum is 500Hz. cpu-profiling-max-depth {depth} 0 (i.e. unlimited) Set the maximum depth of each call stack. cpu-profiling-mode flat, top-down, bottom-up bottom-up Set the output mode of CPU profiling. flat: Show flat profile top-down: Show parent functions at the top bottom-up: Show parent functions at the bottom cpu-profiling-percentage-threshold {threshold} 0 (i.e. unlimited) Filter out the entries that are below the set percentage threshold. The limit should be an integer between 0 and 100, inclusive. cpu-profiling-scope function, instruction function Choose the profiling scope. function: Each level in the stack trace represents a distinct function instruction: Each level in the stack trace represents a distinct instruction address cpu-profiling-show-ccff on, off off Choose whether to print Common Compiler Feedback Format (CCFF) messages embedded in the binary. Note: this option implies --cpu-profiling-scope instruction . cpu-profiling-show-library on, off off Choose whether to print the library name for each sample. cpu-profiling-thread-mode separated, aggregated aggregated Set the thread mode of CPU profiling. separated: Show separate profile for each thread aggregated: Aggregate data from all threads cpu-profiling-unwind-stack on, off on Choose whether to unwind the CPU call-stack at each sample point. openacc-profiling on, off on Enable/disable recording information from the OpenACC profiling interface. Note: if the OpenACC profiling interface is available depends on the OpenACC runtime. See OpenACC for more information. openmp-profiling on, off off Enable/disable recording information from the OpenMP profiling interface. Note: if the OpenMP profiling interface is available depends on the OpenMP runtime. See OpenMP for more information. 3.1.3. Print Options  Option Values Default Description context-name {name} N/A Name of the CUDA context. %i in the context name string is replaced with the ID of the context. %p in the context name string is replaced with the process ID of the application being profiled. %q{<ENV>} in the context name string is replaced with the value of the environment variable <ENV> . If the environment variable is not set it’s an error. %h in the context name string is replaced with the hostname of the system. %% in the context name string is replaced with % . Any other character following % is illegal. csv N/A N/A Use comma-separated values in the output. See CSV for more information. demangling on, off on Turn on/off C++ name demangling of function names. See Demangling for more information. normalized-time-unit (u) s, ms, us, ns, col, auto auto Specify the unit of time that will be used in the output. s: second ms: millisecond us: microsecond ns: nanosecond col: a fixed unit for each column auto: the scale is chosen for each value based on its length. See Adjust Units for more information. openacc-summary-mode exclusive, inclusive exclusive Set how durations are computed in the OpenACC summary. See OpenACC Summary Modes for more information. trace api, gpu N/A Specify the option (or options separated by commas) to be traced. api - only turn on CUDA runtime and driver API tracing gpu - only turn on CUDA GPU tracing print-api-summary N/A N/A Print a summary of CUDA runtime/driver API calls. print-api-trace N/A N/A Print CUDA runtime/driver API trace. See GPU-Trace and API-Trace Modes for more information. print-dependency-analysis-trace N/A N/A Print dependency analysis trace. See Dependency Analysis for more information. print-gpu-summary N/A N/A Print a summary of the activities on the GPU (including CUDA kernels and memcpy’s/memset’s). See Summary Mode for more information. print-gpu-trace N/A N/A Print individual kernel invocations (including CUDA memcpy’s/memset’s) and sort them in chronological order. In event/metric profiling mode, show events/metrics for each kernel invocation. See GPU-Trace and API-Trace Modes for more information. print-openacc-constructs N/A N/A Include parent construct names in OpenACC profile. See OpenACC Options for more information. print-openacc-summary N/A N/A Print a summary of the OpenACC profile. print-openacc-trace N/A N/A Print a trace of the OpenACC profile. print-openmp-summary N/A N/A Print a summary of the OpenMP profile. print-summary (s) N/A N/A Print a summary of the profiling result on screen. Note: This is the default unless --export-profile or other print options are used. print-summary-per-gpu N/A N/A Print a summary of the profiling result for each GPU. process-name {name} N/A Name of the process. %p in the process name string is replaced with the process ID of the application being profiled. %q{<ENV>} in the process name string is replaced with the value of the environment variable <ENV> . If the environment variable is not set it’s an error. %h in the process name string is replaced with the hostname of the system. %% in the process name string is replaced with % . Any other character following % is illegal. quiet N/A N/A Suppress all nvprof output. stream-name {name} N/A Name of the CUDA stream. %i in the stream name string is replaced with the ID of the stream. %p in the stream name string is replaced with the process ID of the application being profiled. %q{<ENV>} in the stream name string is replaced with the value of the environment variable <ENV> . If the environment variable is not set it’s an error. %h in the stream name string is replaced with the hostname of the system. %% in the stream name string is replaced with % . Any other character following % is illegal. 3.1.4. IO Options  Option Values Default Description export-profile (o) {filename} N/A Export the result file which can be imported later or opened by the NVIDIA Visual Profiler. %p in the file name string is replaced with the process ID of the application being profiled. %q{<ENV>} in the file name string is replaced with the value of the environment variable <ENV> . If the environment variable is not set it’s an error. %h in the file name string is replaced with the hostname of the system. %% in the file name string is replaced with % . Any other character following % is illegal. By default, this option disables the summary output. Note: If the application being profiled creates child processes, or if --profile-all-processes is used, the %p format is needed to get correct export files for each process. See Export/Import for more information. force-overwrite (f) N/A N/A Force overwriting all output files (any existing files will be overwritten). import-profile (i) {filename} N/A Import a result profile from a previous run. See Export/Import for more information. log-file {filename} N/A Make nvprof send all its output to the specified file, or one of the standard channels. The file will be overwritten. If the file doesn’t exist, a new one will be created. %1 as the whole file name indicates standard output channel (stdout). %2 as the whole file name indicates standard error channel (stderr). Note: This is the default. %p in the file name string is replaced with the process ID of the application being profiled. %q{<ENV>} in the file name string is replaced with the value of the environment variable <ENV> . If the environment variable is not set it’s an error. %h in the file name string is replaced with the hostname of the system. %% in the file name is replaced with % . Any other character following % is illegal. See Redirecting Output for more information. print-nvlink-topology N/A N/A Print nvlink topology print-pci-topology N/A N/A Print PCI topology help (h) N/A N/A Print help information. version (V) N/A N/A Print version information of this tool. 3.2. Profiling Modes  nvprof operates in one of the modes listed below. 3.2.1. Summary Mode  Summary mode is the default operating mode for nvprof . In this mode, nvprof outputs a single result line for each kernel function and each type of CUDA memory copy/set performed by the application. For each kernel, nvprof outputs the total time of all instances of the kernel or type of memory copy as well as the average, minimum, and maximum time. The time for a kernel is the kernel execution time on the device. By default, nvprof also prints a summary of all the CUDA runtime/driver API calls. Output of nvprof (except for tables) are prefixed with ==<pid>== , <pid> being the process ID of the application being profiled. Here’s a simple example of running nvprof on the CUDA sample matrixMul : $ nvprof matrixMul [ Matrix Multiply Using CUDA ] - Starting ... == 27694 == NVPROF is profiling process 27694 , command : matrixMul GPU Device 0 : \"GeForce GT 640M LE\" with compute capability 3.0 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ... done Performance = 35.35 GFlop / s , Time = 3.708 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example . == 27694 == Profiling application : matrixMul == 27694 == Profiling result : Time ( % ) Time Calls Avg Min Max Name 99.94 % 1.11524 s 301 3.7051 ms 3.6928 ms 3.7174 ms void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) 0.04 % 406.30 us 2 203.15 us 136.13 us 270.18 us [ CUDA memcpy HtoD ] 0.02 % 248.29 us 1 248.29 us 248.29 us 248.29 us [ CUDA memcpy DtoH ] == 27964 == API calls : Time ( % ) Time Calls Avg Min Max Name 49.81 % 285.17 ms 3 95.055 ms 153.32 us 284.86 ms cudaMalloc 25.95 % 148.57 ms 1 148.57 ms 148.57 ms 148.57 ms cudaEventSynchronize 22.23 % 127.28 ms 1 127.28 ms 127.28 ms 127.28 ms cudaDeviceReset 1.33 % 7.6314 ms 301 25.353 us 23.551 us 143.98 us cudaLaunch 0.25 % 1.4343 ms 3 478.09 us 155.84 us 984.38 us cudaMemcpy 0.11 % 601.45 us 1 601.45 us 601.45 us 601.45 us cudaDeviceSynchronize 0.10 % 564.48 us 1505 375 ns 313 ns 3.6790 us cudaSetupArgument 0.09 % 490.44 us 76 6.4530 us 307 ns 221.93 us cuDeviceGetAttribute 0.07 % 406.61 us 3 135.54 us 115.07 us 169.99 us cudaFree 0.02 % 143.00 us 301 475 ns 431 ns 2.4370 us cudaConfigureCall 0.01 % 42.321 us 1 42.321 us 42.321 us 42.321 us cuDeviceTotalMem 0.01 % 33.655 us 1 33.655 us 33.655 us 33.655 us cudaGetDeviceProperties 0.01 % 31.900 us 1 31.900 us 31.900 us 31.900 us cuDeviceGetName 0.00 % 21.874 us 2 10.937 us 8.5850 us 13.289 us cudaEventRecord 0.00 % 16.513 us 2 8.2560 us 2.6240 us 13.889 us cudaEventCreate 0.00 % 13.091 us 1 13.091 us 13.091 us 13.091 us cudaEventElapsedTime 0.00 % 8.1410 us 1 8.1410 us 8.1410 us 8.1410 us cudaGetDevice 0.00 % 2.6290 us 2 1.3140 us 509 ns 2.1200 us cuDeviceGetCount 0.00 % 1.9970 us 2 998 ns 520 ns 1.4770 us cuDeviceGet Note API trace can be turned off, if not needed, by using --profile-api-trace none . This reduces some of the profiling overhead, especially when the kernels are short. If multiple CUDA capable devices are profiled, nvprof --print-summary-per-gpu can be used to print one summary per GPU. nvprof supports CUDA Dynamic Parallelism in summary mode. If your application uses Dynamic Parallelism, the output will contain one column for the number of host-launched kernels and one for the number of device-launched kernels. Here’s an example of running nvprof on the CUDA Dynamic Parallelism sample cdpSimpleQuicksort : $ nvprof cdpSimpleQuicksort == 27325 == NVPROF is profiling process 27325 , command : cdpSimpleQuicksort Running on GPU 0 ( Tesla K20c ) Initializing data : Running quicksort on 128 elements Launching kernel on the GPU Validating results : OK == 27325 == Profiling application : cdpSimpleQuicksort == 27325 == Profiling result : Time ( % ) Time Calls ( host ) Calls ( device ) Avg Min Max Name 99.71 % 1.2114 ms 1 14 80.761 us 5.1200 us 145.66 us cdp_simple_quicksort ( unsigned int * , int , int , int ) 0.18 % 2.2080 us 1 - 2.2080 us 2.2080 us 2.2080 us [ CUDA memcpy DtoH ] 0.11 % 1.2800 us 1 - 1.2800 us 1.2800 us 1.2800 us [ CUDA memcpy HtoD ] 3.2.2. GPU-Trace and API-Trace Modes  GPU-Trace and API-Trace modes can be enabled individually or together. GPU-Trace mode provides a timeline of all activities taking place on the GPU in chronological order. Each kernel execution and memory copy/set instance is shown in the output. For each kernel or memory copy, detailed information such as kernel parameters, shared memory usage and memory transfer throughput are shown. The number shown in the square brackets after the kernel name correlates to the CUDA API that launched that kernel. Here’s an example: $ nvprof -- print - gpu - trace matrixMul == 27706 == NVPROF is profiling process 27706 , command : matrixMul == 27706 == Profiling application : matrixMul [ Matrix Multiply Using CUDA ] - Starting ... GPU Device 0 : \"GeForce GT 640M LE\" with compute capability 3.0 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ... done Performance = 35.36 GFlop / s , Time = 3.707 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example . == 27706 == Profiling result : Start Duration Grid Size Block Size Regs * SSMem * DSMem * Size Throughput Device Context Stream Name 133.81 ms 135.78 us - - - - - 409.60 KB 3.0167 GB / s GeForce GT 640 M 1 2 [ CUDA memcpy HtoD ] 134.62 ms 270.66 us - - - - - 819.20 KB 3.0267 GB / s GeForce GT 640 M 1 2 [ CUDA memcpy HtoD ] 134.90 ms 3.7037 ms ( 20 10 1 ) ( 32 32 1 ) 29 8.1920 KB 0 B - - GeForce GT 640 M 1 2 void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) [ 94 ] 138.71 ms 3.7011 ms ( 20 10 1 ) ( 32 32 1 ) 29 8.1920 KB 0 B - - GeForce GT 640 M 1 2 void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) [ 105 ] < ... more output ... > 1.24341 s 3.7011 ms ( 20 10 1 ) ( 32 32 1 ) 29 8.1920 KB 0 B - - GeForce GT 640 M 1 2 void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) [ 2191 ] 1.24711 s 3.7046 ms ( 20 10 1 ) ( 32 32 1 ) 29 8.1920 KB 0 B - - GeForce GT 640 M 1 2 void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) [ 2198 ] 1.25089 s 248.13 us - - - - - 819.20 KB 3.3015 GB / s GeForce GT 640 M 1 2 [ CUDA memcpy DtoH ] Regs : Number of registers used per CUDA thread . This number includes registers used internally by the CUDA driver and / or tools and can be more than what the compiler shows . SSMem : Static shared memory allocated per CUDA block . DSMem : Dynamic shared memory allocated per CUDA block . nvprof supports CUDA Dynamic Parallelism in GPU-Trace mode. For host kernel launch, the kernel ID will be shown. For device kernel launch, the kernel ID, parent kernel ID and parent block will be shown. Here’s an example: $nvprof -- print - gpu - trace cdpSimpleQuicksort == 28128 == NVPROF is profiling process 28128 , command : cdpSimpleQuicksort Running on GPU 0 ( Tesla K20c ) Initializing data : Running quicksort on 128 elements Launching kernel on the GPU Validating results : OK == 28128 == Profiling application : cdpSimpleQuicksort == 28128 == Profiling result : Start Duration Grid Size Block Size Regs * SSMem * DSMem * Size Throughput Device Context Stream ID Parent ID Parent Block Name 192.76 ms 1.2800 us - - - - - 512 B 400.00 MB / s Tesla K20c ( 0 ) 1 2 - - - [ CUDA memcpy HtoD ] 193.31 ms 146.02 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 0 B - - Tesla K20c ( 0 ) 1 2 2 - - cdp_simple_quicksort ( unsigned int * , int , int , int ) [ 171 ] 193.41 ms 110.53 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -5 2 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.45 ms 125.57 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -6 2 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.48 ms 9.2480 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -7 -5 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.52 ms 107.23 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -8 -5 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.53 ms 93.824 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -9 -6 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.57 ms 117.47 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -10 -6 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.58 ms 5.0560 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -11 -8 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.62 ms 108.06 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -12 -8 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.65 ms 113.34 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -13 -10 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.68 ms 29.536 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -14 -12 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.69 ms 22.848 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -15 -10 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.71 ms 130.85 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -16 -13 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.73 ms 62.432 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -17 -12 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.76 ms 41.024 us ( 1 1 1 ) ( 1 1 1 ) 32 0 B 256 B - - Tesla K20c ( 0 ) 1 2 -18 -13 ( 0 0 0 ) cdp_simple_quicksort ( unsigned int * , int , int , int ) 193.92 ms 2.1760 us - - - - - 512 B 235.29 MB / s Tesla K20c ( 0 ) 1 2 - - - [ CUDA memcpy DtoH ] Regs : Number of registers used per CUDA thread . This number includes registers used internally by the CUDA driver and / or tools and can be more than what the compiler shows . SSMem : Static shared memory allocated per CUDA block . DSMem : Dynamic shared memory allocated per CUDA block . API-trace mode shows the timeline of all CUDA runtime and driver API calls invoked on the host in chronological order. Here’s an example: $nvprof -- print - api - trace matrixMul == 27722 == NVPROF is profiling process 27722 , command : matrixMul == 27722 == Profiling application : matrixMul [ Matrix Multiply Using CUDA ] - Starting ... GPU Device 0 : \"GeForce GT 640M LE\" with compute capability 3.0 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ... done Performance = 35.35 GFlop / s , Time = 3.708 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example . == 27722 == Profiling result : Start Duration Name 108.38 ms 6.2130 us cuDeviceGetCount 108.42 ms 840 ns cuDeviceGet 108.42 ms 22.459 us cuDeviceGetName 108.45 ms 11.782 us cuDeviceTotalMem 108.46 ms 945 ns cuDeviceGetAttribute 149.37 ms 23.737 us cudaLaunch ( void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) [ 2198 ]) 149.39 ms 6.6290 us cudaEventRecord 149.40 ms 1.10156 s cudaEventSynchronize < ... more output ... > 1.25096 s 21.543 us cudaEventElapsedTime 1.25103 s 1.5462 ms cudaMemcpy 1.25467 s 153.93 us cudaFree 1.25483 s 75.373 us cudaFree 1.25491 s 75.564 us cudaFree 1.25693 s 10.901 ms cudaDeviceReset Note Due to the way the profiler is setup, the first “cuInit()” driver API call is never traced. 3.2.3. Event/metric Summary Mode  To see a list of all available events on a particular NVIDIA GPU, use the --query-events option. To see a list of all available metrics on a particular NVIDIA GPU, use the --query-metrics option. nvprof is able to collect multiple events/metrics at the same time. Here’s an example: $ nvprof -- events warps_launched , local_load -- metrics ipc matrixMul [ Matrix Multiply Using CUDA ] - Starting ... == 6461 == NVPROF is profiling process 6461 , command : matrixMul GPU Device 0 : \"GeForce GTX TITAN\" with compute capability 3.5 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ... == 6461 == Warning : Some kernel ( s ) will be replayed on device 0 in order to collect all events / metrics . done Performance = 6.39 GFlop / s , Time = 20.511 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : Result = PASS NOTE : The CUDA Samples are not meant for performance measurements . Results may vary when GPU Boost is enabled . == 6461 == Profiling application : matrixMul == 6461 == Profiling result : == 6461 == Event result : Invocations Event Name Min Max Avg Device \"GeForce GTX TITAN (0)\" Kernel : void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) 301 warps_launched 6400 6400 6400 301 local_load 0 0 0 == 6461 == Metric result : Invocations Metric Name Metric Description Min Max Avg Device \"GeForce GTX TITAN (0)\" Kernel : void matrixMulCUDA < int = 32 > ( float * , float * , float * , int , int ) 301 ipc Executed IPC 1.282576 1.299736 1.291500 If the specified events/metrics can’t be profiled in a single run of the application, nvprof by default replays each kernel multiple times until all the events/metrics are collected. The --replay-mode <mode> option can be used to change the replay mode. In “application replay” mode, nvprof re-runs the whole application instead of replaying each kernel, in order to collect all events/metrics. In some cases this mode can be faster than kernel replay mode if the application allocates large amount of device memory. Replay can also be turned off entirely, in which case the profiler will not collect some events/metrics. To collect all events available on each device, use the option --events all . To collect all metrics available on each device, use the option --metrics all . Note Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU. Note If a large number of events or metrics are requested, no matter which replay mode is chosen, the overall application execution time may increase significantly. 3.2.4. Event/metric Trace Mode  In event/metric trace mode, event and metric values are shown for each kernel execution. By default, event and metric values are aggregated across all units in the GPU. For example, multiprocessor specific events are aggregated across all multiprocessors on the GPU. If --aggregate-mode off is specified, values of each unit are shown. For example, in the following example, the “branch” event value is shown for each multiprocessor on the GPU: $ nvprof -- aggregate - mode off -- events local_load -- print - gpu - trace matrixMul [ Matrix Multiply Using CUDA ] - Starting ... == 6740 == NVPROF is profiling process 6740 , command : matrixMul GPU Device 0 : \"GeForce GTX TITAN\" with compute capability 3.5 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ... done Performance = 16.76 GFlop / s , Time = 7.822 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : Result = PASS NOTE : The CUDA Samples are not meant for performance measurements . Results may vary when GPU Boost is enabled . == 6740 == Profiling application : matrixMul == 6740 == Profiling result : Device Context Stream Kernel local_load ( 0 ) local_load ( 1 ) ... GeForce GTX TIT 1 7 void matrixMulCUDA < i 0 0 ... GeForce GTX TIT 1 7 void matrixMulCUDA < i 0 0 ... < ... more output ... > Note Although --aggregate-mode applies to metrics, some metrics are only available in aggregate mode and some are only available in non-aggregate mode. 3.3. Profiling Controls  3.3.1. Timeout  A timeout (in seconds) can be provided to nvprof . The CUDA application being profiled will be killed by nvprof after the timeout. Profiling result collected before the timeout will be shown. Note Timeout starts counting from the moment the CUDA driver is initialized. If the application doesn’t call any CUDA APIs, timeout won’t be triggered. 3.3.2. Concurrent Kernels  Concurrent-kernel profiling is supported, and is turned on by default. To turn the feature off, use the option --concurrent-kernels off . This forces concurrent kernel executions to be serialized when a CUDA application is run with nvprof . 3.3.3. Profiling Scope  When collecting events/metrics, nvprof profiles all kernels launched on all visible CUDA devices by default. This profiling scope can be limited by the following options. --devices <device IDs> applies to --events , --metrics , --query-events and --query-metrics options that follows it . It limits these options to collect events/metrics only on the devices specified by <device IDs> , which can be a list of device ID numbers separated by comma. --kernels <kernel filter> applies to --events and --metrics options that follows it . It limits these options to collect events/metrics only on the kernels specified by <kernel filter> , which has the following syntax: <kernel name> or <context id/name>:<stream id/name>:<kernel\n        name>:<invocation> Each string in the angle brackets can be a standard Perl regular expression. Empty string matches any number or character combination. Invocation number n indicates the n th invocation of the kernel. If invocation is a positive number, it’s strictly matched against the invocation of the kernel. Otherwise it’s treated as a regular expression. Invocation number is counted separately for each kernel. So for instance :::3 will match the 3rd invocation of every kernel. If the context/stream string is a positive number, it’s strictly matched against the cuda context/stream ID. Otherwise it’s treated as a regular expression and matched against the context/stream name provided by the NVIDIA Tools Extension. Both --devices and --kernels can be specified multiple times, with distinct events/metrics associated. --events , --metrics , --query-events and --query-metrics are controlled by the nearest scope options before them. As an example, the following command, nvprof --devices 0 --metrics ipc\n        --kernels \"1:foo:bar:2\" --events local_load a.out collects metric ipc on all kernels launched on device 0. It also collects event local_load for any kernel whose name contains bar and is the 2nd instance launched on context 1 and on stream named foo on device 0. 3.3.4. Multiprocess Profiling  By default, nvprof only profiles the application specified by the command-line argument. It doesn’t trace child processes launched by that process. To profile all processes launched by an application, use the --profile-child-processes option. Note nvprof cannot profile processes that fork() but do not then exec() . nvprof also has a “profile all processes” mode, in which it profiles every CUDA process launched on the same system by the same user who launched nvprof . Exit this mode by typing “Ctrl-c”. Note CPU profiling is not supported in multi-process mode. 3.3.5. System Profiling  For devices that support system profiling, nvprof can enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application. This feature is turned off by default. To turn on this feature, use --system-profiling on . To see the detail of each sample point, combine the above option with --print-gpu-trace . 3.3.6. Unified Memory Profiling  For GPUs that support Unified Memory, nvprof collects the Unified Memory related memory traffic to and from each GPU on your system. This feature is enabled by default. This feature can be disabled with --unified-memory-profiling off . To see the detail of each memory transfer while this feature is enabled, use --print-gpu-trace . On multi-GPU configurations without P2P support between any pair of devices that support Unified Memory, managed memory allocations are placed in zero-copy memory. In this case Unified Memory profiling is not supported. In certain cases, the environment variable CUDA_MANAGED_FORCE_DEVICE_ALLOC can be set to force managed allocations to be in device memory and to enable migration on these hardware configurations. In this case Unified Memory profiling is supported. Normally, using the environment variable CUDA_VISIBLE_DEVICES is recommended to restrict CUDA to only use those GPUs that have P2P support. Please refer to the environment variables section in the CUDA C++ Programming Guide for further details. 3.3.7. CPU Thread Tracing  In order to allow a correct Dependency Analysis , nvprof can collect information about CPU-side threading APIs. This can be enabled by specifying --cpu-thread-tracing on during measurement. Recording this information is necessary if the application uses multiple CPU threads and at least two of these threads call the CUDA API. Currently, only POSIX threads (Pthreads) are supported. For performance reasons, only selected Pthread API calls may be recorded. nvprof tries to detect which calls are necessary to model the execution behavior and filters others. Filtered calls include pthread_mutex_lock and pthread_mutex_unlock when those do not cause any concurrent thread to block. Note CPU thread tracing is not available on Windows. Note CPU thread tracing starts after the first CUDA API call, from the thread issuing this call. Therefore, the application must call e.g. cuInit from its main thread before spawning any other user threads that call the CUDA API. 3.4. Output  3.4.1. Adjust Units  By default, nvprof adjusts the time units automatically to get the most precise time values. The --normalized-time-unit options can be used to get fixed time units throughout the results. 3.4.2. CSV  For each profiling mode, option --csv can be used to generate output in comma-separated values (CSV) format. The result can be directly imported to spreadsheet software such as Excel. 3.4.3. Export/Import  For each profiling mode, option --export-profile can be used to generate a result file. This file is not human-readable, but can be imported back to nvprof using the option --import-profile , or into the Visual Profiler. Note The profilers use SQLite as the format of the export profiles. Writing files in such format may require more disk operations than writing a plain file. Thus, exporting profiles to slower devices such as a network drive may slow down the execution of the application. 3.4.4. Demangling  By default, nvprof demangles C++ function names. Use option --demangling off to turn this feature off. 3.4.5. Redirecting Output  By default, nvprof sends most of its output to stderr . To redirect the output, use --log-file . --log-file %1 tells nvprof to redirect all output to stdout . --log-file <filename> redirects output to a file. Use %p in the filename to be replaced by the process ID of nvprof , %h by the hostname , %q{ENV} by the value of environment variable ENV , and %% by % . 3.4.6. Dependency Analysis  nvprof can run a Dependency Analysis after the application has been profiled, using the --dependency-analysis option. This analysis can also be applied to imported profiles. It requires to collect the full CUDA API and GPU activity trace during measurement. This is the default for nvprof if not disabled using --profile-api-trace none . For applications using CUDA from multiple CPU threads, CPU Thread Tracing should be enabled, too. The option --print-dependency-analysis-trace can be specified to change from a summary output to a trace output, showing computed metrics such as time on the critical path per function instance rather than per function type. An example for dependency analysis summary output with all computed metrics aggregated per function type is shown below. The table is sorted first by time on the critical path and second by waiting time. The summary contains an entry named Other , referring to all CPU activity that is not tracked by nvprof (e.g. the application’s main function). == 20704 == Dependency Analysis : == 20704 == Analysis progress : 100 % Critical path ( % ) Critical path Waiting time Name % s s 92.06 4.061817 0.000000 clock_block ( long * , long ) 4.54 0.200511 0.000000 cudaMalloc 3.25 0.143326 0.000000 cudaDeviceReset 0.13 5.7273280e-03 0.000000 < Other > 0.01 2.7200900e-04 0.000000 cudaFree 0.00 0.000000 4.062506 pthread_join 0.00 0.000000 4.061790 cudaStreamSynchronize 0.00 0.000000 1.015485 pthread_mutex_lock 0.00 0.000000 1.013711 pthread_cond_wait 0.00 0.000000 0.000000 pthread_mutex_unlock 0.00 0.000000 0.000000 pthread_exit 0.00 0.000000 0.000000 pthread_enter 0.00 0.000000 0.000000 pthread_create 0.00 0.000000 0.000000 pthread_cond_signal 0.00 0.000000 0.000000 cudaLaunch 3.5. CPU Sampling  Sometimes it’s useful to profile the CPU portion of your application, in order to better understand the bottlenecks and identify potential hotspots for the entire CUDA application. For the CPU portion of the application, nvprof is able to sample the program counter and call stacks at a certain frequency. The data is then used to construct a graph, with nodes being frames in each call stack. Function and library symbols are also extracted if available. A sample graph is shown below: ======== CPU profiling result ( bottom up ) : 45.45 % cuInit | 45.45 % cudart :: globalState :: loadDriverInternal ( void ) | 45.45 % cudart :: __loadDriverInternalUtil ( void ) | 45.45 % pthread_once | 45.45 % cudart :: cuosOnce ( int * , void ( * ) ( void )) | 45.45 % cudart :: globalState :: loadDriver ( void ) | 45.45 % cudart :: globalState :: initializeDriver ( void ) | 45.45 % cudaMalloc | 45.45 % main 33.33 % cuDevicePrimaryCtxRetain | 33.33 % cudart :: contextStateManager :: initPrimaryContext ( cudart :: device * ) | 33.33 % cudart :: contextStateManager :: tryInitPrimaryContext ( cudart :: device * ) | 33.33 % cudart :: contextStateManager :: initDriverContext ( void ) | 33.33 % cudart :: contextStateManager :: getRuntimeContextState ( cudart :: contextState ** , bool ) | 33.33 % cudart :: getLazyInitContextState ( cudart :: contextState ** ) | 33.33 % cudart :: doLazyInitContextState ( void ) | 33.33 % cudart :: cudaApiMalloc ( void ** , unsigned long ) | 33.33 % cudaMalloc | 33.33 % main 18.18 % cuDevicePrimaryCtxReset | 18.18 % cudart :: device :: resetPrimaryContext ( void ) | 18.18 % cudart :: cudaApiThreadExit ( void ) | 18.18 % cudaThreadExit | 18.18 % main 3.03 % cudbgGetAPIVersion 3.03 % start_thread 3.03 % clone The graph can be presented in different “views” ( top-down , bottom-up or flat ), allowing the user to analyze the sampling data from different perspectives. For instance, the bottom-up view (shown above) can be useful in identifying the “hot” functions in which the application is spending most of its time. The top-down view gives a break-down of the application execution time, starting from the main function, allowing you to find “call paths” which are executed frequently. By default the CPU sampling feature is disabled. To enable it, use the option --cpu-profiling on . The next section describes all the options controlling the CPU sampling behavior. CPU sampling is supported on Linux and Windows for Intel x86/x86_64 architecture. Note When using the CPU profiling feature on POSIX systems, the profiler samples the application by sending periodic signals. Applications should therefore ensure that system calls are handled appropriately when interrupted. Note On Windows, nvprof requires Visual Studio installation (2010 or later) and compiler-generated .PDB (program database) files to resolve symbol information. When building your application, ensure that .PDB files are created and placed next to the profiled executable and libraries. 3.5.1. CPU Sampling Limitations  The following are known issues with the current release. CPU sampling is not supported on the mobile devices. CPU sampling is currently not supported in multi-process profiling mode. The result stack traces might not be complete under some compiler optimizations, notably frame pointer omission and function inlining. The CPU sampling result does not support CSV mode. 3.6. OpenACC  On 64bit Linux platforms, nvprof supports recording OpenACC activities using the CUPTI Activity API. This allows to investigate the performance on the level of OpenACC constructs in addition to the underlying, compiler-generated CUDA API calls. OpenACC profiling in nvprof requires the targeted application to use PGI OpenACC runtime 19.1 or later. Even though recording OpenACC activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by nvprof . An example for OpenACC summary output is shown below. The CUPTI OpenACC activities are mapped to the original OpenACC constructs using their source file and line information. For acc_enqueue_launch activities, it will furthermore show the launched CUDA kernel name which is generated by the OpenACC compiler. By default, nvprof will demangle kernel names generated by the OpenACC compiler. You can pass --demangling off to disable this behavior. ==20854== NVPROF is profiling process 20854, command: ./acc_saxpy ==20854== Profiling application: ./acc_saxpy\n==20854== Profiling result:\n==20854== OpenACC (excl):\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 33.16%  1.27944s       200  6.3972ms  24.946us  12.770ms  acc_implicit_wait@acc_saxpy.cpp:42\n 33.12%  1.27825s       100  12.783ms  12.693ms  12.787ms  acc_wait@acc_saxpy.cpp:54\n 33.12%  1.27816s       100  12.782ms  12.720ms  12.786ms  acc_wait@acc_saxpy.cpp:61\n  0.14%  5.4550ms       100  54.549us  51.858us  71.461us  acc_enqueue_download@acc_saxpy.cpp:43\n  0.07%  2.5190ms       100  25.189us  23.877us  60.269us  acc_enqueue_launch@acc_saxpy.cpp:50 (kernel2(int, float, float*, float*)_50_gpu)\n  0.06%  2.4988ms       100  24.987us  24.161us  29.453us  acc_enqueue_launch@acc_saxpy.cpp:60 (kernel3(int, float, float*, float*)_60_gpu)\n  0.06%  2.2799ms       100  22.798us  21.654us  56.674us  acc_enqueue_launch@acc_saxpy.cpp:42 (kernel1(int, float, float*, float*)_42_gpu)\n  0.05%  2.1068ms       100  21.068us  20.444us  33.159us  acc_enqueue_download@acc_saxpy.cpp:51\n  0.05%  2.0854ms       100  20.853us  19.453us  23.697us  acc_enqueue_download@acc_saxpy.cpp:61\n  0.04%  1.6265ms       100  16.265us  15.284us  49.632us  acc_enqueue_upload@acc_saxpy.cpp:50\n  0.04%  1.5963ms       100  15.962us  15.052us  19.749us  acc_enqueue_upload@acc_saxpy.cpp:60\n  0.04%  1.5393ms       100  15.393us  14.592us  56.414us  acc_enqueue_upload@acc_saxpy.cpp:42\n  0.01%  558.54us       100  5.5850us  5.3700us  6.2090us  acc_implicit_wait@acc_saxpy.cpp:43\n  0.01%  266.13us       100  2.6610us  2.4630us  4.7590us  acc_compute_construct@acc_saxpy.cpp:42\n  0.01%  211.77us       100  2.1170us  1.9980us  4.1770us  acc_compute_construct@acc_saxpy.cpp:50\n  0.01%  209.14us       100  2.0910us  1.9880us  2.2500us  acc_compute_construct@acc_saxpy.cpp:60\n  0.00%  55.066us         1  55.066us  55.066us  55.066us  acc_enqueue_launch@acc_saxpy.cpp:70 (initVec(int, float, float*)_70_gpu)\n  0.00%  13.209us         1  13.209us  13.209us  13.209us  acc_compute_construct@acc_saxpy.cpp:70\n  0.00%  10.901us         1  10.901us  10.901us  10.901us  acc_implicit_wait@acc_saxpy.cpp:70\n  0.00%       0ns       200       0ns       0ns       0ns  acc_delete@acc_saxpy.cpp:61\n  0.00%       0ns       200       0ns       0ns       0ns  acc_delete@acc_saxpy.cpp:43\n  0.00%       0ns       200       0ns       0ns       0ns  acc_create@acc_saxpy.cpp:60\n  0.00%       0ns       200       0ns       0ns       0ns  acc_create@acc_saxpy.cpp:42\n  0.00%       0ns       200       0ns       0ns       0ns  acc_delete@acc_saxpy.cpp:51\n  0.00%       0ns       200       0ns       0ns       0ns  acc_create@acc_saxpy.cpp:50\n  0.00%       0ns         2       0ns       0ns       0ns  acc_alloc@acc_saxpy.cpp:42 3.6.1. OpenACC Options  Table 1 contains OpenACC profiling related command-line options of nvprof . Table 1. OpenACC Options  Option Description --openacc-profiling <on|off> Turn on/off OpenACC profiling. Note: OpenACC profiling is only supported on x86_64 Linux. Default is on. --print-openacc-summary Print a summary of all recorded OpenACC activities. --print-openacc-trace Print a detailed trace of all recorded OpenACC activities, including each activity’s timestamp and duration. --print-openacc-constructs Include the name of the OpenACC parent construct that caused an OpenACC activity to be emitted. Note that for applications using PGI OpenACC runtime before 19.1, this value will always be unknown . --openacc-summary-mode <exclusive|inclusive> Specify how activity durations are presented in the OpenACC summary. Allowed values: “exclusive” - exclusive durations (default). “inclusive” - inclusive durations. See OpenACC Summary Modes for more information. 3.6.2. OpenACC Summary Modes  nvprof supports two modes for presenting OpenACC activity durations in the OpenACC summary mode (enabled with --print-openacc-summary ): “exclusive” and “inclusive”. Inclusive: In this mode, all durations represent the actual runtime of an activity. This includes the time spent in this activity as well as in all its children (callees). Exclusive: In this mode, all durations represent the time spent solely in this activity. This includes the time spent in this activity but excludes the runtime of all of its children (callees). As an example, consider the OpenACC acc_compute_construct which itself calls acc_enqueue_launch to launch a kernel to the device and acc_implicit_wait , which waits on the completion of this kernel. In “inclusive” mode, the duration for acc_compute_construct will include the time spent in acc_enqueue_launch and acc_implicit_wait . In “exclusive” mode, those two durations are subtracted. In the summary profile, this is helpful to identify if a long acc_compute_construct represents a high launch overhead or rather a long wait (synchronization) time. 3.7. OpenMP  On 64bit Linux platforms, nvprof supports recording OpenMP activities OpenMP profiling in nvprof requires the targeted application to use a runtime supporting the OpenMP Tools interface (OMPT). (PGI version 19.1 or greater using the LLVM code generator supports OMPT). Even though recording OpenMP activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by nvprof . An example for the OpenMP summary output is shown below: == 20854 == NVPROF is profiling process 20854 , command : . / openmp == 20854 == Profiling application : . / openmp == 20854 == Profiling result : No kernels were profiled . No API activities were profiled . Type Time ( % ) Time Calls Avg Min Max Name OpenMP ( incl ) : 99.97 % 277.10 ms 20 13.855 ms 13.131 ms 18.151 ms omp_parallel 0.03 % 72.728 us 19 3.8270 us 2.9840 us 9.5610 us omp_idle 0.00 % 7.9170 us 7 1.1310 us 1.0360 us 1.5330 us omp_wait_barrier 3.7.1. OpenMP Options  Table 2 contains OpenMP profiling related command-line options of nvprof . Table 2. OpenMP Options  Option Description --print-openmp-summary Print a summary of all recorded OpenMP activities. 4. Remote Profiling  Remote profiling is the process of collecting profile data from a remote system that is different than the host system at which that profile data will be viewed and analyzed. There are two ways to perform remote profiling. You can profile your remote application directly from nsight orthe Visual Profiler. Or you can use nvprof to collect the profile data on the remote system and then use nvvp on the host system to view and analyze the data. 4.1. Remote Profiling With Visual Profiler  This section describes how to perform remote profiling by using the remote capabilities of nsight and the Visual Profiler. Nsight Eclipse Edition supports full remote development including remote building, debugging, and profiling. Using these capabilities you can create a project and launch configuration that allows you to remotely profile your application. See the Nsight Eclipse Edition documentation for more information. The Visual Profiler also enables remote profiling. As shown in the following figure, when creating a new session or editing an existing session you can specify that the application being profiled resides on a remote system. Once you have configured your session to use a remote application, you can perform all profiler functions in the same way as you would with a local application, including timeline generation, guided analysis, and event and metric collection. To use the Visual Profiler remote profiling you must install the same version of the CUDA Toolkit on both the host and remote systems. It is not necessary for the host system to have an NVIDIA GPU, but ensure that the CUDA Toolkit installed on the host system supports the target device. The host and remote systems may run different operating systems or have different CPU architectures. Only a remote system running Linux is supported. The remote system must be accessible via SSH. 4.1.1. One-hop remote profiling  In certain remote profiling setups, the machine running the actual CUDA program is not accessible from the machine running the Visual Profiler. These two machines are connected via an intermediate machine, which we refer to as the login node. The host machine is the one which is running the Visual Profiler. The login node is where the one-hop profiling script will run. We only need ssh, scp and perl on this machine. The compute node is where the actual CUDA application will run and profiled. The profiling data generated will be copied over to the login node, so that it can be used by the Visual Profiler on the host. To configure one-hop profiling, you need to do the following one-time setup: Copy the one-hop profiling Perl script onto the login node. In Visual Profiler, add the login node as a new remote connection. In Visual Profiler’s New Session wizard, use the Configure button to open the toolkit configuration window. Here, use the radio button to select the custom script option, and browse to point to the Perl script on the login node. Once this setup is complete, you can profile the application as you would on any remote machine. Copying all data to and from the login and compute nodes happens transparently and automatically. 4.2. Remote Profiling With nvprof  This section describes how to perform remote profiling by running nvprof manually on the remote system and then importing the collected profile data into the Visual Profiler . 4.2.1. Collect Data On Remote System  There are three common remote profiling use cases that can be addressed by using nvprof and the Visual Profiler. Timeline The first use case is to collect a timeline of the application executing on the remote system. The timeline should be collected in a way that most accurately reflects the behavior of the application. To collect the timeline execute the following on the remote system. See ​nvprof for more information on nvprof options. $ nvprof -- export - profile timeline . prof < app > < app args > The profile data will be collected in timeline.prof. You should copy this file back to the host system and then import it into the Visual Profiler as described in the next section. Metrics And Events The second use case is to collect events or metrics for all kernels in an application for which you have already collected a timeline. Collecting events or metrics for all kernels will significantly change the overall performance characteristics of the application because all kernel executions will be serialized on the GPU. Even though overall application performance is changed, the event or metric values for individual kernels will be correct and so you can merge the collected event and metric values onto a previously collected timeline to get an accurate picture of the applications behavior. To collect events or metrics you use the --events or --metrics flag. The following shows an example using just the --metrics flag to collect two metrics. $ nvprof -- metrics achieved_occupancy , ipc - o metrics . prof < app > < app args > You can collect any number of events and metrics for each nvprof invocation, and you can invoke nvprof multiple times to collect multiple metrics.prof files. To get accurate profiling results, it is important that your application conform to the requirements detailed in Application Requirements . The profile data will be collected in the metrics.prof file(s). You should copy these files back to the host system and then import it into the Visual Profiler as described in the next section. Analysis For Individual Kernel The third common remote profiling use case is to collect the metrics needed by the analysis system for an individual kernel. When imported into the Visual Profiler this data will enable the analysis system to analyze the kernel and report optimization opportunities for that kernel. To collect the analysis data execute the following on the remote system. It is important that the --kernels option appear before the --analysis-metrics option so that metrics are collected only for the kernel(s) specified by kernel specifier . See Profiling Scope for more information on the --kernels option. $ nvprof -- kernels < kernel specifier > -- analysis - metrics - o analysis . prof < app > < app args > The profile data will be collected in analysis.prof. You should copy this file back to the host system and then import it into the Visual Profiler as described in the next section. 4.2.2. View And Analyze Data  The collected profile data is viewed and analyzed by importing it into the Visual Profiler on the host system. See Import Session for more information about importing. Timeline, Metrics And Events To view collected timeline data, the timeline.prof file can be imported into the Visual Profiler as described in Import Single-Process nvprof Session . If metric or event data was also collected for the application, the corresponding metrics.prof file(s) can be imported into the Visual Profiler along with the timeline so that the events and metrics collected for each kernel are associated with the corresponding kernel in the timeline. Guided Analysis For Individual Kernel To view collected analysis data for an individual kernel, the analysis.prof file can be imported into the Visual Profiler as described in Import Single-Process nvprof Session . The analysis.prof must be imported by itself. The timeline will show just the individual kernel that we specified during data collection. After importing, the guided analysis system can be used to explore the optimization opportunities for the kernel. 5. NVIDIA Tools Extension  NVIDIA Tools Extension (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications. Applications which integrate NVTX can use the Visual Profiler to capture and visualize these events and ranges. The NVTX API provides two core services: Tracing of CPU events and time ranges. Naming of OS and CUDA resources. NVTX can be quickly integrated into an application. The sample program below shows the use of marker events, range events, and resource naming. void Wait(int waitMilliseconds) {\n  nvtxNameOsThread(“MAIN”);\n  nvtxRangePush(__FUNCTION__);\n  nvtxMark(\"Waiting...\");\n  Sleep(waitMilliseconds);\n  nvtxRangePop();\n}\n\nint main(void) {\n  nvtxNameOsThread(\"MAIN\");\n  nvtxRangePush(__FUNCTION__);\n  Wait();\n  nvtxRangePop();\n} 5.1. NVTX API Overview  Files The core NVTX API is defined in file nvToolsExt.h, whereas CUDA-specific extensions to the NVTX interface are defined in nvToolsExtCuda.h and nvToolsExtCudaRt.h. On Linux the NVTX shared library is called libnvToolsExt.so and on macOS the shared library is called libnvToolsExt.dylib . On Windows the library (.lib) and runtime components (.dll) are named nvToolsExt[bitness=32|64]_[version].{dll|lib} . Function Calls All NVTX API functions start with an nvtx name prefix and may end with one of the three suffixes: A, W, or Ex. NVTX functions with these suffixes exist in multiple variants, performing the same core functionality with different parameter encodings. Depending on the version of the NVTX library, available encodings may include ASCII (A), Unicode (W), or event structure (Ex). The CUDA implementation of NVTX only implements the ASCII (A) and event structure (Ex) variants of the API, the Unicode (W) versions are not supported and have no effect when called. Return Values Some of the NVTX functions are defined to have return values. For example, the nvtxRangeStart() function returns a unique range identifier and nvtxRangePush() function outputs the current stack level. It is recommended not to use the returned values as part of conditional code in the instrumented application. The returned values can differ between various implementations of the NVTX library and, consequently, having added dependencies on the return values might work with one tool, but may fail with another. 5.2. NVTX API Events  Markers are used to describe events that occur at a specific time during the execution of an application, while ranges detail the time span in which they occur. This information is presented alongside all of the other captured data, which makes it easier to understand the collected information. All markers and ranges are identified by a message string. The Ex version of the marker and range APIs also allows category, color, and payload attributes to be associated with the event using the event attributes structure. 5.2.1. NVTX Markers  A marker is used to describe an instantaneous event. A marker can contain a text message or specify additional information using the event attributes structure . Use nvtxMarkA to create a marker containing an ASCII message. Use nvtxMarkEx() to create a marker containing additional attributes specified by the event attribute structure. The nvtxMarkW() function is not supported in the CUDA implementation of NVTX and has no effect if called. Code Example nvtxMarkA ( \"My mark\" ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib . version = NVTX_VERSION ; eventAttrib . size = NVTX_EVENT_ATTRIB_STRUCT_SIZE ; eventAttrib . colorType = NVTX_COLOR_ARGB ; eventAttrib . color = COLOR_RED ; eventAttrib . messageType = NVTX_MESSAGE_TYPE_ASCII ; eventAttrib . message . ascii = \"my mark with attributes\" ; nvtxMarkEx ( & eventAttrib ); 5.2.2. NVTX Range Start/Stop  A start/end range is used to denote an arbitrary, potentially non-nested, time span. The start of a range can occur on a different thread than the end of the range. A range can contain a text message or specify additional information using the event attributes structure . Use nvtxRangeStartA() to create a marker containing an ASCII message. Use nvtxRangeStartEx() to create a range containing additional attributes specified by the event attribute structure. The nvtxRangeStartW() function is not supported in the CUDA implementation of NVTX and has no effect if called. For the correlation of a start/end pair, a unique correlation ID is created that is returned from nvtxRangeStartA() or nvtxRangeStartEx() , and is then passed into nvtxRangeEnd() . Code Example // non-overlapping range nvtxRangeId_t id1 = nvtxRangeStartA ( \"My range\" ); nvtxRangeEnd ( id1 ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib . version = NVTX_VERSION ; eventAttrib . size = NVTX_EVENT_ATTRIB_STRUCT_SIZE ; eventAttrib . colorType = NVTX_COLOR_ARGB ; eventAttrib . color = COLOR_BLUE ; eventAttrib . messageType = NVTX_MESSAGE_TYPE_ASCII ; eventAttrib . message . ascii = \"my start/stop range\" ; nvtxRangeId_t id2 = nvtxRangeStartEx ( & eventAttrib ); nvtxRangeEnd ( id2 ); // overlapping ranges nvtxRangeId_t r1 = nvtxRangeStartA ( \"My range 0\" ); nvtxRangeId_t r2 = nvtxRangeStartA ( \"My range 1\" ); nvtxRangeEnd ( r1 ); nvtxRangeEnd ( r2 ); 5.2.3. NVTX Range Push/Pop  A push/pop range is used to denote nested time span. The start of a range must occur on the same thread as the end of the range. A range can contain a text message or specify additional information using the event attributes structure . Use nvtxRangePushA() to create a marker containing an ASCII message. Use nvtxRangePushEx() to create a range containing additional attributes specified by the event attribute structure. The nvtxRangePushW() function is not supported in the CUDA implementation of NVTX and has no effect if called. Each push function returns the zero-based depth of the range being started. The nvtxRangePop() function is used to end the most recently pushed range for the thread. nvtxRangePop() returns the zero-based depth of the range being ended. If the pop does not have a matching push, a negative value is returned to indicate an error. Code Example nvtxRangePushA ( \"outer\" ); nvtxRangePushA ( \"inner\" ); nvtxRangePop (); // end \"inner\" range nvtxRangePop (); // end \"outer\" range nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib . version = NVTX_VERSION ; eventAttrib . size = NVTX_EVENT_ATTRIB_STRUCT_SIZE ; eventAttrib . colorType = NVTX_COLOR_ARGB ; eventAttrib . color = COLOR_GREEN ; eventAttrib . messageType = NVTX_MESSAGE_TYPE_ASCII ; eventAttrib . message . ascii = \"my push/pop range\" ; nvtxRangePushEx ( & eventAttrib ); nvtxRangePop (); 5.2.4. Event Attributes Structure  The events attributes structure, nvtxEventAttributes_t , is used to describe the attributes of an event. The layout of the structure is defined by a specific version of NVTX and can change between different versions of the Tools Extension library. Attributes Markers and ranges can use attributes to provide additional information for an event or to guide the tool’s visualization of the data. Each of the attributes is optional and if left unspecified, the attributes fall back to a default value. Message The message field can be used to specify an optional string. The caller must set both the messageType and message fields. The default value is NVTX_MESSAGE_UNKNOWN . The CUDA implementation of NVTX only supports ASCII type messages. Category The category attribute is a user-controlled ID that can be used to group events. The tool may use category IDs to improve filtering, or for grouping events. The default value is 0. Color The color attribute is used to help visually identify events in the tool. The caller must set both the colorType and color fields. Payload The payload attribute can be used to provide additional data for markers and ranges. Range events can only specify values at the beginning of a range. The caller must specify valid values for both the payloadType and payload fields. Initialization The caller should always perform the following three tasks when using attributes: Zero the structure Set the version field Set the size field Zeroing the structure sets all the event attributes types and values to the default value. The version and size field are used by NVTX to handle multiple versions of the attributes structure. It is recommended that the caller use the following method to initialize the event attributes structure. nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib . version = NVTX_VERSION ; eventAttrib . size = NVTX_EVENT_ATTRIB_STRUCT_SIZE ; eventAttrib . colorType = NVTX_COLOR_ARGB ; eventAttrib . color = :: COLOR_YELLOW ; eventAttrib . messageType = NVTX_MESSAGE_TYPE_ASCII ; eventAttrib . message . ascii = \"My event\" ; nvtxMarkEx ( & eventAttrib ); 5.2.5. NVTX Synchronization Markers  The NVTX synchronization module provides functions to support tracking additional synchronization details of the target application. Naming OS synchronization primitives may allow users to better understand the data collected by traced synchronization APIs. Additionally, annotating a user-defined synchronization object can allow the user to tell the tools when the user is building their own synchronization system that does not rely on the OS to provide behaviors, and instead uses techniques like atomic operations and spinlocks. Note Synchronization marker support is not available on Windows. Code Example class MyMutex { volatile long bLocked ; nvtxSyncUser_t hSync ; public : MyMutex ( const char * name , nvtxDomainHandle_t d ) { bLocked = 0 ; nvtxSyncUserAttributes_t attribs = { 0 }; attribs . version = NVTX_VERSION ; attribs . size = NVTX_SYNCUSER_ATTRIB_STRUCT_SIZE ; attribs . messageType = NVTX_MESSAGE_TYPE_ASCII ; attribs . message . ascii = name ; hSync = nvtxDomainSyncUserCreate ( d , & attribs ); } ~ MyMutex () { nvtxDomainSyncUserDestroy ( hSync ); } bool Lock () { nvtxDomainSyncUserAcquireStart ( hSync ); //atomic compiler intrinsic bool acquired = __sync_bool_compare_and_swap ( & bLocked , 0 , 1 ); if ( acquired ) { nvtxDomainSyncUserAcquireSuccess ( hSync ); } else { nvtxDomainSyncUserAcquireFailed ( hSync ); } return acquired ; } void Unlock () { nvtxDomainSyncUserReleasing ( hSync ); bLocked = false ; } }; 5.3. NVTX Domains  Domains enable developers to scope annotations. By default all events and annotations are in the default domain. Additional domains can be registered. This allows developers to scope markers and ranges to avoid conflicts. The function nvtxDomainCreateA() or nvtxDomainCreateW() is used to create a named domain. Each domain maintains its own categories thread range stacks registered strings The function nvtxDomainDestroy() marks the end of the domain. Destroying a domain unregisters and destroys all objects associated with it such as registered strings, resource objects, named categories, and started ranges. Note Domain support is not available on Windows. Code Example nvtxDomainHandle_t domain = nvtxDomainCreateA ( \"Domain_A\" ); nvtxMarkA ( \"Mark_A\" ); nvtxEventAttributes_t attrib = { 0 }; attrib . version = NVTX_VERSION ; attrib . size = NVTX_EVENT_ATTRIB_STRUCT_SIZE ; attrib . message . ascii = \"Mark A Message\" ; nvtxDomainMarkEx ( NULL , & attrib ); nvtxDomainDestroy ( domain ); 5.4. NVTX Resource Naming  NVTX resource naming allows custom names to be associated with host OS threads and CUDA resources such as devices, contexts, and streams. The names assigned using NVTX are displayed by the Visual Profiler. OS Thread The nvtxNameOsThreadA() function is used to name a host OS thread. The nvtxNameOsThreadW() function is not supported in the CUDA implementation of NVTX and has no effect if called. The following example shows how the current host OS thread can be named. // Windows nvtxNameOsThread ( GetCurrentThreadId (), \"MAIN_THREAD\" ); // Linux/Mac nvtxNameOsThread ( pthread_self (), \"MAIN_THREAD\" ); CUDA Runtime Resources The nvtxNameCudaDeviceA() and nvtxNameCudaStreamA() functions are used to name CUDA device and stream objects, respectively. The nvtxNameCudaDeviceW() and nvtxNameCudaStreamW() functions are not supported in the CUDA implementation of NVTX and have no effect if called. The nvtxNameCudaEventA() and nvtxNameCudaEventW() functions are also not supported. The following example shows how a CUDA device and stream can be named. nvtxNameCudaDeviceA ( 0 , \"my cuda device 0\" ); cudaStream_t cudastream ; cudaStreamCreate ( & cudastream ); nvtxNameCudaStreamA ( cudastream , \"my cuda stream\" ); CUDA Driver Resources The nvtxNameCuDeviceA() , nvtxNameCuContextA() and nvtxNameCuStreamA() functions are used to name CUDA driver device, context and stream objects, respectively. The nvtxNameCuDeviceW() , nvtxNameCuContextW() and nvtxNameCuStreamW() functions are not supported in the CUDA implementation of NVTX and have no effect if called. The nvtxNameCuEventA() and nvtxNameCuEventW() functions are also not supported. The following example shows how a CUDA device, context and stream can be named. CUdevice device ; cuDeviceGet ( & device , 0 ); nvtxNameCuDeviceA ( device , \"my device 0\" ); CUcontext context ; cuCtxCreate ( & context , 0 , device ); nvtxNameCuContextA ( context , \"my context\" ); cuStream stream ; cuStreamCreate ( & stream , 0 ); nvtxNameCuStreamA ( stream , \"my stream\" ); 5.5. NVTX String Registration  Registered strings are intended to increase performance by lowering instrumentation overhead. String may be registered once and the handle may be passed in place of a string where an the APIs may allow. The nvtxDomainRegisterStringA() function is used to register a string. The nvtxDomainRegisterStringW() function is not supported in the CUDA implementation of NVTX and has no effect if called. nvtxDomainHandle_t domain = nvtxDomainCreateA ( \"Domain_A\" ); nvtxStringHandle_t message = nvtxDomainRegisterStringA ( domain , \"registered string\" ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib . version = NVTX_VERSION ; eventAttrib . size = NVTX_EVENT_ATTRIB_STRUCT_SIZE ; eventAttrib . messageType = NVTX_MESSAGE_TYPE_REGISTERED ; eventAttrib . message . registered = message ; 6. MPI Profiling  6.1. Automatic MPI Annotation with NVTX  You can annotate MPI calls with NVTX markers to profile, trace and visualize them. It can get tedious to wrap every MPI call with NVTX markers, but there are two ways to do this automatically: Built-in annotation nvprof has a built-in option that supports two MPI implementations - OpenMPI and MPICH. If you have either of these installed on your system, you can use the --annotate-mpi option and specify your installed MPI implementation. If you use this option, nvprof will generate NVTX markers every time your application makes MPI calls. Only synchronous MPI calls are annotated using this built-in option. Additionally, we use NVTX to rename the current thread and current device object to indicate the MPI rank. For example if you have OpenMPI installed, you can annotate your application using the command: $ mpirun - np 2 nvprof -- annotate - mpi openmpi . / my_mpi_app This will give you output that looks something like this: NVTX result : Thread \"MPI Rank 0\" ( id = 583411584 ) Domain \"<unnamed>\" Range \"MPI_Reduce\" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 16.652 us 1 16.652 us 16.652 us 16.652 us MPI_Reduce ... Range \"MPI_Scatter\" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 3.0320 ms 1 3.0320 ms 3.0320 ms 3.0320 ms MPI_Scatter ... NVTX result : Thread \"MPI Rank 1\" ( id = 199923584 ) Domain \"<unnamed>\" Range \"MPI_Reduce\" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 21.062 us 1 21.062 us 21.062 us 21.062 us MPI_Reduce ... Range \"MPI_Scatter\" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 85.296 ms 1 85.296 ms 85.296 ms 85.296 ms MPI_Scatter ... Custom annotation If your system has a version of MPI that is not supported by nvprof, or if you want more control over which MPI functions are annotated and how the NVTX markers are generated, you can create your own annotation library, and use the environment variable LD_PRELOAD to intercept MPI calls and wrap them with NVTX markers. You can create this annotation library conveniently using the documentation and open-source scripts located here . 6.2. Manual MPI Profiling  To use nvprof to collect the profiles of the individual MPI processes, you must tell nvprof to send its output to unique files. In CUDA 5.0 and earlier versions, it was recommended to use a script for this. However, you can now easily do it utilizing the %h , %p and %q{ENV} features of the --export-profile argument to the nvprof command. Below is example run using Open MPI. $ mpirun - np 2 - host c0 -0 , c0 -1 nvprof - o output . % h . % p . % q { OMPI_COMM_WORLD_RANK } . / my_mpi_app Alternatively, one can make use of the new feature to turn on profiling on the nodes of interest using the --profile-all-processes argument to nvprof . To do this, you first log into the node you want to profile and start up nvprof there. $ nvprof -- profile - all - processes - o output . % h . % p Then you can just run the MPI job as your normally would. $ mpirun - np 2 - host c0 -0 , c0 -1 . / my_mpi_app Any processes that run on the node where the --profile-all-processes is running will automatically get profiled. The profiling data will be written to the output files. Note that the %q{OMPI_COMM_WORLD_RANK} option will not work here, because this environment variable will not be available in the shell where nvprof is running. Starting CUDA 7.5, you can name threads and CUDA contexts just as you name output files with the options –process-name and –context-name, by passing a string like \"MPI Rank %q{OMPI_COMM_WORLD_RANK}\" as a parameter. This feature is useful to spot resources associated with a specific rank when user imports multiple files into the same time-line in the Visual Profiler. $ mpirun - np 2 - host c0 -0 , c0 -1 nvprof -- process - name \"MPI Rank %q{OMPI_COMM_WORLD_RANK}\" -- context - name \"MPI Rank %q{OMPI_COMM_WORLD_RANK}\" - o output . % h . % p . % q { OMPI_COMM_WORLD_RANK } . / my_mpi_app 6.3. Further Reading  Details about what types of additional arguments to use with nvprof can be found in the Multiprocess Profiling and Redirecting Output section. Additional information about how to view the data with the Visual Profiler can be found in the Import Single-Process nvprof Session and Import Multi-Process nvprof Session sections. The blog post Profiling MPI Applications shows how to use new output file naming of nvprof introduced in CUDA 6.5 and NVTX library to name various resources to analyze the performance of a MPI application. The blog post Track MPI Calls in the Visual Profiler shows how Visual Profiler, combined with PMPI and NVTX can give interesting insights into how the MPI calls in your application interact with the GPU. 7. MPS Profiling  You can collect profiling data for a CUDA application using Multi-Process Service(MPS) with nvprof and then view the timeline by importing the data in the Visual Profiler. 7.1. MPS profiling with Visual Profiler  Visual Profiler can be run on a particular MPS client or for all MPS clients. Timeline profiling can be done for all MPS clients on the same server. Event or metric profiling results in serialization - only one MPS client will execute at a time. To profile a CUDA application using MPS: Launch the MPS daemon. Refer the MPS document for details. nvidia-cuda-mps-control -d In Visual Profiler open “New Session” wizard using main menu “File->New Session”. Select “Profile all processes” option from drop down, press “Next” and then “Finish”. Run the application in a separate terminal To end profiling press the “Cancel” button on progress dialog in Visual Profiler. Note that the profiling output also includes data for the CUDA MPS server processes which have process name nvidia-cuda-mps-server . 7.2. MPS profiling with nvprof  nvprof can be run on a particular MPS client or for all MPS clients. Timeline profiling can be done for all MPS clients on the same server. Event or metric profiling results in serialization - only one MPS client will execute at a time. To profile a CUDA application using MPS: Launch the MPS daemon. Refer to the MPS document for details. nvidia-cuda-mps-control -d Run nvprof with --profile-all-processes argument and to generate separate output files for each process use the %p feature of the --export-profile argument. Note that %p will be replaced by the process id. nvprof --profile-all-processes -o output_%p Run the application in a separate terminal Exit nvprof by typing “Ctrl-c”. Note that the profiling output also includes data for the CUDA MPS server processes which have process name nvidia-cuda-mps-server . 7.3. Viewing nvprof MPS timeline in Visual Profiler  Import the nvprof generated data files for each process using the multi-process import option. Refer the Import Multi-Process Session section. The figure below shows the MPS timeline view for three processes. The MPS context is identified in the timeline row label as Context MPS. Note that the Compute and kernel timeline row shows three kernels overlapping. 8. Dependency Analysis  The dependency analysis feature enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams. It allows to compute the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams. 8.1. Background  The dependency analysis in nvprof and the Visual Profiler is based on execution traces of applications. A trace captures all relevant activities such as API function calls or CUDA kernels along with their timestamps and durations. Given this execution trace and a model of the dependencies between those activities on different threads/streams, a dependency graph can be constructed. Typical dependencies modelled in this graph would be that a CUDA kernel can not start before its respective launch API call or that a blocking CUDA stream synchronization call can not return before all previously enqueued work in this stream has been completed. These dependencies are defined by the CUDA API contract. From this dependency graph and the API model(s), wait states can be computed. A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream. Given the previous stream synchronization example, the synchronizing API call is blocked for the time it has to wait on any GPU activity in the respective CUDA stream. Knowledge about where wait states occur and how long functions are blocked is helpful to identify optimization opportunities for more high-level concurrency in the application. In addition to individual wait states, the critical path through the captured event graph enables to pinpoint those function calls, kernel and memory copies that are responsible for the total application runtime. The critical path is the longest path through an event graph that does not contain wait states, i.e. optimizing activities on this path can directly improve the execution time. 8.2. Metrics  Waiting Time A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream. Waiting time is an inidicator for load-imbalances between execution streams. In the example below, the blocking CUDA synchronization API calls are waiting on their respective kernels to finish executing on the GPU. Instead of waiting immediately, one should attempt to overlap the kernel executions with concurrent CPU work with a similar runtime, thereby reducing the time that any computing device (CPU or GPU) is blocked. Time on Critical Path The critical path is the longest path through an event graph that does not contain wait states, i.e. optimizing activities on this path can directly improve the execution time. Activities with a high time on the critical path have a high direct impact on the application runtime. In the example pictured below, copy_kernel is on the critical path since the CPU is blocked waiting for it to finish in cudeDeviceSynchronize . Reducing the kernel runtime allows the CPU to return earlier from the API call and continue program execution. On the other hand, jacobi_kernel is fully overlapped with CPU work, i.e. the synchronizing API call is triggered after the kernel is already finished. Since no execution stream is waiting on this kernel to finish, reducing its duration will likely not improve the overall application runtime. 8.3. Support  The following programming APIs are currently supported for dependency analysis CUDA runtime and driver API POSIX threads (Pthreads), POSIX mutexes and condition variables Dependency analysis is available in Visual Profiler and nvprof . A Dependency Analysis stage can be selected in the Unguided Application Analysis and new Dependency Analysis Controls are available for the timeline. See section Dependency Analysis on how to use this feature in nvprof . 8.4. Limitations  The dependency and wait time analysis between different threads and CUDA streams only takes into account execution dependencies stated in the respective supported API contracts. This especially does not include synchronization as a result of resource contention. For example, asynchronous memory copies enqueued into independent CUDA streams will not be marked dependent even if the concrete GPU has only a single copy engine. Furthermore, the analysis does not account for synchronization using a not-supported API. For example, a CPU thread actively polling for a value at some memory location (busy-waiting) will not be considered blocked on another concurrent activity. The dependency analysis has only limited support for applications using CUDA Dynamic Parallelism (CDP). CDP kernels can use CUDA API calls from the GPU which are not tracked via the CUPTI Activity API. Therefore, the analysis cannot determine the full dependencies and waiting time for CDP kernels. However, it utilizes the parent-child launch dependencies between CDP kernels. As a result the critical path will always include the last CDP kernel of each host-launched kernel. The POSIX semaphores API is currently not supported. The dependency analysis does not support API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice . Kernel launched by either of these API functions might not be tracked correctly. 9. Metrics Reference  This section contains detailed descriptions of the metrics that can be collected by nvprof and the Visual Profiler. A scope value of “Single-context” indicates that the metric can only be accurately collected when a single context (CUDA or graphic) is executing on the GPU. A scope value of “Multi-context” indicates that the metric can be accurately collected when multiple contexts are executing on the GPU. A scope value of “Device” indicates that the metric will be collected at device level, that is it will include values for all the contexts executing on the GPU. Note that, NVLink metrics collected for kernel mode exhibit the behavior of “Single-context”. 9.1. Metrics for Capability 5.x  Devices with compute capability 5.x implement the metrics shown in the following table. Note that for some metrics the “Multi-context” scope is supported only for specific devices. Such metrics are marked with “Multi-context * ” under the “Scope” column. Refer to the note at the bottom of the table. Table 4. Capability 5.x Metrics  Metric Name Description Scope achieved_occupancy Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor Multi-context atomic_transactions Global memory atomic and reduction transactions Multi-context atomic_transactions_per_request Average number of global memory atomic and reduction transactions performed for each atomic and reduction instruction Multi-context branch_efficiency Ratio of non-divergent branches to total branches expressed as percentage Multi-context cf_executed Number of executed control-flow instructions Multi-context cf_fu_utilization The utilization level of the multiprocessor function units that execute control-flow instructions on a scale of 0 to 10 Multi-context cf_issued Number of issued control-flow instructions Multi-context double_precision_fu_utilization The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10 Multi-context dram_read_bytes Total bytes read from DRAM to L2 cache. This is available for compute capability 5.0 and 5.2. Multi-context * dram_read_throughput Device memory read throughput. This is available for compute capability 5.0 and 5.2. Multi-context * dram_read_transactions Device memory read transactions. This is available for compute capability 5.0 and 5.2. Multi-context * dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context * dram_write_bytes Total bytes written from L2 cache to DRAM. This is available for compute capability 5.0 and 5.2. Multi-context * dram_write_throughput Device memory write throughput. This is available for compute capability 5.0 and 5.2. Multi-context * dram_write_transactions Device memory write transactions. This is available for compute capability 5.0 and 5.2. Multi-context * ecc_throughput ECC throughput from L2 to DRAM. This is available for compute capability 5.0 and 5.2. Multi-context * ecc_transactions Number of ECC transactions between L2 and DRAM. This is available for compute capability 5.0 and 5.2. Multi-context * eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. Multi-context flop_count_dp_add Number of double-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_dp_fma Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_dp_mul Number of double-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add, multiply and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. This is available for compute capability 5.3. Multi-context * flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads. This is available for compute capability 5.3. Multi-context * flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. This is available for compute capability 5.3. Multi-context * flop_count_hp_mul Number of half-precision floating-point multiply operations executed by non-predicated threads. This is available for compute capability 5.3. Multi-context * flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. The count does not include special operations. Multi-context flop_count_sp_add Number of single-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_sp_fma Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_sp_mul Number of single-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_sp_special Number of single-precision floating-point special operations executed by non-predicated threads. Multi-context flop_dp_efficiency Ratio of achieved to peak double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations. This is available for compute capability 5.3. Multi-context * flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage. Multi-context * gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context * gld_transactions Number of global memory load transactions Multi-context * gld_transactions_per_request Average number of global memory load transactions performed for each global memory load. Multi-context * global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global loads in unified l1/tex cache. Metric value maybe wrong if malloc is used in kernel. Multi-context * global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor. This does not include atomic requests. Multi-context gst_efficiency Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage. Multi-context * gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context * gst_transactions Number of global memory store transactions Multi-context * gst_transactions_per_request Average number of global memory store transactions performed for each global memory store Multi-context * half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions and integer instructions on a scale of 0 to 10. This is available for compute capability 5.3. Multi-context * inst_bit_convert Number of bit-conversion instructions executed by non-predicated threads Multi-context inst_compute_ld_st Number of compute load/store instructions executed by non-predicated threads Multi-context inst_control Number of control-flow instructions executed by non-predicated threads (jump, branch, etc.) Multi-context inst_executed The number of instructions executed Multi-context inst_executed_global_atomics Warp level instructions for global atom and atom cas Multi-context inst_executed_global_loads Warp level instructions for global loads Multi-context inst_executed_global_reductions Warp level instructions for global reductions Multi-context inst_executed_global_stores Warp level instructions for global stores Multi-context inst_executed_local_loads Warp level instructions for local loads Multi-context inst_executed_local_stores Warp level instructions for local stores Multi-context inst_executed_shared_atomics Warp level shared instructions for atom and atom CAS Multi-context inst_executed_shared_loads Warp level instructions for shared loads Multi-context inst_executed_shared_stores Warp level instructions for shared stores Multi-context inst_executed_surface_atomics Warp level instructions for surface atom and atom cas Multi-context inst_executed_surface_loads Warp level instructions for surface loads Multi-context inst_executed_surface_reductions Warp level instructions for surface reductions Multi-context inst_executed_surface_stores Warp level instructions for surface stores Multi-context inst_executed_tex_ops Warp level instructions for texture Multi-context inst_fp_16 Number of half-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) This is available for compute capability 5.3. Multi-context * inst_fp_32 Number of single-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_fp_64 Number of double-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_integer Number of integer instructions executed by non-predicated threads Multi-context inst_inter_thread_communication Number of inter-thread communication instructions executed by non-predicated threads Multi-context inst_issued The number of instructions issued Multi-context inst_misc Number of miscellaneous instructions executed by non-predicated threads Multi-context inst_per_warp Average number of instructions executed by each warp Multi-context inst_replay_overhead Average number of replays for each instruction executed Multi-context ipc Instructions executed per cycle Multi-context issue_slot_utilization Percentage of issue slots that issued at least one instruction, averaged across all cycles Multi-context issue_slots The number of issue slots used Multi-context issued_ipc Instructions issued per cycle Multi-context l2_atomic_throughput Memory read throughput seen at L2 cache for atomic and reduction requests Multi-context l2_atomic_transactions Memory read transactions seen at L2 cache for atomic and reduction requests Multi-context * l2_global_atomic_store_bytes Bytes written to L2 from Unified cache for global atomics (ATOM and ATOM CAS) Multi-context * l2_global_load_bytes Bytes read from L2 for misses in Unified Cache for global loads Multi-context * l2_global_reduction_bytes Bytes written to L2 from Unified cache for global reductions Multi-context * l2_local_global_store_bytes Bytes written to L2 from Unified Cache for local and global stores. This does not include global atomics. Multi-context * l2_local_load_bytes Bytes read from L2 for misses in Unified Cache for local loads Multi-context * l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context * l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context * l2_surface_atomic_store_bytes Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS) Multi-context * l2_surface_load_bytes Bytes read from L2 for misses in Unified Cache for surface loads Multi-context * l2_surface_reduction_bytes Bytes written to L2 from Unified Cache for surface reductions Multi-context * l2_surface_store_bytes Bytes written to L2 from Unified Cache for surface stores. This does not include surface atomics. Multi-context * l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context * l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache. This is available for compute capability 5.0 and 5.2. Multi-context * l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context * l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context * l2_tex_write_hit_rate Hit Rate at L2 cache for all write requests from texture cache. This is available for compute capability 5.0 and 5.2. Multi-context * l2_tex_write_throughput Memory write throughput seen at L2 cache for write requests from the texture cache Multi-context * l2_tex_write_transactions Memory write transactions seen at L2 cache for write requests from the texture cache Multi-context * l2_utilization The utilization level of the L2 cache relative to the peak utilization on a scale of 0 to 10 Multi-context * l2_write_throughput Memory write throughput seen at L2 cache for all write requests Multi-context * l2_write_transactions Memory write transactions seen at L2 cache for all write requests Multi-context * ldst_executed Number of executed local, global, shared and texture memory load and store instructions Multi-context ldst_fu_utilization The utilization level of the multiprocessor function units that execute shared load, shared store and constant load instructions on a scale of 0 to 10 Multi-context ldst_issued Number of issued local, global, shared and texture memory load and store instructions Multi-context local_hit_rate Hit rate for local loads and stores Multi-context * local_load_requests Total number of local load requests from Multiprocessor Multi-context * local_load_throughput Local memory load throughput Multi-context * local_load_transactions Number of local memory load transactions Multi-context * local_load_transactions_per_request Average number of local memory load transactions performed for each local memory load Multi-context * local_memory_overhead Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage Multi-context * local_store_requests Total number of local store requests from Multiprocessor Multi-context * local_store_throughput Local memory store throughput Multi-context * local_store_transactions Number of local memory store transactions Multi-context * local_store_transactions_per_request Average number of local memory store transactions performed for each local memory store Multi-context * pcie_total_data_received Total data bytes received through PCIe Device pcie_total_data_transmitted Total data bytes transmitted through PCIe Device shared_efficiency Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage Multi-context * shared_load_throughput Shared memory load throughput Multi-context * shared_load_transactions Number of shared memory load transactions Multi-context * shared_load_transactions_per_request Average number of shared memory load transactions performed for each shared memory load Multi-context * shared_store_throughput Shared memory store throughput Multi-context * shared_store_transactions Number of shared memory store transactions Multi-context * shared_store_transactions_per_request Average number of shared memory store transactions performed for each shared memory store Multi-context * shared_utilization The utilization level of the shared memory relative to peak utilization on a scale of 0 to 10 Multi-context * single_precision_fu_utilization The utilization level of the multiprocessor function units that execute single-precision floating-point instructions and integer instructions on a scale of 0 to 10 Multi-context sm_efficiency The percentage of time at least one warp is active on a specific multiprocessor Multi-context * special_fu_utilization The utilization level of the multiprocessor function units that execute sin, cos, ex2, popc, flo, and similar instructions on a scale of 0 to 10 Multi-context stall_constant_memory_dependency Percentage of stalls occurring because of immediate constant cache miss Multi-context stall_exec_dependency Percentage of stalls occurring because an input required by the instruction is not yet available Multi-context stall_inst_fetch Percentage of stalls occurring because the next assembly instruction has not yet been fetched Multi-context stall_memory_dependency Percentage of stalls occurring because a memory operation cannot be performed due to the required resources not being available or fully utilized, or because too many requests of a given type are outstanding Multi-context stall_memory_throttle Percentage of stalls occurring because of memory throttle Multi-context stall_not_selected Percentage of stalls occurring because warp was not selected Multi-context stall_other Percentage of stalls occurring due to miscellaneous reasons Multi-context stall_pipe_busy Percentage of stalls occurring because a compute operation cannot be performed because the compute pipeline is busy Multi-context stall_sync Percentage of stalls occurring because the warp is blocked at a __syncthreads() call Multi-context stall_texture Percentage of stalls occurring because the texture sub-system is fully utilized or has too many outstanding requests Multi-context surface_atomic_requests Total number of surface atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context surface_load_requests Total number of surface load requests from Multiprocessor Multi-context surface_reduction_requests Total number of surface reduction requests from Multiprocessor Multi-context surface_store_requests Total number of surface store requests from Multiprocessor Multi-context sysmem_read_bytes Number of bytes read from system memory Multi-context * sysmem_read_throughput System memory read throughput Multi-context * sysmem_read_transactions Number of system memory read transactions Multi-context * sysmem_read_utilization The read utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 5.0 and 5.2. Multi-context sysmem_utilization The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 5.0 and 5.2. Multi-context * sysmem_write_bytes Number of bytes written to system memory Multi-context * sysmem_write_throughput System memory write throughput Multi-context * sysmem_write_transactions Number of system memory write transactions Multi-context * sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 5.0 and 5.2. Multi-context * tex_cache_hit_rate Unified cache hit rate Multi-context * tex_cache_throughput Unified cache throughput Multi-context * tex_cache_transactions Unified cache read transactions Multi-context * tex_fu_utilization The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10 Multi-context tex_utilization The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10 Multi-context * texture_load_requests Total number of texture Load requests from Multiprocessor Multi-context warp_execution_efficiency Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor Multi-context warp_nonpred_execution_efficiency Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor Multi-context * The “Multi-context” scope for this metric is supported only for devices with compute capability 5.0 and 5.2. 9.2. Metrics for Capability 6.x  Devices with compute capability 6.x implement the metrics shown in the following table. Table 5. Capability 6.x Metrics  Metric Name Description Scope achieved_occupancy Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor Multi-context atomic_transactions Global memory atomic and reduction transactions Multi-context atomic_transactions_per_request Average number of global memory atomic and reduction transactions performed for each atomic and reduction instruction Multi-context branch_efficiency Ratio of non-divergent branches to total branches expressed as percentage Multi-context cf_executed Number of executed control-flow instructions Multi-context cf_fu_utilization The utilization level of the multiprocessor function units that execute control-flow instructions on a scale of 0 to 10 Multi-context cf_issued Number of issued control-flow instructions Multi-context double_precision_fu_utilization The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10 Multi-context dram_read_bytes Total bytes read from DRAM to L2 cache Multi-context dram_read_throughput Device memory read throughput. This is available for compute capability 6.0 and 6.1. Multi-context dram_read_transactions Device memory read transactions. This is available for compute capability 6.0 and 6.1. Multi-context dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context dram_write_bytes Total bytes written from L2 cache to DRAM Multi-context dram_write_throughput Device memory write throughput. This is available for compute capability 6.0 and 6.1. Multi-context dram_write_transactions Device memory write transactions. This is available for compute capability 6.0 and 6.1. Multi-context ecc_throughput ECC throughput from L2 to DRAM. This is available for compute capability 6.1. Multi-context ecc_transactions Number of ECC transactions between L2 and DRAM. This is available for compute capability 6.1. Multi-context eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. Multi-context flop_count_dp_add Number of double-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_dp_fma Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_dp_mul Number of double-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. Multi-context flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_hp_mul Number of half-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. The count does not include special operations. Multi-context flop_count_sp_add Number of single-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_sp_fma Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_sp_mul Number of single-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_sp_special Number of single-precision floating-point special operations executed by non-predicated threads. Multi-context flop_dp_efficiency Ratio of achieved to peak double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations Multi-context flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage. Multi-context gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context gld_transactions Number of global memory load transactions Multi-context gld_transactions_per_request Average number of global memory load transactions performed for each global memory load. Multi-context global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global loads in unified l1/tex cache. Metric value maybe wrong if malloc is used in kernel. Multi-context global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor. This does not include atomic requests. Multi-context gst_efficiency Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage. Multi-context gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context gst_transactions Number of global memory store transactions Multi-context gst_transactions_per_request Average number of global memory store transactions performed for each global memory store Multi-context half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions on a scale of 0 to 10 Multi-context inst_bit_convert Number of bit-conversion instructions executed by non-predicated threads Multi-context inst_compute_ld_st Number of compute load/store instructions executed by non-predicated threads Multi-context inst_control Number of control-flow instructions executed by non-predicated threads (jump, branch, etc.) Multi-context inst_executed The number of instructions executed Multi-context inst_executed_global_atomics Warp level instructions for global atom and atom cas Multi-context inst_executed_global_loads Warp level instructions for global loads Multi-context inst_executed_global_reductions Warp level instructions for global reductions Multi-context inst_executed_global_stores Warp level instructions for global stores Multi-context inst_executed_local_loads Warp level instructions for local loads Multi-context inst_executed_local_stores Warp level instructions for local stores Multi-context inst_executed_shared_atomics Warp level shared instructions for atom and atom CAS Multi-context inst_executed_shared_loads Warp level instructions for shared loads Multi-context inst_executed_shared_stores Warp level instructions for shared stores Multi-context inst_executed_surface_atomics Warp level instructions for surface atom and atom cas Multi-context inst_executed_surface_loads Warp level instructions for surface loads Multi-context inst_executed_surface_reductions Warp level instructions for surface reductions Multi-context inst_executed_surface_stores Warp level instructions for surface stores Multi-context inst_executed_tex_ops Warp level instructions for texture Multi-context inst_fp_16 Number of half-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_fp_32 Number of single-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_fp_64 Number of double-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_integer Number of integer instructions executed by non-predicated threads Multi-context inst_inter_thread_communication Number of inter-thread communication instructions executed by non-predicated threads Multi-context inst_issued The number of instructions issued Multi-context inst_misc Number of miscellaneous instructions executed by non-predicated threads Multi-context inst_per_warp Average number of instructions executed by each warp Multi-context inst_replay_overhead Average number of replays for each instruction executed Multi-context ipc Instructions executed per cycle Multi-context issue_slot_utilization Percentage of issue slots that issued at least one instruction, averaged across all cycles Multi-context issue_slots The number of issue slots used Multi-context issued_ipc Instructions issued per cycle Multi-context l2_atomic_throughput Memory read throughput seen at L2 cache for atomic and reduction requests Multi-context l2_atomic_transactions Memory read transactions seen at L2 cache for atomic and reduction requests Multi-context l2_global_atomic_store_bytes Bytes written to L2 from Unified cache for global atomics (ATOM and ATOM CAS) Multi-context l2_global_load_bytes Bytes read from L2 for misses in Unified Cache for global loads Multi-context l2_global_reduction_bytes Bytes written to L2 from Unified cache for global reductions Multi-context l2_local_global_store_bytes Bytes written to L2 from Unified Cache for local and global stores. This does not include global atomics. Multi-context l2_local_load_bytes Bytes read from L2 for misses in Unified Cache for local loads Multi-context l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context l2_surface_atomic_store_bytes Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS) Multi-context l2_surface_load_bytes Bytes read from L2 for misses in Unified Cache for surface loads Multi-context l2_surface_reduction_bytes Bytes written to L2 from Unified Cache for surface reductions Multi-context l2_surface_store_bytes Bytes written to L2 from Unified Cache for surface stores. This does not include surface atomics. Multi-context l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache. This is available for compute capability 6.0 and 6.1. Multi-context l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context l2_tex_write_hit_rate Hit Rate at L2 cache for all write requests from texture cache. This is available for compute capability 6.0 and 6.1. Multi-context l2_tex_write_throughput Memory write throughput seen at L2 cache for write requests from the texture cache Multi-context l2_tex_write_transactions Memory write transactions seen at L2 cache for write requests from the texture cache Multi-context l2_utilization The utilization level of the L2 cache relative to the peak utilization on a scale of 0 to 10 Multi-context l2_write_throughput Memory write throughput seen at L2 cache for all write requests Multi-context l2_write_transactions Memory write transactions seen at L2 cache for all write requests Multi-context ldst_executed Number of executed local, global, shared and texture memory load and store instructions Multi-context ldst_fu_utilization The utilization level of the multiprocessor function units that execute shared load, shared store and constant load instructions on a scale of 0 to 10 Multi-context ldst_issued Number of issued local, global, shared and texture memory load and store instructions Multi-context local_hit_rate Hit rate for local loads and stores Multi-context local_load_requests Total number of local load requests from Multiprocessor Multi-context local_load_throughput Local memory load throughput Multi-context local_load_transactions Number of local memory load transactions Multi-context local_load_transactions_per_request Average number of local memory load transactions performed for each local memory load Multi-context local_memory_overhead Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage Multi-context local_store_requests Total number of local store requests from Multiprocessor Multi-context local_store_throughput Local memory store throughput Multi-context local_store_transactions Number of local memory store transactions Multi-context local_store_transactions_per_request Average number of local memory store transactions performed for each local memory store Multi-context nvlink_overhead_data_received Ratio of overhead data to the total data, received through NVLink. This is available for compute capability 6.0. Device nvlink_overhead_data_transmitted Ratio of overhead data to the total data, transmitted through NVLink. This is available for compute capability 6.0. Device nvlink_receive_throughput Number of bytes received per second through NVLinks. This is available for compute capability 6.0. Device nvlink_total_data_received Total data bytes received through NVLinks including headers. This is available for compute capability 6.0. Device nvlink_total_data_transmitted Total data bytes transmitted through NVLinks including headers. This is available for compute capability 6.0. Device nvlink_total_nratom_data_transmitted Total non-reduction atomic data bytes transmitted through NVLinks. This is available for compute capability 6.0. Device nvlink_total_ratom_data_transmitted Total reduction atomic data bytes transmitted through NVLinks This is available for compute capability 6.0. Device nvlink_total_response_data_received Total response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests. This is available for compute capability 6.0. Device nvlink_total_write_data_transmitted Total write data bytes transmitted through NVLinks. This is available for compute capability 6.0. Device nvlink_transmit_throughput Number of Bytes Transmitted per second through NVLinks. This is available for compute capability 6.0. Device nvlink_user_data_received User data bytes received through NVLinks, doesn’t include headers. This is available for compute capability 6.0. Device nvlink_user_data_transmitted User data bytes transmitted through NVLinks, doesn’t include headers. This is available for compute capability 6.0. Device nvlink_user_nratom_data_transmitted Total non-reduction atomic user data bytes transmitted through NVLinks. This is available for compute capability 6.0. Device nvlink_user_ratom_data_transmitted Total reduction atomic user data bytes transmitted through NVLinks. This is available for compute capability 6.0. Device nvlink_user_response_data_received Total user response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests. This is available for compute capability 6.0. Device nvlink_user_write_data_transmitted User write data bytes transmitted through NVLinks. This is available for compute capability 6.0. Device pcie_total_data_received Total data bytes received through PCIe Device pcie_total_data_transmitted Total data bytes transmitted through PCIe Device shared_efficiency Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage Multi-context shared_load_throughput Shared memory load throughput Multi-context shared_load_transactions Number of shared memory load transactions Multi-context shared_load_transactions_per_request Average number of shared memory load transactions performed for each shared memory load Multi-context shared_store_throughput Shared memory store throughput Multi-context shared_store_transactions Number of shared memory store transactions Multi-context shared_store_transactions_per_request Average number of shared memory store transactions performed for each shared memory store Multi-context shared_utilization The utilization level of the shared memory relative to peak utilization on a scale of 0 to 10 Multi-context single_precision_fu_utilization The utilization level of the multiprocessor function units that execute single-precision floating-point instructions and integer instructions on a scale of 0 to 10 Multi-context sm_efficiency The percentage of time at least one warp is active on a specific multiprocessor Multi-context special_fu_utilization The utilization level of the multiprocessor function units that execute sin, cos, ex2, popc, flo, and similar instructions on a scale of 0 to 10 Multi-context stall_constant_memory_dependency Percentage of stalls occurring because of immediate constant cache miss Multi-context stall_exec_dependency Percentage of stalls occurring because an input required by the instruction is not yet available Multi-context stall_inst_fetch Percentage of stalls occurring because the next assembly instruction has not yet been fetched Multi-context stall_memory_dependency Percentage of stalls occurring because a memory operation cannot be performed due to the required resources not being available or fully utilized, or because too many requests of a given type are outstanding Multi-context stall_memory_throttle Percentage of stalls occurring because of memory throttle Multi-context stall_not_selected Percentage of stalls occurring because warp was not selected Multi-context stall_other Percentage of stalls occurring due to miscellaneous reasons Multi-context stall_pipe_busy Percentage of stalls occurring because a compute operation cannot be performed because the compute pipeline is busy Multi-context stall_sync Percentage of stalls occurring because the warp is blocked at a __syncthreads() call Multi-context stall_texture Percentage of stalls occurring because the texture sub-system is fully utilized or has too many outstanding requests Multi-context surface_atomic_requests Total number of surface atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context surface_load_requests Total number of surface load requests from Multiprocessor Multi-context surface_reduction_requests Total number of surface reduction requests from Multiprocessor Multi-context surface_store_requests Total number of surface store requests from Multiprocessor Multi-context sysmem_read_bytes Number of bytes read from system memory Multi-context sysmem_read_throughput System memory read throughput Multi-context sysmem_read_transactions Number of system memory read transactions Multi-context sysmem_read_utilization The read utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 6.0 and 6.1. Multi-context sysmem_utilization The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 6.0 and 6.1. Multi-context sysmem_write_bytes Number of bytes written to system memory Multi-context sysmem_write_throughput System memory write throughput Multi-context sysmem_write_transactions Number of system memory write transactions Multi-context sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 6.0 and 6.1. Multi-context tex_cache_hit_rate Unified cache hit rate Multi-context tex_cache_throughput Unified cache throughput Multi-context tex_cache_transactions Unified cache read transactions Multi-context tex_fu_utilization The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10 Multi-context tex_utilization The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10 Multi-context texture_load_requests Total number of texture Load requests from Multiprocessor Multi-context unique_warps_launched Number of warps launched. Value is unaffected by compute preemption. Multi-context warp_execution_efficiency Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor Multi-context warp_nonpred_execution_efficiency Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor Multi-context 9.3. Metrics for Capability 7.x  Devices with compute capability 7.x implement the metrics shown in the following table. (7.x refers to 7.0 and 7.2 here.) Table 6. Capability 7.x (7.0 and 7.2) Metrics  Metric Name Description Scope achieved_occupancy Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor Multi-context atomic_transactions Global memory atomic and reduction transactions Multi-context atomic_transactions_per_request Average number of global memory atomic and reduction transactions performed for each atomic and reduction instruction Multi-context branch_efficiency Ratio of branch instruction to sum of branch and divergent branch instruction Multi-context cf_executed Number of executed control-flow instructions Multi-context cf_fu_utilization The utilization level of the multiprocessor function units that execute control-flow instructions on a scale of 0 to 10 Multi-context cf_issued Number of issued control-flow instructions Multi-context double_precision_fu_utilization The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10 Multi-context dram_read_bytes Total bytes read from DRAM to L2 cache Multi-context dram_read_throughput Device memory read throughput Multi-context dram_read_transactions Device memory read transactions Multi-context dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context dram_write_bytes Total bytes written from L2 cache to DRAM Multi-context dram_write_throughput Device memory write throughput Multi-context dram_write_transactions Device memory write transactions Multi-context eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. Multi-context flop_count_dp_add Number of double-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_dp_fma Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_dp_mul Number of double-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate contributes 2 or 4 to the count based on the number of inputs. Multi-context flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate contributes 2 or 4 to the count based on the number of inputs. Multi-context flop_count_hp_mul Number of half-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. The count does not include special operations. Multi-context flop_count_sp_add Number of single-precision floating-point add operations executed by non-predicated threads. Multi-context flop_count_sp_fma Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. Multi-context flop_count_sp_mul Number of single-precision floating-point multiply operations executed by non-predicated threads. Multi-context flop_count_sp_special Number of single-precision floating-point special operations executed by non-predicated threads. Multi-context flop_dp_efficiency Ratio of achieved to peak double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations Multi-context flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage. Multi-context gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context gld_transactions Number of global memory load transactions Multi-context gld_transactions_per_request Average number of global memory load transactions performed for each global memory load. Multi-context global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global load and store in unified l1/tex cache Multi-context global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor. This does not include atomic requests. Multi-context gst_efficiency Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage. Multi-context gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context gst_transactions Number of global memory store transactions Multi-context gst_transactions_per_request Average number of global memory store transactions performed for each global memory store Multi-context half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions on a scale of 0 to 10. Note that this doesn’t specify the utilization level of tensor core unit Multi-context inst_bit_convert Number of bit-conversion instructions executed by non-predicated threads Multi-context inst_compute_ld_st Number of compute load/store instructions executed by non-predicated threads Multi-context inst_control Number of control-flow instructions executed by non-predicated threads (jump, branch, etc.) Multi-context inst_executed The number of instructions executed Multi-context inst_executed_global_atomics Warp level instructions for global atom and atom cas Multi-context inst_executed_global_loads Warp level instructions for global loads Multi-context inst_executed_global_reductions Warp level instructions for global reductions Multi-context inst_executed_global_stores Warp level instructions for global stores Multi-context inst_executed_local_loads Warp level instructions for local loads Multi-context inst_executed_local_stores Warp level instructions for local stores Multi-context inst_executed_shared_atomics Warp level shared instructions for atom and atom CAS Multi-context inst_executed_shared_loads Warp level instructions for shared loads Multi-context inst_executed_shared_stores Warp level instructions for shared stores Multi-context inst_executed_surface_atomics Warp level instructions for surface atom and atom cas Multi-context inst_executed_surface_loads Warp level instructions for surface loads Multi-context inst_executed_surface_reductions Warp level instructions for surface reductions Multi-context inst_executed_surface_stores Warp level instructions for surface stores Multi-context inst_executed_tex_ops Warp level instructions for texture Multi-context inst_fp_16 Number of half-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_fp_32 Number of single-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_fp_64 Number of double-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) Multi-context inst_integer Number of integer instructions executed by non-predicated threads Multi-context inst_inter_thread_communication Number of inter-thread communication instructions executed by non-predicated threads Multi-context inst_issued The number of instructions issued Multi-context inst_misc Number of miscellaneous instructions executed by non-predicated threads Multi-context inst_per_warp Average number of instructions executed by each warp Multi-context inst_replay_overhead Average number of replays for each instruction executed Multi-context ipc Instructions executed per cycle Multi-context issue_slot_utilization Percentage of issue slots that issued at least one instruction, averaged across all cycles Multi-context issue_slots The number of issue slots used Multi-context issued_ipc Instructions issued per cycle Multi-context l2_atomic_throughput Memory read throughput seen at L2 cache for atomic and reduction requests Multi-context l2_atomic_transactions Memory read transactions seen at L2 cache for atomic and reduction requests Multi-context l2_global_atomic_store_bytes Bytes written to L2 from L1 for global atomics (ATOM and ATOM CAS) Multi-context l2_global_load_bytes Bytes read from L2 for misses in L1 for global loads Multi-context l2_local_global_store_bytes Bytes written to L2 from L1 for local and global stores. This does not include global atomics. Multi-context l2_local_load_bytes Bytes read from L2 for misses in L1 for local loads Multi-context l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context l2_surface_load_bytes Bytes read from L2 for misses in L1 for surface loads Multi-context l2_surface_store_bytes Bytes read from L2 for misses in L1 for surface stores Multi-context l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache Multi-context l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context l2_tex_write_hit_rate Hit Rate at L2 cache for all write requests from texture cache Multi-context l2_tex_write_throughput Memory write throughput seen at L2 cache for write requests from the texture cache Multi-context l2_tex_write_transactions Memory write transactions seen at L2 cache for write requests from the texture cache Multi-context l2_utilization The utilization level of the L2 cache relative to the peak utilization on a scale of 0 to 10 Multi-context l2_write_throughput Memory write throughput seen at L2 cache for all write requests Multi-context l2_write_transactions Memory write transactions seen at L2 cache for all write requests Multi-context ldst_executed Number of executed local, global, shared and texture memory load and store instructions Multi-context ldst_fu_utilization The utilization level of the multiprocessor function units that execute shared load, shared store and constant load instructions on a scale of 0 to 10 Multi-context ldst_issued Number of issued local, global, shared and texture memory load and store instructions Multi-context local_hit_rate Hit rate for local loads and stores Multi-context local_load_requests Total number of local load requests from Multiprocessor Multi-context local_load_throughput Local memory load throughput Multi-context local_load_transactions Number of local memory load transactions Multi-context local_load_transactions_per_request Average number of local memory load transactions performed for each local memory load Multi-context local_memory_overhead Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage Multi-context local_store_requests Total number of local store requests from Multiprocessor Multi-context local_store_throughput Local memory store throughput Multi-context local_store_transactions Number of local memory store transactions Multi-context local_store_transactions_per_request Average number of local memory store transactions performed for each local memory store Multi-context nvlink_overhead_data_received Ratio of overhead data to the total data, received through NVLink. Device nvlink_overhead_data_transmitted Ratio of overhead data to the total data, transmitted through NVLink. Device nvlink_receive_throughput Number of bytes received per second through NVLinks. Device nvlink_total_data_received Total data bytes received through NVLinks including headers. Device nvlink_total_data_transmitted Total data bytes transmitted through NVLinks including headers. Device nvlink_total_nratom_data_transmitted Total non-reduction atomic data bytes transmitted through NVLinks. Device nvlink_total_ratom_data_transmitted Total reduction atomic data bytes transmitted through NVLinks. Device nvlink_total_response_data_received Total response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests. Device nvlink_total_write_data_transmitted Total write data bytes transmitted through NVLinks. Device nvlink_transmit_throughput Number of Bytes Transmitted per second through NVLinks. Device nvlink_user_data_received User data bytes received through NVLinks, doesn’t include headers. Device nvlink_user_data_transmitted User data bytes transmitted through NVLinks, doesn’t include headers. Device nvlink_user_nratom_data_transmitted Total non-reduction atomic user data bytes transmitted through NVLinks. Device nvlink_user_ratom_data_transmitted Total reduction atomic user data bytes transmitted through NVLinks. Device nvlink_user_response_data_received Total user response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests. Device nvlink_user_write_data_transmitted User write data bytes transmitted through NVLinks. Device pcie_total_data_received Total data bytes received through PCIe Device pcie_total_data_transmitted Total data bytes transmitted through PCIe Device shared_efficiency Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage Multi-context shared_load_throughput Shared memory load throughput Multi-context shared_load_transactions Number of shared memory load transactions Multi-context shared_load_transactions_per_request Average number of shared memory load transactions performed for each shared memory load Multi-context shared_store_throughput Shared memory store throughput Multi-context shared_store_transactions Number of shared memory store transactions Multi-context shared_store_transactions_per_request Average number of shared memory store transactions performed for each shared memory store Multi-context shared_utilization The utilization level of the shared memory relative to peak utilization on a scale of 0 to 10 Multi-context single_precision_fu_utilization The utilization level of the multiprocessor function units that execute single-precision floating-point instructions on a scale of 0 to 10 Multi-context sm_efficiency The percentage of time at least one warp is active on a specific multiprocessor Multi-context special_fu_utilization The utilization level of the multiprocessor function units that execute sin, cos, ex2, popc, flo, and similar instructions on a scale of 0 to 10 Multi-context stall_constant_memory_dependency Percentage of stalls occurring because of immediate constant cache miss Multi-context stall_exec_dependency Percentage of stalls occurring because an input required by the instruction is not yet available Multi-context stall_inst_fetch Percentage of stalls occurring because the next assembly instruction has not yet been fetched Multi-context stall_memory_dependency Percentage of stalls occurring because a memory operation cannot be performed due to the required resources not being available or fully utilized, or because too many requests of a given type are outstanding Multi-context stall_memory_throttle Percentage of stalls occurring because of memory throttle Multi-context stall_not_selected Percentage of stalls occurring because warp was not selected Multi-context stall_other Percentage of stalls occurring due to miscellaneous reasons Multi-context stall_pipe_busy Percentage of stalls occurring because a compute operation cannot be performed because the compute pipeline is busy Multi-context stall_sleeping Percentage of stalls occurring because warp was sleeping Multi-context stall_sync Percentage of stalls occurring because the warp is blocked at a __syncthreads() call Multi-context stall_texture Percentage of stalls occurring because the texture sub-system is fully utilized or has too many outstanding requests Multi-context surface_atomic_requests Total number of surface atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context surface_load_requests Total number of surface load requests from Multiprocessor Multi-context surface_reduction_requests Total number of surface reduction requests from Multiprocessor Multi-context surface_store_requests Total number of surface store requests from Multiprocessor Multi-context sysmem_read_bytes Number of bytes read from system memory Multi-context sysmem_read_throughput System memory read throughput Multi-context sysmem_read_transactions Number of system memory read transactions Multi-context sysmem_read_utilization The read utilization level of the system memory relative to the peak utilization on a scale of 0 to 10 Multi-context sysmem_utilization The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10 Multi-context sysmem_write_bytes Number of bytes written to system memory Multi-context sysmem_write_throughput System memory write throughput Multi-context sysmem_write_transactions Number of system memory write transactions Multi-context sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10 Multi-context tensor_precision_fu_utilization The utilization level of the multiprocessor function units that execute tensor core instructions on a scale of 0 to 10 Multi-context tensor_int_fu_utilization The utilization level of the multiprocessor function units that execute tensor core int8 instructions on a scale of 0 to 10. This metric is only available for device with compute capability 7.2. Multi-context tex_cache_hit_rate Unified cache hit rate Multi-context tex_cache_throughput Unified cache to Multiprocessor read throughput Multi-context tex_cache_transactions Unified cache to Multiprocessor read transactions Multi-context tex_fu_utilization The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10 Multi-context tex_utilization The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10 Multi-context texture_load_requests Total number of texture Load requests from Multiprocessor Multi-context warp_execution_efficiency Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor Multi-context warp_nonpred_execution_efficiency Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor Multi-context 10. Warp State  This section contains a description of each warp state. The warp can have following states: Instruction issued - An instruction or a pair of independent instructions was issued from a warp. Stalled - Warp can be stalled for one of the following reasons. The stall reason distribution can be seen at source level in PC Sampling View or at kernel level in Latency analysis using ‘Examine Stall Reasons’ Stalled for instruction fetch - The next instruction was not yet available. To reduce instruction fetch stalls: If large loop have been unrolled in kernel, try reducing them. If the kernel contains many calls to small function, try inlining more of them with the __inline__ or __forceinline__ qualifiers. Conversely, if inlining many functions or large functions, try __noinline__ to disable inlining of those functions. For very short kernels, consider fusing into a single kernels. If blocks with fewer threads are used, consider using fewer blocks of more threads. Occasional calls to __syncthreads() will then keep the warps in sync which may improve instruction cache hit rate. Stalled for execution dependency - The next instruction is waiting for one or more of its inputs to be computed by earlier instruction(s). To reduce execution dependency stalls, try to increase instruction-level parallelism (ILP). This can be done by, for example, increasing loop unrolling or processing several elements per thread. This prevents the thread from idling through the full latency of each instruction. Stalled for memory dependency - The next instruction is waiting for a previous memory accesses to complete. To reduce the memory dependency stalls Try to improve memory coalescing and/or efficiency of bytes fetched (alignment, etc.). Look at the source level analysis ‘Global Memory Access Pattern’ and/or the metrics gld_efficiency and gst_efficiency. Try to increase memory-level parallelism (MLP): the number of independent memory operations in flight per thread. Loop unrolling, loading vector types such as float4, and processing multiple elements per thread are all ways to increase memory-level parallelism. Consider moving frequently-accessed data closer to SM, such as by use of shared memory or read-only data cache. Consider re-computing data where possible instead of loading it from device memory. If local memory accesses are high, consider increasing register count per thread to reduce spilling, even at the expense of occupancy since local memory accesses are cached only in L2 for GPUs with compute capability major = 5. Stalled for memory throttle - A large number of outstanding memory requests prevents forward progress. On GPUs with compute capability major = 3, memory throttle indicates high number of memory replays. To reduce memory throttle stalls: Try to find ways to combine several memory transactions into one (e.g., use 64-bit memory requests instead of two 32-bit requests). Check for un-coalesced memory accesses using the source level analysis ‘Global Memory Access Pattern’ and/or the profiler metrics gld_efficiency and gst_efficiency; minimize them wherever possible. On GPUs with compute capability major >= 3, consider using read-only data cache using LDG for un-coalesced global reads Stalled for texture - The texture sub-system is fully utilized or has too many outstanding requests. To reduce texture stalls: Consider combining several texture fetch operations into one (e.g., packing data in texture and unpacking in SM or using vector loads). Consider moving frequently-accessed data closer to SM by use of shared memory. Consider re-computing data where possible instead of fetching it from memory. On GPUs with compute capability major < 5: Consider changing some texture accesses into regular global loads to reduce pressure on the texture unit, especially if you do not use texture-specific features such as interpolation. On GPUs with compute capability major = 3: If global loads through the read-only data cache (LDG) are the source of texture accesses for this kernel, consider changing some of them back to regular global loads. Note that if LDG is being generated due to use of the __ldg() intrinsic, this simply means changing back to a normal pointer dereference, but if LDG is being generated automatically by the compiler due to the use of the const and __restrict__ qualifiers, this may be more difficult. Stalled for sync - The warp is waiting for all threads to synchronize after a barrier instruction. To reduce sync stalls: Try to improve load balancing i.e. try to increase work done between synchronization points; consider reducing thread block size. Minimize use of threadfence_*(). On GPUs with compute capability major >= 3: If __syncthreads() is being used because of data exchange through shared memory within a threadblock, consider whether warp shuffle operations can be used in place of some of these exchange/synchronize sequences. Stalled for constant memory dependency - The warp is stalled on a miss in the cache for __constant__ memory and immediate. This may be high the first time each constant is accessed (e.g., at the beginning of a kernel). To reduce these stalls, Consider reducing use of __constant__ or increase kernel runtime by increasing block count Consider increasing number of items processed per thread Consider merging several kernels that use the same __constant__ data to amortize the cost of misses in the constant cache. Try using regular global memory accesses instead of constant memory accesses. Stalled for pipe busy - The warp is stalled because the functional unit required to execute the next instruction is busy. To reduce stalls due to pipe busy: Prefer high-throughput operations over low-throughput operations. If precision doesn’t matter, use float instead of double precision arithmetic. Look for arithmetic improvements (e.g., order-of-operations changes) that may be mathematically valid but unsafe for the compiler to do automatically. Due to e.g. floating-point non-associativity. Stalled for not selected - Warp was ready but did not get a chance to issue as some other warp was selected for issue. This reason generally indicates that kernel is possibly optimized well but in some cases, you may be able to decrease occupancy without impacting latency hiding, and doing so may help improve cache hit rates. Stalled for other - Warp is blocked for an uncommon reason like compiler or hardware reasons. Developers do not have control over these stalls. 11. Migrating to Nsight Tools from Visual Profiler and nvprof  Visual Profiler and nvprof will be deprecated in a future CUDA release. It is recommended to use next-generation tools NVIDIA Nsight Systems for GPU and CPU sampling and tracing and NVIDIA Nsight Compute for GPU kernel profiling. The new tools still offer the same profiling / optimization / deployment workflow. The type of data you need to look at is the same. The commands have changed and the output looks a little different. The new tools are powerful, fast, and feature rich, allowing you to find solutions even more quickly. NVIDIA Nsight Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs; from large servers to our smallest SoC. Refer to the Migrating from NVIDIA nvprof section section in the NVIDIA Nsight Systems User Guide NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. In addition, its baseline feature allows users to compare results within the tool. Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results. Refer to the nvprof Transition Guide section in the Nsight Compute CLI document. Refer to the Visual Profiler Transition Guide section in the Nsight Compute document. Also refer to the blog posts on how to move your development to the next-generation tools: Migrating to Nsight Tools from Visual Profiler and nvprof Transitioning to Nsight Systems from Visual Profiler and nvprof Using Nsight Compute to Inspect your Kernels Table 7. Which tools are available on which GPU architectures  GPU architecture Visual Profiler and nvprof Nsight Systems Nsight Compute Maxwell Yes No No Pascal Yes Yes No Volta Yes Yes Yes Turing Yes* Yes Yes Ampere and later GPU architectures No Yes Yes * Only Tracing functionality is supported - Timeline, Activity, API. CUDA kernel profiling functionality i.e. collecting GPU performance metrics is not supported. The following table maps the key features of Visual Profiler and nvprof to the NVIDIA Nsight tools Table 8. Mapping of key Visual Profiler and nvprof features  Visual Profiler/nvprof feature categories Nsight Systems Nsight Compute Timeline/Activity/API Tracing Yes CPU Sampling Yes OpenACC Yes OpenMP Yes MPI Yes MPS Yes Application Dependency Analysis Unified Memory Transfers Yes Unified Memory Page Faults Yes Application Unified Memory Analysis Application NVLink Analysis Yes (per kernel) Events and Metrics (per kernel) Yes Guided and Unguided Kernel Analysis Yes Kernel Source-Disassembly View Yes Kernel PC Sampling Yes NVTX Yes Yes Remote Profiling Yes Yes 12. Profiler Known Issues  The following are known issues with the current release. Visual Profiler and nvprof don’t support devices with compute capability 8.0 and higher. Next-gen tools NVIDIA Nsight Compute and NVIDIA Nsight Systems should be used instead. Starting with the CUDA 11.0, Visual Profiler and nvprof won’t support macOS as the target platform. However Visual Profiler supports remote profiling from the macOS host. This support is deprecated in the CUDA 12.5 release. Visual Profiler is provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on macOS. See Developer Tools for macOS for download instructions. Starting with CUDA 10.2, Visual Profiler and nvprof use dynamic/shared CUPTI library. Thus it’s required to set the path to the CUPTI library before launching Visual Profiler and nvprof on Windows. CUPTI library can be found at \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\<cuda-toolkit>\\extras\\CUPTI\\lib64\" for Windows. A security vulnerability issue required profiling tools to disable features using GPU performance counters for non-root or non-admin users when using a Windows 419.17 or Linux 418.43 or later driver. By default, NVIDIA drivers require elevated permissions to access GPU performance counters. On Tegra platforms, profile as root or using sudo. On other platforms, you can either start profiling as root or using sudo, or by enabling non-admin profiling. More details about the issue and the solutions can be found on the ERR_NVGPUCTRPERM web page . Note Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms only, Tegra platforms require root or sudo access. Use of the environment variable LD_PRELOAD to load some versions of MPI libraries may result in a crash on Linux platforms. The workaround is to start the profiling session as a root user. For the normal user, the SUID permission for nvprof must be set. To ensure that all profile data is collected and flushed to a file, cudaDeviceSynchronize() followed by either cudaProfilerStop() or cuProfilerStop() should be called before the application exits. Refer the section Flush Profile Data . Concurrent kernel mode can add significant overhead if used on kernels that execute a large number of blocks and that have short execution durations. If the kernel launch rate is very high, the device memory used to collect profiling data can run out. In such a case some profiling data might be dropped. This will be indicated by a warning. When profiling an application that uses CUDA Dynamic Parallelism (CDP) there are several limitations to the profiling tools. CDP kernel launch tracing has a limitation for devices with compute capability 7.0 and higher. Profiler traces all the host launched kernels until it encounters a host launched kernel which launches child kernels. Subsequent kernels are not traced. Source level analysis is not supported on devices with compute capability 7.0 and higher. The Visual Profiler timeline does not display CUDA API calls invoked from within device-launched kernels. The Visual Profiler does not display detailed event, metric, and source-level results for device-launched kernels. Event, metric, and source-level results collected for CPU-launched kernels will include event, metric, and source-level results for the entire call-tree of kernels launched from within that kernel. The nvprof event/metric output does not include results for device-launched kernels. Events/metrics collected for CPU-launched kernels will include events/metrics for the entire call-tree of kernels launched from within that kernel. Profiling APK binaries is not supported. Unified memory profiling is not supported on the ARM architecture (aarch64). When profiling an application in which a device kernel was stopped due to an assertion the profiling data will be incomplete and a warning or error message is displayed. But the message is not precise as the exact cause of the failure is not detected. For dependency analysis, in cases where activity timestamps in the trace are slightly distorted such that they violate the programming model constraints, no dependencies or waiting times can be analyzed. Devices with compute capability 6.0 and higher introduce a new feature, compute preemption, to give fair chance for all compute contexts while running long tasks. With compute preemption feature- If multiple contexts are running in parallel it is possible that long kernels will get preempted. Some kernels may get preempted occasionally due to timeslice expiry for the context. If kernel has been preempted, the time the kernel spends preempted is still counted towards kernel duration. This can affect the kernel optimization priorities given by Visual Profiler as there is randomness introduced due to preemption. Compute preemption can affect events and metrics collection. The following are known issues with the current release: Events and metrics collection for a MPS client can result in higher counts than expected on devices with compute capability 7.0 and higher, since MPS client may get preempted due to termination of another MPS client. Events warps_launched and sm_cta_launched and metric inst_per_warp might provide higher counts than expected on devices with compute capability 6.0 and 6.1. Metric unique_warps_launched can be used in place of warps_launched to get correct count of actual warps launched as it is not affected by compute preemption. To avoid compute preemption affecting profiler results try to isolate the context being profiled: Run the application on secondary GPU where display is not connected. On Linux if the application is running on the primary GPU where the display driver is connected then unload the display driver. Run only one process that uses GPU at one time. Devices with compute capability 6.0 and higher support demand paging. When the kernel is scheduled for the first time, all the pages allocated using cudaMallocManaged and that are required for execution of the kernel are fetched in the global memory when GPU faults are generated. Profiler requires multiple passes to collect all the metrics required for kernel analysis. The kernel state needs to be saved and restored for each kernel replay pass. For devices with compute capability 6.0 and higher and platforms supporting Unified memory, in the first kernel iteration the GPU faults will be generated and all pages will be fetched in the global memory. Second iteration onwards GPU page faults will not occur. This will significantly affect the memory related events and timing. The time taken from trace will include the time required to fetch the pages but most of the metrics profiled in multiple iterations will not include time/cycles required to fetch the pages. This causes inconsistency in the profiler results. CUDA device enumeration and order, typically controlled through environment variables CUDA_VISIBLE_DEVICES and CUDA_DEVICE_ORDER , should remain the same for the profiler and the application. CUDA profiling might not work on systems that contain a mixture of supported and unsupported GPUs. On such systems, either set option --devices to supported devices in nvprof , or set environment variable CUDA_VISIBLE_DEVICES before launching nvprof or the Visual Profiler. Because of the low resolution of the timer on Windows, the start and end timestamps can be same for activities having short execution duration on Windows. As a result, the nvprof and Visual Profiler report the following warning: “Found N invalid records in the result.” Profiler cannot interoperate with other Nvidia tools such as cuda-gdb, cuda-memcheck, Nsight Systems and Nsight Compute. OpenACC profiling might fail when OpenACC library is linked statically in the user application. This happens due to the missing definition of the OpenACC API routines needed for the OpenACC profiling, as compiler might ignore definitions for the functions not used in the application. This issue can be mitigated by linking the OpenACC library dynamically. Visual Profiler and nvprof versions shipped in the CUDA Toolkit 11.7 and CUDA Toolkit 11.8 don’t support Kepler (sm_35 and sm_37) devices. This issue can be resolved by upgrading the CUPTI library. Refer to the webpages CUPTI 11.7 and CUPTI 11.8 for location of the CUPTI packages having the support for these Kepler devices. Profiler is not supported on below system configurations: 64-bit ARM Server CPU architecture (arm64 SBSA). Virtual GPUs (vGPU). Windows Subsystem for Linux (WSL). NVIDIA Crypto Mining Processors (CMP). For more information, please visit the web page . Visual Profiler The following are known issues related to Visual Profiler: Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system. However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install the required version of JRE 1.8 in order to use Visual Profiler. Refer to the section Setting up Java Runtime Environment for more information. Some analysis results require metrics that are not available on all devices. When these analyses are attempted on a device where the metric is not available the analysis results will show that the required data is “not available”. Using the mouse wheel button to scroll does not work within the Visual Profiler on Windows. Since Visual Profiler uses nvprof for collecting profiling data, nvprof limitations also apply to Visual Profiler. Visual Profiler cannot load profiler data larger than the memory size limited by JVM or available memory on the system. Refer Improve Loading of Large Profiles for more information. Visual Profiler global menus do not show properly or are empty on some versions of Ubuntu. One workaround is to set environment variable “UBUNTU_MENUPROXY=0” before running Visual Profiler In the Visual Profiler the NVLink Analysis diagram can be incorrect after scrolling the diagram. This can be corrected by horizontally resizing the diagram panel. Visual Profiler might not be able to show NVLink events on the timeline when large number of samples are collected. To work around this issue, refresh the timeline by doing zoom-in or zoom-out. Alternate solution is to save and open the session. For unified memory profiling on a remote setup having different version of GCC than host machine, Visual Profiler might not be able to show the source code location for CPU page fault events. For unified memory profiling on a remote setup having different architecture than the host machine (x86 versus POWER), Visual Profiler might not be able to show the source code location for CPU page fault and allocation tracking events. Visual Profiler is not supported on the ARM architecture (aarch64). You can use Remote Profiling. Refer the Remote Profiling section for more information. Visual Profiler doesn’t support remote profiling for the Android target. The workaround is to run nvprof on the target and load the nvprof output in the Visual Profiler. For remote profiling, the CUDA Toolkit installed on the host system must support the target device on the remote system. Visual Profiler might show strange symbol fonts on platforms which don’t have required fonts installed. The splash screen that appears on Visual Profiler start-up is disabled on macOS. When using remote profiling if there is a connection failure due to key exchange failure, then you will get an error message “Unable to establish shell connection to ‘user @ xxx ’”. You can follow these steps to mitigate the issue. Check the SSH daemon config file (default path is /etc/ssh/sshd_config) on the target Comment out lines starting with: KexAlgorithms HostbasedAcceptedKeyTypes Ciphers HostKey AuthorizedKeysFile Re-generate keys sudo ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key Restart sshd service sudo services sshd restart Accessing the local help document from Visual Profiler leads to HTTP Error 500. The workaround is to refer to this document (online document or pdf). Visual Profiler can’t remote into a target machine running Ubuntu 20.04 and later. nvprof The following are known issues related to nvprof : nvprof cannot profile processes that fork() but do not then exec() . nvprof assumes it has access to the temporary directory on the system, which it uses to store temporary profiling data. On Linux the default is /tmp . On Windows it’s specified by the system environment variables. To specify a custom location, change $TMPDIR on Linux or %TMP% on Windows. When multiple nvprof processes are run simultaneously on the same node, there is an issue of contention for files under the temporary directory. One workaround is to set a different temporary directory for each process. Multiple nvprof processes running concurrently using application replay may generate incorrect results or no results at all. To work around this issue you need to set a unique temporary directory per process. Set NVPROF_TMPDIR before launching nvprof . To profile application on Android $TMPDIR environment variable has to be defined and point to a user-writable folder. Profiling results might be inconsistent when auto boost is enabled. nvprof tries to disable auto boost by default, it might fail to do so in some conditions, but profiling will continue. nvprof will report a warning when auto boost cannot be disabled. Note that auto boost is supported only on certain Tesla devices from the Kepler+ family. Profiling a C++ application which overloads the new operator at the global scope and uses any CUDA APIs like cudaMalloc() or cudaMallocManaged() inside the overloaded new operator will result in a hang. NVTX annotations will not work when profiling all processes using the nvprof option --profile-all-processes . It is advised to set the environment variable NVTX_INJECTION64_PATH to point to the profiler injection library, libcuinj64.so on Linux and cuinj64_*.dll on Windows, before launching the application. Events and Metrics The following are known issues related to Events and Metrics profiling: Profiling features for devices with compute capability 7.5 and higher are supported in the NVIDIA Nsight Compute . Visual Profiler does not support Guided Analysis, some stages under Unguided Analysis and events and metrics collection for devices with compute capability 7.5 and higher. One can launch the NVIDIA Nsight Compute UI for devices with compute capability 7.5 and higher from Visual Profiler. Also nvprof does not support query and collection of events and metrics, source level analysis and other options used for profiling on devices with compute capability 7.5 and higher. The NVIDIA Nsight Compute command line interface can be used for these features. Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU. In event or metric profiling, kernel launches are blocking. Thus kernels waiting on updates from host or another kernel may hang. This includes synchronization between the host and the device build upon value-based CUDA stream synchronization APIs such as cuStreamWaitValue32() and cuStreamWriteValue32() . Event and metric collection requiring multiple passes will not work with the nvprof kernel replay option for any kernel performing IPC or data communication between the kernel and CPU, kernel and regular CPU allocated memory, kernel and Peer GPU, or kernel and other Peer devices (e.g. GPU direct). For some metrics, the required events can only be collected for a single CUDA context. For an application that uses multiple CUDA contexts, these metrics will only be collected for one of the contexts. The metrics that can be collected only for a single CUDA context are indicated in the metric reference tables . Some metric values are calculated assuming a kernel is large enough to occupy all device multiprocessors with approximately the same amount of work. If a kernel launch does not have this characteristic, then those metric values may not be accurate. Some metrics are not available on all devices. To see a list of all available metrics on a particular NVIDIA GPU, type nvprof --query-metrics . You can also refer to the metric reference tables . The profilers may fail to collect events or metrics when “application replay” mode is turned on. This is most likely to happen if the application is multi-threaded and non-deterministic. Instead use “kernel replay” mode in this case. For applications that allocate large amount of device memory, the profiler may take significant time to collect all events or metrics when “kernel replay” mode is used. Instead use “application replay” mode in this case. Here are a couple of reasons why Visual Profiler may fail to gather metric or event information. More than one tool is trying to access the GPU. To fix this issue please make sure only one tool is using the GPU at any given point. Tools include Nsight Compute, Nsight Systems, Nsight Graphics, and applications that use either CUPTI or PerfKit API (NVPM) to read event values. More than one application is using the GPU at the same time Visual Profiler is profiling a CUDA application. To fix this issue please close all applications and just run the one with Visual Profiler. Interacting with the active desktop should be avoided while the application is generating event information. Please note that for some types of event Visual Profiler gathers events for only one context if the application is using multiple contexts within the same application. When collecting events or metrics with the --events , --metrics , or --analysis-metrics options, nvprof will use kernel replay to execute each kernel multiple times as needed to collect all the requested data. If a large number of events or metrics are requested then a large number of replays may be required, resulting in a significant increase in application execution time. Some events are not available on all devices. To see a list of all available events on a particular device, type nvprof --query-events . Enabling certain events can cause GPU kernels to run longer than the driver’s watchdog time-out limit. In these cases the driver will terminate the GPU kernel resulting in an application error and profiling data will not be available. Please disable the driver watchdog time out before profiling such long running CUDA kernels On Linux, setting the X Config option Interactive to false is recommended. For Windows, detailed information about TDR (Timeout Detection and Recovery) and how to disable it is available at https://docs.microsoft.com/en-us/windows-hardware/drivers/display/timeout-detection-and-recovery nvprof can give out of memory error for event and metrics profiling, it could be due to large number of instructions in the kernel. Profiling results might be incorrect for CUDA applications compiled with nvcc version older than 9.0 for devices with compute capability 6.0 and 6.1. It is advised to recompile the application with nvcc version 9.0 or later. Ignore this warning if code is already compiled with the recommended nvcc version. PC Sampling is not supported on Tegra platforms. Profiling is not supported for multidevice cooperative kernels, that is, kernels launched by using the API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice. Profiling is not supported for CUDA kernel nodes launched by a CUDA Graph. 13. Changelog  Profiler changes in CUDA 12.5 List of changes done as part of the CUDA Toolkit 12.5 release. Visual Profiler’s remote profiling support from the macOS host is deprecated. It will be dropped in an upcoming release. Support for IBM Power architecture is dropped. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 12.4 List of changes done as part of the CUDA Toolkit 12.4 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 12.3 List of changes done as part of the CUDA Toolkit 12.3 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 12.2 List of changes done as part of the CUDA Toolkit 12.2 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 12.1 List of changes done as part of the CUDA Toolkit 12.1 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 12.0 List of changes done as part of the CUDA Toolkit 12.0 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.8 List of changes done as part of the CUDA Toolkit 11.8 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.7 List of changes done as part of the CUDA Toolkit 11.7 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.6 List of changes done as part of the CUDA Toolkit 11.6 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.5 List of changes done as part of the CUDA Toolkit 11.5 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.4 List of changes done as part of the CUDA Toolkit 11.4 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.3 List of changes done as part of the CUDA Toolkit 11.3 release. Visual Profiler extends remote profiling support to macOS host running version 11 (Big Sur) on Intel x86_64 architecture. General bug fixes. Profiler changes in CUDA 11.2 List of changes done as part of the CUDA Toolkit 11.2 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.1 List of changes done as part of the CUDA Toolkit 11.1 release. General bug fixes. No new feature is added in this release. Profiler changes in CUDA 11.0 List of changes done as part of the CUDA Toolkit 11.0 release. Visual Profiler and nvprof don’t support devices with compute capability 8.0 and higher. Next-gen tools NVIDIA Nsight Compute and NVIDIA Nsight Systems should be used instead. Starting with the CUDA 11.0, Visual Profiler and nvprof won’t support Mac as the target platform. However Visual Profiler will continue to support remote profiling from the Mac host. Visual Profiler will be provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on Mac. Added support to trace Optix applications. Fixed the nvprof option –annotate-mpi which was broken since CUDA 10.0. Profiler changes in CUDA 10.2 List of changes done as part of the CUDA Toolkit 10.2 release. Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms. Note that events and metrics profiling is still restricted for non-root and non-admin users. More details about the issue and the solutions can be found on this web page . Starting with CUDA 10.2, Visual Profiler and nvprof use dynamic/shared CUPTI library. Thus it’s required to set the path to the CUPTI library before launching Visual Profiler and nvprof. CUPTI library can be found at /usr/local/<cuda-toolkit>/extras/CUPTI/lib64 or /usr/local/<cuda-toolkit>/targets/<arch>/lib for POSIX platforms and \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\<cuda-toolkit>\\extras\\CUPTI\\lib64\" for Windows. Profilers no longer turn off the performance characteristics of CUDA Graph when tracing the application. Added an option to enable/disable the OpenMP profiling in Visual Profiler. Fixed the incorrect timing issue for the asynchronous cuMemset/cudaMemset activity. Profiler changes in CUDA 10.1 Update 2 List of changes done as part of the CUDA Toolkit 10.1 Update 2 release. This release is focused on bug fixes and stability of the profiling tools. A security vulnerability issue required profiling tools to disable all the features for non-root or non-admin users. As a result, Visual Profiler and nvprof cannot profile the application when using a Windows 419.17 or Linux 418.43 or later driver. More details about the issue and the solutions can be found on this web page . Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system. However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install the required version of JRE 1.8 in order to use Visual Profiler. Refer to the section Setting up Java Runtime Environment for more information. Profiler changes in CUDA 10.1 List of changes done as part of the CUDA Toolkit 10.1 release. This release is focused on bug fixes and stability of the profiling tools. Support for NVTX string registration API nvtxDomainRegisterStringA(). Profiler changes in CUDA 10.0 List of changes done as part of the CUDA Toolkit 10.0 release. Added tracing support for devices with compute capability 7.5. Profiling features for devices with compute capability 7.5 and higher are supported in the NVIDIA Nsight Compute . Visual Profiler does not support Guided Analysis, some stages under Unguided Analysis and events and metrics collection for devices with compute capability 7.5 and higher. One can launch the NVIDIA Nsight Compute UI for devices with compute capability 7.5 and higher from Visual Profiler. Also nvprof does not support query and collection of events and metrics, source level analysis and other options used for profiling on devices with compute capability 7.5 and higher. The NVIDIA Nsight Compute command line interface can be used for these features. Visual Profiler and nvprof now support OpenMP profiling where available. See OpenMP for more information. Tracing support for CUDA kernels, memcpy and memset nodes launched by a CUDA Graph. Profiler supports version 3 of NVIDIA Tools Extension API (NVTX). This is a header-only implementation of NVTX version 2. Profiler changes in CUDA 9.2 List of changes done as part of the CUDA Toolkit 9.2 release. The Visual Profiler allows to switch multiple segments to non-segment mode for Unified Memory profiling on the timeline. Earlier it was restircted to single segment only. The Visual Profiler shows a summary view of the memory hierarchy of the CUDA programming model. This is available for devices with compute capability 5.0 and higher. Refer Memory Statistics for more information. The Visual Profiler can correctly import profiler data generated by nvprof when the option --kernels kernel-filter is used. nvprof supports display of basic PCIe topolgy including PCI bridges between NVIDIA GPUs and Host Bridge. To view and analyze bandwidth of memory transfers over PCIe topologies, new set of metrics to collect total data bytes transmitted and recieved through PCIe are added. Those give accumulated count for all devices in the system. These metrics are collected at the device level for the entire application. And those are made available for devices with compute capability 5.2 and higher. The Visual Profiler and nvprof added support for new metrics: Instruction executed for different types of load and store Total number of cached global/local load requests from SM to texture cache Global atomic/non-atomic/reduction bytes written to L2 cache from texture cache Surface atomic/non-atomic/reduction bytes written to L2 cache from texture cache Hit rate at L2 cache for all requests from texture cache Device memory (DRAM) read and write bytes The utilization level of the multiprocessor function units that execute tensor core instructions for devices with compute capability 7.0 nvprof allows to collect tracing infromation along with the profiling information in the same pass. Use new option --trace <api|gpu> to enable trace along with collection of events/metrics. Profiler changes in CUDA 9.1 List of changes done as part of the CUDA Toolkit 9.1 release. The Visual Profiler shows the breakdown of the time spent on the CPU for each thread in the CPU Details View . The Visual Profiler supports a new option to select the PC sampling frequency. The Visual Profiler shows NVLink version in the NVLink topology. nvprof provides the correlation ID when profiling data is generated in CSV format. Profiler changes in CUDA 9.0 List of changes done as part of the CUDA Toolkit 9.0 release. Visual Profiler and nvprof now support profiling on devices with compute capability 7.0. Tools and extensions for profiling are hosted on Github at https://github.com/NVIDIA/cuda-profiler There are several enhancements to Unified Memory profiling: The Visual Profiler now associates unified memory events with the source code at which the memory is allocated. The Visual Profiler now correlates a CPU page fault to the source code resulting in the page fault. New Unified Memory profiling events for page thrashing, throttling and remote map are added. The Visual Profiler provides an option to switch between segment and non-segment mode on the timeline. The Visual Profiler supports filtering of Unified Memory profiling events based on the virtual address, migration reason or the page fault access type. CPU page fault support is extended to Mac platforms. Tracing and profiling of cooperative kernel launches is supported. The Visual Profiler shows NVLink events on the timeline. The Visual Profiler color codes links in the NVLink topology diagram based on throughput. The Visual Profiler supports new options to make it easier to do multi-hop remote profiling. nvprof supports a new option to select the PC sampling frequency. The Visual Profiler supports remote profiling to systems supporting ssh key exchange algorithms with a key length of 2048 bits. OpenACC profiling is now also supported on non-NVIDIA systems. nvprof flushes all profiling data when a SIGINT or SIGKILL signal is encountered. Profiler changes in CUDA 8.0 List of changes done as part of the CUDA Toolkit 8.0 release. Visual Profiler and nvprof now support NVLink analysis for devices with compute capability 6.0. See NVLink view for more information. Visual Profiler and nvprof now support dependency analysis which enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams. It allows computing the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams. See Dependency Analysis for more information. Visual Profiler and nvprof now support OpenACC profiling. See OpenACC for more information. Visual Profiler now supports CPU profiling. Refer CPU Details View and CPU Source View for more information. Unified Memory profiling now provides GPU page fault information on devices with compute capability 6.0 and 64 bit Linux platforms. Unified Memory profiling now provides CPU page fault information on 64 bit Linux platforms. Unified Memory profiling support is extended to the Mac platform. The Visual Profiler source-disassembly view has several enhancements. There is now a single integrated view for the different source level analysis results collected for a kernel instance. Results of different analysis steps can be viewed together. See Source-Disassembly View for more information. The PC sampling feature is enhanced to point out the true latency issues for devices with compute capability 6.0 and higher. Support for 16-bit floating point (FP16) data format profiling. If the new NVIDIA Tools Extension API(NVTX) feature of domains is used then Visual Profiler and nvprof will show the NVTX markers and ranges grouped by domain. The Visual Profiler now adds a default file extension .nvvp if an extension is not specified when saving or opening a session file. The Visual Profiler now supports timeline filtering options in create new session and import dialogs. Refer “Timeline Options” section under Creating a Session for more details. Profiler changes in CUDA 7.5 List of changes done as part of the CUDA Toolkit 7.5 release. Visual Profiler now supports PC sampling for devices with compute capability 5.2. Warp state including stall reasons are shown at source level for kernel latency analysis. See PC Sampling View for more information. Visual Profiler now supports profiling child processes and profiling all processes launched on the same system. See Creating a Session for more information on the new multi-process profiling options. For profiling CUDA applications using Multi-Process Service(MPS) see MPS profiling with Visual Profiler Visual Profiler import now supports browsing and selecting files on a remote system. nvprof now supports CPU profiling. See CPU Sampling for more information. All events and metrics for devices with compute capability 5.2 can now be collected accurately in presence of multiple contexts on the GPU. Profiler changes in CUDA 7.0 The profiling tools contain a number of changes and new features as part of the CUDA Toolkit 7.0 release. The Visual Profiler has been updated with several enhancements: Performance is improved when loading large data file. Memory usage is also reduced. Visual Profiler timeline is improved to view multi-gpu MPS profile data. Unified memory profiling is enhanced by providing fine grain data transfers to and from the GPU, coupled with more accurate timestamps with each transfer. nvprof has been updated with several enhancements: All events and metrics for devices with compute capability 3.x and 5.0 can now be collected accurately in presence of multiple contexts on the GPU. Profiler changes in CUDA 6.5 List of changes done as part of the CUDA Toolkit 6.5 release. The Visual Profiler kernel memory analysis has been updated with several enhancements: ECC overhead is added which provides a count of memory transactions required for ECC Under L2 cache a split up of transactions for L1 Reads, L1 Writes, Texture Reads, Atomic and Noncoherent reads is shown Under L1 cache a count of Atomic transactions is shown The Visual Profiler kernel profile analysis view has been updated with several enhancements: Initially the instruction with maximum execution count is highlighted A bar is shown in the background of the counter value for the “Exec Count” column to make it easier to identify instruction with high execution counts The current assembly instruction block is highlighted using two horizontal lines around the block. Also “next” and “previous” buttons are added to move to the next or previous block of assembly instructions. Syntax highlighting is added for the CUDA C source. Support is added for showing or hiding columns. A tooltip describing each column is added. nvprof now supports a new application replay mode for collecting multiple events and metrics. In this mode the application is run multiple times instead of using kernel replay. This is useful for cases when the kernel uses a large amount of device memory and use of kernel replay can be slow due to a high overhead of saving and restoring device memory for each kernel replay run. See Event/metric Summary Mode for more information. Visual Profiler also supports this new application replay mode and it can enabled in the Visual Profiler “New Session” dialog. Visual Profiler now displays peak single precision flops and double precision flops for a GPU under device properties. Improved source-to-assembly code correlation for CUDA Fortran applications compiled by the PGI CUDA Fortran compiler. Profiler changes in CUDA 6.0 List of changes done as part of the CUDA Toolkit 6.0 release. Unified Memory is fully supported by both the Visual Profiler and nvprof . Both profilers allow you to see the Unified Memory related memory traffic to and from each GPU on your system. The standalone Visual Profiler, nvvp , now provides a multi-process timeline view. You can import multiple timeline data sets collected with nvprof into nvvp and view them on the same timeline to see how they are sharing the GPU(s). This multi-process import capability also includes support for CUDA applications using MPS. See MPS Profiling for more information. The Visual Profiler now supports a remote profiling mode that allows you to collect a profile on a remote Linux system and view the timeline, analysis results, and detailed results on your local Linux, Mac, or Windows system. See Remote Profiling for more information. The Visual Profiler analysis system now includes a side-by-side source and disassembly view annotated with instruction execution counts, inactive thread counts, and predicated instruction counts. This new view enables you to find hotspots and inefficient code sequences within your kernels. The Visual Profiler analysis system has been updated with several new analysis passes: 1) kernel instructions are categorized into classes so that you can see if instruction mix matches your expectations, 2) inefficient shared memory access patterns are detected and reported, and 3) per-SM activity level is presented to help you detect detect load-balancing issues across the blocks of your kernel. The Visual Profiler guided analysis system can now generate a kernel analysis report. The report is a PDF version of the per-kernel information presented by the guided analysis system. Both nvvp and nvprof can now operate on a system that does not have an NVIDIA GPU. You can import profile data collected from another system and view and analyze it on your GPU-less system. Profiling overheads for both nvvp and nvprof have been significantly reduced. 14. Notices  14.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 14.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 14.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cufft/contents.html", "parent_url": "https://docs.nvidia.com/cuda/cufft/contents.html", "content_type": "text/html", "text": "Contents — cuFFT 12.5 documentation 1. Introduction 2. Using the cuFFT API 3. cuFFT API Reference 4. cuFFT Code Examples 5. Multiple GPU Data Organization 6. FFTW Conversion Guide 7. FFTW Interface to cuFFT 8. Deprecated Functionality 9. Notices cuFFT » Contents v12.5 | PDF | Archive Contents  1. Introduction 2. Using the cuFFT API 2.1. Accessing cuFFT 2.2. Fourier Transform Setup 2.2.1. Free Memory Requirement 2.2.2. Plan Initialization Time 2.3. Fourier Transform Types 2.3.1. Half-precision cuFFT Transforms 2.3.2. Bfloat16-precision cuFFT Transforms 2.4. Data Layout 2.5. Multidimensional Transforms 2.6. Advanced Data Layout 2.7. Streamed cuFFT Transforms 2.8. Multiple GPU cuFFT Transforms 2.8.1. Plan Specification and Work Areas 2.8.2. Helper Functions 2.8.3. Multiple GPU 2D and 3D Transforms on Permuted Input 2.8.4. Supported Functionality 2.9. cuFFT Callback Routines 2.9.1. Overview of the cuFFT Callback Routine Feature 2.9.2. Specifying Load and Store Callback Routines 2.9.3. Callback Routine Function Details 2.9.4. Coding Considerations for the cuFFT Callback Routine Feature 2.9.4.1. No Ordering Guarantees Within a Kernel 2.10. Thread Safety 2.11. CUDA Graphs Support 2.12. Static Library and Callback Support 2.12.1. Static library without callback support 2.13. Accuracy and Performance 2.14. Caller Allocated Work Area Support 2.15. cuFFT Link-Time Optimized Kernels 2.15.1. Overview of the cuFFT Callback Routine Feature 3. cuFFT API Reference 3.1. Return value cufftResult 3.2. cuFFT Basic Plans 3.2.1. cufftPlan1d() 3.2.2. cufftPlan2d() 3.2.3. cufftPlan3d() 3.2.4. cufftPlanMany() 3.3. cuFFT Extensible Plans 3.3.1. cufftCreate() 3.3.2. cufftDestroy() 3.3.3. cufftMakePlan1d() 3.3.4. cufftMakePlan2d() 3.3.5. cufftMakePlan3d() 3.3.6. cufftMakePlanMany() 3.3.7. cufftMakePlanMany64() 3.3.8. cufftXtMakePlanMany() 3.4. cuFFT Plan Properties 3.4.1. cufftSetPlanPropertyInt64() 3.4.2. cufftGetPlanPropertyInt64() 3.4.3. cufftResetPlanProperty() 3.5. cuFFT Estimated Size of Work Area 3.5.1. cufftEstimate1d() 3.5.2. cufftEstimate2d() 3.5.3. cufftEstimate3d() 3.5.4. cufftEstimateMany() 3.6. cuFFT Refined Estimated Size of Work Area 3.6.1. cufftGetSize1d() 3.6.2. cufftGetSize2d() 3.6.3. cufftGetSize3d() 3.6.4. cufftGetSizeMany() 3.6.5. cufftGetSizeMany64() 3.6.6. cufftXtGetSizeMany() 3.7. cufftGetSize() 3.8. cuFFT Caller Allocated Work Area Support 3.8.1. cufftSetAutoAllocation() 3.8.2. cufftSetWorkArea() 3.8.3. cufftXtSetWorkAreaPolicy() 3.9. cuFFT Execution 3.9.1. cufftExecC2C() and cufftExecZ2Z() 3.9.2. cufftExecR2C() and cufftExecD2Z() 3.9.3. cufftExecC2R() and cufftExecZ2D() 3.9.4. cufftXtExec() 3.9.5. cufftXtExecDescriptor() 3.10. cuFFT and Multiple GPUs 3.10.1. cufftXtSetGPUs() 3.10.2. cufftXtSetWorkArea() 3.10.3. cuFFT Multiple GPU Execution 3.10.3.1. cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z() 3.10.3.2. cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z() 3.10.3.3. cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D() 3.10.4. Memory Allocation and Data Movement Functions 3.10.4.1. cufftXtMalloc() 3.10.4.1.1. Parameter cufftXtSubFormat 3.10.4.2. cufftXtFree() 3.10.4.3. cufftXtMemcpy() 3.10.4.3.1. Parameter cufftXtCopyType 3.10.5. General Multiple GPU Descriptor Types 3.10.5.1. cudaXtDesc 3.10.5.2. cudaLibXtDesc 3.11. cuFFT Callbacks 3.11.1. cufftXtSetCallback() 3.11.2. cufftXtClearCallback() 3.11.3. cufftXtSetCallbackSharedSize() 3.12. cufftSetStream() 3.13. cufftGetVersion() 3.14. cufftGetProperty() 3.15. cuFFT Types 3.15.1. Parameter cufftType 3.15.2. Parameters for Transform Direction 3.15.3. Type definitions for callbacks 3.15.4. Other cuFFT Types 3.15.4.1. cufftHandle 3.15.4.2. cufftReal 3.15.4.3. cufftDoubleReal 3.15.4.4. cufftComplex 3.15.4.5. cufftDoubleComplex 3.16. Common types 3.16.1. cudaDataType 3.16.2. libraryPropertyType 4. cuFFT Code Examples 5. Multiple GPU Data Organization 5.1. Multiple GPU Data Organization for Batched Transforms 5.2. Multiple GPU Data Organization for Single 2D and 3D Transforms 5.3. Multiple-GPU Data Organization for Single 1D Transforms 6. FFTW Conversion Guide 7. FFTW Interface to cuFFT 8. Deprecated Functionality 9. Notices 9.1. Notice 9.2. OpenCL 9.3. Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jun 20, 2024."}, {"url": "https://docs.nvidia.com/cuda/ada-tuning-guide/contents.html", "parent_url": "https://docs.nvidia.com/cuda/ada-tuning-guide/contents.html", "content_type": "text/html", "text": "Contents — Ada Tuning Guide 12.5 documentation 1. NVIDIA Ada GPU Architecture Tuning Guide 2. Revision History 3. Notices Ada Tuning Guide » Contents v12.5 | PDF | Archive Contents  1. NVIDIA Ada GPU Architecture Tuning Guide 1.1. NVIDIA Ada GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ada GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Improved Tensor Core Operations 1.4.1.3. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased L2 capacity 1.4.2.2. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jun 20, 2024."}, {"url": "https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/contents.html", "parent_url": "https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/contents.html", "content_type": "text/html", "text": "Contents — PTX Interoperability 12.5 documentation 1. Introduction 2. Data Representation 3. Function Calling Sequence 4. System Calls 5. Debug Information 6. Example 7. C++ 8. Notices PTX Interoperability » Contents v12.5 | PDF | Archive Contents  1. Introduction 2. Data Representation 2.1. Fundamental Types 2.2. Aggregates and Unions 2.3. Bit Fields 2.4. Texture, Sampler, and Surface Types 3. Function Calling Sequence 3.1. Registers 3.2. Stack Frame 3.3. Parameter Passing 4. System Calls 5. Debug Information 5.1. Generation of Debug Information 5.2. CUDA-Specific DWARF Definitions 6. Example 7. C++ 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jun 20, 2024."}, {"url": "https://docs.nvidia.com/cuda/nvblas/contents.html", "parent_url": "https://docs.nvidia.com/cuda/nvblas/contents.html", "content_type": "text/html", "text": "Contents — NVBLAS 12.5 documentation 1. Introduction 2. NVBLAS Overview 3. GPU Accelerated Routines 4. BLAS Symbols Interception 5. Device Memory Support 6. Security Precaution 7. Configuration 8. NVBLAS Installation 9. Usage 10. Notices NVBLAS » Contents v12.5 | PDF | Archive Contents  1. Introduction 2. NVBLAS Overview 3. GPU Accelerated Routines 4. BLAS Symbols Interception 5. Device Memory Support 6. Security Precaution 7. Configuration 7.1. NVBLAS_CONFIG_FILE Environment Variable 7.2. Configuration Keywords 7.2.1. NVBLAS_LOGFILE 7.2.2. NVBLAS_TRACE_LOG_ENABLED 7.2.3. NVBLAS_CPU_BLAS_LIB 7.2.4. NVBLAS_GPU_LIST 7.2.5. NVBLAS_TILE_DIM 7.2.6. NVBLAS_GPU_DISABLED_<BLAS_FUNC_NAME> 7.2.7. NVBLAS_CPU_RATIO_<BLAS_FUNC_NAME> 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED 7.2.9. Configuration File Example 8. NVBLAS Installation 9. Usage 10. Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jun 20, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/notices.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/notices.html", "content_type": "text/html", "text": "13. Notices — CUDA Math API Reference Manual 12.5 documentation 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices CUDA Math API Reference Manual » 13. Notices v12.5 | PDF | Archive 13. Notices  13.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 13.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 13.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jun 20, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/structs.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/structs.html", "content_type": "text/html", "text": "12. Structs — CUDA Math API Reference Manual 12.5 documentation 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 12.1. __half 12.2. __half2 12.3. __half2_raw 12.4. __half_raw 12.5. __nv_bfloat16 12.6. __nv_bfloat162 12.7. __nv_bfloat162_raw 12.8. __nv_bfloat16_raw 12.9. __nv_fp8_e4m3 12.10. __nv_fp8_e5m2 12.11. __nv_fp8x2_e4m3 12.12. __nv_fp8x2_e5m2 12.13. __nv_fp8x4_e4m3 12.14. __nv_fp8x4_e5m2 13. Notices CUDA Math API Reference Manual » 12. Structs v12.5 | PDF | Archive 12. Structs  __half __half data type __half2 __half2 data type __half2_raw __half2_raw data type __half_raw __half_raw data type __nv_bfloat16 nv_bfloat16 datatype __nv_bfloat162 nv_bfloat162 datatype __nv_bfloat162_raw __nv_bfloat162_raw data type __nv_bfloat16_raw __nv_bfloat16_raw data type __nv_fp8_e4m3 __nv_fp8_e4m3 datatype __nv_fp8_e5m2 __nv_fp8_e5m2 datatype __nv_fp8x2_e4m3 __nv_fp8x2_e4m3 datatype __nv_fp8x2_e5m2 __nv_fp8x2_e5m2 datatype __nv_fp8x4_e4m3 __nv_fp8x4_e4m3 datatype __nv_fp8x4_e5m2 __nv_fp8x4_e5m2 datatype Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/nsight-compute/NsightCompute/index.html", "parent_url": "https://docs.nvidia.com/nsight-compute/NsightCompute/index.html", "content_type": "text/html", "text": "3. Nsight Compute — NsightCompute 12.5 documentation Nsight Compute 1. Release Notes 2. Kernel Profiling Guide 3. Nsight Compute 3.1. Introduction 3.1.1. Overview 3.2. Quickstart 3.2.1. Interactive Profile Activity 3.2.2. Non-Interactive Profile Activity 3.2.3. System Trace Activity 3.2.4. Navigate the Report 3.3. Connection Dialog 3.3.1. Remote Connections 3.3.2. Interactive Profile Activity 3.3.3. Profile Activity 3.3.4. Reset 3.4. Main Menu and Toolbar 3.4.1. Main Menu 3.4.2. Main Toolbar 3.4.3. Status Banners 3.5. Tool Windows 3.5.1. API Statistics 3.5.2. API Stream 3.5.3. Baselines 3.5.4. Metric Details 3.5.5. Launch Details Header Body 3.5.6. NVTX 3.5.7. CPU Call Stack 3.5.8. Resources Memory Allocations Graphviz DOT and SVG exports 3.5.9. Metric Selection 3.6. Profiler Report 3.6.1. Header 3.6.2. Report Pages Summary Page Details Page Source Page Navigation Metrics Profiles Limitations Context Page Comments Page Raw Page Session Page 3.6.3. Metrics and Units 3.7. Baselines 3.8. Standalone Source Viewer 3.9. Source Comparison 3.10. Occupancy Calculator 3.10.1. Tables 3.10.2. Graphs 3.10.3. GPU Data 3.11. Acceleration Structure Viewer 3.11.1. Navigation 3.11.2. Filtering and Highlighting 3.11.3. Rendering Options 3.11.4. Exporting 3.12. Options 3.12.1. Profile 3.12.2. Environment 3.12.3. Connection Target Connection Properties Host Connection Properties 3.12.4. Source Lookup 3.12.5. Send Feedback 3.13. Projects 3.13.1. Project Dialogs 3.13.2. Project Explorer 3.14. Visual Profiler Transition Guide 3.14.1. Trace 3.14.2. Sessions 3.14.3. Timeline 3.14.4. Analysis 3.14.5. Command Line Arguments 3.15. Visual Studio Integration Guide 3.15.1. Visual Studio Integration Overview 4. Nsight Compute CLI Developer Interfaces 1. Customization Guide 2. NvRules API Training Training Release Information Archives Copyright and Licenses Copyright and Licenses NsightCompute » 3. Nsight Compute v2024.2.1 | Archive 3. Nsight Compute  The User Guide for Nsight Compute. 3.1. Introduction  For users migrating from Visual Profiler to NVIDIA Nsight Compute, please see the Visual Profiler Transition Guide for comparison of features and workflows. 3.1.1. Overview  This document is a user guide to the next-generation NVIDIA Nsight Compute profiling tools. NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. In addition, its baseline feature allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results. Important Features Interactive kernel profiler and API debugger Graphical profile report Result comparison across one or multiple reports within the tool Fast Data Collection UI and Command Line interface Fully customizable reports and analysis rules 3.2. Quickstart  The following sections provide brief step-by-step guides of how to setup and run NVIDIA Nsight Compute to collect profile information. All directories are relative to the base directory of NVIDIA Nsight Compute, unless specified otherwise. The UI executable is called ncu-ui. A shortcut with this name is located in the base directory of the NVIDIA Nsight Compute installation. The actual executable is located in the folder host\\windows-desktop-win7-x64 on Windows or host/linux-desktop-glibc_2_11_3-x64 on Linux. By default, when installing from a Linux .run file, NVIDIA Nsight Compute is located in /usr/local/cuda-<cuda-version>/nsight-compute-<version> . When installing from a .deb or .rpm package, it is located in /opt/nvidia/nsight-compute/<version> to be consistent with Nsight Systems . In Windows, the default path is C:\\Program Files\\NVIDIA Corporation\\Nsight Compute <version> . After starting NVIDIA Nsight Compute, by default the Welcome Page is opened. The Start section allows the user to start a new activity, open an existing report, create a new project or load an existing project. The Continue section provides links to recently opened reports and projects. The Explore section provides information about what is new in the latest release, as well as links to additional training. See Environment on how to change the start-up action. Welcome Page  3.2.1. Interactive Profile Activity  Launch the target application from NVIDIA Nsight Compute When starting NVIDIA Nsight Compute, the Welcome Page will appear. Click on Quick Launch to open the Connection dialog. If the Connection dialog doesn’t appear, you can open it using the Connect button from the main toolbar, as long as you are not currently connected. Select your target platform on the left-hand side and your connection target (machine) from the Connection drop down. If you have your local target platform selected, localhost will become available as a connection. Use the + button to add a new connection target. Then, continue by filling in the details in the Launch tab. In the Activity panel, select the Interactive Profile activity to initiate a session that allows controlling the execution of the target application and selecting the kernels of interest interactively. Press Launch to start the session. Launch the target application with tools instrumentation from the command line The ncu can act as a simple wrapper that forces the target application to load the necessary libraries for tools instrumentation. The parameter --mode=launch specifies that the target application should be launched and suspended before the first instrumented API call. That way the application waits until we connect with the UI. $ ncu --mode=launch CuVectorAddDrv.exe Launch NVIDIA Nsight Compute and connect to target application Select the target machine at the top of the dialog to connect and update the list of attachable applications. By default, localhost is pre-selected if the target matches your current local platform. Select the Attach tab and the target application of interest and press Attach . Once connected, the layout of NVIDIA Nsight Compute changes into stepping mode that allows you to control the execution of any calls into the instrumented API. When connected, the API Stream window indicates that the target application waits before the very first API call. Control application execution Use the API Stream window to step the calls into the instrumented API. The dropdown at the top allows switching between different CPU threads of the application. Step In (F11), Step Over (F10), and Step Out (Shift + F11) are available from the Debug menu or the corresponding toolbar buttons. While stepping, function return values and function parameters are captured. Use Resume (F5) and Pause to allow the program to run freely. Freeze control is available to define the behavior of threads currently not in focus, i.e. selected in the thread drop down. By default, the API Stream stops on any API call that returns an error code. This can be toggled in the Debug menu by Break On API Error . Isolate a kernel launch To quickly isolate a kernel launch for profiling, use the Run to Next Kernel button in the toolbar of the API Stream window to jump to the next kernel launch. The execution will stop before the kernel launch is executed. Profile a kernel launch Once the execution of the target application is suspended at a kernel launch, additional actions become available in the UI. These actions are either available from the menu or from the toolbar. Please note that the actions are disabled, if the API stream is not at a qualifying state (not at a kernel launch or launching on an unsupported GPU). To profile, press Profile Kernel and wait until the result is shown in the Profiler Report . Profiling progress is reported in the lower right corner status bar. Instead of manually selecting Profile , it is also possible to enable Auto Profile from the Profile menu. If enabled, each kernel matching the current kernel filter (if any) will be profiled using the current section configuration. This is especially useful if an application is to be profiled unattended, or the number of kernel launches to be profiled is very large. Sections can be enabled or disabled using the Metric Selection tool window. Profile Series allows to configure the collection of a set of profile results at once. Each result in the set is profiled with varying parameters. Series are useful to investigate the behavior of a kernel across a large set of parameters without the need to recompile and rerun the application many times. For a detailed description of the options available in this activity, see Interactive Profile Activity . 3.2.2. Non-Interactive Profile Activity  Launch the target application from NVIDIA Nsight Compute When starting NVIDIA Nsight Compute, the Welcome Page will appear. Click on Quick Launch to open the Connection dialog. If the Connection dialog doesn’t appear, you can open it using the Connect button from the main toolbar, as long as you are not currently connected. Select your target platform on the left-hand side and your localhost from the Connection drop down. Then, fill in the launch details. In the Activity panel, select the Profile activity to initiate a session that pre-configures the profile session and launches the command line profiler to collect the data. Provide the Output File name to enable starting the session with the Launch button. Additional Launch Options For more details on these options, see Command Line Options . The options are grouped into tabs: The Filter tab exposes the options to specify which kernels should be profiled. Options include the kernel regex filter, the number of launches to skip, and the total number of launches to profile. The Sections tab allows you to select which sections should be collected for each kernel launch. Hover over a section to see its description as a tool-tip. To change the sections that are enabled by default, use the Metric Selection tool window. The Sampling tab allows you to configure sampling options for each kernel launch. The Other tab includes the option to collect NVTX information or custom metrics via the --metrics option. For a detailed description of the options available in this activity, see Profile Activity . 3.2.3. System Trace Activity  Launch the target application from NVIDIA Nsight Compute When starting NVIDIA Nsight Compute, the Welcome Page will appear. Click on Quick Launch to open the Connection dialog. If the Connection dialog doesn’t appear, you can open it using the Connect button from the main toolbar, as long as you are not currently connected. Select your local target platform on the left-hand side and your localhost from the Connection drop down. Then, fill in the launch details. In the Activity panel, select the System Trace activity to initiate a session with pre-configured settings. Press Launch to start the session. Additional Launch Options For more details on these options, see System-Wide Profiling Options . Once the session is completed, the Nsight Systems report is opened in a new document. By default, the timeline view is shown. It provides detailed information of the activity of the CPU and GPUs and helps understanding the overall behavior and performance of application. Once a CUDA kernel is identified to be on the critical path and not meeting the performance expectations, right click on the kernel launch on timeline and select Profile Kernel from the context menu. A new Connection Dialog opens up that is already preconfigured to profile the selected kernel launch. Proceed with optimizing the selected kernel using Non-Interactive Profile Activity 3.2.4. Navigate the Report  Navigate the report The profile report comes up by default on the Summary page. It shows an overview table to summarize all results in the report. It also shows rule information for the selected row. You can switch between different Report Pages using the tab bar on the top-left of the report.\nYou can also use Ctrl + Shift + N and Ctrl + Shift + P shortcut keys or corresponding toolbar button to navigate next and previous pages, respectively.\nA report can contain any number of results. The Current dropdown allows switching between the different results in a report. Diffing multiple results On the Details page, use the Compare - Add Baseline button for the current result to become the baseline all other results from this report and any other report opened in the same instance of NVIDIA Nsight Compute get compared to.\nWhen a baseline is set, every element on the Details page shows two values: The current value of the result in focus and the corresponding value of the baseline or the percentage of change from the corresponding baseline value. Use the Clear Baselines entry from the same group button, the Profile menu or the corresponding toolbar button to remove all baselines. For more information see Baselines . Following rules On the Details page, many sections provide rules with valuable information on detected problems and optimization suggestions.\nRules can be user-defined too. For more information, see the Customization Guide . 3.3. Connection Dialog  Use the Connection Dialog to launch and attach to applications on your local and remote platforms. Start by selecting the Target Platform for profiling. By default (and if supported) your local platform will be selected. Select the platform on which you would like to start the target application or connect to a running process. When using a remote platform, you will be asked to select or create a Connection in the top drop down. To create a new connection, select + and enter your connection details. When using the local platform, localhost will be selected as the default and no further connection settings are required. You can still create or select a remote connection, if profiling will be on a remote system of the same platform. Depending on your target platform, select either Launch or Remote Launch to launch an application for profiling on the target. Note that Remote Launch will only be available if supported on the target platform. Fill in the following launch details for the application: Application Executable: Specifies the root application to launch. Note that this may not be the final application that you wish to profile. It can be a script or launcher that creates other processes. Working Directory: The directory in which the application will be launched. Command Line Arguments: Specify the arguments to pass to the application executable. Environment: The environment variables to set for the launched application. Select Attach to attach the profiler to an application already running on the target platform. This application must have been started using another NVIDIA Nsight Compute CLI instance. The list will show all application processes running on the target system which can be attached. Select the refresh button to re-create this list. Finally, select the Activity to be run on the target for the launched or attached application. Note that not all activities are necessarily compatible with all targets and connection options. Currently, the following activities exist: Interactive Profile Activity Profile Activity System Trace Activity Occupancy Calculator 3.3.1. Remote Connections  Remote devices that support SSH can also be configured as a target in the Connection Dialog . To configure a remote device, ensure an SSH-capable Target Platform is selected, then press the + button. The following configuration dialog will be presented. NVIDIA Nsight Compute supports both password and private key authentication methods. In this dialog, select the authentication method and enter the following information: Password IP/Host Name: The IP address or host name of the target device. User Name: The user name to be used for the SSH connection. Password: The user password to be used for the SSH connection. Port: The port to be used for the SSH connection. (The default value is 22) Deployment Directory: The directory to use on the target device to deploy supporting files. The specified user must have write permissions to this location. Connection Name: The name of the remote connection that will show up in the Connection Dialog . If not set, it will default to <User>@<Host>:<Port>. Private Key IP/Host Name: The IP address or host name of the target device. User Name: The user name to be used for the SSH connection. SSH Private Key: The private key that is used to authenticate to SSH server. SSH Key Passphrase: The passphrase for your private key. Port: The port to be used for the SSH connection. (The default value is 22) Deployment Directory: The directory to use on the target device to deploy supporting files. The specified user must have write permissions to this location. Connection Name: The name of the remote connection that will show up in the Connection Dialog . If not set, it will default to <User>@<Host>:<Port>. In addition to keyfiles specified by path and plain password authentication, NVIDIA Nsight Compute supports keyboard-interactive authentication, standard keyfile path searching and SSH agents. When all information is entered, click the Add button to make use of this new connection. When a remote connection is selected in the Connection Dialog , the Application Executable file browser will browse the remote file system using the configured SSH connection, allowing the user to select the target application on the remote device. When an activity is launched on a remote device, the following steps are taken: The command line profiler and supporting files are copied into the Deployment Directory on the the remote device. (Only files that do not exist or are out of date are copied.) Communication channels are opened to prepare for the traffic between the UI and the Application Executable . For Interactive Profile activities, a SOCKS proxy is started on the host machine. For Non-Interactive Profile activities, a remote forwarding channel is opened on the target machine to tunnel profiling information back to the host. The Application Executable is executed on the remote device. For Interactive Profile activities, a connection is established to the remote application and the profiling session begins. For Non-Interactive Profile activities, the remote application is executed under the command line profiler and the specified report file is generated. For non-interactive profiling activities, the generated report file is copied back to the host, and opened. The progress of each of these steps is presented in the Progress Log . Progress Log  Note that once either activity type has been launched remotely, the tools necessary for further profiling sessions can be found in the Deployment Directory on the remote device. On Linux and Mac host platforms, NVIDIA Nsight Compute supports SSH remote profiling on target machines which are not directly addressable from the machine the UI is running on through the ProxyJump and ProxyCommand SSH options. These options can be used to specify intermediate hosts to connect to or actual commands to run to obtain a socket connected to the SSH server on the target host and can be added to your SSH configuration file. Note that for both options, NVIDIA Nsight Compute runs external commands and does not implement any mechanism to authenticate to the intermediate hosts using the credentials entered in the Connection Dialog . These credentials will only be used to authenticate to the final target in the chain of machines. When using the ProxyJump option NVIDIA Nsight Compute uses the OpenSSH client to establish the connection to the intermediate hosts. This means that in order to use ProxyJump or ProxyCommand , a version of OpenSSH supporting these options must be installed on the host machine. A common way to authenticate to the intermediate hosts in this case is to use a SSH agent and have it hold the private keys used for authentication. Since the OpenSSH SSH client is used, you can also use the SSH askpass mechanism to handle these authentications in an interactive manner. It might happen on slow networks that connections used for remote profiling through SSH time out. If this is the case, the ConnectTimeout option can be used to set the desired timeout value. A known limitation of the remote profiling through SSH is that problems may arise if NVIDIA Nsight Compute tries to do remote profiling through SSH by connecting to the same machine it is running on. In this case, the workaround is to do local profiling through localhost . For more information about available options for the OpenSSH client and the ecosystem of tools it can be used with for authentication refer to the official manual pages . 3.3.2. Interactive Profile Activity  The Interactive Profile activity allows you to initiate a session that controls the execution of the target application, similar to a debugger. You can step API calls and workloads (CUDA kernels), pause and resume, and interactively select the kernels of interest and which metrics to collect. This activity does currently not support profiling or attaching to child processes. Enable CPU Call Stack Collect the CPU-sided Call Stack at the location of each profiled kernel launch. CPU Call Stack Types If “Enable CPU Call Stack” is set to “Yes”, the type(s) of call stack may be selected here. Enable NVTX Support Collect NVTX information provided by the application or its libraries. Required to support stepping to specific NVTX contexts. Disable Profiling Start/Stop Ignore calls to cu(da)ProfilerStart or cu(da)ProfilerStop made by the application. Enable Profiling From Start Enables profiling from the application start. Disabling this is useful if the application calls cu(da)ProfilerStart and kernels before the first call to this API should not be profiled. Note that disabling this does not prevent you from manually profiling kernels. Cache Control Control the behavior of the GPU caches during profiling. Allowed values: For Flush All , all GPU caches are flushed before each kernel replay iteration during profiling. While metric values in the execution environment of the application might be slightly different without invalidating the caches, this mode offers the most reproducible metric results across the replay passes and also across multiple runs of the target application. For Flush None , no GPU caches are flushed during profiling. This can improve performance and better replicates the application behavior if only a single kernel replay pass is necessary for metric collection. However, some metric results will vary depending on prior GPU work, and between replay iterations. This can lead to inconsistent and out-of-bounds metric values. Clock Control Control the behavior of the GPU clocks during profiling. Allowed values: For Base , GPC and memory clocks are locked to their respective base frequency during profiling. This has no impact on thermal throttling. For None , no GPC or memory frequencies are changed during profiling. Import Source Enables permanently importing available source files into the report. Missing source files are searched in Source Lookup folders. Source information must be embedded in the executable, e.g. via the -lineinfo compiler option. Imported files are used in the CUDA-C view on the Source Page . Graph Profiling Set if CUDA graphs should be stepped and profiled as individual Nodes or as complete Graphs . See the Kernel Profiling Guide for more information on this mode. 3.3.3. Profile Activity  The Profile activity provides a traditional, pre-configurable profiler. After configuring which kernels to profile, which metrics to collect, etc, the application is run under the profiler without interactive control. The activity completes once the application terminates. For applications that normally do not terminate on their own, e.g. interactive user interfaces, you can cancel the activity once all expected kernels are profiled. This activity does not support attaching to processes previously launched via NVIDIA Nsight Compute. These processes will be shown grayed out in the Attach tab. Output File Path to report file where the collected profile should be stored. If not present, the report extension .ncu-rep is added automatically. The placeholder %i is supported for the filename component. It is replaced by a sequentially increasing number to create a unique filename. This maps to the --export command line option. Force Overwrite If set, existing report file are overwritten. This maps to the --force-overwrite command line option. Target Processes Select the processes you want to profile. In mode Application Only , only the root application process is profiled. In mode all , the root application process and all its child processes are profiled. This maps to the --target-processes command line option. Replay Mode Select the method for replaying kernel launches multiple times. In mode Kernel , individual kernel launches are replayed transparently during the single execution of the target application. In mode Application , the entire target application is relaunched multiple times. In each iteration, additional data for the target kernel launches is collected. Application replay requires the program execution to be deterministic. This maps to the --replay-mode command line option. See the Kernel Profiling Guide for more details on the replay modes. Graph Profiling Set if CUDA graphs should be profiled as individual Nodes or as complete Graphs . Additional Options All remaining options map to their command line profiler equivalents. See the Command Line Options for details. 3.3.4. Reset  Entries in the connection dialog are saved as part of the current project . When working in a custom project, simply close the project to reset the dialog. When not working in a custom project, entries are stored as part of the default project . You can delete all information from the default project by closing NVIDIA Nsight Compute and then deleting the project file from disk . 3.4. Main Menu and Toolbar  Information on the main menu and toolbar. 3.4.1. Main Menu  File New Project Create new profiling Projects with the New Project Dialog . Open Project Open an existing profiling project. Recent Projects Open an existing profiling project from the list of recently used projects. Save Project Save the current profiling project. Save Project As Save the current profiling project with a new filename. Close Project Close the current profiling project. New File Create a new file. Open File Open an existing file. Open Remote File Download an existing file from a remote host and open it locally. The opened file will only exist in memory and will not be written to the local machine’s disk unless the user explicitly saves it. For more information concerning the selection of a remote host to download the file from, see the section about Remote Connections . Only a subset of file types that are supported locally can be opened from a remote target. The following table lists file types that can be opened remotely. Remote File Type Support  Extensions Description Supported ncu-rep Nsight Compute Profiler Report Yes ncu-occ Occupancy Calculator File Yes ncu-bvh OptiX AS Viewer File Yes (except on MacOSX) section Section Description No cubin Cubin File No cuh,h,hpp Header File No c,cpp,cu Source File No txt Text file No nsight-cuprof-report Nsight Compute Profiler Report (legacy) Yes Save Save the current file Save As Save a copy of the current file with a different name or type or in a different location. Save All Files Save all open files. Close Close the current file. Close All Files Close all open files. Recent Files Open an existing file from the list of recently used files. Exit Exit Nsight Compute. Connection Connect Open the Connection Dialog to launch or attach to a target application. Disabled when already connected. Disconnect Disconnect from the current target application, allows the application to continue normally and potentially re-attach. Terminate Disconnect from and terminate the current target application immediately. Debug Pause Pause the target application at the next intercepted API call or launch. Resume Resume the target application. Step In Step into the current API call or launch to the next nested call, if any, or the subsequent API call, otherwise. Step Over Step over the current API call or launch and suspend at the next, non-nested API call or launch. Step Out Step out of the current nested API call or launch to the next, non-parent API call or launch one level above. Freeze API When disabled, all CPU threads are enabled and continue to run during stepping or resume, and all threads stop as soon as at least one thread arrives at the next API call or launch. This also means that during stepping or resume the currently selected thread might change as the old selected thread makes no forward progress and the API Stream automatically switches to the thread with a new API call or launch. When enabled, only the currently selected CPU thread is enabled. All other threads are disabled and blocked. Stepping now completes if the current thread arrives at the next API call or launch. The selected thread never changes. However, if the selected thread does not call any further API calls or waits at a barrier for another thread to make progress, stepping may not complete and hang indefinitely. In this case, pause, select another thread, and continue stepping until the original thread is unblocked. In this mode, only the selected thread will ever make forward progress. Break On API Error When enabled, during resume or stepping, execution is suspended as soon as an API call returns an error code. Run to Next Kernel See API Stream tool window. Run to Next API Call See API Stream tool window. Run to Next Range Start See API Stream tool window. Run to Next Range End See API Stream tool window. API Statistics Opens the API Statistics tool window API Stream Opens the API Stream tool window Resources Opens the Resources tool window NVTX Opens the NVTX tool window Profile Profile Kernel When suspended at a kernel launch, select the profile using the current configuration. Profile Series When suspended at a kernel launch, open the Profile Series configuration dialog to setup and collect a series of profile results. Auto Profile Enable or disable auto profiling. If enabled, each kernel matching the current kernel filter (if any) will be profiled using the current section configuration. Baselines Opens the Baselines tool window. Clear Baselines Clear all current baselines. Import Source Permanently import resolved source files into the report. Existing content may be overwritten. Section/Rules Info Opens the Metric Selection tool window. Tools Project Explorer Opens the Project Explorer tool window. Output Messages Opens the Output Messages tool window. Options Opens the Options dialog. Window Save Window Layout Allows you to specify a name for the current layout. The layouts are saved to a Layouts folder in the documents directory as named “.nvlayout” files. Apply Window Layout Once you have saved a layout, you can restore them by using the “Apply Window Layout” menu entry. Simply select the entry from sub-menu you want to apply. Manage Window Layout Allows you to delete or rename old layouts. Restore Default Layout Restore views to their original size and position. Show Welcome Page Opens the Welcome Page . Help Documentation Opens the latest documentation for NVIDIA Nsight Compute online. Documentation (local) Opens the local HTML documentation for NVIDIA Nsight Compute that has shipped with the tool. Check For Updates Checks online if a newer version of NVIDIA Nsight Compute is available for download. Reset Application Data Reset all NVIDIA Nsight Compute configuration data saved on disk, including option settings, default paths, recent project references etc. This will not delete saved reports. Send Feedback Opens a dialog that allows you to send bug reports and suggestions for features. Optionally, the feedback includes basic system information, screenshots, or additional files (such as profile reports). About Opens the About dialog with information about the version of NVIDIA Nsight Compute. 3.4.2. Main Toolbar  The main toolbar shows commonly used operations from the main menu. See Main Menu for their description. 3.4.3. Status Banners  Status banners are used to display important messages, such as profiler errors. The message can be dismissed by clicking the ‘X’ button. The number of banners shown at the same time is limited and old messages can get dismissed automatically if new ones appear. Use the Output Messages window to see the complete message history. 3.5. Tool Windows  3.5.1. API Statistics  The API Statistics window is available when NVIDIA Nsight Compute is connected to a target application. It opens by default as soon as the connection is established. It can be re-opened using Debug > API Statistics from the main menu. Whenever the target application is suspended, it shows a summary of tracked API calls with some statistical information, such as the number of calls, their total, average, minimum and maximum duration. Note that this view cannot be used as a replacement for Nsight Systems when trying to optimize CPU performance of your application. The Reset button deletes all statistics collected to the current point and starts a new collection. Use the Export to CSV button to export the current statistics to a CSV file. 3.5.2. API Stream  The API Stream window is available when NVIDIA Nsight Compute is connected to a target application. It opens by default as soon as the connection is established. It can be re-opened using Debug > API Stream from the main menu. Whenever the target application is suspended, the window shows the history of API calls and traced kernel launches. The currently suspended API call or kernel launch (activity) is marked with a yellow arrow. If the suspension is at a subcall, the parent call is marked with a green arrow. The API call or kernel is suspended before being executed. For each activity, further information is shown such as the kernel name or the function parameters ( Func Parameters ) and return value ( Func Return ). Note that the function return value will only become available once you step out or over the API call. Use the Current Thread dropdown to switch between the active threads. The dropdown shows the thread ID followed by the current API name. One of several options can be chosen in the trigger dropdown, which are executed by the adjacent >> button. Run to Next Kernel resumes execution until the next kernel launch is found in any enabled thread. Run to Next API Call resumes execution until the next API call matching Next Trigger is found in any enabled thread. Run to Next Range Start resumes execution until the next start of an active profiler range is found. Profiler ranges are defined by using the cu(da)ProfilerStart/Stop API calls. Run to Next Range Stop resumes execution until the next stop of an active profiler range is found. The API Level dropdown changes which API levels are shown in the stream. The Export to CSV button exports the currently visible stream to a CSV file. 3.5.3. Baselines  The Baselines tool window can be opened by clicking the Baselines entry in the Profile menu. It provides a centralized place from which to manage configured baselines. (Refer to Baselines , for information on how to create baselines from profile results.) The baseline visibility can be controlled by clicking on the check box in a table row. When the check box is checked, the baseline will be visible in the summary header as well as all graphs in all sections. When unchecked the baseline will be hidden and will not contribute to metric difference calculations. The baseline color can be changed by double-clicking on the color swatch in the table row. The color dialog which is opened provides the ability to choose an arbitrary color as well as offers a palette of predefined colors associated with the stock baseline color rotation. The baseline name can be changed by double-clicking on the Name column in the table row. The name must not be empty and must be less than the Maximum Baseline Name Length as specified in the options dialog. The z-order of a selected baseline can be changed by clicking the Move Baseline Up and Move Baseline Down buttons in the tool bar. When a baseline is moved up or down its new position will be reflected in the report header as well as in each graph. Currently, only one baseline may be moved at a time. The selected baselines may be removed by clicking on the Clear Selected Baselines button in the tool bar. All baselines can be removed at once by clicking on the Clear All Baselines button, from either the global tool bar or the tool window tool bar. The configured baselines can be saved to a file by clicking on the Save Baselines button in the tool bar. By default baseline files use the .ncu-bln extension. Baseline files can be opened locally and/or shared with other users. Baseline information can be loaded by clicking on the Load Baselines button in the tool bar. When a baseline file is loaded, currently configured baselines will be replaced. A dialog will be presented to the user to confirm this operation when necessary. Differences between the current result and the baselines can be visualized with graphical bars for metrics in Details page section headers. Use the Difference Bars drop down to select the visualization mode. Bars are extending from left to right and have a fixed maximum. 3.5.4. Metric Details  The Metric Details tool window can be opened using the Metric Details entry in the Profile menu or the respective tool bar button. When a report and the tool window are open, a metric can be selected in the report to display additional information in the tool window. It also contains a search bar to look up metrics in the focused report. Report metrics can be selected in the Details Page or the Raw Page . The window will show basic information (name, unit and raw value of the metric) as well as additional information, such as its extended description. The search bar can be used to open metrics in the focused report. It shows available matches as you type. The entered string must match from the start of the metric name. By default, selecting or searching for a new metric updates the current Default Tab . You can click the pin button located in the upper-left corner of the tab control to create a copy of the default tab, unless the same metric is already pinned. This makes it possible to save multiple tabs and quickly switch between them to compare values. Some metrics contain Instance Values . When available, they are listed in the tool window. Instance values can have a Correlation ID that allows correlating the individual value with its associated entity, e.g. a function address or instruction name. For metrics collected with PM sampling , the correlation ID is the GPU timestamp in nanoseconds. It is shown as an absolute value and relative to the first timestamp for this metric. 3.5.5. Launch Details  The Launch Details tool window can be opened using the Launch Details entry\nin the Profile menu or the respective tool bar button. When a result\ncontaining multiple sub-launches is selected and this tool window is open, it\nwill display information about each sub-launch contained in the result. This tool window is split into two sections: a header displaying information applying to the result as a whole a body displaying information specific to the viewed sub-launch Header  On the left side of its header, this tool window displays the selected result’s\nname and the number of sub-launches it is comprised of. The right side contains a combo box that allows selection of the sub-launch the\nbody should represent. Each element of the combo box contains an index for the\nsub-launch as well as the name of the function that it launched if available. Body  The body of this tool window displays a table with sub-launch-specific metrics.\nThis table has four columns: Metric Name : the name of the metric Metric Unit : the unit for metric values Instance Value : the value of this metric for the selected sub-launch Aggregate Value : the aggregate value for this metric over all sub-launches\nin the selected result 3.5.6. NVTX  The NVTX window is available when NVIDIA Nsight Compute is connected to a target application. If closed, it can be re-opened using Debug > NVTX from the main menu. Whenever the target application is suspended, the window shows the state of all active NVTX domains and ranges in the currently selected thread. Note that NVTX information is only tracked if the launching command line profiler instance was started with --nvtx or NVTX was enabled in the NVIDIA Nsight Compute launch dialog. Use the Current Thread dropdown in the API Stream window to change the currently selected thread. NVIDIA Nsight Compute supports NVTX named resources, such as threads, CUDA devices, CUDA contexts, etc. If a resource is named using NVTX, the appropriate UI elements will be updated. 3.5.7. CPU Call Stack  The CPU Call Stack window is available when NVIDIA Nsight Compute is connected to a target application.\nIf closed, it can be re-opened using Debug > CPU Call Stack from the main menu.\nWhenever the target application is suspended, the window shows all enabled CPU call stacks for the currently selected thread. Use the Call Stack Type dropdown menu to switch between stack types in case multiple stack types were enabled (e.g., Native , Python ).\nNote that Python call stack collection requires CPython version 3.9 or later. 3.5.8. Resources  The Resources window is available when NVIDIA Nsight Compute is connected to a target application. It shows information about the currently known resources, such as CUDA devices, CUDA streams or kernels. The window is updated every time the target application is suspended. If closed, it can be re-opened using Debug > Resources from the main menu. Using the dropdown on the top, different views can be selected, where each view is specific to one kind of resource (context, stream, kernel, …). The Filter edit allows you to create filter expressions using the column headers of the currently selected resource. The resource table shows all information for each resource instance. Each instance has a unique ID, the API Call ID when this resource was created, its handle, associated handles, and further parameters. When a resource is destroyed, it is removed from its table. Memory Allocations  When using the asynchronous malloc/free APIs, the resource view for Memory Allocation will also include the memory objects created in this manner. These memory objects have a non-zero memory pool handle. The Mode column will indicate which code path was taken during the allocation of the corresponding object. The modes are: REUSE_STREAM_SUBPOOL: The memory object was allocated in memory that was previously freed. The memory was backed by the memory pool set as current for the stream on which the allocation was made. USE_EXISTING_POOL_MEMORY: The memory object was allocated in memory that was previously freed. The memory is backed by the default memory pool of the stream on which the allocation was made. REUSE_EVENT_DEPENDENCIES: The memory object was allocated in memory that was previously freed in another stream of the same context. A stream ordering dependency of the allocating stream on the free action existed. Cuda events and null stream interactions can create the required stream ordered dependencies. REUSE_OPPORTUNISTIC: The memory object was allocated in memory that was previously freed in another stream of the same context. However, no dependency between the free and allocation existed. This mode requires that the free be already committed at the time the allocation is requested. Changes in execution behavior might result in different modes for multiple runs of the application. REUSE_INTERNAL_DEPENDENCIES: The memory object was allocated in memory that was previously freed in another stream of the same context. New internal stream dependencies may have been added in order to establish the stream ordering required to reuse a piece of memory previously released. REQUEST_NEW_ALLOCATION: New memory had to be allocated for this memory object as no viable reusable pool memory was found. The allocation performance is comparable to using the non-asynchronous malloc/free APIs. Graphviz DOT and SVG exports  Some of the shown Resources can also be exported to GraphViz DOT or SVG* files using the Export to GraphViz or Export to SVG buttons. When exporting OptiX traversable handles , the traversable graph node types will be encoded using shapes and colors as described in the following table. Table 2. OptiX Traversable Graph Node Types  Node Type Shape Color IAS Hexagon #8DD3C7 Triangle GAS Box #FFFFB3 AABB GAS Box #FCCDE5 Curve GAS Box #CCEBC5 Sphere GAS Box #BEBADA Static Transform Diamond #FB8072 SRT Transform Diamond #FDB462 Matrix Motion Transform Diamond #80B1D3 Error Paralellogram #D9D9D9 3.5.9. Metric Selection  The Metric Selection window can be opened from the main menu using Profile > Metric Selection . It tracks all metric sets, sections and rules currently loaded in NVIDIA Nsight Compute, independent from a specific connection or report. The directory to load those files from can be configured in the Profile options dialog. It is used to inspect available sets, sections and rules, as well as to configure which should be collected, and which rules should be applied. You can also specify a comma separated list of individual metrics, that should be collected. The window has two views, which can be selected using the dropdown in its header. The Metric Sets view shows all available metric sets. Each set is associated with a number of metrics sections. You can choose a set appropriate to the level of detail for which you want to collect performance metrics. Sets which collect more detailed information normally incur higher runtime overhead during profiling. When enabling a set in this view, the associated metric sections are enabled in the Metric Sections/Rules view. When disabling a set in this view, the associated sections in the Metric Sections/Rules view are disabled. If no set is enabled, or if sections are manually enabled/disabled in the Metric Sections/Rules view, the < custom > entry is marked active to represent that no section set is currently enabled. Note that the basic set is enabled by default. Whenever a kernel is profiled manually, or when auto-profiling is enabled, only sections enabled in the Metric Sections/Rules view and individual metrics specified in input box are collected. Similarly, whenever rules are applied, only rules enabled in this view are active. The enabled states of sections and rules are persisted across NVIDIA Nsight Compute launches. The Reload button reloads all sections and rules from disk again. If a new section or rule is found, it will be enabled if possible. If any errors occur while loading a rule, they will be listed in an extra entry with a warning icon and a description of the error. Use the Enable All and Disable All checkboxes to enable or disable all sections and rules at once. The Filter text box can be used to filter what is currently shown in the view. It does not alter activation of any entry. The table shows sections and rules with their activation status, their relationship and further parameters, such as associated metrics or the original file on disk. Rules associated with a section are shown as children of their section entry. Rules independent of any section are shown under an additional Independent Rules entry. Double-clicking an entry in the table’s Filename column opens this file as a document. It can be edited and saved directly in NVIDIA Nsight Compute. After editing the file, Reload must be selected to apply those changes. When a section or rule file is modified, the entry in the State column will show User Modified to reflect that it has been modified from its default state. When a User Modified row is selected, the Restore button will be enabled. Clicking the Restore button will restore the entry to its default state and automatically Reload the sections and rules. Similarly, when a stock section or rule file is removed from the configured Sections Directory (specified in the Profile options dialog), the State column will show User Deleted . User Deleted files can also be restored using the Restore button. Section and rule files that are created by the user (and not shipped with NVIDIA Nsight Compute) will show up as User Created in the state column . See the Sections and Rules for the list of default sections for NVIDIA Nsight Compute. 3.6. Profiler Report  The profiler report contains all the information collected during profiling for each kernel launch. In the user interface, it consists of a header with general information, as well as controls to switch between report pages or individual collected launches. 3.6.1. Header  The top of the report shows a table with information about the selected profile result (as Current ) and potentially additional baselines .\nFor many values in this table, tooltips provide additional information or data, e.g., the tooltip of the column Attributes provides additional information about the context type and resources used for the launch. The Result dropdown can be used to switch between all collected kernel launches. The information displayed in each page commonly represents the selected launch instance. On some pages (e.g. Raw ), information for all launches is shown and the selected instance is highlighted. You can type in this dropdown to quickly filter and find a kernel launch. The Apply Filters button opens the filter dialog. You can use more than one filter to narrow down your results. On the filter dialog, enter your filter parameters and press OK button. The Launch dropdown, Summary Page table, and Raw Page table will be filtered accordingly. Select the arrow dropdown to access the Clear Filters button, which removes all filters. Filter Dialog  Underneath the current and baseline results are the tabs for switching between the report pages. The pages themselves are explained in detail in the next section . Each group button to the right of the page tabs opens a context menu that features related actions.\nSome actions may be enabled only when the related report page is selected. Compare Add Baseline promotes the current result in focus to become the baseline of all other results from this report and any other report opened in the same instance of NVIDIA Nsight Compute. Clear Baselines removes all currently active baselines. You may also use the Baselines tool window to manage baseline for comparison. Source Comparison navigates to the Source Comparison document in case at least two profile results are available for comparison. Tools Occupancy Calculator opens the Occupancy Calculator in a new document. Metric Details Windows opens the Metric Details tool window. When the window is open and a metric is selected elsewhere in the report, it shows detailed information about it. Launch Details Windows opens the Launch Details tool window. When the window is open and a result containing multiple sub-launches is selected, it displays information about each sub-launch in the result. View Show/Hide Rules Output toggles the visibility of rule results. Show/Hide Section Descriptions toggles the visibiliy of section descriptions on the Details page. Expand Sections expands all sections to show their body contents, not only header and rule output. Note that sections may have multiple bodies and the visible one can be chosen using the dropdown in the section header. Collapse Sections collapses all sections to show only their header and rule output. Export Copy as Image - Copies the contents of the page to the clipboard as an image. Save as Image - Saves the contents of the page to a file as an image. Save as PDF - Saves the contents of the page to a file as a PDF. Export to CSV - Exports the contents of the page to CSV format. More (three bars icon) Apply Rules applies all rules available for this report. If rules had been applied previously, those results will be replaced. By default, rules are applied immediately once the kernel launch has been profiled. This can be changed in the options under Tools > Options > Profile > Report UI > Apply Applicable Rules Automatically . Reset to Default resets the page to a default state by removing any persisted settings. 3.6.2. Report Pages  Use the Page dropdown in the header to switch between the report pages. By default, when opening a report with a single profile result, the Details Page is shown. When opening a report with multiple results, the Summary Page is selected instead. You can change the default report page in the Profile options. Summary Page  The Summary page shows a table of all collected results in the report, as well as a list of the most important rule outputs ( Prioritized Rules ) which are ordered by the estimated speedup that could potential be obtained by following their guidance. Prioritized Rules are shown by default and can be toggled with the [R] button on the upper right of the page. Summary page with Summary Table and Prioritized Rules.  The Summary Table gives you a quick comparison overview across all profiled workloads. It contains a number of important, pre-selected metrics which can be customized as explained below. Its columns can be sorted by clicking the column header. You can transpose the table with the Transpose button. Aggregate of all results per each counter metric is shown in the table header along with the column name. You can change the aggregated values by selecting the desired results for multiple metrics simultaneously. When selecting any entry by single-click, a list of its Prioritized Rules will be shown below the table. Double-click any entry to make the result the currently active one and switch to the Details Page page to inspect its performance data.\nBy default, kernel demangled names are simplified, renamed and shown in an optimized manner. This behavior can be changed with Rename Demangled Names option. If an auto-simplified name is not useful, you can rename it through a configuration file.\nYou can also persist the updated names directly in the report by double-clicking on the name, renaming and saving the report.\nUse Rename Kernels Config Path option to specify the configuration file which should be used while importing renamed kernels or exporting demangled names with mappings to rename them. To export names to a new file, click Export button and use Rename Kernels Config option.\nSee Kernel Renaming for more details on configuration file usage. You can configure the list of metrics included in this table in the Profile options dialog. If a metric has multiple instance values, the number of instances is shown after its standard value. A metric with ten instance values could for example look like this: 35.48 {10} . In the Profile options dialog, you can select that all instance values should be shown individually. You can also inspect the instances values of a metric result in the Metric Details tool window. In addition to metrics, you can also configure the table to include any of the following properties: Properties Properties  property__api_call_id ID of the API call associated with this profile result. property__block_size Block Size. If the result contains multiple launches, this will contain the maximum value for each dimension of the block. property__creation_time Local collection time. property__demangled_name Kernel demangled name, potentially renamed. property__device_name GPU device name. property__estimated_speedup Maximal relative speedup achievable for this profile result as estimated by the guided analysis rules. property__function_name Kernel function name or range name. property__grid_dimensions Grid Dimensions. If the result contains multiple launches, this will contain the maximum value for each dimension of the grid. property__grid_offset Grid Offset. property__grid_size Grid Size. If the result contains multiple launches, this will contain the maximum value for each dimension of the grid. property__issues_detected Number of issues detected by guided analysis rules for this profile result. property__kernel_id Kernel ID. property__mangled_name Kernel mangled name. property__original_demangled_name Original kernel demangled name without any renaming. property__process_name Process name. property__runtime_improvement Runtime improvement corresponding to the estimated speedup. property__series_id ID of the profile series. property__series_parameters Profile series parameters. property__thread_id CPU thread ID. For Range Replay reports, a smaller set of columns is shown by default, as not all apply to such results. For the currently selected metric result the Prioritized Rules show the most impactful rule results with respect to the estimated potential speedup. Clicking on any of the rule names on the left allows you to easily navigate to the containing section on the details page. With the downward-facing arrow on the right a table with the relevant key performance indicators can be toggled. This table contains the metrics which should be tracked when optimizing performance according to the rule guidance. Prioritized Rules with key performance indicators table.  Details Page  Overview The Details page is the main page for all metric data collected during a kernel launch. The page is split into individual sections. Each section consists of a header table and an optional body that can be expanded. The sections are completely user defined and can be changed easily by updating their respective files. For more information on customizing sections, see the Customization Guide . For a list of sections shipped with NVIDIA Nsight Compute, see Sections and Rules . By default, once a new profile result is collected, all applicable rules are applied. Any rule results will be shown as Recommendations on this page. Most rule results will contain an optimization advice along with an estimate of the improvement that could be achieved when successfully implementing this advice. Other rule results will be purely informative or have a warning icon to indicate a problem that occurred during execution (e.g., an optional metric that could not be collected). Results with error icons typically indicate an error while applying the rule. Estimates of potential improvement are shown below the rule result’s name and exist in two types. Global estimates (“Est. Speedup”) are an approximation of the decrease in workload runtime, whereas local estimates (“Est. Local Speedup”) are an approximation of the increase in efficiency of the hardware utilization of the particular performance problem the rule addresses. Rule results often point out performance problems and guide through the analysis process.  If a rule result references another report section, it will appear as a link in the recommendation. Select the link to scroll to the respective section. If the section was not collected in the same profile result, enable it in the Metric Selection tool window. You can add or edit comments in each section of the Details view by clicking on the comment button (speech bubble). The comment icon will be highlighted in sections that contain a comment. Comments are persisted in the report and are summarized in the Comments Page . Use the Comments button to annotate sections.  Besides their header, sections typically have one or more bodies with additional charts or tables. Click the triangle Expander icon in the top-left corner of each section to show or hide those. If a section has multiple bodies, a dropdown in their top-right corner allows you to switch between them. Sections with multiple bodies have a dropdown to switch between them.  Memory If enabled, the Memory Workload Analysis section contains a Memory chart that visualizes data transfers, cache hit rates, instructions and memory requests. More information on how to use and read this chart can be found in the Kernel Profiling Guide . Occupancy You can open the Occupancy Calculator by clicking on the calculator button in the report header or in the header of the Occupancy Section . Range Replay Note that for Range Replay results some UI elements, analysis rules, metrics or section body items such as charts or tables might not be available, as they only apply to kernel launch-based results. The filters can be checked in the corresponding section files. Rooflines If enabled, the GPU Speed Of Light Roofline Chart section contains a Roofline chart that is particularly helpful for visualizing kernel performance at a glance. (To enable roofline charts in the report, ensure that the section is enabled when profiling.) More information on how to use and read this chart can be found in Roofline Charts . NVIDIA Nsight Compute ships with several different definitions for roofline charts, including hierarchical rooflines. These additional rooflines are defined in different section files. While not part of the full section set, a new section set called roofline was added to collect and show all rooflines in one report. The idea of hierarchical rooflines is that they define multiple ceilings that represent the limiters of a hardware hierarchy. For example, a hierarchical roofline focusing on the memory hierarchy could have ceilings for the throughputs of the L1 cache, L2 cache and device memory. If the achieved performance of a kernel is limited by one of the ceilings of a hierarchical roofline, it can indicate that the corresponding unit of the hierarchy is a potential bottleneck. Sample roofline chart.  The roofline chart can be zoomed and panned for more effective data analysis, using the controls in the table below. Table 3. Roofline Chart Zoom and Pan Controls  Zoom In Zoom Out Zoom Reset Pan Click the Zoom In button in the top right corner of the chart. Click the left mouse button and drag to create a rectangle that bounds the area of interest. Press the plus (+) key. Use Ctrl + MouseWheel (Windows and Linux only) Click the Zoom Out button in the top right corner of the chart. Click the right mouse button. Press the minus (-) key. Use Ctrl + MouseWheel (Windows and Linux only) Click the Zoom Reset button in the top right corner of the chart. Press the Escape (Esc) key. Use Ctrl (Command on Mac) + LeftMouseButton to grab the chart, then move the mouse. Use the cursor keys. Source Sections such as Source Counters can contain source hot spot tables. These tables indicate the N highest or lowest values of one or more metrics in your kernel source code. Select the location links to navigate directly to this location in the Source Page . Hover the mouse over a value to see which metrics contribute to it. Hot spot tables point out performance problems in your source.  Timelines When collecting metrics with PM sampling , they can be viewed in a timeline . The timeline shows metrics selected in the respective section file or on the command line with their labels/names and their values over time. Different metrics may be collected in different passes (replays) of the workload, as only a limited number of them can be sampled in the same pass. Context switch trace is used to filter the collected data to only include samples from the profiled contexts and to align it in the timeline. You can hover the mouse over a metric row label to see further information on the metrics in the row. Hovering over a sample on the timeline shows the metric values at that timestamp within the current row. With the Metric Details tool window open, click to select a value on the timeline and show the metric and all its raw timestamps (absolute and relative) correlated values in the tool window. You can also use the Metric Details tool window to inspect profiler metrics generated during PM sampling. These provide information about the used sampling intervals, buffer sizes, dropped samples and other properties for each collection pass. A detailed list can be found in the metrics reference . The timeline has a context menu for further actions regarding copying content and zooming.\nIn addition, the Enable/Disable Context Switch Filter option can be used to enable or disable the filtering of the timeline data with context switch information, if it is available.\nWhen the context switch filter is enabled (the default), samples from each pass group are only shown for the active contexts.\nWhen the context switch filter is disabled, the raw collected sampling data is shown along with a separate row for each pass group’s context switch trace. When the context menu option is not available, the report does not include context switch trace data.\nIn this case, the option Enable/Disable Trim Filter is shown instead, which, when enabled, tries to align based on the first non-zero value in any sampling metric in this pass group.\nHowever, this fallback does not take into account actual context switches. The timeline row Workload Execution shows each kernel’s start and end timestamp. When the context switch filter is enabled, kernel execution is only shown for one of the passes for the active contexts. When the context switch filter is disabled, kernel execution is shown for all the passes. Source Page  The Source page correlates assembly (SASS) with high-level code such as CUDA-C, Python or PTX. In addition, it displays instruction-correlated metrics to help pinpoint performance problems in your code. Source Correlation  The page can be switched between different Views to focus on a specific source layer or see two layers side-by-side. This includes SASS, PTX and Source (CUDA-C, Fortran, Python, …), as well as their combinations. Which options are available depends on the source information embedded into the executable. The high-level Source (CUDA-C) view is available if the application was built with the -lineinfo or --generate-line-info nvcc flag to correlate SASS and source. When using separate linking at the ELF level, there is no PTX available in the ELF that would correspond to the final SASS. As such, NVIDIA Nsight Compute does not show any PTX even though it would be available statically in the executable and could be shown with cuobjdump -all -lptx . However, this is a pre-linked version of the PTX and cannot be reliably used for correlation. The code in the different Views can also contain warnings, errors or just notifications that are displayed as Source Markers in the left header, as shown below. These can be generated from multiple systems, but as of now only NvRules are supported. Source Markers  Navigation  The View dropdown can be used to select different code (correlation) options: SASS, PTX and Source (CUDA-C, Fortran, Python, …). In side-by-side views, when selecting a line in the left-hand- or right-hand-side, any correlated lines in the opposite view are highlighted. However, when the Show Single File For Multi-File Sources option is set to Yes , the target file or source object must already be selected in the respective view for those correlated lines to be shown. In side-by-side views, the view which has blue border is called focused view . Focused view can be changed by clicking anywhere in the other view. Control groups which are bordered with blue color will work for a focused view only. The Source dropdown allows you to switch between the files or functions that provide the content in the view. When a different source entry is selected, the view scrolls to the start of this file or function. If a view contains multiple source files or functions, [+] and [-] buttons are shown. These can be used to expand or collapse the view, thereby showing or hiding the file or function content except for its header. These will work for both the views if that group is in the linked state otherwise in an unlinked state it will work for a focused view. If collapsed, all metrics are shown aggregated to provide a quick overview. Collapsed Source View  You can use the Find (source code) line edit to search the Source column of a focused view. Enter the text to search and use the associated buttons to find the next or previous occurrence in this column. While the line edit is selected, you can also use the Enter or Shift*+*Enter keys to search for the next or previous occurrence, respectively. The SASS view is filtered to only show functions that were executed in the launch. You can toggle the Show Only Executed Functions option to change this, but performance of this page may be negatively affected for large binaries. It is possible that some SASS instructions are shown as N/A . Those instructions are not currently exposed publicly. In side-by-side views, the Navigate By dropdowns are linked with each other by default, thereby changing column names from one dropdown will change it in the other view only if column is available. These dropdowns can be unlinked with the link-unlink button provided just before it. Linked state of “Navigate By” Dropdowns  Only filenames are shown in the view, together with a File Not Found error, if the source files cannot be found in their original location. This can occur, for example, if the report was moved to a different system. Select a filename and click the Resolve button above to specify where this source can be found on the local filesystem. However, the view always shows the source files if the import source option was selected during profiling, and the files were available at that time. If a file is found in its original or any source lookup location, but its attributes don’t match, a File Mismatch error is shown. See the Source Lookup options for changing file lookup behavior. Resolve Source  If the report was collected using remote profiling, and automatic resolution of remote files is enabled in the Profile options, NVIDIA Nsight Compute will attempt to load the source from the remote target. If the connection credentials are not yet available in the current NVIDIA Nsight Compute instance, they are prompted in a dialog. Loading from a remote target is currently only available for Linux x86_64 targets and Linux and Windows hosts. Metrics  Metrics Correlation The page is most useful when inspecting performance information and metrics correlated with your code. Metrics are shown in columns, which can be enabled or disabled using the Column Chooser accessible using the column header right click menu. Column Chooser  To not move out of view when scrolling horizontally, columns can be fixed. By default, the Source column is fixed to the left, enabling easy inspection of all metrics correlated to a source line. To change fixing of columns, right click the column header and select Freeze or Unfreeze , respectively. Column Freezing/Unfreezing  The heatmap on the right-hand side of each view can be used to quickly identify locations with high metric values of the currently selected metric in the dropdown. The heatmap uses a black-body radiation color scale where black denotes the lowest mapped value and white the highest, respectively. The current scale is shown when clicking and holding the heatmap with the right mouse button. Heatmap Color Scale  By default, applicable metrics are shown as percentage values relative to their sum across the launch. A bar is filling from left to right to indicate the value at a specific source location relative to this metric’s maximum within the launch. The [%] and [+-] buttons can be used to switch the display from relative to absolute and from abbreviated absolute to full-precision absolute, respectively. For relative values and bars, the [circle/pie] button can be used to switch the display between relative to global (launch) and relative to local (function/file) scope. This button is disabled when the view is collapsed, as percentages are always relative to the global launch scope in this case. Relative and Absolute Metric Values.  Pre-Defined Source Metrics Live Registers Number of registers that need to be kept valid by the compiler. A high value indicates that many registers are required at this code location, potentially increasing the register pressure and the maximum number of register required by the kernel. The total number of registers reported as launch__registers_per_thread may be significantly higher than the maximum live registers. The compiler may need to allocate specific registers that can creates holes in the allocation, thereby affecting launch__registers_per_thread , even if the maximum live registers is smaller. This may happen due to ABI restrictions, or restrictions enforced by particular hardware instructions. The compiler may not have a complete picture of which registers may be used in either callee or caller and has to obey ABI conventions, thereby allocating different registers even if some register could have theoretically been re-used. Warp Stall Sampling (All Samples) 1 The number of samples from the Statistical Sampler at this program location. Warp Stall Sampling (Not-issued Samples) 2 The number of samples from the Statistical Sampler at this program location on cycles the warp scheduler issued no instructions. Note that (Not Issued) samples may be taken on a different profiling pass than (All) samples mentioned above, so their values do not strictly correlate. This metric is only available on devices with compute capability 7.0 or higher. Instructions Executed Number of times the source (instruction) was executed per individual warp, independent of the number of participating threads within each warp. Thread Instructions Executed Number of times the source (instruction) was executed by any thread, regardless of predicate presence or evaluation. Predicated-On Thread Instructions Executed Number of times the source (instruction) was executed by any active, predicated-on thread. For instructions that are executed unconditionally (i.e. without predicate), this is the number of active threads in the warp, multiplied with the respective Instructions Executed value. Avg. Threads Executed Average number of thread-level executed instructions per warp, regardless of their predicate. Avg. Predicated-On Threads Executed Average number of predicated-on thread-level executed instructions per warp. Divergent Branches Number of divergent branch targets, including fallthrough. Incremented only when there are two or more active threads with divergent targets. Divergent branches can lead to warp stalls due to resolving the branch or instruction cache misses. Information on Memory Operations Label Name Description Address Space memory_type The accessed address space (global/local/shared). Access Operation memory_access_type The type of memory access (e.g. load or store). Access Size memory_access_size_type The size of the memory access, in bits. L1 Tag Requests Global memory_l1_tag_requests_global Number of L1 tag requests generated by global memory instructions. L1 Conflicts Shared N-Way derived__memory_l1_conflicts_shared_nway Average N-way conflict in L1 per shared memory instruction. A 1-way access has no conflicts and resolves in a single pass. Note: This is a derived metric which can not be collected directly. L1 Wavefronts Shared Excessive derived__memory_l1_wavefronts_shared_excessive Excessive number of wavefronts in L1 from shared memory instructions, because not all not predicated-off threads performed the operation. Note: This is a derived metric which can not be collected directly. L1 Wavefronts Shared memory_l1_wavefronts_shared Number of wavefronts in L1 from shared memory instructions. L1 Wavefronts Shared Ideal memory_l1_wavefronts_shared_ideal Ideal number of wavefronts in L1 from shared memory instructions, assuming each not predicated-off thread performed the operation. L2 Theoretical Sectors Global Excessive derived__memory_l2_theoretical_sectors_global_excessive Excessive theoretical number of sectors requested in L2 from global memory instructions, because not all not predicated-off threads performed the operation. Note: This is a derived metric which can not be collected directly. L2 Theoretical Sectors Global memory_l2_theoretical_sectors_global Theoretical number of sectors requested in L2 from global memory instructions. L2 Theoretical Sectors Global Ideal memory_l2_theoretical_sectors_global_ideal Ideal number of sectors requested in L2 from global memory instructions, assuming each not predicated-off thread performed the operation. L2 Theoretical Sectors Local memory_l2_theoretical_sectors_local Theoretical number of sectors requested in L2 from local memory instructions. All L1/L2 Sectors/Wavefronts/Requests metrics give the number of achieved (actually required), ideal, and excessive (achieved - ideal) sectors/wavefronts/requests. Ideal metrics indicate the number that would needed, given each not predicated-off thread performed the operation of given width. Excessive metrics indicate the required surplus over the ideal case. Reducing divergence between threads can reduce the excess amount and result in less work for the respective HW units. Several of the above metrics on memory operations were renamed in version 2021.2 as follows: Old name New name memory_l2_sectors_global memory_l2_theoretical_sectors_global memory_l2_sectors_global_ideal memory_l2_theoretical_sectors_global_ideal memory_l2_sectors_local memory_l2_theoretical_sectors_local memory_l1_sectors_global memory_l1_tag_requests_global memory_l1_sectors_shared memory_l1_wavefronts_shared memory_l1_sectors_shared_ideal memory_l1_wavefronts_shared_ideal L2 Explicit Evict Policy Metrics Starting with the NVIDIA Ampere architecture the eviction policy of the L2 cache can be tuned to match the kernel’s access pattern. The eviction policy can be either set implicitly for a memory window (for more details see CUaccessProperty ) or set explicitly per executed memory instruction. If set explicitly, the desired eviction behavior for the cases of an L2 cache hit or miss are passed as input to the instruction. For more details refer to CUDA’s Cache Eviction Priority Hints . Label Name Description L2 Explicit Evict Policies smsp__inst_executed_memdesc_explicit_evict_type Comma separated list of configured explicit eviction policies. As the policies can be set dynamically at runtime, this list includes all policies that were part of any executed instruction. L2 Explicit Hit Policy Evict First smsp__inst_executed_memdesc_explicit_hitprop_evict_first Number of times a memory instruction was executed by any warp which had the evict_first policy set in case the access leads to a cache hit in L2. Data cached with this policy will be first in the eviction priority order and will likely be evicted when cache eviction is required. This policy is suitable for streaming data. L2 Explicit Hit Policy Evict Last smsp__inst_executed_memdesc_explicit_hitprop_evict_last Number of times a memory instruction was executed by any warp which had the evict_last policy set in case the access leads to a cache hit in L2. Data cached with this policy will be last in the eviction priority order and will likely be evicted only after other data with evict_normal or evict_first eviction policy is already evicted. This policy is suitable for data that should remain persistent in cache. L2 Explicit Hit Policy Evict Normal smsp__inst_executed_memdesc_explicit_hitprop_evict_normal Number of times a memory instruction was executed by any warp which had the evict_normal (default) policy set in case the access leads to a cache hit in L2. L2 Explicit Hit Policy Evict Normal Demote smsp__inst_executed_memdesc_explicit_hitprop_evict_normal_demote Number of times a memory instruction was executed by any warp which had the evict_normal_demote policy set in case the access leads to a cache hit in L2. L2 Explicit Miss Policy Evict First smsp__inst_executed_memdesc_explicit_missprop_evict_first Number of times a memory instruction was executed by any warp which had the evict_first policy set in case the access leads to a cache miss in L2. Data cached with this policy will be first in the eviction priority order and will likely be evicted cache eviction is required. This policy is suitable for streaming data. L2 Explicit Miss Policy Evict Normal smsp__inst_executed_memdesc_explicit_missprop_evict_normal Number of times a memory instruction was executed by any warp which had the evict_normal (default) policy set in case the access leads to a cache miss in L2. Individual Warp Stall Sampling Metrics All stall_* metrics show the information combined in Warp Stall Sampling individually. See Statistical Sampler for their descriptions. See the Customization Guide on how to add additional metrics for this view and the Metrics Reference for further information on available metrics. Register Dependencies Dependencies between registers are displayed in the SASS view. When a register is read, all the potential addresses where it could have been written are found. The links between these lines are drawn in the view. All dependencies for registers, predicates, uniform registers and uniform predicates are shown in their respective columns. The picture above shows some dependencies for a simple CUDA kernel. On the first row, which is line 9 of the SASS code, we can see writes on registers R2 and R3, represented by filled triangles pointing to the left . These registers are then read on lines 17, 20 and 23, and this is represented by regular triangles pointing to the right . There are also some lines where both types of triangles are on the same line, which means that a read and a write occured for the same register. Dependencies across source files and functions are not tracked. The Register Dependencies Tracking feature is enabled by default, but can be disabled completely in Tools > Options > Profile > Report Source Page > Enable Register Dependencies . 1 This metric was previously called Sampling Data (All). 2 This metric was previously called Sampling Data (Not Issued). Profiles  The icon next to the View dropdown can be used to manage Source View Profiles . This button opens a dialog that shows you the list of saved source view profiles. Such profiles can be created using the Create button in the dialog. Profiles let you store the column properties of all views in the report to a file. Such properties include column visibility, freeze state, width, order and the selected navigation metric. A saved profile can be applied to any opened report using the Apply button. This updates the column properties mentioned above from the selected profile in all views. Profiles are useful for configuring views to your preferences, or for a certain use case. Start by choosing metric columns from the Column Chooser . Next, configure other properties like freezing column, changing width or order and setting a heatmap metric in the Navigation dropdown before creating the profile. Once a profile is created, you can always use this profile on any opened report to hide all non-required columns or to restore your configured properties. Simply select the profile from the source view profiles dialog and click the Apply button. Note that the column properties are stored separately for each View in the profile and when applied, only those views will be updated which are present in the selected profile. You will not see the metric columns that are not available in your report even if those were configured to be visible in the source profile you have applied. Limitations  Range Replay When using Range Replay mode, instruction-level source metrics are not available. Graph Profiling When profiling complete CUDA graphs, instruction-level source metrics are not available. Context Page  The CPU Call Stack section of this report page shows the CPU call stack(s) for the executing CPU thread at the time the kernel was launched. For this information to show up in the profiler report, the option to collect CPU call stacks had to be enabled in the Connection Dialog or using the corresponding NVIDIA Nsight Compute CLI command line parameter. NVIDIA Nsight Compute supports to collect native CPU call stacks as well as call stacks for Python applications.\nEither or both types can be selected in the Activity menu of the Connection Dialog (via the “CPU Call Stack Types” option),\nor using the NVIDIA Nsight Compute CLI command line parameter –call-stack-type .\nIn case both types are enabled, a dropdown menu will appear to select the desired call stack type. Note that Python call stack collection requires CPython version 3.9 or later. The NVTX State section of this report page shows the NVTX context when the kernel was launched. All thread-specific information is with respect to the thread of the kernel’s launch API call. Note that NVTX information is only collected if the profiler is started with NVTX support enabled, either in the Connection Dialog or using the NVIDIA Nsight Compute CLI command line parameter. This page has been renamed from “Call Stack / NVTX Page”. Comments Page  The Comments page aggregates all section comments in a single view and allows the user to edit those comments on any launch instance or section, as well as on the overall report. Comments are persisted with the report. If a section comment is added, the comment icon of the respective section in the Details Page will be highlighted. Raw Page  The Raw page shows a list of all collected metrics with their units per profiled kernel launch. It can be exported, for example, to CSV format for further analysis. The page features a filter edit to quickly find specific metrics. You can transpose the table of kernels and metrics by using the Transpose button. If a metric has multiple instance values, the number of instances is shown after the standard value. This metric for example has ten instance values: 35.48 {10} . You can select in the Profile options dialog that all instance values should be shown individually or inspect the metric result in the Metric Details tool window. Session Page  This Session page contains basic information about the report and the machine, as well as device attributes of all devices for which launches were profiled. When switching between launch instances, the respective device attributes are highlighted. 3.6.3. Metrics and Units  Numeric metric values are shown in various places in the report, including the header and tables and charts on most pages. NVIDIA Nsight Compute supports various ways to display those metrics and their values. When available and applicable to the UI component, metrics are shown along with their unit. This is to make it apparent if a metric represents cycles, threads, bytes/s, and so on. The unit will normally be shown in rectangular brackets, e.g. Metric Name [bytes] 128 . By default, units are scaled automatically so that metric values are shown with a reasonable order of magnitude. Units are scaled using their SI-factors, i.e. byte-based units are scaled using a factor of 1000 and the prefixes K, M, G, etc. Time-based units are also scaled using a factor of 1000, with the prefixes n, u and m. This scaling can be disabled in the Profile options. Metrics which could not be collected are shown as n/a and assigned a warning icon. If the metric floating point value is out of the regular range (i.e. nan (Not a number) or inf (infinite)), they are also assigned a warning icon. The exception are metrics for which these values are expected and which are allow-listed internally. 3.7. Baselines  NVIDIA Nsight Compute supports diffing collected results across one or multiple reports using Baselines. Each result in any report can be promoted to a baseline. This causes metric values from all results in all reports to show the difference to the baseline. If multiple baselines are selected simultaneously, metric values are compared to the average across all current baselines. Baselines are not stored with a report and are only available as long as the same NVIDIA Nsight Compute instance is open, unless they are saved to a ncu-bln file from the Baselines tool window . Profiler report with one baseline  Select Add Baseline to promote the current result in focus to become a baseline. If a baseline is set, most metrics on the Details Page , Raw Page and Summary Page show two values: the current value of the result in focus, and the corresponding value of the baseline or the percentage of change from the corresponding baseline value. (Note that an infinite percentage gain, inf% , may be displayed when the baseline value for the metric is zero, while the focus value is not.) If multiple baselines are selected, each metric will show the following notation: <focus value> (<difference to baselines average [%]>, z=<standard score>@<number of values>) The standard score is the difference between the current value and the average across all baselines, normalized by the standard deviation. If the number of metric values contributing to the standard score equals the number of results (current and all baselines), the @<number of values> notation is omitted. Profiler report with multiple baselines  Baseline added for the current result in focus is always shown on the top. However, the actual order of added baselines is shown in the Baselines tool window . Baselines tool window with mutliple baselines  Double-clicking on a baseline name allows the user to edit the displayed name. Edits are committed by pressing Enter/Return or upon loss of focus, and abandoned by pressing Esc . Hovering over the baseline color icon allows the user to remove this specific baseline from the list. Use the Clear Baselines entry from the dropdown button, the Profile menu, or the corresponding toolbar button to remove all baselines. Baseline changes can also be made in the Baselines tool window . 3.8. Standalone Source Viewer  NVIDIA Nsight Compute includes a standalone source viewer for cubin files. This view is identical to the Source Page , except that it won’t include any performance metrics. Cubin files can be opened from the File > Open main menu command. The SM Selection dialog will be shown before opening the standalone source view. If available, the SM version present in the file name is pre-selected. For example, if your file name is mergeSort.sm_80.cubin then SM 8.0 will be pre-selected in the dialog. Choose the appropriate SM version from the drop down menu if it’s not included in the file name. SM Selection Dialog  Click Ok button to open Standalone Source Viewer . Standalone Source Viewer  3.9. Source Comparison  Source comparison provides a way to see the source files of two profile results side by side. It enables to quickly identify source differences and understand changes in metric values. To compare two results side by side add one result as a baseline, navigate to the other result, and then click the Source Comparison button located in the report header. For example, if you want to compare kernel XYZ from report R1 with kernel XYZ from report R2, first open report R1, add the profile result for kernel XYZ as baseline, open report R2, choose kernel XYZ, and then click the Source Comparison button. Source comparison will be shown only with first added baseline result. Source Comparison Button  Source Comparison  Currently only high-level Source (CUDA-C) view and SASS view are supported for comparison. Navigation to the previous or next difference is supported using the navigation buttons or the keyboard shortcuts Ctrl + 1 and Ctrl + 2 . Source Comparison Navigation Buttons  3.10. Occupancy Calculator  NVIDIA Nsight Compute provides an Occupancy Calculator that allows you to compute the multiprocessor occupancy of a GPU for a given CUDA kernel. It offers feature parity to the CUDA Occupancy Calculator spreadsheet . The Occupancy Calculator can be opened directly from a profile report or as a new activity. The occupancy calculator data can be saved to a file using File > Save . By default, the file uses the .ncu-occ extension. The occupancy calculator file can be opened using File > Open File Launching from the Connection Dialog Select the Occupancy Calculator activity from the connection dialog. You can optionally specify an occupancy calculator data file, which is used to initialize the calculator with the data from the saved file. Click the Launch button to open the Occupancy Calculator. Launching from the Profiler Report The Occupancy Calculator can be opened from the Profiler Report using the calculator button located in the report header or in the header of the Occupancy section on the Detail Page . Details page header  Occupancy section header  The user interface consists of an input section as well as tables and graphs that display information about GPU occupancy. To use the calculator, change the input values in the input section, click the Apply button and examine the tables and graphs. 3.10.1. Tables  The tables show the occupancy, as well as the number of active threads, warps, and thread blocks per multiprocessor, and the maximum number of active blocks on the GPU. Tables  3.10.2. Graphs  The graphs show the occupancy for your chosen block size as a blue circle, and for all other possible block sizes as a line graph. Graphs  3.10.3. GPU Data  The GPU Data shows the properties of all supported devices. GPU Data  3.11. Acceleration Structure Viewer  The Acceleration Structure Viewer allows inspection of acceleration structures built using the OptiX API. In modern ray tracing APIs like OptiX, acceleration structures are data structures describing the rendered scene’s geometries that will be intersected when performing ray tracing operations. More information concerning acceleration structures can be found in the OptiX programming guide . It is the responsibility of the user to set these up and pass them to the OptiX API which translates them to internal data structures that perform well on modern GPUs. The description created by the user can be very error-prone and it is sometimes hard to understand why the rendered result is not as expected. The Acceleration Structure Viewer is a component allowing OptiX users to inspect the acceleration structures they build before launching a ray tracing pipeline. The Acceleration Structure Viewer is opened through a button in the Resources window. The button will only be available when the currently viewed resource is OptiX: TraversableHandles . It opens the currently selected handle. The viewer is multi-paned: it shows a hierarchical view of the acceleration structure on the left, a graphical view of the acceleration structure in the middle, and controls and options on the right. In the hierarchical tree view on the left of the viewer the instance acceleration structures (IAS) , geometry acceleration structures (GAS) , child instances and child geometries are shown. In addition to this, some general properties for each of them is shown such as their primitive count, surface area and size on the device. In the hierarchical view on the left of the Acceleration Structure Viewer , the following information is displayed where applicable. Table 4. Acceleration Structure Hierarchical Columns  Column Description Name An identifier for each row in the hierarchy. Click on the check box next to the name to show or hide the selected geometry or hierarchy. Double-click on this entry to jump to the item in the rendering view. # Prims The number of primitives that make up this acceleration structure. Surface Area A calculation of the total surface area for the AABB that bounds the particular entry. Size The size of the output buffer on the device holding this acceleration structure . Performance analysis tools are accessible in the bottom left corner on the main view. These tools help identify potential performance problems that are outlined in the RTX Ray Tracing Best Practices Guide . These analysis tools aim to give a broad picture of acceleration structures that may exhibit sub-optimal performance. To find the most optimal solution, profiling and experimentation is recommended but these tools may paint a better picture as to why one structure performs poorly compared to another. Table 5. Acceleration Structure Analysis Tools  Action Description Instance Overlaps Identifies instance AABBs that overlap with other instances in 3D. Consider merging GASes when instance world-space AABBs overlap significantly to potentially increase performance. Instance Heatmap This allows you to set the threshold used by the AABB heatmap rendered in the visualizer. 3.11.1. Navigation  The Acceleration Structure Viewer supports multiple navigation modes. The navigation mode can be changed using the combo box in the camera controls pane, to the right of the rendering pane. The keyboard and mouse bindings for each mode are as follows: Table 6. Acceleration Structure Key Bindings  Binding Fly Camera Dolly Camera Orbit Camera WASD/Arrow Keys Move forward, backward, left, right Move forward, backward, left, right Track (Move up, down, left, right) E/Q Move up/down Move up/down n/a Z/C Increase/decrease field of view Increase/decrease field of view Increase/decrease field of view Shift/Ctrl Move faster/slower Move faster/slower Move faster/slower Mousewheel Zoom in/out Zoom in/out Zoom in/out LMB + Drag Rotate in place Rotate left/right, move forward/backward Rotate around the geometry RMB + Drag Zoom in/out Rotate in place Zoom in/out MMB + Drag Track (Move up, down, left, right) Track (Move up, down, left, right) Track (Move up, down, left, right) Alt Temporarily switch to Orbit Camera Temporarily switch to Orbit Camera n/a F/Double Click Focus on the selected geometry Focus on the selected geometry Focus on the selected geometry Based on the coordinate system of the input geometry, you may need to change the Up Direction setting to Z-Axis or the Coordinates setting to RHS. To reset the camera to its original location, click Reset Camera . There are also a selection of Camera Controls for fast and precise navigation. To save a position, use the bookmarks controls. Each node within the acceleration structure hierarchy can also be double-clicked to quickly navigate to that location. 3.11.2. Filtering and Highlighting  The acceleration structure view supports acceleration structure filtering as well as highlighting of data matching particular characteristics. The checkboxes next to each geometry allow users to toggle the rendering of each traversable. Geometry instances can also be selected by clicking on them in the main graphical view. Additionally, right clicking in the main graphical view gives options to hide or show all geometry, hide the selected geometry, or hide all but the selected geometry. Beyond filtering, the view also supports highlight-based identification of geometry specified with particular flags. Checking each highlight option will identify those resources matching that flag, colorizing for easy identification. Clicking an entry in this section will dim all geometry that does not meet the filter criteria allowing items that match the filter to standout. Selecting multiple filters requires the passing geometry to meet all selected filters (e.g., AND logic). Additionally, the heading text will be updated to reflect the number of items that meet this filter criteria. 3.11.3. Rendering Options  Under the highlight controls, additional rendering options are available. These include methods to control the geometry colors and the ability to toggle the drawing of wireframes for meshes and AABBs. 3.11.4. Exporting  The data displayed in the acceleration structure viewer document can be saved to file. Exporting an Acceleration Structure Viewer document allows for persisting the data you have collected beyond the immediate analysis session. This capability is particularly valuable for comparing different revisions of your geometry or sharing with others. Bookmarks are persisted as well. 3.12. Options  NVIDIA Nsight Compute options can be accessed via the main menu under Tools > Options . All options are persisted on disk and available the next time NVIDIA Nsight Compute is launched. When an option is changed from its default setting, its label will become bold. You can use the Restore Defaults button to restore all options to their default values. Profile options  3.12.1. Profile  Table 7. NVIDIA Nsight Compute Profile Options  Name Description Values Sections Directory Directory from which to import section files and rules. Relative paths are with respect to the NVIDIA Nsight Compute installation directory. Include Sub-Directories Recursively include section files and rules from sub-directories. Yes (Default)/No Apply Applicable Rules Automatically Automatically apply active and applicable rules. Yes (Default)/No Reload Rules Before Applying Force a rule reload before applying the rule to ensure changes in the rule script are recognized. Yes/No (Default) Default Report Page The report page to show when a report is generated or opened. Auto lets the tool decide the best page to show when opening a report. Session Summary Details Source Comments Call Stack/NVTX Raw Auto (default) Function Name Mode Determines how function/kernel names are shown. Auto (default): each component uses its preferred mode Demangled: kernel names are shown demangled with all parameters Function: kernel names are shown with their demangled function name without parameters Mangled: kernel names are shown with their mangled name, if applicable NVTX Rename Mode Determines how NVTX information is used for renaming. Range replay results are always renamed when possible. None: no renaming Kernel: kernel names are renamed using the most recent enclosing push/pop range Resources (default): resources like CPU threads or CUDA contexts and streams are renamed All: Kernel and Resources Show Metrics Aggregation Show aggregate of all results per each counter metric in the table header. Also, show aggregated value of all randomly selected metrics from summary or raw page table in the bottom-right label. Yes (Default)/No Rename Demangled Names Perform auto-simplification on kernel demangled names or import renamed names from a configuration file. See more unique keywords in kernel demangled names first while expanding the column. Yes (Default)/No Rename Kernels Config Path Use a configuration file to rename multiple demangled names. Also, export demangled names from the report to the specified file with mappings for renaming them. ncu-kernel-renames.yaml Maximum Baseline Name Length The maximum length of baseline names. 1..N (Default: 40) Number of Full Baselines to Display Number of baselines to display in the report header with all details in addition to the current result or the baseline added for the current result 0..N (Default: 2) Auto-Convert Metric Units Auto-adjust displayed metric units and values (e.g. Bytes to KBytes). Yes (Default)/No Show Instanced Metric Values Show the individual values of instanced metrics in tables. Yes/No (Default) Show Metrics As Floating Point Show all numeric metrics as floating-point numbers. Yes/No (Default) Show Knowledge Base Information Show information from the knowledge base in (metric) tooltips to explain terminology. Note: Nsight Compute needs to be restarted for this option to take effect. Yes (Default)/No Metrics/Properties List of metrics and properties to show on the summary page. Comma-separated list of metric entries. Each entry has the format {Label:MetricName}. Delay Load ‘Source’ Page Delays loading the content of the report page until the page becomes visible. Avoids processing costs and memory overhead until the report page is opened. Yes/No (Default) Show Single File For Multi-File Sources Shows a single file in each Source page view, even for multi-file sources. Yes/No (Default) Show Only Executed Functions Shows only executed functions in the source page views. Disabling this can impact performance. Yes (Default)/No Auto-Resolve Remote Source Files Automatically try to resolve remote source files on the source page (e.g. via SSH) if the connection is still registered. Yes/No (Default) Enable Register Dependencies Track dependencies between SASS registers/predicates and display them in the SASS view. Yes (Default)/No Kernel Analysis Size Threshold (KB) Enable SASS flow graph analysis for functions below this threshold. SASS analysis is required for Live Register and Register Dependency information. Set to -1 to enable analysis for all functions. -1..N (Default: 1024) Enable ELF Verification Enable ELF (cubin) verification to run every time before SASS analysis. This should only be enabled when working with applications compiled before CUDA 11.0 or when encountering source page issues. Yes/No (Default) API Call History Number of recent API calls shown in API Stream View. 1..N (Default: 100) 3.12.2. Environment  Table 8. NVIDIA Nsight Compute Environment Options  Name Description Values Color Theme The currently selected UI color theme. Dark (Default) Light Mixed DPI Scaling Disable Mixed DPI Scaling if unwanted artifacts are detected when using monitors with different DPIs. Auto (Default) Off Default Document Folder Directory where documents unassociated with a project will be saved. At Startup What to do when NVIDIA Nsight Compute is launched. Show welcome page (Default) Show quick launch dialog Load last project Show empty environment Show version update notifications Show notifications when a new version of this product is available. Yes (Default) No 3.12.3. Connection  Connection properties are grouped into Target Connection Options and Host Connection Properties . Target Connection Properties  The Target Connection Properties determine how the host connects to the target application during an Interactive Profile Activity . This connection is used to transfer profile information to the host during the profile session. Table 9. NVIDIA Nsight Compute Target Connection Properties  Name Description Values Base Port Base port used to establish a connection from the host to the target application during an Interactive Profile activity (both local and remote). 1-65535 (Default: 49152) Maximum Ports Maximum number of ports to try (starting from Base Port ) when attempting to connect to the target application. 2-65534 (Default: 64) Host Connection Properties  The Host Connection Properties determine how the command line profiler will connect to the host application during a Profile Activity . This connection is used to transfer profile information to the host during the profile session. Table 10. NVIDIA Nsight Compute Host Connection Options  Name Description Values Base Port Base port used to establish a connection from the command line profiler to the host application during a Profile activity (both local and remote). 1-65535 (Default: 50152) Maximum Ports Maximum number of ports to try (starting from Base Port ) when attempting to connect to the host application. 1-100 (Default: 10) 3.12.4. Source Lookup  Table 11. NVIDIA Nsight Compute Source Lookup Options  Name Description Values Program Source Locations Set program source search paths. These paths are used to resolve CUDA-C source files on the Source page if the respective file cannot be found in its original location. Files which cannot be found are marked with a File Not Found error. See the Ignore File Properties option for files that are found but don’t match. Ignore File Properties Ignore file properties (e.g. timestamp, size) for source resolution. If this is disabled, all file properties like modification timestamp and file size are checked against the information stored by the compiler in the application during compilation. If a file with the same name exists on a source lookup path, but not all properties match, it won’t be used for resolution (and a File Mismatch error will be shown). Yes/No (Default) 3.12.5. Send Feedback  Table 12. NVIDIA Nsight Compute Send Feedback Options  Name Description Values Collect Usage and Platform Data Choose whether or not you wish to allow NVIDIA Nsight Compute to collect usage and platform data. Yes No (Default) 3.13. Projects  NVIDIA Nsight Compute uses Project Files to group and organize profiling reports. At any given time, only one project can be open in NVIDIA Nsight Compute. Collected reports are automatically assigned to the current project. Reports stored on disk can be assigned to a project at any time. In addition to profiling reports, related files such as notes or source code can be associated with the project for future reference. Note that only references to reports or other files are saved in the project file. Those references can become invalid, for example when associated files are deleted, removed or not available on the current system, in case the project file was moved itself. NVIDIA Nsight Compute uses the ncu-proj file extension for project files. When no custom project is current, a default project is used to store e.g. the current Connection Dialog entries. To remove all information from the default project, you must close NVIDIA Nsight Compute and then delete the file from disk. On Windows, the file is located at <USER>\\AppData\\Local\\NVIDIA Corporation\\NVIDIA Nsight Compute\\ On Linux, the file is located at <USER>/.local/share/NVIDIA Corporation/NVIDIA Nsight Compute/ On MacOSX, the file is located at <USER>/Library/Application Support/NVIDIA Corporation/NVIDIA Nsight Compute/ 3.13.1. Project Dialogs  New Project Creates a new project. The project must be given a name, which will also be used for the project file. You can select the location where the project file should be saved on disk. Select whether a new directory with the project name should be created in that location. 3.13.2. Project Explorer  The Project Explorer window allows you to inspect and manage the current project. It shows the project name as well as all Items (profile reports and other files) associated with it. Right-click on any entry to see further actions, such as adding, removing or grouping items. Type in the Search project toolbar at the top to filter the currently shown entries. Project Explorer  3.14. Visual Profiler Transition Guide  This guide provides tips for moving from Visual Profiler to NVIDIA Nsight Compute. NVIDIA Nsight Compute tries to provide as much parity as possible with Visual Profiler’s kernel profiling features, but some functionality is now covered by different tools. 3.14.1. Trace  NVIDIA Nsight Compute does not support tracing GPU or API activities on an accurate timeline. This functionality is covered by NVIDIA Nsight Systems . In the Interactive Profile Activity , the API Stream tool window provides a stream of recent API calls on each thread. However, since all tracked API calls are serialized by default, it does not collect accurate timestamps. 3.14.2. Sessions  Instead of sessions, NVIDIA Nsight Compute uses Projects to launch and gather connection details and collected reports. Executable and Import Sessions Use the Project Explorer or the Main Menu to create a new project. Reports collected from the command line, i.e. using NVIDIA Nsight Compute CLI, can be opened directly using the main menu. In addition, you can use the Project Explorer to associate existing reports as well as any other artifacts such as executables, notes, etc., with the project. Note that those associations are only references; in other words, moving or deleting the project file on disk will not update its artifacts. nvprof or command-line profiler output files, as well as Visual Profiler sessions, cannot be imported into NVIDIA Nsight Compute. 3.14.3. Timeline  Since trace analysis is now covered by Nsight Systems, NVIDIA Nsight Compute does not provide views of the application timeline. The API Stream tool window does show a per-thread stream of the last captured CUDA API calls. However, those are serialized and do not maintain runtime concurrency or provide accurate timing information. 3.14.4. Analysis  Guided Analysis All trace-based analysis is now covered by NVIDIA Nsight Systems . This means that NVIDIA Nsight Compute does not include analysis regarding concurrent CUDA streams or (for example) UVM events. For per-kernel analysis, NVIDIA Nsight Compute provides recommendations based on collected performance data on the Details Page . These rules currently require you to collect the required metrics via their sections up front, and do not support partial on-demand profiling. To use the rule-based recommendations, enable the respective rules in the Metric Selection . Before profiling, enable Apply Rules in the Profile Options , or click the Apply Rules button in the report afterward. Unguided Analysis All trace-based analysis is now covered by Nsight Systems. For per-kernel analysis, Python-based rules provide analysis and recommendations. See Guided Analysis above for more details. PC Sampling View Source-correlated PC sampling information can now be viewed in the Source Page . Aggregated warp states are shown on the Details Page in the Warp State Statistics section. Memory Statistics Memory Statistics are located on the Details Page . Enable the Memory Workload Analysis sections to collect the respective information. NVLink View NVLink topology diagram and NVLink property table are located on the Details Page . Enable the NVLink Topology and NVLink Table sections to collect the respective information. Refer to the Known Issues section for the limitations related to NVLink. Source-Disassembly View Source correlated with PTX and SASS disassembly is shown on the Source Page . Which information is available depends on your application’s compilation/JIT flags. GPU Details View NVIDIA Nsight Compute does not automatically collect data for each executed kernel, and it does not collect any data for device-side memory copies. Summary information for all profiled kernel launches is shown on the Summary Page . Comprehensive information on all collected metrics for all profiled kernel launches is shown on the Raw Page . CPU Details View CPU callstack sampling is now covered by NVIDIA Nsight Systems . OpenACC Details View OpenACC performance analysis with NVIDIA Nsight Compute is available to limited extent. OpenACC parallel regions are not explicitly recognized, but CUDA kernels generated by the OpenACC compiler can be profiled as regular CUDA kernels. See the NVIDIA Nsight Systems release notes to check its latest support status. OpenMP Details View OpenMP performance analysis is not supported by NVIDIA Nsight Compute. See the NVIDIA Nsight Systems release notes to check its latest support status. Properties View NVIDIA Nsight Compute does not collect CUDA API and GPU activities and their properties. Performance data for profiled kernel launches is reported (for example) on the Details Page . Console View NVIDIA Nsight Compute does not currently collect stdout/stderr application output. Settings View Application launch settings are specified in the Connection Dialog . For reports collected from the UI, launch settings can be inspected on the Session Page after profiling. CPU Source View Source for CPU-only APIs is not available. Source for profiled GPU kernel launches is shown on the Source Page . 3.14.5. Command Line Arguments  Please execute ncu-ui with the -h parameter within a shell window to see the currently supported command line arguments for the NVIDIA Nsight Compute UI. To open a collected profile report with ncu-ui, simply pass the path to the report file as a parameter to the shell command. 3.15. Visual Studio Integration Guide  This guide provides information on using NVIDIA Nsight Compute within Microsoft Visual Studio, using the NVIDIA Nsight Integration Visual Studio extension, allowing for a seamless development workflow. 3.15.1. Visual Studio Integration Overview  NVIDIA Nsight Integration is a Visual Studio extension that allows you to access the power of NVIDIA Nsight Compute from within Visual Studio. When NVIDIA Nsight Compute is installed along with NVIDIA Nsight Integration, NVIDIA Nsight Compute activities will appear under the NVIDIA ‘Nsight’ menu in the Visual Studio menu bar. These activities launch NVIDIA Nsight Compute with the current project settings and executable. For more information about using NVIDIA Nsight Compute from within Visual Studio, please visit NVIDIA Nsight Integration Overview NVIDIA Nsight Integration User Guide Notices Notices ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. Information furnished is believed to be accurate and reliable. However, NVIDIA Corporation assumes no responsibility for the consequences of use of such information or for any infringement of patents or other rights of third parties that may result from its use. No license is granted by implication of otherwise under any patent rights of NVIDIA Corporation. Specifications mentioned in this publication are subject to change without notice. This publication supersedes and replaces all other information previously supplied. NVIDIA Corporation products are not authorized as critical components in life support devices or systems without express written approval of NVIDIA Corporation. Trademarks NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. © Copyright 2018-2024, NVIDIA Corporation & Affiliates. All rights reserved. Last updated on Jun 03, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH__INTRINSIC__SIMD.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH__INTRINSIC__SIMD.html", "content_type": "text/html", "text": "11. SIMD Intrinsics — CUDA Math API Reference Manual 12.5 documentation 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices CUDA Math API Reference Manual » 11. SIMD Intrinsics v12.5 | PDF | Archive 11. SIMD Intrinsics  This section describes SIMD intrinsic functions that are only supported in device code. To use these functions you do not need to include any additional header files in your program. Functions __device__ unsigned int __vabs2 (unsigned int a) Computes per-halfword absolute value. __device__ unsigned int __vabs4 (unsigned int a) Computes per-byte absolute value. __device__ unsigned int __vabsdiffs2 (unsigned int a, unsigned int b) Computes per-halfword sum of absolute difference of signed integer. __device__ unsigned int __vabsdiffs4 (unsigned int a, unsigned int b) Computes per-byte absolute difference of signed integer. __device__ unsigned int __vabsdiffu2 (unsigned int a, unsigned int b) Performs per-halfword absolute difference of unsigned integer computation: |a - b|. __device__ unsigned int __vabsdiffu4 (unsigned int a, unsigned int b) Computes per-byte absolute difference of unsigned integer. __device__ unsigned int __vabsss2 (unsigned int a) Computes per-halfword absolute value with signed saturation. __device__ unsigned int __vabsss4 (unsigned int a) Computes per-byte absolute value with signed saturation. __device__ unsigned int __vadd2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed addition, with wrap-around: a + b. __device__ unsigned int __vadd4 (unsigned int a, unsigned int b) Performs per-byte (un)signed addition. __device__ unsigned int __vaddss2 (unsigned int a, unsigned int b) Performs per-halfword addition with signed saturation. __device__ unsigned int __vaddss4 (unsigned int a, unsigned int b) Performs per-byte addition with signed saturation. __device__ unsigned int __vaddus2 (unsigned int a, unsigned int b) Performs per-halfword addition with unsigned saturation. __device__ unsigned int __vaddus4 (unsigned int a, unsigned int b) Performs per-byte addition with unsigned saturation. __device__ unsigned int __vavgs2 (unsigned int a, unsigned int b) Performs per-halfword signed rounded average computation. __device__ unsigned int __vavgs4 (unsigned int a, unsigned int b) Computes per-byte signed rounded average. __device__ unsigned int __vavgu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned rounded average computation. __device__ unsigned int __vavgu4 (unsigned int a, unsigned int b) Performs per-byte unsigned rounded average. __device__ unsigned int __vcmpeq2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed comparison. __device__ unsigned int __vcmpeq4 (unsigned int a, unsigned int b) Performs per-byte (un)signed comparison. __device__ unsigned int __vcmpges2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison: a >= b ? 0xffff : 0. __device__ unsigned int __vcmpges4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vcmpgeu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned comparison: a >= b ? 0xffff : 0. __device__ unsigned int __vcmpgeu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vcmpgts2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison: a > b ? 0xffff : 0. __device__ unsigned int __vcmpgts4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vcmpgtu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned comparison: a > b ? 0xffff : 0. __device__ unsigned int __vcmpgtu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vcmples2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison: a <= b ? 0xffff : 0. __device__ unsigned int __vcmples4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vcmpleu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned comparison: a <= b ? 0xffff : 0. __device__ unsigned int __vcmpleu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vcmplts2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison: a < b ? 0xffff : 0. __device__ unsigned int __vcmplts4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vcmpltu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned comparison: a < b ? 0xffff : 0. __device__ unsigned int __vcmpltu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vcmpne2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed comparison: a != b ? 0xffff : 0. __device__ unsigned int __vcmpne4 (unsigned int a, unsigned int b) Performs per-byte (un)signed comparison. __device__ unsigned int __vhaddu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned average computation. __device__ unsigned int __vhaddu4 (unsigned int a, unsigned int b) Computes per-byte unsigned average. __host__ __device__ unsigned int __viaddmax_s16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(a + b, c) __host__ __device__ unsigned int __viaddmax_s16x2_relu (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(max(a + b, c), 0) __host__ __device__ int __viaddmax_s32 (const int a, const int b, const int c) Computes max(a + b, c) __host__ __device__ int __viaddmax_s32_relu (const int a, const int b, const int c) Computes max(max(a + b, c), 0) __host__ __device__ unsigned int __viaddmax_u16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(a + b, c) __host__ __device__ unsigned int __viaddmax_u32 (const unsigned int a, const unsigned int b, const unsigned int c) Computes max(a + b, c) __host__ __device__ unsigned int __viaddmin_s16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword min(a + b, c) __host__ __device__ unsigned int __viaddmin_s16x2_relu (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(min(a + b, c), 0) __host__ __device__ int __viaddmin_s32 (const int a, const int b, const int c) Computes min(a + b, c) __host__ __device__ int __viaddmin_s32_relu (const int a, const int b, const int c) Computes max(min(a + b, c), 0) __host__ __device__ unsigned int __viaddmin_u16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword min(a + b, c) __host__ __device__ unsigned int __viaddmin_u32 (const unsigned int a, const unsigned int b, const unsigned int c) Computes min(a + b, c) __host__ __device__ unsigned int __vibmax_s16x2 (const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo) Performs per-halfword max(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a >= b). __host__ __device__ int __vibmax_s32 (const int a, const int b, bool *const pred) Computes max(a, b), also sets the value pointed to by pred to (a >= b). __host__ __device__ unsigned int __vibmax_u16x2 (const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo) Performs per-halfword max(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a >= b). __host__ __device__ unsigned int __vibmax_u32 (const unsigned int a, const unsigned int b, bool *const pred) Computes max(a, b), also sets the value pointed to by pred to (a >= b). __host__ __device__ unsigned int __vibmin_s16x2 (const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo) Performs per-halfword min(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a <= b). __host__ __device__ int __vibmin_s32 (const int a, const int b, bool *const pred) Computes min(a, b), also sets the value pointed to by pred to (a <= b). __host__ __device__ unsigned int __vibmin_u16x2 (const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo) Performs per-halfword min(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a <= b). __host__ __device__ unsigned int __vibmin_u32 (const unsigned int a, const unsigned int b, bool *const pred) Computes min(a, b), also sets the value pointed to by pred to (a <= b). __host__ __device__ unsigned int __vimax3_s16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(max(a, b), c) __host__ __device__ unsigned int __vimax3_s16x2_relu (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(max(max(a, b), c), 0) __host__ __device__ int __vimax3_s32 (const int a, const int b, const int c) Computes max(max(a, b), c) __host__ __device__ int __vimax3_s32_relu (const int a, const int b, const int c) Computes max(max(max(a, b), c), 0) __host__ __device__ unsigned int __vimax3_u16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(max(a, b), c) __host__ __device__ unsigned int __vimax3_u32 (const unsigned int a, const unsigned int b, const unsigned int c) Computes max(max(a, b), c) __host__ __device__ unsigned int __vimax_s16x2_relu (const unsigned int a, const unsigned int b) Performs per-halfword max(max(a, b), 0) __host__ __device__ int __vimax_s32_relu (const int a, const int b) Computes max(max(a, b), 0) __host__ __device__ unsigned int __vimin3_s16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword min(min(a, b), c) __host__ __device__ unsigned int __vimin3_s16x2_relu (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword max(min(min(a, b), c), 0) __host__ __device__ int __vimin3_s32 (const int a, const int b, const int c) Computes min(min(a, b), c) __host__ __device__ int __vimin3_s32_relu (const int a, const int b, const int c) Computes max(min(min(a, b), c), 0) __host__ __device__ unsigned int __vimin3_u16x2 (const unsigned int a, const unsigned int b, const unsigned int c) Performs per-halfword min(min(a, b), c) __host__ __device__ unsigned int __vimin3_u32 (const unsigned int a, const unsigned int b, const unsigned int c) Computes min(min(a, b), c) __host__ __device__ unsigned int __vimin_s16x2_relu (const unsigned int a, const unsigned int b) Performs per-halfword max(min(a, b), 0) __host__ __device__ int __vimin_s32_relu (const int a, const int b) Computes max(min(a, b), 0) __device__ unsigned int __vmaxs2 (unsigned int a, unsigned int b) Performs per-halfword signed maximum computation. __device__ unsigned int __vmaxs4 (unsigned int a, unsigned int b) Computes per-byte signed maximum. __device__ unsigned int __vmaxu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned maximum computation. __device__ unsigned int __vmaxu4 (unsigned int a, unsigned int b) Computes per-byte unsigned maximum. __device__ unsigned int __vmins2 (unsigned int a, unsigned int b) Performs per-halfword signed minimum computation. __device__ unsigned int __vmins4 (unsigned int a, unsigned int b) Computes per-byte signed minimum. __device__ unsigned int __vminu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned minimum computation. __device__ unsigned int __vminu4 (unsigned int a, unsigned int b) Computes per-byte unsigned minimum. __device__ unsigned int __vneg2 (unsigned int a) Computes per-halfword negation. __device__ unsigned int __vneg4 (unsigned int a) Performs per-byte negation. __device__ unsigned int __vnegss2 (unsigned int a) Computes per-halfword negation with signed saturation. __device__ unsigned int __vnegss4 (unsigned int a) Performs per-byte negation with signed saturation. __device__ unsigned int __vsads2 (unsigned int a, unsigned int b) Performs per-halfword sum of absolute difference of signed. __device__ unsigned int __vsads4 (unsigned int a, unsigned int b) Computes per-byte sum of abs difference of signed. __device__ unsigned int __vsadu2 (unsigned int a, unsigned int b) Computes per-halfword sum of abs diff of unsigned. __device__ unsigned int __vsadu4 (unsigned int a, unsigned int b) Computes per-byte sum of abs difference of unsigned. __device__ unsigned int __vseteq2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed comparison. __device__ unsigned int __vseteq4 (unsigned int a, unsigned int b) Performs per-byte (un)signed comparison. __device__ unsigned int __vsetges2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison. __device__ unsigned int __vsetges4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vsetgeu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned minimum unsigned comparison. __device__ unsigned int __vsetgeu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vsetgts2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison. __device__ unsigned int __vsetgts4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vsetgtu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned comparison. __device__ unsigned int __vsetgtu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vsetles2 (unsigned int a, unsigned int b) Performs per-halfword unsigned minimum computation. __device__ unsigned int __vsetles4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vsetleu2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison. __device__ unsigned int __vsetleu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vsetlts2 (unsigned int a, unsigned int b) Performs per-halfword signed comparison. __device__ unsigned int __vsetlts4 (unsigned int a, unsigned int b) Performs per-byte signed comparison. __device__ unsigned int __vsetltu2 (unsigned int a, unsigned int b) Performs per-halfword unsigned comparison. __device__ unsigned int __vsetltu4 (unsigned int a, unsigned int b) Performs per-byte unsigned comparison. __device__ unsigned int __vsetne2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed comparison. __device__ unsigned int __vsetne4 (unsigned int a, unsigned int b) Performs per-byte (un)signed comparison. __device__ unsigned int __vsub2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed subtraction, with wrap-around. __device__ unsigned int __vsub4 (unsigned int a, unsigned int b) Performs per-byte subtraction. __device__ unsigned int __vsubss2 (unsigned int a, unsigned int b) Performs per-halfword (un)signed subtraction, with signed saturation. __device__ unsigned int __vsubss4 (unsigned int a, unsigned int b) Performs per-byte subtraction with signed saturation. __device__ unsigned int __vsubus2 (unsigned int a, unsigned int b) Performs per-halfword subtraction with unsigned saturation. __device__ unsigned int __vsubus4 (unsigned int a, unsigned int b) Performs per-byte subtraction with unsigned saturation. 11.1. Functions  __device__ unsigned int __vabs2 ( unsigned int a )  Computes per-halfword absolute value. Splits 4 bytes of argument into 2 parts, each consisting of 2 bytes, then computes absolute value for each of parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabs4 ( unsigned int a )  Computes per-byte absolute value. Splits argument by bytes. Computes absolute value of each byte. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabsdiffs2 ( unsigned int a , unsigned int b )  Computes per-halfword sum of absolute difference of signed integer. Splits 4 bytes of each into 2 parts, each consisting of 2 bytes. For corresponding parts function computes absolute difference. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabsdiffs4 ( unsigned int a , unsigned int b )  Computes per-byte absolute difference of signed integer. Splits 4 bytes of each into 4 parts, each consisting of 1 byte. For corresponding parts function computes absolute difference. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabsdiffu2 ( unsigned int a , unsigned int b )  Performs per-halfword absolute difference of unsigned integer computation: |a - b|. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes absolute difference. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabsdiffu4 ( unsigned int a , unsigned int b )  Computes per-byte absolute difference of unsigned integer. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes absolute difference. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabsss2 ( unsigned int a )  Computes per-halfword absolute value with signed saturation. Splits 4 bytes of argument into 2 parts, each consisting of 2 bytes, then computes absolute value with signed saturation for each of parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vabsss4 ( unsigned int a )  Computes per-byte absolute value with signed saturation. Splits 4 bytes of argument into 4 parts, each consisting of 1 byte, then computes absolute value with signed saturation for each of parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vadd2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed addition, with wrap-around: a + b. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes, then performs unsigned addition on corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vadd4 ( unsigned int a , unsigned int b )  Performs per-byte (un)signed addition. Splits ‘a’ into 4 bytes, then performs unsigned addition on each of these bytes with the corresponding byte from ‘b’, ignoring overflow. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vaddss2 ( unsigned int a , unsigned int b )  Performs per-halfword addition with signed saturation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes, then performs addition with signed saturation on corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vaddss4 ( unsigned int a , unsigned int b )  Performs per-byte addition with signed saturation. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte, then performs addition with signed saturation on corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vaddus2 ( unsigned int a , unsigned int b )  Performs per-halfword addition with unsigned saturation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes, then performs addition with unsigned saturation on corresponding parts. Returns Returns computed value. __device__ unsigned int __vaddus4 ( unsigned int a , unsigned int b )  Performs per-byte addition with unsigned saturation. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte, then performs addition with unsigned saturation on corresponding parts. Returns Returns computed value. __device__ unsigned int __vavgs2 ( unsigned int a , unsigned int b )  Performs per-halfword signed rounded average computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes, then computes signed rounded average of corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vavgs4 ( unsigned int a , unsigned int b )  Computes per-byte signed rounded average. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. then computes signed rounded average of corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vavgu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned rounded average computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes, then computes unsigned rounded average of corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vavgu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned rounded average. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. then computes unsigned rounded average of corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vcmpeq2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if they are equal, and 0000 otherwise. For example __vcmpeq2(0x1234aba5, 0x1234aba6) returns 0xffff0000. Returns Returns 0xffff computed value. __device__ unsigned int __vcmpeq4 ( unsigned int a , unsigned int b )  Performs per-byte (un)signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if they are equal, and 00 otherwise. For example __vcmpeq4(0x1234aba5, 0x1234aba6) returns 0xffffff00. Returns Returns 0xff if a = b, else returns 0. __device__ unsigned int __vcmpges2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison: a >= b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part >= ‘b’ part, and 0000 otherwise. For example __vcmpges2(0x1234aba5, 0x1234aba6) returns 0xffff0000. Returns Returns 0xffff if a >= b, else returns 0. __device__ unsigned int __vcmpges4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part >= ‘b’ part, and 00 otherwise. For example __vcmpges4(0x1234aba5, 0x1234aba6) returns 0xffffff00. Returns Returns 0xff if a >= b, else returns 0. __device__ unsigned int __vcmpgeu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned comparison: a >= b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part >= ‘b’ part, and 0000 otherwise. For example __vcmpgeu2(0x1234aba5, 0x1234aba6) returns 0xffff0000. Returns Returns 0xffff if a >= b, else returns 0. __device__ unsigned int __vcmpgeu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part >= ‘b’ part, and 00 otherwise. For example __vcmpgeu4(0x1234aba5, 0x1234aba6) returns 0xffffff00. Returns Returns 0xff if a >= b, else returns 0. __device__ unsigned int __vcmpgts2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison: a > b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part > ‘b’ part, and 0000 otherwise. For example __vcmpgts2(0x1234aba5, 0x1234aba6) returns 0x00000000. Returns Returns 0xffff if a > b, else returns 0. __device__ unsigned int __vcmpgts4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part > ‘b’ part, and 00 otherwise. For example __vcmpgts4(0x1234aba5, 0x1234aba6) returns 0x00000000. Returns Returns 0xff if a > b, else returns 0. __device__ unsigned int __vcmpgtu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned comparison: a > b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part > ‘b’ part, and 0000 otherwise. For example __vcmpgtu2(0x1234aba5, 0x1234aba6) returns 0x00000000. Returns Returns 0xffff if a > b, else returns 0. __device__ unsigned int __vcmpgtu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part > ‘b’ part, and 00 otherwise. For example __vcmpgtu4(0x1234aba5, 0x1234aba6) returns 0x00000000. Returns Returns 0xff if a > b, else returns 0. __device__ unsigned int __vcmples2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison: a <= b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part <= ‘b’ part, and 0000 otherwise. For example __vcmples2(0x1234aba5, 0x1234aba6) returns 0xffffffff. Returns Returns 0xffff if a <= b, else returns 0. __device__ unsigned int __vcmples4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part <= ‘b’ part, and 00 otherwise. For example __vcmples4(0x1234aba5, 0x1234aba6) returns 0xffffffff. Returns Returns 0xff if a <= b, else returns 0. __device__ unsigned int __vcmpleu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned comparison: a <= b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part <= ‘b’ part, and 0000 otherwise. For example __vcmpleu2(0x1234aba5, 0x1234aba6) returns 0xffffffff. Returns Returns 0xffff if a <= b, else returns 0. __device__ unsigned int __vcmpleu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part <= ‘b’ part, and 00 otherwise. For example __vcmpleu4(0x1234aba5, 0x1234aba6) returns 0xffffffff. Returns Returns 0xff if a <= b, else returns 0. __device__ unsigned int __vcmplts2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison: a < b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part < ‘b’ part, and 0000 otherwise. For example __vcmplts2(0x1234aba5, 0x1234aba6) returns 0x0000ffff. Returns Returns 0xffff if a < b, else returns 0. __device__ unsigned int __vcmplts4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part < ‘b’ part, and 00 otherwise. For example __vcmplts4(0x1234aba5, 0x1234aba6) returns 0x000000ff. Returns Returns 0xff if a < b, else returns 0. __device__ unsigned int __vcmpltu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned comparison: a < b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part < ‘b’ part, and 0000 otherwise. For example __vcmpltu2(0x1234aba5, 0x1234aba6) returns 0x0000ffff. Returns Returns 0xffff if a < b, else returns 0. __device__ unsigned int __vcmpltu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part < ‘b’ part, and 00 otherwise. For example __vcmpltu4(0x1234aba5, 0x1234aba6) returns 0x000000ff. Returns Returns 0xff if a < b, else returns 0. __device__ unsigned int __vcmpne2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed comparison: a != b ? 0xffff : 0. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts result is ffff if ‘a’ part != ‘b’ part, and 0000 otherwise. For example __vcmplts2(0x1234aba5, 0x1234aba6) returns 0x0000ffff. Returns Returns 0xffff if a != b, else returns 0. __device__ unsigned int __vcmpne4 ( unsigned int a , unsigned int b )  Performs per-byte (un)signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts result is ff if ‘a’ part != ‘b’ part, and 00 otherwise. For example __vcmplts4(0x1234aba5, 0x1234aba6) returns 0x000000ff. Returns Returns 0xff if a != b, else returns 0. __device__ unsigned int __vhaddu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned average computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes, then computes unsigned average of corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vhaddu4 ( unsigned int a , unsigned int b )  Computes per-byte unsigned average. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. then computes unsigned average of corresponding parts. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __viaddmax_s16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(a + b, c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs an add and compare: max(a_part + b_part), c_part) Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __viaddmax_s16x2_relu ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(max(a + b, c), 0) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs an add, followed by a max with relu: max(max(a_part + b_part), c_part), 0) Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ int __viaddmax_s32 ( const int a , const int b , const int c )  Computes max(a + b, c) Calculates the sum of signed integers a and b and takes the max with c . Returns Returns computed value. __host__ __device__ int __viaddmax_s32_relu ( const int a , const int b , const int c )  Computes max(max(a + b, c), 0) Calculates the sum of signed integers a and b and takes the max with c . If the result is less than 0 then 0 is returned. Returns Returns computed value. __host__ __device__ unsigned int __viaddmax_u16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(a + b, c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as unsigned shorts. For corresponding parts function performs an add and compare: max(a_part + b_part), c_part) Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __viaddmax_u32 ( const unsigned int a , const unsigned int b , const unsigned int c )  Computes max(a + b, c) Calculates the sum of unsigned integers a and b and takes the max with c . Returns Returns computed value. __host__ __device__ unsigned int __viaddmin_s16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword min(a + b, c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs an add and compare: min(a_part + b_part), c_part) Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __viaddmin_s16x2_relu ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(min(a + b, c), 0) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs an add, followed by a min with relu: max(min(a_part + b_part), c_part), 0) Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ int __viaddmin_s32 ( const int a , const int b , const int c )  Computes min(a + b, c) Calculates the sum of signed integers a and b and takes the min with c . Returns Returns computed value. __host__ __device__ int __viaddmin_s32_relu ( const int a , const int b , const int c )  Computes max(min(a + b, c), 0) Calculates the sum of signed integers a and b and takes the min with c . If the result is less than 0 then 0 is returned. Returns Returns computed value. __host__ __device__ unsigned int __viaddmin_u16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword min(a + b, c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as unsigned shorts. For corresponding parts function performs an add and compare: min(a_part + b_part), c_part) Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __viaddmin_u32 ( const unsigned int a , const unsigned int b , const unsigned int c )  Computes min(a + b, c) Calculates the sum of unsigned integers a and b and takes the min with c . Returns Returns computed value. __host__ __device__ unsigned int __vibmax_s16x2 ( const unsigned int a , const unsigned int b , bool * const pred_hi , bool * const pred_lo )  Performs per-halfword max(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a >= b). Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a maximum ( = max(a_part, b_part) ). Partial results are recombined and returned as unsigned int. Sets the value pointed to by pred_hi to the value (a_high_part >= b_high_part). Sets the value pointed to by pred_lo to the value (a_low_part >= b_low_part). Returns Returns computed values. __host__ __device__ int __vibmax_s32 ( const int a , const int b , bool * const pred )  Computes max(a, b), also sets the value pointed to by pred to (a >= b). Calculates the maximum of a and b of two signed ints. Also sets the value pointed to by pred to the value (a >= b). Returns Returns computed values. __host__ __device__ unsigned int __vibmax_u16x2 ( const unsigned int a , const unsigned int b , bool * const pred_hi , bool * const pred_lo )  Performs per-halfword max(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a >= b). Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as unsigned shorts. For corresponding parts function performs a maximum ( = max(a_part, b_part) ). Partial results are recombined and returned as unsigned int. Sets the value pointed to by pred_hi to the value (a_high_part >= b_high_part). Sets the value pointed to by pred_lo to the value (a_low_part >= b_low_part). Returns Returns computed values. __host__ __device__ unsigned int __vibmax_u32 ( const unsigned int a , const unsigned int b , bool * const pred )  Computes max(a, b), also sets the value pointed to by pred to (a >= b). Calculates the maximum of a and b of two unsigned ints. Also sets the value pointed to by pred to the value (a >= b). Returns Returns computed values. __host__ __device__ unsigned int __vibmin_s16x2 ( const unsigned int a , const unsigned int b , bool * const pred_hi , bool * const pred_lo )  Performs per-halfword min(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a <= b). Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a maximum ( = max(a_part, b_part) ). Partial results are recombined and returned as unsigned int. Sets the value pointed to by pred_hi to the value (a_high_part <= b_high_part). Sets the value pointed to by pred_lo to the value (a_low_part <= b_low_part). Returns Returns computed values. __host__ __device__ int __vibmin_s32 ( const int a , const int b , bool * const pred )  Computes min(a, b), also sets the value pointed to by pred to (a <= b). Calculates the minimum of a and b of two signed ints. Also sets the value pointed to by pred to the value (a <= b). Returns Returns computed values. __host__ __device__ unsigned int __vibmin_u16x2 ( const unsigned int a , const unsigned int b , bool * const pred_hi , bool * const pred_lo )  Performs per-halfword min(a, b), also sets the value pointed to by pred_hi and pred_lo to the per-halfword result of (a <= b). Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as unsigned shorts. For corresponding parts function performs a maximum ( = max(a_part, b_part) ). Partial results are recombined and returned as unsigned int. Sets the value pointed to by pred_hi to the value (a_high_part <= b_high_part). Sets the value pointed to by pred_lo to the value (a_low_part <= b_low_part). Returns Returns computed values. __host__ __device__ unsigned int __vibmin_u32 ( const unsigned int a , const unsigned int b , bool * const pred )  Computes min(a, b), also sets the value pointed to by pred to (a <= b). Calculates the minimum of a and b of two unsigned ints. Also sets the value pointed to by pred to the value (a <= b). Returns Returns computed values. __host__ __device__ unsigned int __vimax3_s16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(max(a, b), c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a 3-way max ( = max(max(a_part, b_part), c_part) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __vimax3_s16x2_relu ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(max(max(a, b), c), 0) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a three-way max with relu ( = max(a_part, b_part, c_part, 0) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ int __vimax3_s32 ( const int a , const int b , const int c )  Computes max(max(a, b), c) Calculates the 3-way max of signed integers a , b and c . Returns Returns computed value. __host__ __device__ int __vimax3_s32_relu ( const int a , const int b , const int c )  Computes max(max(max(a, b), c), 0) Calculates the maximum of three signed ints, if this is less than 0 then 0 is returned. Returns Returns computed value. __host__ __device__ unsigned int __vimax3_u16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(max(a, b), c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as unsigned shorts. For corresponding parts function performs a 3-way max ( = max(max(a_part, b_part), c_part) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __vimax3_u32 ( const unsigned int a , const unsigned int b , const unsigned int c )  Computes max(max(a, b), c) Calculates the 3-way max of unsigned integers a , b and c . Returns Returns computed value. __host__ __device__ unsigned int __vimax_s16x2_relu ( const unsigned int a , const unsigned int b )  Performs per-halfword max(max(a, b), 0) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a max with relu ( = max(a_part, b_part, 0) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ int __vimax_s32_relu ( const int a , const int b )  Computes max(max(a, b), 0) Calculates the maximum of a and b of two signed ints, if this is less than 0 then 0 is returned. Returns Returns computed value. __host__ __device__ unsigned int __vimin3_s16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword min(min(a, b), c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a 3-way min ( = min(min(a_part, b_part), c_part) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __vimin3_s16x2_relu ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword max(min(min(a, b), c), 0) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a three-way min with relu ( = max(min(a_part, b_part, c_part), 0) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ int __vimin3_s32 ( const int a , const int b , const int c )  Computes min(min(a, b), c) Calculates the 3-way min of signed integers a , b and c . Returns Returns computed value. __host__ __device__ int __vimin3_s32_relu ( const int a , const int b , const int c )  Computes max(min(min(a, b), c), 0) Calculates the minimum of three signed ints, if this is less than 0 then 0 is returned. Returns Returns computed value. __host__ __device__ unsigned int __vimin3_u16x2 ( const unsigned int a , const unsigned int b , const unsigned int c )  Performs per-halfword min(min(a, b), c) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as unsigned shorts. For corresponding parts function performs a 3-way min ( = min(min(a_part, b_part), c_part) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ unsigned int __vimin3_u32 ( const unsigned int a , const unsigned int b , const unsigned int c )  Computes min(min(a, b), c) Calculates the 3-way min of unsigned integers a , b and c . Returns Returns computed value. __host__ __device__ unsigned int __vimin_s16x2_relu ( const unsigned int a , const unsigned int b )  Performs per-halfword max(min(a, b), 0) Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. These 2 byte parts are interpreted as signed shorts. For corresponding parts function performs a min with relu ( = max(min(a_part, b_part), 0) ). Partial results are recombined and returned as unsigned int. Returns Returns computed value. __host__ __device__ int __vimin_s32_relu ( const int a , const int b )  Computes max(min(a, b), 0) Calculates the minimum of a and b of two signed ints, if this is less than 0 then 0 is returned. Returns Returns computed value. __device__ unsigned int __vmaxs2 ( unsigned int a , unsigned int b )  Performs per-halfword signed maximum computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes signed maximum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vmaxs4 ( unsigned int a , unsigned int b )  Computes per-byte signed maximum. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes signed maximum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vmaxu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned maximum computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes unsigned maximum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vmaxu4 ( unsigned int a , unsigned int b )  Computes per-byte unsigned maximum. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes unsigned maximum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vmins2 ( unsigned int a , unsigned int b )  Performs per-halfword signed minimum computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes signed minimum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vmins4 ( unsigned int a , unsigned int b )  Computes per-byte signed minimum. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes signed minimum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vminu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned minimum computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes unsigned minimum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vminu4 ( unsigned int a , unsigned int b )  Computes per-byte unsigned minimum. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes unsigned minimum. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vneg2 ( unsigned int a )  Computes per-halfword negation. Splits 4 bytes of argument into 2 parts, each consisting of 2 bytes. For each part function computes negation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vneg4 ( unsigned int a )  Performs per-byte negation. Splits 4 bytes of argument into 4 parts, each consisting of 1 byte. For each part function computes negation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vnegss2 ( unsigned int a )  Computes per-halfword negation with signed saturation. Splits 4 bytes of argument into 2 parts, each consisting of 2 bytes. For each part function computes negation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vnegss4 ( unsigned int a )  Performs per-byte negation with signed saturation. Splits 4 bytes of argument into 4 parts, each consisting of 1 byte. For each part function computes negation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsads2 ( unsigned int a , unsigned int b )  Performs per-halfword sum of absolute difference of signed. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes absolute difference and sum it up. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsads4 ( unsigned int a , unsigned int b )  Computes per-byte sum of abs difference of signed. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes absolute difference and sum it up. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsadu2 ( unsigned int a , unsigned int b )  Computes per-halfword sum of abs diff of unsigned. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function computes absolute differences and returns sum of those differences. Returns Returns computed value. __device__ unsigned int __vsadu4 ( unsigned int a , unsigned int b )  Computes per-byte sum of abs difference of unsigned. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function computes absolute differences and returns sum of those differences. Returns Returns computed value. __device__ unsigned int __vseteq2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part == ‘b’ part. If both equalities are satisfied, function returns 1. Returns Returns 1 if a = b, else returns 0. __device__ unsigned int __vseteq4 ( unsigned int a , unsigned int b )  Performs per-byte (un)signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part == ‘b’ part. If both equalities are satisfied, function returns 1. Returns Returns 1 if a = b, else returns 0. __device__ unsigned int __vsetges2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part >= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a >= b, else returns 0. __device__ unsigned int __vsetges4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part >= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a >= b, else returns 0. __device__ unsigned int __vsetgeu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned minimum unsigned comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part >= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a >= b, else returns 0. __device__ unsigned int __vsetgeu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part >= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a >= b, else returns 0. __device__ unsigned int __vsetgts2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part > ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a > b, else returns 0. __device__ unsigned int __vsetgts4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part > ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a > b, else returns 0. __device__ unsigned int __vsetgtu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part > ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a > b, else returns 0. __device__ unsigned int __vsetgtu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part > ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a > b, else returns 0. __device__ unsigned int __vsetles2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned minimum computation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a <= b, else returns 0. __device__ unsigned int __vsetles4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a <= b, else returns 0. __device__ unsigned int __vsetleu2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a <= b, else returns 0. __device__ unsigned int __vsetleu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 part, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a <= b, else returns 0. __device__ unsigned int __vsetlts2 ( unsigned int a , unsigned int b )  Performs per-halfword signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a < b, else returns 0. __device__ unsigned int __vsetlts4 ( unsigned int a , unsigned int b )  Performs per-byte signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a < b, else returns 0. __device__ unsigned int __vsetltu2 ( unsigned int a , unsigned int b )  Performs per-halfword unsigned comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a < b, else returns 0. __device__ unsigned int __vsetltu4 ( unsigned int a , unsigned int b )  Performs per-byte unsigned comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part <= ‘b’ part. If both inequalities are satisfied, function returns 1. Returns Returns 1 if a < b, else returns 0. __device__ unsigned int __vsetne2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed comparison. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs comparison ‘a’ part != ‘b’ part. If both conditions are satisfied, function returns 1. Returns Returns 1 if a != b, else returns 0. __device__ unsigned int __vsetne4 ( unsigned int a , unsigned int b )  Performs per-byte (un)signed comparison. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs comparison ‘a’ part != ‘b’ part. If both conditions are satisfied, function returns 1. Returns Returns 1 if a != b, else returns 0. __device__ unsigned int __vsub2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed subtraction, with wrap-around. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs subtraction. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsub4 ( unsigned int a , unsigned int b )  Performs per-byte subtraction. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs subtraction. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsubss2 ( unsigned int a , unsigned int b )  Performs per-halfword (un)signed subtraction, with signed saturation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs subtraction with signed saturation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsubss4 ( unsigned int a , unsigned int b )  Performs per-byte subtraction with signed saturation. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs subtraction with signed saturation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsubus2 ( unsigned int a , unsigned int b )  Performs per-halfword subtraction with unsigned saturation. Splits 4 bytes of each argument into 2 parts, each consisting of 2 bytes. For corresponding parts function performs subtraction with unsigned saturation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. __device__ unsigned int __vsubus4 ( unsigned int a , unsigned int b )  Performs per-byte subtraction with unsigned saturation. Splits 4 bytes of each argument into 4 parts, each consisting of 1 byte. For corresponding parts function performs subtraction with unsigned saturation. Partial results are recombined and returned as unsigned int. Returns Returns computed value. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y1f.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y1f.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.317. __nv_y1f Prototype : float @__nv_y1f(float %x) Description : Calculate the value of the Bessel function of the second kind of order 1 for the input argument x , Y 1 ( x ) . Returns: Returns the value of the Bessel function of the second kind of order 1. __nv_y1f(0) returns − ∞ . __nv_y1f( x ) returns NaN for x < 0. __nv_y1f( + ∞ ) returns +0. __nv_y1f(NaN) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ynf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ynf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.319. __nv_ynf Prototype : float @__nv_ynf(i32 %n, float %x) Description : Calculate the value of the Bessel function of the second kind of order n for the input argument x , Y n ( x ) . Returns: Returns the value of the Bessel function of the second kind of order n . __nv_ynf( n , x ) returns NaN for n < 0. __nv_ynf( n , 0) returns − ∞ . __nv_ynf( n , x ) returns NaN for x < 0. __nv_ynf( n , + ∞ ) returns +0. __nv_ynf( n , NaN) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y1.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y1.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.316. __nv_y1 Prototype : double @__nv_y1(double %x) Description : Calculate the value of the Bessel function of the second kind of order 1 for the input argument x , Y 1 ( x ) . Returns: Returns the value of the Bessel function of the second kind of order 1. __nv_y1(0) returns − ∞ . __nv_y1( x ) returns NaN for x < 0. __nv_y1( + ∞ ) returns +0. __nv_y1(NaN) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_yn.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_yn.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.318. __nv_yn Prototype : double @__nv_yn(i32 %n, double %x) Description : Calculate the value of the Bessel function of the second kind of order n for the input argument x , Y n ( x ) . Returns: Returns the value of the Bessel function of the second kind of order n . __nv_yn( n , x ) returns NaN for n < 0. __nv_yn( n , 0) returns − ∞ . __nv_yn( n , x ) returns NaN for x < 0. __nv_yn( n , + ∞ ) returns +0. __nv_yn( n , NaN) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y0f.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y0f.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.315. __nv_y0f Prototype : float @__nv_y0f(float %x) Description : Calculate the value of the Bessel function of the second kind of order 0 for the input argument x , Y 0 ( x ) . Returns: Returns the value of the Bessel function of the second kind of order 0. __nv_y0f(0) returns − ∞ . __nv_y0f( x ) returns NaN for x < 0. __nv_y0f( + ∞ ) returns +0. __nv_y0f(NaN) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umulhi.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umulhi.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.311. __nv_umulhi Prototype : i32 @__nv_umulhi(i32 %x, i32 %y) Description : Calculate the most significant 32 bits of the 64-bit product x * y , where x and y are 32-bit unsigned integers. Returns: Returns the most significant 32 bits of the product x * y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umul64hi.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umul64hi.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.310. __nv_umul64hi Prototype : i64 @__nv_umul64hi(i64 %x, i64 %y) Description : Calculate the most significant 64 bits of the 128-bit product x * y , where x and y are 64-bit unsigned integers. Returns: Returns the most significant 64 bits of the product x * y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y0.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_y0.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.314. __nv_y0 Prototype : double @__nv_y0(double %x) Description : Calculate the value of the Bessel function of the second kind of order 0 for the input argument x , Y 0 ( x ) . Returns: Returns the value of the Bessel function of the second kind of order 0. __nv_y0(0) returns − ∞ . __nv_y0( x ) returns NaN for x < 0. __nv_y0( + ∞ ) returns +0. __nv_y0(NaN) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_usad.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_usad.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.313. __nv_usad Prototype : i32 @__nv_usad(i32 %x, i32 %y, i32 %z) Description : Calculate | x − y | + z , the 32-bit sum of the third argument z plus and the absolute value of the difference between the first argument, x , and second argument, y . Inputs x , y , and z are unsigned 32-bit integers. Returns: Returns | x − y | + z . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umin.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umin.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.308. __nv_umin Prototype : i32 @__nv_umin(i32 %x, i32 %y) Description : Determine the minimum value of the two 32-bit unsigned integers x and y . Returns: Returns the minimum value of the two 32-bit unsigned integers x and y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umax.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umax.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.307. __nv_umax Prototype : i32 @__nv_umax(i32 %x, i32 %y) Description : Determine the maximum value of the two 32-bit unsigned integers x and y . Returns: Returns the maximum value of the two 32-bit unsigned integers x and y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umul24.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_umul24.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.309. __nv_umul24 Prototype : i32 @__nv_umul24(i32 %x, i32 %y) Description : Calculate the least significant 32 bits of the product of the least significant 24 bits of x and y . The high order 8 bits of x and y are ignored. Returns: Returns the least significant 32 bits of the product x * y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_urhadd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_urhadd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.312. __nv_urhadd Prototype : i32 @__nv_urhadd(i32 %x, i32 %y) Description : Compute average of unsigned input arguments x and y as ( x + y + 1 ) >> 1, avoiding overflow in the intermediate sum. Returns: Returns an unsigned integer value representing the unsigned rounded average value of the two inputs. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ullmin.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ullmin.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.306. __nv_ullmin Prototype : i64 @__nv_ullmin(i64 %x, i64 %y) Description : Determine the minimum value of the two 64-bit unsigned integers x and y . Returns: Returns the minimum value of the two 64-bit unsigned integers x and y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ullmax.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ullmax.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.305. __nv_ullmax Prototype : i64 @__nv_ullmax(i64 %x, i64 %y) Description : Determine the maximum value of the two 64-bit unsigned integers x and y . Returns: Returns the maximum value of the two 64-bit unsigned integers x and y . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_rz.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_rz.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.304. __nv_ull2float_rz Prototype : float @__nv_ull2float_rz(i64 %l) Description : Convert the unsigned integer value x to a single-precision floating point value in round-towards-zero mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_ru.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_ru.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.303. __nv_ull2float_ru Prototype : float @__nv_ull2float_ru(i64 %l) Description : Convert the unsigned integer value x to a single-precision floating point value in round-up (to positive infinity) mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_rn.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_rn.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.302. __nv_ull2float_rn Prototype : float @__nv_ull2float_rn(i64 %l) Description : Convert the unsigned integer value x to a single-precision floating point value in round-to-nearest-even mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_rd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2float_rd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.301. __nv_ull2float_rd Prototype : float @__nv_ull2float_rd(i64 %l) Description : Convert the unsigned integer value x to a single-precision floating point value in round-down (to negative infinity) mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_rz.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_rz.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.300. __nv_ull2double_rz Prototype : double @__nv_ull2double_rz(i64 %l) Description : Convert the unsigned 64-bit integer value x to a double-precision floating point value in round-towards-zero mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_rd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_rd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.297. __nv_ull2double_rd Prototype : double @__nv_ull2double_rd(i64 %l) Description : Convert the unsigned 64-bit integer value x to a double-precision floating point value in round-down (to negative infinity) mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/struct____nv__fp8__e4m3.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/struct____nv__fp8__e4m3.html", "content_type": "text/html", "text": "CUDA Math API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.4.1 CUDA Math API 1. Modules 1.1. FP8 Intrinsics 1.1.1. FP8 Conversion and Data Movement 1.1.2. C++ struct for handling fp8 data type of e5m2 kind. 1.1.3. C++ struct for handling vector type of two fp8 values of e5m2 kind. 1.1.4. C++ struct for handling vector type of four fp8 values of e5m2 kind. 1.1.5. C++ struct for handling fp8 data type of e4m3 kind. 1.1.6. C++ struct for handling vector type of two fp8 values of e4m3 kind. 1.1.7. C++ struct for handling vector type of four fp8 values of e4m3 kind. 1.2. Half Precision Intrinsics 1.2.1. Half Arithmetic Constants 1.2.2. Half Arithmetic Functions 1.2.3. Half2 Arithmetic Functions 1.2.4. Half Comparison Functions 1.2.5. Half2 Comparison Functions 1.2.6. Half Precision Conversion and Data Movement 1.2.7. Half Math Functions 1.2.8. Half2 Math Functions 1.3. Bfloat16 Precision Intrinsics 1.3.1. Bfloat16 Arithmetic Constants 1.3.2. Bfloat16 Arithmetic Functions 1.3.3. Bfloat162 Arithmetic Functions 1.3.4. Bfloat16 Comparison Functions 1.3.5. Bfloat162 Comparison Functions 1.3.6. Bfloat16 Precision Conversion and Data Movement 1.3.7. Bfloat16 Math Functions 1.3.8. Bfloat162 Math Functions 1.4. Mathematical Functions 1.5. Single Precision Mathematical Functions 1.6. Double Precision Mathematical Functions 1.7. Integer Mathematical Functions 1.8. Single Precision Intrinsics 1.9. Double Precision Intrinsics 1.10. Integer Intrinsics 1.11. Type Casting Intrinsics 1.12. SIMD Intrinsics 2. Data Structures 2.1. __half 2.2. __half2 2.3. __half2_raw 2.4. __half_raw 2.5. __nv_bfloat16 2.6. __nv_bfloat162 2.7. __nv_bfloat162_raw 2.8. __nv_bfloat16_raw 2.9. __nv_fp8_e4m3 2.10. __nv_fp8_e5m2 2.11. __nv_fp8x2_e4m3 2.12. __nv_fp8x2_e5m2 2.13. __nv_fp8x4_e4m3 2.14. __nv_fp8x4_e5m2 3. Data Fields Search Results < Previous | Next > CUDA Math API\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.4.1\n                  ( older )\n                  -\n                  Last updated April 3, 2024\n                  - Send Feedback 2.9. __nv_fp8_e4m3 Struct Reference [ C++ struct for handling fp8 data type of e4m3 kind. ] This structure implements the datatype for storing fp8 floating-point numbers of e4m3 kind: with 1 sign, 4 exponent, 1 implicit and 3 explicit mantissa bits. The encoding doesn't support Infinity. NaNs are limited\n                        to 0x7F and 0xFF values. The structure implements converting constructors and operators. Public Constructors __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const long long int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const long int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const short int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const unsigned long long int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const unsigned long int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const unsigned int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const unsigned short int val ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const double f ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const float f ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const __nv_bfloat16 f ) __host__ ​ __device__ ​ __nv_fp8_e4m3 (  const __half f ) __nv_fp8_e4m3 (  ) Public Member Functions __host__ ​ __device__ ​ operator __half (  )  const __host__ ​ __device__ ​ operator __nv_bfloat16 (  )  const __host__ ​ __device__ ​ operator bool (  )  const __host__ ​ __device__ ​ operator char (  )  const __host__ ​ __device__ ​ operator double (  )  const __host__ ​ __device__ ​ operator float (  )  const __host__ ​ __device__ ​ operator int (  )  const __host__ ​ __device__ ​ operator long int (  )  const __host__ ​ __device__ ​ operator long long int (  )  const __host__ ​ __device__ ​ operator short int (  )  const __host__ ​ __device__ ​ operator signed char (  )  const __host__ ​ __device__ ​ operator unsigned char (  )  const __host__ ​ __device__ ​ operator unsigned int (  )  const __host__ ​ __device__ ​ operator unsigned long int (  )  const __host__ ​ __device__ ​ operator unsigned long long int (  )  const __host__ ​ __device__ ​ operator unsigned short int (  )  const Public Variables __nv_fp8_storage_t __x Constructors __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const long long int val )  [inline, explicit] Description Constructor from long long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const long int val )  [inline, explicit] Description Constructor from long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const int val )  [inline, explicit] Description Constructor from int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const short int val )  [inline, explicit] Description Constructor from short int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const unsigned long long int val )  [inline, explicit] Description Constructor from unsigned long long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const unsigned long int val )  [inline, explicit] Description Constructor from unsigned long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const unsigned int val )  [inline, explicit] Description Constructor from unsigned int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const unsigned short int val )  [inline, explicit] Description Constructor from unsigned short int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const double f )  [inline, explicit] Description Constructor from double data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const float f )  [inline, explicit] Description Constructor from float data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const __nv_bfloat16 f )  [inline, explicit] Description Constructor from __nv_bfloat16 data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  const __half f )  [inline, explicit] Description Constructor from __half data type, relies on __NV_SATFINITE behavior for out-of-range values. __nv_fp8_e4m3 ::__nv_fp8_e4m3 (  ) Description Constructor by default. Member Functions __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator __half (  )  const [inherited, inline] Description Conversion operator to __half data type. __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator __nv_bfloat16 (  )  const [inherited, inline] Description Conversion operator to __nv_bfloat16 data type. __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator bool (  )  const [inherited, inline] Description Conversion operator to bool data type. +0 and -0 inputs convert to false . Non-zero inputs convert to true . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator char (  )  const [inherited, inline] Description Conversion operator to an implementation defined char data type. Detects signedness of the char type and proceeds accordingly, see further details in signed and unsigned char operators. Clamps inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator double (  )  const [inherited, inline] Description Conversion operator to double data type. __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator float (  )  const [inherited, inline] Description Conversion operator to float data type. __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator int (  )  const [inherited, inline] Description Conversion operator to int data type. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator long int (  )  const [inherited, inline] Description Conversion operator to long int data type. Clamps too large inputs to the output range. NaN inputs convert to zero if output type is 32-bit. NaN inputs convert to 0x8000000000000000ULL if output type is 64-bit. __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator long long int (  )  const [inherited, inline] Description Conversion operator to long long int data type. NaN inputs convert to 0x8000000000000000LL . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator short int (  )  const [inherited, inline] Description Conversion operator to short int data type. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator signed char (  )  const [inherited, inline] Description Conversion operator to signed char data type. Clamps too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator unsigned char (  )  const [inherited, inline] Description Conversion operator to unsigned char data type. Clamps negative and too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator unsigned int (  )  const [inherited, inline] Description Conversion operator to unsigned int data type. Clamps negative inputs to zero. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator unsigned long int (  )  const [inherited, inline] Description Conversion operator to unsigned long int data type. Clamps negative and too large inputs to the output range. NaN inputs convert to zero if output type is 32-bit. NaN inputs convert to 0x8000000000000000ULL if output type is 64-bit. __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator unsigned long long int (  )  const [inherited, inline] Description Conversion operator to unsigned long long int data type. Clamps negative inputs to zero. NaN inputs convert to 0x8000000000000000ULL . __host__ ​ __device__ ​ __nv_fp8_e4m3 :: operator unsigned short int (  )  const [inherited, inline] Description Conversion operator to unsigned short int data type. Clamps negative inputs to zero. NaN inputs convert to zero . Variables __nv_fp8_storage_t __nv_fp8_e4m3 :: __x [inherited] Storage variable contains the fp8 floating-point data. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact"}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/struct____nv__fp8__e5m2.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/struct____nv__fp8__e5m2.html", "content_type": "text/html", "text": "CUDA Math API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.4.1 CUDA Math API 1. Modules 1.1. FP8 Intrinsics 1.1.1. FP8 Conversion and Data Movement 1.1.2. C++ struct for handling fp8 data type of e5m2 kind. 1.1.3. C++ struct for handling vector type of two fp8 values of e5m2 kind. 1.1.4. C++ struct for handling vector type of four fp8 values of e5m2 kind. 1.1.5. C++ struct for handling fp8 data type of e4m3 kind. 1.1.6. C++ struct for handling vector type of two fp8 values of e4m3 kind. 1.1.7. C++ struct for handling vector type of four fp8 values of e4m3 kind. 1.2. Half Precision Intrinsics 1.2.1. Half Arithmetic Constants 1.2.2. Half Arithmetic Functions 1.2.3. Half2 Arithmetic Functions 1.2.4. Half Comparison Functions 1.2.5. Half2 Comparison Functions 1.2.6. Half Precision Conversion and Data Movement 1.2.7. Half Math Functions 1.2.8. Half2 Math Functions 1.3. Bfloat16 Precision Intrinsics 1.3.1. Bfloat16 Arithmetic Constants 1.3.2. Bfloat16 Arithmetic Functions 1.3.3. Bfloat162 Arithmetic Functions 1.3.4. Bfloat16 Comparison Functions 1.3.5. Bfloat162 Comparison Functions 1.3.6. Bfloat16 Precision Conversion and Data Movement 1.3.7. Bfloat16 Math Functions 1.3.8. Bfloat162 Math Functions 1.4. Mathematical Functions 1.5. Single Precision Mathematical Functions 1.6. Double Precision Mathematical Functions 1.7. Integer Mathematical Functions 1.8. Single Precision Intrinsics 1.9. Double Precision Intrinsics 1.10. Integer Intrinsics 1.11. Type Casting Intrinsics 1.12. SIMD Intrinsics 2. Data Structures 2.1. __half 2.2. __half2 2.3. __half2_raw 2.4. __half_raw 2.5. __nv_bfloat16 2.6. __nv_bfloat162 2.7. __nv_bfloat162_raw 2.8. __nv_bfloat16_raw 2.9. __nv_fp8_e4m3 2.10. __nv_fp8_e5m2 2.11. __nv_fp8x2_e4m3 2.12. __nv_fp8x2_e5m2 2.13. __nv_fp8x4_e4m3 2.14. __nv_fp8x4_e5m2 3. Data Fields Search Results < Previous | Next > CUDA Math API\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.4.1\n                  ( older )\n                  -\n                  Last updated April 3, 2024\n                  - Send Feedback 2.10. __nv_fp8_e5m2 Struct Reference [ C++ struct for handling fp8 data type of e5m2 kind. ] This structure implements the datatype for handling fp8 floating-point numbers of e5m2 kind: with 1 sign, 5 exponent, 1 implicit and 2 explicit mantissa bits. The structure implements converting constructors and operators. Public Constructors __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const long long int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const long int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const short int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const unsigned long long int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const unsigned long int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const unsigned int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const unsigned short int val ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const double f ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const float f ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const __nv_bfloat16 f ) __host__ ​ __device__ ​ __nv_fp8_e5m2 (  const __half f ) __nv_fp8_e5m2 (  ) Public Member Functions __host__ ​ __device__ ​ operator __half (  )  const __host__ ​ __device__ ​ operator __nv_bfloat16 (  )  const __host__ ​ __device__ ​ operator bool (  )  const __host__ ​ __device__ ​ operator char (  )  const __host__ ​ __device__ ​ operator double (  )  const __host__ ​ __device__ ​ operator float (  )  const __host__ ​ __device__ ​ operator int (  )  const __host__ ​ __device__ ​ operator long int (  )  const __host__ ​ __device__ ​ operator long long int (  )  const __host__ ​ __device__ ​ operator short int (  )  const __host__ ​ __device__ ​ operator signed char (  )  const __host__ ​ __device__ ​ operator unsigned char (  )  const __host__ ​ __device__ ​ operator unsigned int (  )  const __host__ ​ __device__ ​ operator unsigned long int (  )  const __host__ ​ __device__ ​ operator unsigned long long int (  )  const __host__ ​ __device__ ​ operator unsigned short int (  )  const Public Variables __nv_fp8_storage_t __x Constructors __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const long long int val )  [inline, explicit] Description Constructor from long long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const long int val )  [inline, explicit] Description Constructor from long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const int val )  [inline, explicit] Description Constructor from int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const short int val )  [inline, explicit] Description Constructor from short int data type. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const unsigned long long int val )  [inline, explicit] Description Constructor from unsigned long long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const unsigned long int val )  [inline, explicit] Description Constructor from unsigned long int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const unsigned int val )  [inline, explicit] Description Constructor from unsigned int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const unsigned short int val )  [inline, explicit] Description Constructor from unsigned short int data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const double f )  [inline, explicit] Description Constructor from double data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const float f )  [inline, explicit] Description Constructor from float data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const __nv_bfloat16 f )  [inline, explicit] Description Constructor from __nv_bfloat16 data type, relies on __NV_SATFINITE behavior for out-of-range values. __host__ ​ __device__ ​ __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  const __half f )  [inline, explicit] Description Constructor from __half data type, relies on __NV_SATFINITE behavior for out-of-range values. __nv_fp8_e5m2 ::__nv_fp8_e5m2 (  ) Description Constructor by default. Member Functions __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator __half (  )  const [inherited, inline] Description Conversion operator to __half data type. __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator __nv_bfloat16 (  )  const [inherited, inline] Description Conversion operator to __nv_bfloat16 data type. __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator bool (  )  const [inherited, inline] Description Conversion operator to bool data type. +0 and -0 inputs convert to false . Non-zero inputs convert to true . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator char (  )  const [inherited, inline] Description Conversion operator to an implementation defined char data type. Detects signedness of the char type and proceeds accordingly, see further details in signed and unsigned char operators. Clamps inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator double (  )  const [inherited, inline] Description Conversion operator to double data type. __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator float (  )  const [inherited, inline] Description Conversion operator to float data type. __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator int (  )  const [inherited, inline] Description Conversion operator to int data type. Clamps too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator long int (  )  const [inherited, inline] Description Conversion operator to long int data type. Clamps too large inputs to the output range. NaN inputs convert to zero if output type is 32-bit. NaN inputs convert to 0x8000000000000000ULL if output type is 64-bit. __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator long long int (  )  const [inherited, inline] Description Conversion operator to long long int data type. Clamps too large inputs to the output range. NaN inputs convert to 0x8000000000000000LL . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator short int (  )  const [inherited, inline] Description Conversion operator to short int data type. Clamps too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator signed char (  )  const [inherited, inline] Description Conversion operator to signed char data type. Clamps too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator unsigned char (  )  const [inherited, inline] Description Conversion operator to unsigned char data type. Clamps negative and too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator unsigned int (  )  const [inherited, inline] Description Conversion operator to unsigned int data type. Clamps negative and too large inputs to the output range. NaN inputs convert to zero . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator unsigned long int (  )  const [inherited, inline] Description Conversion operator to unsigned long int data type. Clamps negative and too large inputs to the output range. NaN inputs convert to zero if output type is 32-bit. NaN inputs convert to 0x8000000000000000ULL if output type is 64-bit. __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator unsigned long long int (  )  const [inherited, inline] Description Conversion operator to unsigned long long int data type. Clamps negative and too large inputs to the output range. NaN inputs convert to 0x8000000000000000ULL . __host__ ​ __device__ ​ __nv_fp8_e5m2 :: operator unsigned short int (  )  const [inherited, inline] Description Conversion operator to unsigned short int data type. Clamps negative and too large inputs to the output range. NaN inputs convert to zero . Variables __nv_fp8_storage_t __nv_fp8_e5m2 :: __x [inherited] Storage variable contains the fp8 floating-point data. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_rn.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_rn.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.298. __nv_ull2double_rn Prototype : double @__nv_ull2double_rn(i64 %l) Description : Convert the unsigned 64-bit integer value x to a double-precision floating point value in round-to-nearest-even mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_ru.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_ull2double_ru.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.299. __nv_ull2double_ru Prototype : double @__nv_ull2double_ru(i64 %l) Description : Convert the unsigned 64-bit integer value x to a double-precision floating point value in round-up (to positive infinity) mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_rz.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_rz.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.296. __nv_uint2float_rz Prototype : float @__nv_uint2float_rz(i32 %in) Description : Convert the unsigned integer value x to a single-precision floating point value in round-towards-zero mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_rd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_rd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.293. __nv_uint2float_rd Prototype : float @__nv_uint2float_rd(i32 %in) Description : Convert the unsigned integer value x to a single-precision floating point value in round-down (to negative infinity) mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_ru.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_ru.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.295. __nv_uint2float_ru Prototype : float @__nv_uint2float_ru(i32 %in) Description : Convert the unsigned integer value x to a single-precision floating point value in round-up (to positive infinity) mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2double_rn.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2double_rn.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.292. __nv_uint2double_rn Prototype : double @__nv_uint2double_rn(i32 %i) Description : Convert the unsigned integer value x to a double-precision floating point value. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uhadd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uhadd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.291. __nv_uhadd Prototype : i32 @__nv_uhadd(i32 %x, i32 %y) Description : Compute average of unsigned input arguments x and y as ( x + y ) >> 1, avoiding overflow in the intermediate sum. Returns: Returns an unsigned integer value representing the unsigned average value of the two inputs. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_rn.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_uint2float_rn.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.294. __nv_uint2float_rn Prototype : float @__nv_uint2float_rn(i32 %in) Description : Convert the unsigned integer value x to a single-precision floating point value in round-to-nearest-even mode. Returns: Returns converted value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_truncf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_truncf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.290. __nv_truncf Prototype : float @__nv_truncf(float %x) Description : Round x to the nearest integer value that does not exceed x in magnitude. Returns: Returns truncated integer value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_trunc.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_trunc.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.289. __nv_trunc Prototype : double @__nv_trunc(double %x) Description : Round x to the nearest integer value that does not exceed x in magnitude. Returns: Returns truncated integer value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tgammaf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tgammaf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.288. __nv_tgammaf Prototype : float @__nv_tgammaf(float %x) Description : Calculate the gamma function of the input argument x , namely the value of ∫ 0 ∞ e − t t x − 1 d t . Returns: __nv_tgammaf( ± 0 ) returns ± ∞ . __nv_tgammaf(2) returns +0. __nv_tgammaf( x ) returns ± ∞ if the correctly calculated value is outside the double floating point range. __nv_tgammaf( x ) returns NaN if x < 0. __nv_tgammaf( − ∞ ) returns NaN. __nv_tgammaf( + ∞ ) returns + ∞ . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tgamma.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tgamma.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.287. __nv_tgamma Prototype : double @__nv_tgamma(double %x) Description : Calculate the gamma function of the input argument x , namely the value of ∫ 0 ∞ e − t t x − 1 d t . Returns: __nv_tgamma( ± 0 ) returns ± ∞ . __nv_tgamma(2) returns +0. __nv_tgamma( x ) returns ± ∞ if the correctly calculated value is outside the double floating point range. __nv_tgamma( x ) returns NaN if x < 0. __nv_tgamma( − ∞ ) returns NaN. __nv_tgamma( + ∞ ) returns + ∞ . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tanhf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tanhf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.286. __nv_tanhf Prototype : float @__nv_tanhf(float %x) Description : Calculate the hyperbolic tangent of the input argument x . Returns: __nv_tanhf( ± 0 ) returns ± 0 . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tanh.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tanh.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.285. __nv_tanh Prototype : double @__nv_tanh(double %x) Description : Calculate the hyperbolic tangent of the input argument x . Returns: __nv_tanh( ± 0 ) returns ± 0 . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tanf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tanf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.284. __nv_tanf Prototype : float @__nv_tanf(float %x) Description : Calculate the tangent of the input argument x (measured in radians). Returns: __nv_tanf( ± 0 ) returns ± 0 . __nv_tanf( ± ∞ ) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tan.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_tan.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.283. __nv_tan Prototype : double @__nv_tan(double %x) Description : Calculate the tangent of the input argument x (measured in radians). Returns: __nv_tan( ± 0 ) returns ± 0 . __nv_tan( ± ∞ ) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sqrtf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sqrtf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.282. __nv_sqrtf Prototype : float @__nv_sqrtf(float %x) Description : Calculate the nonnegative square root of x , x . Returns: Returns x . __nv_sqrtf( ± 0 ) returns ± 0 . __nv_sqrtf( + ∞ ) returns + ∞ . __nv_sqrtf( x ) returns NaN if x is less than 0. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sqrt.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sqrt.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.281. __nv_sqrt Prototype : double @__nv_sqrt(double %x) Description : Calculate the nonnegative square root of x , x . Returns: Returns x . __nv_sqrt( ± 0 ) returns ± 0 . __nv_sqrt( + ∞ ) returns + ∞ . __nv_sqrt( x ) returns NaN if x is less than 0. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinpif.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinpif.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.280. __nv_sinpif Prototype : float @__nv_sinpif(float %x) Description : Calculate the sine of x × π (measured in radians), where x is the input argument. Returns: __nv_sinpif( ± 0 ) returns ± 0 . __nv_sinpif( ± ∞ ) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinpi.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinpi.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.279. __nv_sinpi Prototype : double @__nv_sinpi(double %x) Description : Calculate the sine of x × π (measured in radians), where x is the input argument. Returns: __nv_sinpi( ± 0 ) returns ± 0 . __nv_sinpi( ± ∞ ) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinhf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinhf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.278. __nv_sinhf Prototype : float @__nv_sinhf(float %x) Description : Calculate the hyperbolic sine of the input argument x . Returns: __nv_sinhf( ± 0 ) returns ± 0 . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinh.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinh.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.277. __nv_sinh Prototype : double @__nv_sinh(double %x) Description : Calculate the hyperbolic sine of the input argument x . Returns: __nv_sinh( ± 0 ) returns ± 0 . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sinf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.276. __nv_sinf Prototype : float @__nv_sinf(float %x) Description : Calculate the sine of the input argument x (measured in radians). Returns: __nv_sinf( ± 0 ) returns ± 0 . __nv_sinf( ± ∞ ) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincospif.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincospif.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.275. __nv_sincospif Prototype : void @__nv_sincospif(float %x, float* %sptr, float* %cptr) Description : Calculate the sine and cosine of the first input argument, x (measured in radians), × π . The results for sine and cosine are written into the second argument, sptr , and, respectively, third argument, zptr . Returns: none See __nv_sinpif() and __nv_cospif(). Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincospi.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincospi.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.274. __nv_sincospi Prototype : void @__nv_sincospi(double %x, double* %sptr, double* %cptr) Description : Calculate the sine and cosine of the first input argument, x (measured in radians), × π . The results for sine and cosine are written into the second argument, sptr , and, respectively, third argument, zptr . Returns: none See __nv_sinpi() and __nv_cospi(). Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincosf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincosf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.273. __nv_sincosf Prototype : void @__nv_sincosf(float %x, float* %sptr, float* %cptr) Description : Calculate the sine and cosine of the first input argument x (measured in radians). The results for sine and cosine are written into the second argument, sptr , and, respectively, third argument, zptr . Returns: none See __nv_sinf() and __nv_cosf(). Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincos.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sincos.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.272. __nv_sincos Prototype : void @__nv_sincos(double %x, double* %sptr, double* %cptr) Description : Calculate the sine and cosine of the first input argument x (measured in radians). The results for sine and cosine are written into the second argument, sptr , and, respectively, third argument, zptr . Returns: none See __nv_sin() and __nv_cos(). Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sin.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sin.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.271. __nv_sin Prototype : double @__nv_sin(double %x) Description : Calculate the sine of the input argument x (measured in radians). Returns: __nv_sin( ± 0 ) returns ± 0 . __nv_sin( ± ∞ ) returns NaN. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_signbitf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_signbitf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.270. __nv_signbitf Prototype : i32 @__nv_signbitf(float %x) Description : Determine whether the floating-point value x is negative. Returns: Returns a nonzero value if and only if x is negative. Reports the sign bit of all values including infinities, zeros, and NaNs. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_signbitd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_signbitd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.269. __nv_signbitd Prototype : i32 @__nv_signbitd(double %x) Description : Determine whether the floating-point value x is negative. Returns: Returns a nonzero value if and only if x is negative. Reports the sign bit of all values including infinities, zeros, and NaNs. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/cublasdx/", "parent_url": "https://docs.nvidia.com/cuda/cublasdx/", "content_type": "text/html", "text": "NVIDIA cuBLASDx — cuBLASDx 0.1.0 documentation cuBLASDx 0.1.0 Documentation home User guide: Requirements and Functionality Requirements Supported Compilers Supported Functionality Quick Installation Guide cuBLASDx In Your Project cuBLASDx In Your CMake Project Using Custom CUTLASS Defined Variables General Matrix Multiply Using cuBLASDx Defining GEMM Operation Executing GEMM Launching GEMM Kernel Compilation Achieving High Performance General Advice Memory Management Advanced Further Reading References API reference Operators Description Operators Size Operator Type Operator Precision Operator TransposeMode Operator LeadingDimension Operator Function Operator SM Operator Execution Operators Block Operator Block Configuration Operators Traits Description Traits Size Trait Type Trait Precision Trait Function Trait Transpose Mode Trait SM Trait is_blas Trait is_blas_execution Trait is_complete_blas Trait is_complete_blas_execution Trait Execution Traits Block Traits Other Traits is_supported suggested_leading_dimension_of Execution Methods Block Execute Method Value Format Input/Output Data Format Shared Memory Usage Examples Introduction Examples Simple GEMM Examples NVRTC Examples GEMM Performance Advanced Examples Release Notes 0.1.0 New Features Known Issues Software License Agreement Third Party License Agreements CUTLASS cuBLASDx NVIDIA cuBLASDx NVIDIA cuBLASDx ¶ The cuBLAS Device Extensions (cuBLASDx) library enables you to perform selected linear algebra functions known from cuBLAS inside your CUDA kernel.\nThis is currently limited only to General Matrix Multiplication (GEMM).\nFusing linear algebra routines with other operations can decrease the latency and improve the overall performance of\nyour application. cuBLASDx is a part of the MathDx package which also includes cuFFTDx for FFT calculations.\nBoth libraries are designed to work together.\nExamples of such fusion are included in the package.\nWhen using multiple device extensions libraries in a single project they should all come from the same MathDx release. The documentation consists of three main components: Requirements and supported features . A quick start guide, General Matrix Multiply Using cuBLASDx . An API reference for a comprehensive overview of the provided functionality. Highlights ¶ The cuBLASDx library currently provides: BLAS GEMM routine embeddable into a CUDA kernel. High performance, no unnecessary data movement from and to global memory. Customizability, options to adjust GEMM routine for different needs (size, precision, type, targeted CUDA architecture, etc.). Ability to fuse BLAS kernels with other operations in order to save global memory trips. Compatibility with future versions of the CUDA Toolkit. User guide: Requirements and Functionality Requirements Supported Functionality Quick Installation Guide cuBLASDx In Your Project cuBLASDx In Your CMake Project General Matrix Multiply Using cuBLASDx Defining GEMM Operation Executing GEMM Launching GEMM Kernel Compilation Achieving High Performance General Advice Memory Management Advanced Further Reading API reference Operators Traits Execution Methods Examples Introduction Examples Simple GEMM Examples NVRTC Examples GEMM Performance Advanced Examples Release Notes 0.1.0 Software License Agreement Third Party License Agreements Next Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2023-2024, NVIDIA Corporation & Affiliates. All rights reserved."}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_scalbnf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_scalbnf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.268. __nv_scalbnf Prototype : float @__nv_scalbnf(float %x, i32 %y) Description : Scale x by 2 n by efficient manipulation of the floating-point exponent. Returns: Returns x * 2 n . __nv_scalbnf( ± 0 , n ) returns ± 0 . __nv_scalbnf( x , 0) returns x . __nv_scalbnf( ± ∞ , n ) returns ± ∞ . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_scalbn.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_scalbn.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.267. __nv_scalbn Prototype : double @__nv_scalbn(double %x, i32 %y) Description : Scale x by 2 n by efficient manipulation of the floating-point exponent. Returns: Returns x * 2 n . __nv_scalbn( ± 0 , n ) returns ± 0 . __nv_scalbn( x , 0) returns x . __nv_scalbn( ± ∞ , n ) returns ± ∞ . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_saturatef.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_saturatef.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.266. __nv_saturatef Prototype : float @__nv_saturatef(float %x) Description : Clamp the input argument x to be within the interval [+0.0, 1.0]. Returns: __nv_saturatef( x ) returns 0 if x < 0. __nv_saturatef( x ) returns 1 if x > 1. __nv_saturatef( x ) returns x if 0 ≤ x ≤ 1 . __nv_saturatef(NaN) returns 0. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rsqrtf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rsqrtf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.264. __nv_rsqrtf Prototype : float @__nv_rsqrtf(float %x) Description : Calculate the reciprocal of the nonnegative square root of x , 1 / x . Returns: Returns 1 / x . __nv_rsqrtf( + ∞ ) returns +0. __nv_rsqrtf( ± 0 ) returns ± ∞ . __nv_rsqrtf( x ) returns NaN if x is less than 0. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sad.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_sad.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.265. __nv_sad Prototype : i32 @__nv_sad(i32 %x, i32 %y, i32 %z) Description : Calculate | x − y | + z , the 32-bit sum of the third argument z plus and the absolute value of the difference between the first argument, x , and second argument, y . Inputs x and y are signed 32-bit integers, input z is a 32-bit unsigned integer. Returns: Returns | x − y | + z . Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rsqrt.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rsqrt.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.263. __nv_rsqrt Prototype : double @__nv_rsqrt(double %x) Description : Calculate the reciprocal of the nonnegative square root of x , 1 / x . Returns: Returns 1 / x . __nv_rsqrt( + ∞ ) returns +0. __nv_rsqrt( ± 0 ) returns ± ∞ . __nv_rsqrt( x ) returns NaN if x is less than 0. Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Double-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_round.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_round.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.261. __nv_round Prototype : double @__nv_round(double %x) Description : Round x to the nearest integer value in floating-point format, with halfway cases rounded away from zero. Returns: Returns rounded integer value. Note: This function may be slower than alternate rounding methods. See rint(). Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rintf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rintf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.260. __nv_rintf Prototype : float @__nv_rintf(float %x) Description : Round x to the nearest integer value in floating-point format, with halfway cases rounded to the nearest even integer value. Returns: Returns rounded integer value. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rhadd.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_rhadd.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.258. __nv_rhadd Prototype : i32 @__nv_rhadd(i32 %x, i32 %y) Description : Compute average of signed input arguments x and y as ( x + y + 1 ) >> 1, avoiding overflow in the intermediate sum. Returns: Returns a signed integer value representing the signed rounded average value of the two inputs. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_remquof.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_remquof.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.257. __nv_remquof Prototype : float @__nv_remquof(float %x, float %y, i32* %quo) Description : Compute a double-precision floating-point remainder in the same way as the remainder() function. Argument quo returns part of quotient upon division of x by y . Value quo has the same sign as x y and may not be the exact quotient but agrees with the exact quotient in the low order 3 bits. Returns: Returns the remainder. __nv_remquof( x , 0, quo ) returns NaN. __nv_remquof( ± ∞ , y , quo ) returns NaN. __nv_remquof( x , ± ∞ , quo ) returns x . Note: For accuracy information see the CUDA C++ Programming Guide, Mathematical Functions Appendix, Single-Precision Floating-Point\n                              Functions section. Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_roundf.html", "parent_url": "https://docs.nvidia.com/cuda/libdevice-users-guide/__nv_roundf.html", "content_type": "text/html", "text": "libdevice User's Guide :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 libdevice User's Guide 1. Introduction 1.1. What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318. __nv_yn 3.319. __nv_ynf Search Results < Previous | Next > libdevice User's Guide\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 3.262. __nv_roundf Prototype : float @__nv_roundf(float %x) Description : Round x to the nearest integer value in floating-point format, with halfway cases rounded away from zero. Returns: Returns rounded integer value. Note: This function may be slower than alternate rounding methods. See rint(). Library Availability : Compute 2.0: Yes Compute 3.0: Yes Compute 3.5: Yes Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation"}]