[{"url": "https://docs.nvidia.com/cuda/", "parent_url": "https://docs.nvidia.com/cuda/", "content_type": "text/html", "text": "CUDA Toolkit Documentation 12.5 Release Notes CUDA Features Archive EULA Installation Guides Quick Start Guide Installation Guide Windows Installation Guide Linux Programming Guides Programming Guide Best Practices Guide Maxwell Compatibility Guide Pascal Compatibility Guide Volta Compatibility Guide Turing Compatibility Guide NVIDIA Ampere GPU Architecture Compatibility Guide Hopper Compatibility Guide Ada Compatibility Guide Maxwell Tuning Guide Pascal Tuning Guide Volta Tuning Guide Turing Tuning Guide NVIDIA Ampere GPU Architecture Tuning Guide Hopper Tuning Guide Ada Tuning Guide PTX ISA Video Decoder PTX Interoperability Inline PTX Assembly CUDA API References CUDA Runtime API CUDA Driver API CUDA Math API cuBLAS cuDLA API NVBLAS nvJPEG cuFFT CUB CUDA C++ Standard Library cuFile API Reference Guide cuRAND cuSPARSE NPP nvJitLink nvFatbin NVRTC (Runtime Compilation) Thrust cuSOLVER PTX Compiler API References PTX Compiler APIs Miscellaneous CUDA Demo Suite CUDA on WSL CUDA on EFLOW Multi-Instance GPU (MIG) CUDA Compatibility CUPTI Debugger API GPUDirect RDMA GPUDirect Storage vGPU Tools NVCC CUDA-GDB Compute Sanitizer Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Systems Nsight Compute Nsight Visual Studio Edition Profiler CUDA Binary Utilities White Papers Floating Point and IEEE 754 Incomplete-LU and Cholesky Preconditioned Iterative Methods Application Notes CUDA for Tegra Compiler SDK libNVVM API libdevice User’s Guide NVVM IR landing » CUDA Toolkit Documentation 12.5 Update 1 CUDA Toolkit Archive - Send Feedback CUDA Toolkit Documentation 12.5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated\r\napplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\r\nembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\r\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime\r\nlibrary to deploy your application. Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers\r\ncan develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs. Release Notes The Release Notes for the CUDA Toolkit. CUDA Features Archive The list of CUDA features by release. EULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. Installation Guides  Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system. Installation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems. Installation Guide Linux This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems. Programming Guides  Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface. It then describes the hardware implementation, and provides guidance on how to achieve maximum performance. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API. Best Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit. Maxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture. This document provides guidance to ensure that your software applications are compatible with Maxwell. Pascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture. This document provides guidance to ensure that your software applications are compatible with Pascal. Volta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture. This document provides guidance to ensure that your software applications are compatible with Volta. Turing Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture. This document provides guidance to ensure that your software applications are compatible with Turing. NVIDIA Ampere GPU Architecture Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture. This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture. Hopper Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs. This document provides guidance to ensure that your software applications are compatible with Hopper architecture. Ada Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs. This document provides guidance to ensure that your software applications are compatible with Ada architecture. Maxwell Tuning Guide Maxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features. Pascal Tuning Guide Pascal is NVIDIA’s 5th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features. Volta Tuning Guide Volta is NVIDIA’s 6th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features. Turing Tuning Guide Turing is NVIDIA’s 7th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features. NVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features. Hopper Tuning Guide Hopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features. Ada Tuning Guide The NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features. PTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device. Video Decoder NVIDIA Video Decoder (NVCUVID) is deprecated. Instead, use the NVIDIA Video Codec SDK ( https://developer.nvidia.com/nvidia-video-codec-sdk ). PTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code. Inline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter. CUDA API References  CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration. CUDA Driver API Fields in structures might appear in order that is different from the order of declaration. CUDA Math API The CUDA math API. cuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs. cuDLA API The cuDLA API. NVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library. nvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. cuFFT The cuFFT library user guide. CUB The user guide for CUB. CUDA C++ Standard Library The API reference for libcu++, the CUDA C++ standard library. cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. cuRAND The cuRAND library user guide. cuSPARSE The cuSPARSE library user guide. NPP NVIDIA NPP is a library of functions for performing CUDA accelerated processing. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. The NPP library is written to maximize flexibility, while maintaining high performance. nvJitLink The user guide for the nvJitLink library. nvFatbin The user guide for the nvFatbin library. NVRTC (Runtime Compilation) NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. This facility can often provide optimizations and performance not possible in a purely offline static compilation. Thrust The C++ parallel algorithms library. cuSOLVER The cuSOLVER library user guide. PTX Compiler API References  PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library. Miscellaneous  CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite. CUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment. Multi-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU. CUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade. CUPTI The CUPTI-API. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications. Debugger API The CUDA debugger API. GPUDirect RDMA A technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express. This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model. GPUDirect Storage The documentation for GPUDirect Storage. vGPU vGPUs that support CUDA. Tools  NVCC This is a reference document for nvcc, the CUDA compiler driver. nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. CUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger. Compute Sanitizer The user guide for Compute Sanitizer. Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems. Nsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. Nsight Visual Studio Edition The documentation for Nsight Visual Studio Edition. Profiler This is the guide to the Profiler. CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, and nvprune. White Papers  Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide. Incomplete-LU and Cholesky Preconditioned Iterative Methods In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms. Application Notes  CUDA for Tegra This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability. Compiler SDK  libNVVM API The libNVVM API. libdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels. NVVM IR NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/nvfatbin/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvfatbin/index.html", "content_type": "text/html", "text": "nvFatbin 1. Introduction 2. Getting Started 2.1. System Requirements 2.2. Installation 3. User Interface 3.1. Error codes 3.1.1. Enumerations 3.1.2. Functions 3.2. Fatbinary Creation 3.2.1. Functions 3.2.2. Typedefs 3.3. Supported Options 4. Basic Usage 5. Compatibility 6. Example: Runtime fatbin creation 6.1. Code (online.cpp) 6.2. Build Instructions 6.3. Notices 6.3.1. Notice 6.3.2. OpenCL 6.3.3. Trademarks nvFatbin » 1. Introduction v12.5 | Archive nvFatbin The User guide to nvFatbin library. 1. Introduction  The Fatbin Creator APIs are a set of APIs which can be used at runtime to combine multiple CUDA objects into one CUDA fat binary (fatbin). The APIs accept inputs in multiple formats, either device cubins, PTX, or LTO-IR.\nThe output is a fatbin that can be loaded by cuModuleLoadData of the CUDA Driver API. The functionality in this library is similar to the fatbinary offline tool in the CUDA toolkit, with the following advantages: Support for runtime fatbin creation. The clients get fine grain control over the input process. Supports direct input from memory, rather than requiring inputs be written to files. 2. Getting Started  2.1. System Requirements  The Fatbin Creator library requires no special system configuration. It does not require a GPU. 2.2. Installation  The Fatbin Creator library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include\\nvFatbin.h lib\\x64\\nvFatbin.dll lib\\x64\\nvFatbin_static.lib doc\\pdf\\nvFatbin_User_Guide.pdf On Linux: include/nvFatbin.h lib64/libnvfatbin.so lib64/libnvfatbin_static.a doc/pdf/nvFatbin_User_Guide.pdf 3. User Interface  This chapter presents the Fatbin Creator APIs. Basic usage of the API is explained in Basic Usage . Error codes Creation Supported Options 3.1. Error codes  Enumerations nvFatbinResult The enumerated type nvFatbinResult defines API call result codes. Functions const char * nvFatbinGetErrorString (nvFatbinResult result) nvFatbinGetErrorString returns an error description string for each error code. 3.1.1. Enumerations  enum nvFatbinResult  The enumerated type nvFatbinResult defines API call result codes. nvFatbin APIs return nvFatbinResult codes to indicate the result. Values: enumerator NVFATBIN_SUCCESS  enumerator NVFATBIN_ERROR_INTERNAL  enumerator NVFATBIN_ERROR_ELF_ARCH_MISMATCH  enumerator NVFATBIN_ERROR_ELF_SIZE_MISMATCH  enumerator NVFATBIN_ERROR_MISSING_PTX_VERSION  enumerator NVFATBIN_ERROR_NULL_POINTER  enumerator NVFATBIN_ERROR_COMPRESSION_FAILED  enumerator NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED  enumerator NVFATBIN_ERROR_UNRECOGNIZED_OPTION  enumerator NVFATBIN_ERROR_INVALID_ARCH  enumerator NVFATBIN_ERROR_INVALID_NVVM  enumerator NVFATBIN_ERROR_EMPTY_INPUT  enumerator NVFATBIN_ERROR_MISSING_PTX_ARCH  enumerator NVFATBIN_ERROR_PTX_ARCH_MISMATCH  enumerator NVFATBIN_ERROR_MISSING_FATBIN  enumerator NVFATBIN_ERROR_INVALID_INDEX  enumerator NVFATBIN_ERROR_IDENTIFIER_REUSE  3.1.2. Functions  const char * nvFatbinGetErrorString ( nvFatbinResult result )  nvFatbinGetErrorString returns an error description string for each error code. Parameters result – [in] error code Returns nullptr, if result is NVFATBIN_SUCCESS a string, if result is not NVFATBIN_SUCCESS 3.2. Fatbinary Creation  Functions nvFatbinResult nvFatbinAddCubin (nvFatbinHandle handle, const void *code, size_t size, const char *arch, const char *identifier) nvFatbinAddCubin adds a CUDA binary to the fatbinary. nvFatbinResult nvFatbinAddIndex (nvFatbinHandle handle, const void *code, size_t size, const char *identifier) nvFatbinAddIndex adds an index file to the fatbinary. nvFatbinResult nvFatbinAddLTOIR (nvFatbinHandle handle, const void *code, size_t size, const char *arch, const char *identifier, const char *optionsCmdLine) nvFatbinAddLTOIR adds LTOIR to the fatbinary. nvFatbinResult nvFatbinAddPTX (nvFatbinHandle handle, const char *code, size_t size, const char *arch, const char *identifier, const char *optionsCmdLine) nvFatbinAddPTX adds PTX to the fatbinary. nvFatbinResult nvFatbinAddReloc (nvFatbinHandle handle, const void *code, size_t size) nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary. nvFatbinResult nvFatbinCreate (nvFatbinHandle *handle_indirect, const char **options, size_t optionsCount) nvFatbinCreate creates a new handle nvFatbinResult nvFatbinDestroy (nvFatbinHandle *handle_indirect) nvFatbinDestroy destroys the handle. nvFatbinResult nvFatbinGet (nvFatbinHandle handle, void *buffer) nvFatbinGet returns the completed fatbinary. nvFatbinResult nvFatbinSize (nvFatbinHandle handle, size_t *size) nvFatbinSize returns the fatbinary's size. nvFatbinResult nvFatbinVersion (unsigned int *major, unsigned int *minor) nvFatbinVersion returns the current version of nvFatbin Typedefs nvFatbinHandle nvFatbinHandle is the unit of fatbin creation, and an opaque handle for a program. 3.2.1. Functions  nvFatbinResult nvFatbinAddCubin ( nvFatbinHandle handle , const void * code , size_t size , const char * arch , const char * identifier )  nvFatbinAddCubin adds a CUDA binary to the fatbinary. User is responsible for making sure all strings are well-formed. Parameters handle – [in] nvFatbin handle. code – [in] The cubin. size – [in] The size of the cubin. arch – [in] The architecture that this cubin is for. identifier – [in] Name of the cubin, useful when extracting the fatbin with tools like cuobjdump. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_ELF_ARCH_MISMATCH NVFATBIN_ERROR_ELF_SIZE_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddIndex ( nvFatbinHandle handle , const void * code , size_t size , const char * identifier )  nvFatbinAddIndex adds an index file to the fatbinary. User is responsible for making sure all strings are well-formed. Parameters handle – [in] nvFatbin handle. code – [in] The index. size – [in] The size of the index. identifier – [in] Name of the index, useful when extracting the fatbin with tools like cuobjdump. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_INVALID_INDEX NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddLTOIR ( nvFatbinHandle handle , const void * code , size_t size , const char * arch , const char * identifier , const char * optionsCmdLine )  nvFatbinAddLTOIR adds LTOIR to the fatbinary. User is responsible for making sure all strings are well-formed. Parameters handle – [in] nvFatbin handle. code – [in] The LTOIR code. size – [in] The size of the LTOIR code. arch – [in] The architecture that this LTOIR is for. identifier – [in] Name of the LTOIR, useful when extracting the fatbin with tools like cuobjdump. optionsCmdLine – [in] Options used during JIT compilation. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddPTX ( nvFatbinHandle handle , const char * code , size_t size , const char * arch , const char * identifier , const char * optionsCmdLine )  nvFatbinAddPTX adds PTX to the fatbinary. User is responsible for making sure all string are well-formed. The size should be inclusive of the terminating null character (‘\\0’). If the final character is not ‘\\0’, one will be added automatically, but in doing so, the code will be copied if it hasn’t already been copied. Parameters handle – [in] nvFatbin handle. code – [in] The PTX code. size – [in] The size of the PTX code. arch – [in] The architecture that this PTX is for. identifier – [in] Name of the PTX, useful when extracting the fatbin with tools like cuobjdump. optionsCmdLine – [in] Options used during JIT compilation. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_PTX_ARCH_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_MISSING_PTX_VERSION NVFATBIN_ERROR_MISSING_PTX_ARCH NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddReloc ( nvFatbinHandle handle , const void * code , size_t size )  nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary. Note that each relocatable ptx source must have a unique identifier (the identifiers are taken from the object’s entries). This is enforced as only one entry per sm of each unique identifier. Note also that handle options are ignored for this operation. Instead, the host object’s options are copied over from each of its entries. Parameters handle – [in] nvFatbin handle. code – [in] The host object image. size – [in] The size of the host object image code. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_PTX_ARCH_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_MISSING_PTX_VERSION NVFATBIN_ERROR_MISSING_PTX_ARCH NVFATBIN_ERROR_IDENTIFIER_REUSE NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinCreate ( nvFatbinHandle * handle_indirect , const char * * options , size_t optionsCount )  nvFatbinCreate creates a new handle Parameters handle_indirect – [out] Address of nvFatbin handle options – [in] An array of strings, each containing a single option. optionsCount – [in] Number of options. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinDestroy ( nvFatbinHandle * handle_indirect )  nvFatbinDestroy destroys the handle. Use of any other pointers to the handle after calling this will result in undefined behavior. The passed in handle will be set to nullptr. Parameters handle_indirect – [in] Pointer to the handle. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinGet ( nvFatbinHandle handle , void * buffer )  nvFatbinGet returns the completed fatbinary. User is responsible for making sure the buffer is appropriately sized for the fatbinary . You must call nvFatbinSize before using this, otherwise, it will return an error. See also nvFatbinSize Parameters handle – [in] nvFatbin handle. buffer – [out] memory to store fatbinary. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinSize ( nvFatbinHandle handle , size_t * size )  nvFatbinSize returns the fatbinary’s size. Parameters handle – [in] nvFatbin handle. size – [out] The fatbinary’s size Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinVersion ( unsigned int * major , unsigned int * minor )  nvFatbinVersion returns the current version of nvFatbin Parameters major – [out] The major version. minor – [out] The minor version. Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL 3.2.2. Typedefs  typedef struct _nvFatbinHandle * nvFatbinHandle  nvFatbinHandle is the unit of fatbin creation, and an opaque handle for a program. To create a fatbin, an instance of nvFatbinHandle must be created first with nvFatbinCreate() . 3.3. Supported Options  nvFatbin supports the options below. Option names are prefixed with a single dash ( - ). Options that take a value have an assignment operator ( = ) followed by the option value, with no spaces, e.g. \"-host=windows\" . The supported options are: -32 Make entries 32 bit. -64 Make entries 64 bit. -c Has no effect. (Deprecated, will be removed in the next major release. Didn’t do anything from the start.) -compress=<bool> Enable (true) / disable (false) compression (default: true). -compress-all Compress everything in the fatbin, even if it’s small. -cuda Specify CUDA (rather than OpenCL). -g Generate debug information. -host=<name> Specify host operating system. Valid options are “linux”, “windows”, and “mac” (deprecated). -opencl Specify OpenCL (rather than CUDA). 4. Basic Usage  This section of the document uses a simple example to explain how to use the Fatbin Creator APIs to link a program. For brevity and readability, error checks on the API return values are not shown. This example assumes we want to create a fatbin with a CUBIN for sm_52, PTX for sm_61, and LTOIR for sm_70. We can create an instance of the fatbin creator and obtain an api handle to it as shown in Figure 1 . Figure 1. Fatbin Creator creation and initialization of a program nvFatbinHandle handle ; nvFatbinCreate ( & handle , nullptr , 0 ); Assume that we already have three inputs stored in std::vector ‘s (CUBIN, PTX, and LTOIR), which could be from code created with nvrtc and stored into vectors. (They do not have to be in vectors, this merely illustrates that both the data itself and its size are needed.) We can add the inputs as shown in Figure 2 . Figure 2. Inputs to the fatbin creator nvFatbinAddCubin ( handle , cubin . data (), cubin . size (), \"52\" , nullptr ); nvFatbinAddPTX ( handle , ptx . data (), ptx . size (), \"61\" , nullptr , nullptr ); nvFatbinAddLTOIR ( handle , ltoir . data (), ltoir . size (), \"70\" , nullptr , nullptr ); The fatbin can now be obtained. To obtain this we first allocate memory for it. And to allocate memory, we need to query the size of the fatbin which is done as shown in Figure 3 . Figure 3. Query size of the created fatbin nvFatbinSize ( linker , & fatbinSize ); The fatbin can now be queried as shown in Figure 4 . This fatbin can then be executed on the GPU by passing this to the CUDA Driver APIs. Figure 4. Query the created fatbin void * fatbin = malloc ( fatbinSize ); nvFatbinGet ( handle , fatbin ); When the fatbin creator is not needed anymore, it can be destroyed as shown in Figure 5 . Figure 5. Destroy the fatbin creator nvFatbinDestroy ( & handle ); 5. Compatibility  The nvFatbin library is compatible across releases. The library major version itself must be >= the maximum major version of the inputs. For example, you can create a fatbin from a cubin created with 11.8 and one with 12.4 if your nvFatbin library is at least version 12.x. 6. Example: Runtime fatbin creation  This section demonstrates runtime fatbin creation. There are two cubins. The cubins are generated online using NVRTC. These two cubins are then passed to nvFatbin* API functions, which put the cubins into a fatbin. Note that this example requires a compatible GPU with drivers and NVRTC to work, even though the library doesn’t require either. 6.1. Code (online.cpp)  #include <nvrtc.h> #include <cuda.h> #include <nvFatbin.h> #include <nvrtc.h> #include <iostream> #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x)                                        \\ do {                                                              \\ nvrtcResult result = x;                                        \\ if (result != NVRTC_SUCCESS) {                                 \\ std::cerr << \"\\nerror: \" #x \" failed with error \"           \\ << nvrtcGetErrorString(result) << '\\n';            \\ exit(1);                                                    \\ }                                                              \\ } while(0) #define CUDA_SAFE_CALL(x)                                         \\ do {                                                              \\ CUresult result = x;                                           \\ if (result != CUDA_SUCCESS) {                                  \\ const char *msg;                                            \\ cuGetErrorName(result, &msg);                               \\ std::cerr << \"\\nerror: \" #x \" failed with error \"           \\ << msg << '\\n';                                    \\ exit(1);                                                    \\ }                                                              \\ } while(0) #define NVFATBIN_SAFE_CALL(x)                            \\ do                                                       \\ {                                                        \\ nvFatbinResult result = x;                            \\ if (result != NVFATBIN_SUCCESS)                       \\ {                                                     \\ std::cerr << \"\\nerror: \" #x \" failed with error \"  \\ << nvFatbinGetErrorString(result) << '\\n';\\ exit(1);                                           \\ }                                                     \\ } while (0) const char * fatbin_saxpy = \" \\n \\ __device__  float compute(float a, float x, float y) { \\n \\ return a * x + y; \\n \\ } \\n \\ \\n \\ extern \\\" C \\\" __global__ \\n \\ void saxpy(float a, float *x, float *y, float *out, size_t n) \\n \\ { \\n \\ size_t tid = blockIdx.x * blockDim.x + threadIdx.x; \\n \\ if (tid < n) { \\n \\ out[tid] = compute(a, x[tid], y[tid]); \\n \\ } \\n \\ } \\n \" ; size_t process ( const void * input , const char * input_name , void ** output , const char * arch ) { // Create an instance of nvrtcProgram with the code string. nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog , // prog ( const char * ) input , // buffer input_name , // name 0 , // numHeaders NULL , // headers NULL )); // includeNames const char * opts [ 1 ]; opts [ 0 ] = arch ; nvrtcResult compileResult = nvrtcCompileProgram ( prog , // prog 1 , // numOptions opts ); // options // Obtain compilation log from the program. size_t logSize ; NVRTC_SAFE_CALL ( nvrtcGetProgramLogSize ( prog , & logSize )); char * log = new char [ logSize ]; NVRTC_SAFE_CALL ( nvrtcGetProgramLog ( prog , log )); std :: cout << log << '\\n' ; delete [] log ; if ( compileResult != NVRTC_SUCCESS ) { exit ( 1 ); } // Obtain generated CUBIN from the program. size_t CUBINSize ; NVRTC_SAFE_CALL ( nvrtcGetCUBINSize ( prog , & CUBINSize )); char * CUBIN = new char [ CUBINSize ]; NVRTC_SAFE_CALL ( nvrtcGetCUBIN ( prog , CUBIN )); // Destroy the program. NVRTC_SAFE_CALL ( nvrtcDestroyProgram ( & prog )); * output = ( void * ) CUBIN ; return CUBINSize ; } int main ( int argc , char * argv []) { void * known = NULL ; size_t known_size = process ( fatbin_saxpy , \"fatbin_saxpy.cu\" , & known , \"-arch=sm_52\" ); CUdevice cuDevice ; CUcontext context ; CUmodule module ; CUfunction kernel ; CUDA_SAFE_CALL ( cuInit ( 0 )); CUDA_SAFE_CALL ( cuDeviceGet ( & cuDevice , 0 )); CUDA_SAFE_CALL ( cuCtxCreate ( & context , 0 , cuDevice )); // Dynamically determine the arch to make one of the entries of the fatbin with int major = 0 ; int minor = 0 ; CUDA_SAFE_CALL ( cuDeviceGetAttribute ( & major , CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR , cuDevice )); CUDA_SAFE_CALL ( cuDeviceGetAttribute ( & minor , CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR , cuDevice )); int arch = major * 10 + minor ; char smbuf [ 16 ]; sprintf ( smbuf , \"-arch=sm_%d\" , arch ); void * dynamic = NULL ; size_t dynamic_size = process ( fatbin_saxpy , \"fatbin_saxpy.cu\" , & dynamic , smbuf ); sprintf ( smbuf , \"%d\" , arch ); // Load the dynamic CUBIN and the statically known arch CUBIN // and put them in a fatbin together. nvFatbinHandle handle ; const char * fatbin_options [] = { \"-cuda\" }; NVFATBIN_SAFE_CALL ( nvFatbinCreate ( & handle , fatbin_options , 1 )); NVFATBIN_SAFE_CALL ( nvFatbinAddCubin ( handle , ( void * ) dynamic , dynamic_size , smbuf , \"dynamic\" )); NVFATBIN_SAFE_CALL ( nvFatbinAddCubin ( handle , ( void * ) known , known_size , \"52\" , \"known\" )); size_t fatbinSize ; NVFATBIN_SAFE_CALL ( nvFatbinSize ( handle , & fatbinSize )); void * fatbin = malloc ( fatbinSize ); NVFATBIN_SAFE_CALL ( nvFatbinGet ( handle , fatbin )); NVFATBIN_SAFE_CALL ( nvFatbinDestroy ( & handle )); CUDA_SAFE_CALL ( cuModuleLoadData ( & module , fatbin )); CUDA_SAFE_CALL ( cuModuleGetFunction ( & kernel , module , \"saxpy\" )); // Generate input for execution, and create output buffers. #define NUM_THREADS 128 #define NUM_BLOCKS 32 size_t n = NUM_THREADS * NUM_BLOCKS ; size_t bufferSize = n * sizeof ( float ); float a = 5.1f ; float * hX = new float [ n ], * hY = new float [ n ], * hOut = new float [ n ]; for ( size_t i = 0 ; i < n ; ++ i ) { hX [ i ] = static_cast < float > ( i ); hY [ i ] = static_cast < float > ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize )); // Execute SAXPY. void * args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 , // grid dim NUM_THREADS , 1 , 1 , // block dim 0 , NULL , // shared mem and stream args , 0 )); // arguments CUDA_SAFE_CALL ( cuCtxSynchronize ()); // Retrieve and print output. CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << \" * \" << hX [ i ] << \" + \" << hY [ i ] << \" = \" << hOut [ i ] << '\\n' ; } // Release resources. CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); delete [] hX ; delete [] hY ; delete [] hOut ; // Release resources. free ( fatbin ); delete [] (( char * ) known ); delete [] (( char * ) dynamic ); return 0 ; } 6.2. Build Instructions  Assuming the environment variable CUDA_PATH points to CUDA Toolkit installation directory, build this example as: With nvFatbin shared library (note that if the test didn’t use nvrtc or run the code then it would not need to link with nvrtc or the CUDA driver API): Windows: cl.exe online.cpp /Feonline ^\n      /I \"%CUDA_PATH%\\include\" ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvrtc.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvfatbin.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\cuda.lib Linux: g++ online.cpp -o online \\\n      -I $CUDA_PATH/include \\\n      -L $CUDA_PATH/lib64 \\\n      -lnvrtc -lnvfatbin -lcuda \\\n      -Wl,-rpath,$CUDA_PATH/lib64 With nvFatbin static library: Windows: cl.exe online.cpp /Feonline  ^\n      /I \"%CUDA_PATH%\"\\include ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvrtc_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvrtc-builtins_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvfatbin_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvptxcompiler_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\cuda.lib user32.lib Ws2_32.lib Linux: g++ online.cpp -o online \\\n      -I $CUDA_PATH/include \\\n      -L $CUDA_PATH/lib64 \\\n      -lnvrtc_static -lnvrtc-builtins_static -lnvfatbin_static -lnvptxcompiler_static -lcuda \\\n      -lpthread 6.3. Notices  6.3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 6.3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. © 2023 NVIDIA Corporation & affiliates. All rights reserved. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2023-2024, NVIDIA Corporation & Affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/nvjitlink/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvjitlink/index.html", "content_type": "text/html", "text": "nvJitLink 1. Introduction 2. Getting Started 2.1. System Requirements 2.2. Installation 3. User Interface 3.1. Error codes 3.1.1. Enumerations 3.2. Linking 3.2.1. Enumerations 3.2.2. Functions 3.2.3. Typedefs 3.3. Supported Link Options 4. Basic Usage 5. Compatibility 6. Example: Device LTO (link time optimization) 6.1. Code (offline.cu) 6.2. Code (online.cpp) 6.3. Build Instructions 6.4. Notices 6.4.1. Notice 6.4.2. OpenCL 6.4.3. Trademarks nvJitLink » 1. Introduction v12.5 | Archive nvJitLink The User guide to nvJitLink library. 1. Introduction  The JIT Link APIs are a set of APIs which can be used at runtime to link together GPU devide code. The APIs accept inputs in multiple formats, either host objects, host libraries, fatbins, device cubins, PTX, or LTO-IR. The output is a linked cubin that can be loaded by cuModuleLoadData and cuModuleLoadDataEx of the CUDA Driver API. Link Time Optimization can also be performed when given LTO-IR or higher level formats that include LTO-IR. If an input does not contain GPU assembly code, it is first compiled and then linked. The functionality in this library is similar to the cuLink* APIs in the CUDA Driver, with the following advantages: Support for Link Time Optimization Allow users to use runtime linking with the latest Toolkit version that is supported as part of CUDA Toolkit release. This support may not be available in the CUDA Driver APIs if the application is running with an older driver installed in the system. Refer to CUDA Compatibility for more details. The clients get fine grain control and can specify low-level compiler options during linking. 2. Getting Started  2.1. System Requirements  The JIT Link library requires the following system configuration: POSIX threads support for non-Windows platform. GPU: Any GPU with CUDA Compute Capability 3.5 or higher. CUDA Toolkit and Driver. 2.2. Installation  The JIT Link library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include\\nvJitLink.h lib\\x64\\nvJitLink.dll lib\\x64\\nvJitLink_static.lib doc\\pdf\\nvJitLink_User_Guide.pdf On Linux: include/nvJitLink.h lib64/libnvJitLink.so lib64/libnvJitLink_static.a doc/pdf/nvJitLink_User_Guide.pdf 3. User Interface  This chapter presents the JIT Link APIs. Basic usage of the API is explained in Basic Usage . Error codes Linking Supported Link Options 3.1. Error codes  Enumerations nvJitLinkResult The enumerated type nvJitLinkResult defines API call result codes. 3.1.1. Enumerations  enum nvJitLinkResult  The enumerated type nvJitLinkResult defines API call result codes. nvJitLink APIs return nvJitLinkResult codes to indicate the result. Values: enumerator NVJITLINK_SUCCESS  enumerator NVJITLINK_ERROR_UNRECOGNIZED_OPTION  enumerator NVJITLINK_ERROR_MISSING_ARCH  enumerator NVJITLINK_ERROR_INVALID_INPUT  enumerator NVJITLINK_ERROR_PTX_COMPILE  enumerator NVJITLINK_ERROR_NVVM_COMPILE  enumerator NVJITLINK_ERROR_INTERNAL  enumerator NVJITLINK_ERROR_THREADPOOL  enumerator NVJITLINK_ERROR_UNRECOGNIZED_INPUT  3.2. Linking  Enumerations nvJitLinkInputType The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd* APIs. Functions nvJitLinkResult nvJitLinkAddData (nvJitLinkHandle handle, nvJitLinkInputType inputType, const void *data, size_t size, const char *name) nvJitLinkAddData adds data image to the link. nvJitLinkResult nvJitLinkAddFile (nvJitLinkHandle handle, nvJitLinkInputType inputType, const char *fileName) nvJitLinkAddFile reads data from file and links it in. nvJitLinkResult nvJitLinkComplete (nvJitLinkHandle handle) nvJitLinkComplete does the actual link. nvJitLinkResult nvJitLinkCreate (nvJitLinkHandle *handle, uint32_t numOptions, const char **options) nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle . nvJitLinkResult nvJitLinkDestroy (nvJitLinkHandle *handle) nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL. nvJitLinkResult nvJitLinkGetErrorLog (nvJitLinkHandle handle, char *log) nvJitLinkGetErrorLog puts any error messages in the log. nvJitLinkResult nvJitLinkGetErrorLogSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetErrorLogSize gets the size of the error log. nvJitLinkResult nvJitLinkGetInfoLog (nvJitLinkHandle handle, char *log) nvJitLinkGetInfoLog puts any info messages in the log. nvJitLinkResult nvJitLinkGetInfoLogSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetInfoLogSize gets the size of the info log. nvJitLinkResult nvJitLinkGetLinkedCubin (nvJitLinkHandle handle, void *cubin) nvJitLinkGetLinkedCubin gets the linked cubin. nvJitLinkResult nvJitLinkGetLinkedCubinSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetLinkedCubinSize gets the size of the linked cubin. nvJitLinkResult nvJitLinkGetLinkedPtx (nvJitLinkHandle handle, char *ptx) nvJitLinkGetLinkedPtx gets the linked ptx. nvJitLinkResult nvJitLinkGetLinkedPtxSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetLinkedPtxSize gets the size of the linked ptx. nvJitLinkResult nvJitLinkVersion (unsigned int *major, unsigned int *minor) nvJitLinkVersion returns the current version of nvJitLink. Typedefs nvJitLinkHandle nvJitLinkHandle is the unit of linking, and an opaque handle for a program. 3.2.1. Enumerations  enum nvJitLinkInputType  The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd* APIs. Values: enumerator NVJITLINK_INPUT_NONE  enumerator NVJITLINK_INPUT_CUBIN  enumerator NVJITLINK_INPUT_PTX  enumerator NVJITLINK_INPUT_LTOIR  enumerator NVJITLINK_INPUT_FATBIN  enumerator NVJITLINK_INPUT_OBJECT  enumerator NVJITLINK_INPUT_LIBRARY  enumerator NVJITLINK_INPUT_ANY  3.2.2. Functions  static inline nvJitLinkResult nvJitLinkAddData ( nvJitLinkHandle handle , nvJitLinkInputType inputType , const void * data , size_t size , const char * name )  nvJitLinkAddData adds data image to the link. Parameters handle – [in] nvJitLink handle. inputType – [in] kind of input. data – [in] pointer to data image in memory. size – [in] size of the data. name – [in] name of input object. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkAddFile ( nvJitLinkHandle handle , nvJitLinkInputType inputType , const char * fileName )  nvJitLinkAddFile reads data from file and links it in. Parameters handle – [in] nvJitLink handle. inputType – [in] kind of input. fileName – [in] name of file. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkComplete ( nvJitLinkHandle handle )  nvJitLinkComplete does the actual link. Parameters handle – [in] nvJitLink handle. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkCreate ( nvJitLinkHandle * handle , uint32_t numOptions , const char * * options )  nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle . It supports options listed in Supported Link Options . See also nvJitLinkDestroy Parameters handle – [out] Address of nvJitLink handle. numOptions – [in] Number of options passed. options – [in] Array of size numOptions of option strings. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_UNRECOGNIZED_OPTION NVJITLINK_ERROR_MISSING_ARCH NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkDestroy ( nvJitLinkHandle * handle )  nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL. See also nvJitLinkCreate Parameters handle – [in] Address of nvJitLink handle. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLog ( nvJitLinkHandle handle , char * log )  nvJitLinkGetErrorLog puts any error messages in the log. User is responsible for allocating enough space to hold the log . See also nvJitLinkGetErrorLogSize Parameters handle – [in] nvJitLink handle. log – [out] The error log. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLogSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetErrorLogSize gets the size of the error log. See also nvJitLinkGetErrorLog Parameters handle – [in] nvJitLink handle. size – [out] Size of the error log. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLog ( nvJitLinkHandle handle , char * log )  nvJitLinkGetInfoLog puts any info messages in the log. User is responsible for allocating enough space to hold the log . See also nvJitLinkGetInfoLogSize Parameters handle – [in] nvJitLink handle. log – [out] The info log. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLogSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetInfoLogSize gets the size of the info log. See also nvJitLinkGetInfoLog Parameters handle – [in] nvJitLink handle. size – [out] Size of the info log. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedCubin ( nvJitLinkHandle handle , void * cubin )  nvJitLinkGetLinkedCubin gets the linked cubin. User is responsible for allocating enough space to hold the cubin . See also nvJitLinkGetLinkedCubinSize Parameters handle – [in] nvJitLink handle. cubin – [out] The linked cubin. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedCubinSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetLinkedCubinSize gets the size of the linked cubin. See also nvJitLinkGetLinkedCubin Parameters handle – [in] nvJitLink handle. size – [out] Size of the linked cubin. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedPtx ( nvJitLinkHandle handle , char * ptx )  nvJitLinkGetLinkedPtx gets the linked ptx. Linked PTX is only available when using the -lto option. User is responsible for allocating enough space to hold the ptx . See also nvJitLinkGetLinkedPtxSize Parameters handle – [in] nvJitLink handle. ptx – [out] The linked PTX. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedPtxSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetLinkedPtxSize gets the size of the linked ptx. Linked PTX is only available when using the -lto option. See also nvJitLinkGetLinkedPtx Parameters handle – [in] nvJitLink handle. size – [out] Size of the linked PTX. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL nvJitLinkResult nvJitLinkVersion ( unsigned int * major , unsigned int * minor )  nvJitLinkVersion returns the current version of nvJitLink. Parameters major – [out] The major version. minor – [out] The minor version. Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL 3.2.3. Typedefs  typedef struct nvJitLink * nvJitLinkHandle  nvJitLinkHandle is the unit of linking, and an opaque handle for a program. To link inputs, an instance of nvJitLinkHandle must be created first with nvJitLinkCreate(). 3.3. Supported Link Options  nvJitLink supports the link options below. Option names are prefixed with a single dash ( - ). Options that take a value have an assignment operator ( = ) followed by the option value, with no spaces, e.g. \"-arch=sm_90\" . The supported options are: -arch=sm_<N> Pass SM architecture value. See nvcc for valid values of <N>. Can use compute_<N> value instead if only generating PTX. This is a required option. -maxrregcount=<N> Maximum register count. -time Print timing information to InfoLog. -verbose Print verbose messages to InfoLog. -lto Do link time optimization. -ptx Emit ptx after linking instead of cubin; only supported with -lto -O<N> Optimization level. Only 0 and 3 are accepted. -g Generate debug information. -lineinfo Generate line information. -ftz=<n> Flush to zero. -prec-div=<n> Precise divide. -prec-sqrt=<n> Precise square root. -fma=<n> Fast multiply add. -kernels-used=<name> Pass list of kernels that are used; any not in the list can be removed. This option can be specified multiple times. -variables-used=<name> Pass list of variables that are used; any not in the list can be removed. This option can be specified multiple times. -optimize-unused-variables Normally device code optimization is limited by not knowing what the host code references. With this option it can assume that if a variable is not referenced in device code then it can be removed. -Xptxas=<opt> Pass <opt> to ptxas. This option can be called multiple times. -split-compile=<N> Split compilation maximum thread count. Use 0 to use all available processors. Value of 1 disables split compilation (default). -split-compile-extended=<N> [Experimental] A more aggressive form of split compilation. Accepts a maximum thread count value. Use 0 to use all available processors. Value of 1 disables extended split compilation (default). -jump-table-density=<N> When doing LTO, specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement. Default value is 101. The percentage ranges from 0 to 101 inclusively. -no-cache Don’t cache the intermediate steps of nvJitLink. 4. Basic Usage  This section of the document uses a simple example to explain how to use the JIT Link APIs to link a program. For brevity and readability, error checks on the API return values are not shown. This example assumes we want to link for sm_80, but whatever arch is installed on the system should be used. We can create the linker and obtain a handle to it as shown in Figure 1 . Figure 1. Linker creation and initialization of a program nvJitLink_t linker ; const char * link_options [] = { \"-arch=sm_80\" }; nvJitLinkCreate ( & linker , 1 , link_options ); Assume that we already have two relocatable input files (a.o and b.o), which could be created with the nvcc -dc command. We can add the input files as show in Figure 2 . Figure 2. Inputs to linker nvJitLinkAddFile ( linker , NVJITLINK_INPUT_OBJECT , \"a.o\" ); nvJitLinkAddFile ( linker , NVJITLINK_INPUT_OBJECT , \"b.o\" ); Now the actual link can be done as shown in Figure 3 . Figure 3. Linking of the PTX program nvJitLinkComplete ( linker ); The linked GPU assembly code can now be obtained. To obtain this we first allocate memory for it. And to allocate memory, we need to query the size of the image of the linked GPU assembly code which is done as shown in Figure 4 . Figure 4. Query size of the linked assembly image nvJitLinkGetLinkedCubinSize ( linker , & cubinSize ); The image of the linked GPU assembly code can now be queried as shown in Figure 5 . This image can then be executed on the GPU by passing this image to the CUDA Driver APIs. Figure 5. Query the linked assembly image elf = ( char * ) malloc ( cubinSize ); nvJitLinkGetLinkedCubin ( linker , ( void * ) elf ); When the linker is not needed anymore, it can be destroyed as shown in Figure 6 . Figure 6. Destroy the linker nvJitLinkDestroy ( & linker ); 5. Compatibility  The nvJitLink library is compatible across minor versions in a release, but may not be compatible across major versions. The library version itself must be >= the maximum version of the inputs, and the shared library version must be >= the version that was linked with. For example, you can link an object created with 12.0 and one with 12.1 if your nvJitLink library is version 12.x where x >= 1. If it was linked with 12.1, then you can replace and use the nvJitLink shared library with any version 12.x where x >= 1. On the flip side, you cannot use 12.0 to link 12.1 objects, nor use 12.0 nvJitLink library to run 12.1 code. Linking across major versions (like 11.x with 12.x) works for ELF and PTX inputs, but does not work with LTOIR inputs. If using LTO, then compatibility is only guaranteed within a major release. 6. Example: Device LTO (link time optimization)  This section demonstrates device link time optimization (LTO). There are two units of LTO IR. The first unit is generated offline using nvcc , by specifying the architecture as ‘ -arch lto_XX ’ (see offline.cu). The generated LTO IR is packaged in a fatbinary. The second unit is generated online using NVRTC, by specifying the flag ‘ -dlto ’ (see online.cpp). These two units are then passed to libnvJitLink* API functions, which link together the LTO IR, run the optimizer on the linked IR, and generate a cubin (see online.cpp). The cubin is then loaded on the GPU and executed. 6.1. Code (offline.cu)  __device__ float compute ( float a , float x , float y ) { return a * x + y ; } 6.2. Code (online.cpp)  #include <nvrtc.h> #include <cuda.h> #include <nvJitLink.h> #include <nvrtc.h> #include <iostream> #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x)                                        \\ do {                                                            \\ nvrtcResult result = x;                                       \\ if (result != NVRTC_SUCCESS) {                                \\ std::cerr << \"\\nerror: \" #x \" failed with error \"           \\ << nvrtcGetErrorString(result) << '\\n';           \\ exit(1);                                                    \\ }                                                             \\ } while(0) #define CUDA_SAFE_CALL(x)                                         \\ do {                                                            \\ CUresult result = x;                                          \\ if (result != CUDA_SUCCESS) {                                 \\ const char *msg;                                            \\ cuGetErrorName(result, &msg);                               \\ std::cerr << \"\\nerror: \" #x \" failed with error \"           \\ << msg << '\\n';                                   \\ exit(1);                                                    \\ }                                                             \\ } while(0) #define NVJITLINK_SAFE_CALL(h,x)                                  \\ do {                                                            \\ nvJitLinkResult result = x;                                   \\ if (result != NVJITLINK_SUCCESS) {                            \\ std::cerr << \"\\nerror: \" #x \" failed with error \"           \\ << result << '\\n';                                \\ size_t lsize;                                               \\ result = nvJitLinkGetErrorLogSize(h, &lsize);               \\ if (result == NVJITLINK_SUCCESS && lsize > 0) {             \\ char *log = (char*)malloc(lsize);                         \\ result = nvJitLinkGetErrorLog(h, log);                        \\ if (result == NVJITLINK_SUCCESS) {                            \\ std::cerr << \"error: \" << log << '\\n';                      \\ free(log);                                                  \\ }                                                             \\ }                                                           \\ exit(1);                                                    \\ }                                                             \\ } while(0) const char * lto_saxpy = \" \\n \\ extern __device__ float compute(float a, float x, float y); \\n \\ \\n \\ extern \\\" C \\\" __global__ \\n \\ void saxpy(float a, float *x, float *y, float *out, size_t n) \\n \\ { \\n \\ size_t tid = blockIdx.x * blockDim.x + threadIdx.x; \\n \\ if (tid < n) { \\n \\ out[tid] = compute(a, x[tid], y[tid]); \\n \\ } \\n \\ } \\n \" ; int main ( int argc , char * argv []) { size_t numBlocks = 32 ; size_t numThreads = 128 ; // Create an instance of nvrtcProgram with the code string. nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog , // prog lto_saxpy , // buffer \"lto_saxpy.cu\" , // name 0 , // numHeaders NULL , // headers NULL )); // includeNames // specify that LTO IR should be generated for LTO operation const char * opts [] = { \"-dlto\" , \"--relocatable-device-code=true\" }; nvrtcResult compileResult = nvrtcCompileProgram ( prog , // prog 2 , // numOptions opts ); // options // Obtain compilation log from the program. size_t logSize ; NVRTC_SAFE_CALL ( nvrtcGetProgramLogSize ( prog , & logSize )); char * log = new char [ logSize ]; NVRTC_SAFE_CALL ( nvrtcGetProgramLog ( prog , log )); std :: cout << log << '\\n' ; delete [] log ; if ( compileResult != NVRTC_SUCCESS ) { exit ( 1 ); } // Obtain generated LTO IR from the program. size_t LTOIRSize ; NVRTC_SAFE_CALL ( nvrtcGetLTOIRSize ( prog , & LTOIRSize )); char * LTOIR = new char [ LTOIRSize ]; NVRTC_SAFE_CALL ( nvrtcGetLTOIR ( prog , LTOIR )); // Destroy the program. NVRTC_SAFE_CALL ( nvrtcDestroyProgram ( & prog )); CUdevice cuDevice ; CUcontext context ; CUmodule module ; CUfunction kernel ; CUDA_SAFE_CALL ( cuInit ( 0 )); CUDA_SAFE_CALL ( cuDeviceGet ( & cuDevice , 0 )); CUDA_SAFE_CALL ( cuCtxCreate ( & context , 0 , cuDevice )); // Load the generated LTO IR and the LTO IR generated offline // and link them together. nvJitLinkHandle handle ; // Dynamically determine the arch to link for int major = 0 ; int minor = 0 ; CUDA_SAFE_CALL ( cuDeviceGetAttribute ( & major , CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR , cuDevice )); CUDA_SAFE_CALL ( cuDeviceGetAttribute ( & minor , CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR , cuDevice )); int arch = major * 10 + minor ; char smbuf [ 16 ]; sprintf ( smbuf , \"-arch=sm_%d\" , arch ); const char * lopts [] = { \"-lto\" , smbuf }; NVJITLINK_SAFE_CALL ( handle , nvJitLinkCreate ( & handle , 2 , lopts )); // NOTE: assumes \"offline.fatbin\" is in the current directory // The fatbinary contains LTO IR generated offline using nvcc NVJITLINK_SAFE_CALL ( handle , nvJitLinkAddFile ( handle , NVJITLINK_INPUT_FATBIN , \"offline.fatbin\" )); NVJITLINK_SAFE_CALL ( handle , nvJitLinkAddData ( handle , NVJITLINK_INPUT_LTOIR , ( void * ) LTOIR , LTOIRSize , \"lto_online\" )); // The call to nvJitLinkComplete causes linker to link together the two // LTO IR modules (offline and online), do optimization on the linked LTO IR, // and generate cubin from it. NVJITLINK_SAFE_CALL ( handle , nvJitLinkComplete ( handle )); size_t cubinSize ; NVJITLINK_SAFE_CALL ( handle , nvJitLinkGetLinkedCubinSize ( handle , & cubinSize )); void * cubin = malloc ( cubinSize ); NVJITLINK_SAFE_CALL ( handle , nvJitLinkGetLinkedCubin ( handle , cubin )); NVJITLINK_SAFE_CALL ( handle , nvJitLinkDestroy ( & handle )); CUDA_SAFE_CALL ( cuModuleLoadData ( & module , cubin )); CUDA_SAFE_CALL ( cuModuleGetFunction ( & kernel , module , \"saxpy\" )); // Generate input for execution, and create output buffers. size_t n = NUM_THREADS * NUM_BLOCKS ; size_t bufferSize = n * sizeof ( float ); float a = 5.1f ; float * hX = new float [ n ], * hY = new float [ n ], * hOut = new float [ n ]; for ( size_t i = 0 ; i < n ; ++ i ) { hX [ i ] = static_cast < float > ( i ); hY [ i ] = static_cast < float > ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize )); // Execute SAXPY. void * args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 , // grid dim NUM_THREADS , 1 , 1 , // block dim 0 , NULL , // shared mem and stream args , 0 )); // arguments CUDA_SAFE_CALL ( cuCtxSynchronize ()); // Retrieve and print output. CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << \" * \" << hX [ i ] << \" + \" << hY [ i ] << \" = \" << hOut [ i ] << '\\n' ; } // Release resources. CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); free ( cubin ); delete [] hX ; delete [] hY ; delete [] hOut ; delete [] LTOIR ; return 0 ; } 6.3. Build Instructions  Assuming the environment variable CUDA_PATH points to CUDA Toolkit installation directory, build this example as: Compile offline.cu to fatbinary containing LTO IR (change lto_52 to a different lto_XX architecture as appropriate). nvcc -arch lto_52 -rdc=true -fatbin offline.cu With nvJitLink shared library (note that if test didn’t use nvrtc then it would not need to link with nvrtc): Windows: cl.exe online.cpp /Feonline ^\n      /I \"%CUDA_PATH%\\include\" ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvrtc.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvJitLink.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\cuda.lib Linux: g++ online.cpp -o online \\\n      -I $CUDA_PATH/include \\\n      -L $CUDA_PATH/lib64 \\\n      -lnvrtc -lnvJitLink -lcuda \\\n      -Wl,-rpath,$CUDA_PATH/lib64 With nvJitLink static library (when linking with static library then need to also link with nvptxcompiler_static, but with is implicitly included): Windows: cl.exe online.cpp /Feonline  ^\n      /I \"%CUDA_PATH%\"\\include ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvrtc_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvrtc-builtins_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvJitLink_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\nvptxcompiler_static.lib ^\n      \"%CUDA_PATH%\"\\lib\\x64\\cuda.lib user32.lib Ws2_32.lib Linux: g++ online.cpp -o online \\\n      -I $CUDA_PATH/include \\\n      -L $CUDA_PATH/lib64 \\\n      -lnvrtc_static -lnvrtc-builtins_static -lnvJitLink_static -lnvptxcompiler_static -lcuda \\\n      -lpthread 6.4. Notices  6.4.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.4.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 6.4.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. © 2022-2022 NVIDIA Corporation & affiliates. All rights reserved. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/nvblas/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvblas/index.html", "content_type": "text/html", "text": "NVBLAS 1. Introduction 2. NVBLAS Overview 3. GPU Accelerated Routines 4. BLAS Symbols Interception 5. Device Memory Support 6. Security Precaution 7. Configuration 7.1. NVBLAS_CONFIG_FILE Environment Variable 7.2. Configuration Keywords 7.2.1. NVBLAS_LOGFILE 7.2.2. NVBLAS_TRACE_LOG_ENABLED 7.2.3. NVBLAS_CPU_BLAS_LIB 7.2.4. NVBLAS_GPU_LIST 7.2.5. NVBLAS_TILE_DIM 7.2.6. NVBLAS_GPU_DISABLED_<BLAS_FUNC_NAME> 7.2.7. NVBLAS_CPU_RATIO_<BLAS_FUNC_NAME> 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED 7.2.9. Configuration File Example 8. NVBLAS Installation 9. Usage 10. Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks NVBLAS » 1. Introduction v12.5 | PDF | Archive NVBLAS The User guide for NVBLAS, drop-in BLAS replacement, multi-GPUs accelerated 1. Introduction  The NVBLAS Library is a GPU-accelerated Libary that implements BLAS (Basic Linear Algebra Subprograms). It can accelerate most BLAS Level-3 routines by dynamically routing BLAS calls to one or more NVIDIA GPUs present in the system, when the charateristics of the call make it speed up on a GPU. 2. NVBLAS Overview  The NVBLAS Library is built on top of the cuBLAS Library using only the CUBLASXT API (refer to the CUBLASXT API section of the cuBLAS Documentation for more details). NVBLAS also requires the presence of a CPU BLAS lirbary on the system. Currently NVBLAS intercepts only compute intensive BLAS Level-3 calls (see table below). Depending on the charateristics of those BLAS calls, NVBLAS will redirect the calls to the GPUs present in the system or to CPU. That decision is based on a simple heuristic that estimates if the BLAS call will execute for long enough to amortize the PCI transfers of the input and output data to the GPU. Because NVBLAS does not support all standard BLAS routines, it might be necessary to associate it with an existing full BLAS Library. Please refer to the Usage section for more details. 3. GPU Accelerated Routines  NVBLAS offloads only the compute-intensive BLAS3 routines which have the best potential for acceleration on GPUs. The following table shows the currently supported routines: Routine Types Operation gemm S,D,C,Z Multiplication of 2 matrices syrk S,D,C,Z Symmetric rank-k update herk C,Z Hermitian rank-k update syr2k S,D,C,Z Symmetric rank-2k update her2k C,Z Hermitian rank-2k update trsm S,D,C,Z Triangular solve with multiple right-hand sides trmm S,D,C,Z Triangular matrix-matrix multiplication symm S,D,C,Z Symmetric matrix-matrix multiplication hemm C,Z Hermitian matrix-matrix multiplication 4. BLAS Symbols Interception  Standard BLAS Library implementations usually expose multiple symbols for the same routines. Let’s say func is a BLAS routine name, func_ or/and func are usually defined as extern symbols. Some BLAS Libraries might also expose some symbols with a proprietary appended prefix. NVBLAS intercepts only the symbols func_ and func . The user needs to make sure that the application intended to be GPU-accelerated by NVBLAS actually calls those defined symbols. Any other symbols will not be intercepted and the original BLAS routine will be executed for those cases. 5. Device Memory Support  Starting with Release 8.0, data can be located on any GPU device, even on GPU devices that are not configured to be part of the computation. When any of the data is located on a GPU, the computation will be exclusively done on GPU whatever the size of the problem. Also, this feature has to be used with caution: the user has to be sure that the BLAS call will indeed be intercepted by NVBLAS, otherwise it will result in a crash when the CPU BLAS tries to execute it. 6. Security Precaution  Because the NVBLAS Library relies on a symbols interception mechanism, it is essential to make sure it has not been compromised. In that regard, NVBLAS should never be used from a process running at elevated privileges, such as Administrator on Windows or root on Linux. 7. Configuration  Because NVBLAS is a drop-in replacement of BLAS, it must be configured through an ASCII text file that describes how many and which GPUs can participate in the intercepted BLAS calls. The configuration file is parsed at the time of the loading of the library. The format of the configuration file is based on keywords optionally followed by one or more user-defined parameters. At most one keyword per line is allowed. Blank lines or lines beginning with the character # are ignored. 7.1. NVBLAS_CONFIG_FILE Environment Variable  The location and name of the configuration file must be defined by the environment variable NVBLAS_CONFIG_FILE . By default, if NVBLAS_CONFIG_FILE is not defined, NVBLAS will try to open the file nvblas.conf in the current directory. For a safe use of NVBLAS, the configuration file should have have restricted write permissions. 7.2. Configuration Keywords  The configuration keywords syntax is described in the following subsections. 7.2.1. NVBLAS_LOGFILE  This keyword defines the file where NVBLAS should print status and error messages. By default, if not defined, the standard error output file (eg. stderr) will be used. It is advised to define this keyword early in the configuration to capture errors in parsing that file itself. 7.2.2. NVBLAS_TRACE_LOG_ENABLED  When this keyword is defined, every intercepted BLAS calls will be logged into the NVBLAS_LOGFILE. This feature, even though intrusive, can be useful for debugging purposes. 7.2.3. NVBLAS_CPU_BLAS_LIB  This keyword defines the CPU BLAS dynamic library file (for example, .so file on Linux or .dll on Windows) that NVBLAS should open to find the CPU BLAS symbols definitions. This keyword must be defined for NVBLAS to work. Because CPU Blas libraries are often composed of multiple files, even though this keyword is set to the full path to the main file of the CPU library, it might still be necessary to define the right path to find the rest of the library files in the environment of your system. On Linux, this can be done by setting the environment variable LD_LIBRARY_PATH whereas on Windows, this can be done by setting the environment variable PATH . For a safe use of NVBLAS, the following precautions are strongly advised: The CPU BLAS Library should be located where ordinary users do not have write permissions. The path specified should be absolute, not relative. 7.2.4. NVBLAS_GPU_LIST  This keyword defines the list of GPUs that should participate in the computation of the intercepted BLAS calls. If not defined, only GPU device 0 is used, since that is normally the most compute-capable GPU installed in the system. This keyword can be set to a list of device numbers separated by blank characters. Also the following wildcard keywords are also accepted for simplicity : Keyword Meaning ALL All compute-capable GPUs detected on the system will be used by NVBLAS ALL0 GPU device 0, AND all others GPUs detected that have the same compute-capabilities as device 0 will be used by NVBLAS Note In the current release of CUBLAS, the CUBLASXT API supports two GPUs if they are on the same board such as Tesla K10 or GeForce GTX690 and one GPU otherwise. Because NVBLAS is built on top of the CUBLASXT API, NVBLAS has the same restriction. If access to more GPUs devices is needed, details of the licensing are described at cublasXt . 7.2.5. NVBLAS_TILE_DIM  This keyword defines the tile dimension that should be used to divide the matrices involved in the computation. This definition maps directly to a call of the cublasXt API routine cublasXtSetBlockDim . Refer to cuBLAS documentation to understand the tradeoffs associated with setting this to a larger or a smaller value. 7.2.6. NVBLAS_GPU_DISABLED_<BLAS_FUNC_NAME>  This keyword, appended with the name of a BLAS routine disables NVBLAS from running a specified routine on the GPU. This feature is intended mainly for debugging purposes. By default, all supported BLAS routines are enabled. 7.2.7. NVBLAS_CPU_RATIO_<BLAS_FUNC_NAME>  This keyword, appended with the name of ta BLAS routine defines the ratio of the workload that should remain on the CPU in the event that the NVBLAS decides to offload work for that routine on the GPU. This functionality is directly mapped to the cublasXt API routine cublasXtSetCpuRatio . By default, the ratio is defined to zero for all routines. Please refer to the cuBLAS documentation for details and for the list of routines which support this feature. 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED  This keyword enables the Pinning Memory mode. This functionality is directly mapped to the cublasXt API routine cublasXtSetPinningMemMode . If this keyowrd is not present in the configuration file, the Pinning Memory mode will be set to CUBLASXT_PINNING_DISABLED . Note There are some restrictions to use this feature as specified in the cuBLAS documentation of the underlying routine cublasXtSetPinningMemMode . Specifically when NVBLAS is used in a multi-threaded applications, this option should not be used if there is a chance that matrices used by different threads overlaps while calling NVBLAS. Please refer to the cuBLAS Documentation of the routine `cublasXtSetPinningMemMode < https://docs.nvidia.com/cuda/cublas/index.html#cublasxt_setPinningMemMode >`__ for details. 7.2.9. Configuration File Example  The following example shows a typical NVBLAS configuration file : # This is the configuration file to use NVBLAS Library\n# Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file.\n# By default, if NVBLAS_CONFIG_FILE is not defined,\n# NVBLAS Library will try to open the file \"nvblas.conf\" in its current directory\n# Example : NVBLAS_CONFIG_FILE  /home/cuda_user/my_nvblas.conf\n# The config file should have restricted write permissions accesses\n\n# Specify which output log file (default is stderr)\nNVBLAS_LOGFILE  nvblas.log\n\n# Enable trace log of every intercepted BLAS calls\nNVBLAS_TRACE_LOG_ENABLED\n\n#Put here the CPU BLAS fallback Library of your choice\n#It is strongly advised to use full path to describe the location of the CPU Library\nNVBLAS_CPU_BLAS_LIB  /usr/lib/libopenblas.so\n#NVBLAS_CPU_BLAS_LIB  <mkl_path_installtion>/libmkl_rt.so\n\n# List of GPU devices Id to participate to the computation\n# Use ALL if you want all your GPUs to contribute\n# Use ALL0, if you want all your GPUs of the same type as device 0 to contribute\n# However, NVBLAS consider that all GPU have the same performance and PCI bandwidth\n# By default if no GPU are listed, only device 0 will be used\n\n#NVBLAS_GPU_LIST 0 2 4\n#NVBLAS_GPU_LIST ALL\nNVBLAS_GPU_LIST ALL0\n\n# Tile Dimension\nNVBLAS_TILE_DIM 2048\n\n# Autopin Memory\nNVBLAS_AUTOPIN_MEM_ENABLED\n\n#List of BLAS routines that are prevented from running on GPU (use for debugging purpose\n# The current list of BLAS routines supported by NVBLAS are\n# GEMM, SYRK, HERK, TRSM, TRMM, SYMM, HEMM, SYR2K, HER2K\n\n#NVBLAS_GPU_DISABLED_SGEMM\n#NVBLAS_GPU_DISABLED_DGEMM\n#NVBLAS_GPU_DISABLED_CGEMM\n#NVBLAS_GPU_DISABLED_ZGEMM\n\n# Computation can be optionally hybridized between CPU and GPU\n# By default, GPU-supported BLAS routines are ran fully on GPU\n# The option NVBLAS_CPU_RATIO_<BLAS_ROUTINE> give the ratio [0,1]\n# of the amount of computation that should be done on CPU\n# CAUTION : this option should be used wisely because it can actually\n# significantly reduced the overall performance if too much work is given to CPU\n\n#NVBLAS_CPU_RATIO_CGEMM 0.07 8. NVBLAS Installation  The NVBLAS Library is part of the CUDA Toolkit, and will be installed along all the other CUDA libraries. It is available on 64-bit operating systems. NVBLAS Library is built on top of cuBLAS, so the cuBLAS library needs to be accessible by NVBLAS. 9. Usage  To use the NVBLAS Library, the user application must be relinked against NVBLAS in addition to the original CPU Blas (technically only NVBLAS is needed unless some BLAS routines not supported by NVBLAS are used by the application). To be sure that the linker links against the exposed symbols of NVBLAS and not the ones from the CPU BLAS, the NVBLAS Library needs to be put before the CPU BLAS on the linkage command line. On Linux, an alternative way to use NVBLAS Library is to use the LD_PRELOAD environment variable; this technique has the advantage of avoiding the relinkage step. However, the user should avoid defining that environment variable globally because it will cause the NVBLAS library to be loaded by every shell command executed on the system, thus leading to a lack of responsiveness of the system. Finally, mathematical tools and libraries often offer the opportunity to specify the BLAS Library to be used through an environment variable or a configuration file. Because NVBLAS does not support all the standard BLAS routines, it might be necessary to pair NVBLAS with a full BLAS library, even though your application only calls supported NVBLAS routines. Fortunately, those tools and libraries usually offer a way to specify multiple BLAS Libraries. Please refer to the documentation of the appropriate tools and libraries for details. 10. Notices  10.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/npp/index.html", "parent_url": "https://docs.nvidia.com/cuda/npp/index.html", "content_type": "text/html", "text": "NVIDIA 2D Image and Signal Processing Performance Primitives (NPP) Contents: What is NPP ? General Conventions Image Processing Conventions Signal Processing Conventions Data Types, Structs, Enums, and Constants Image Arithmetic And Logical Operations Image Color Conversion Functions Image Data Exchange And Initialization Functions Image Filtering Functions Image Geometry Transforms Functions Image Linear Transforms Functions Image Morphological Operations Image Statistics Functions Image Threshold And Compare Operations Image Memory Management Functions Signal Arithmetic And Logical Operations Signal Conversion Functions Signal Filtering Functions Signal Initialization Functions Signal Statistical Functions Signal Memory Management Functions npp » NVIDIA 2D Image and Signal Processing Performance Primitives (NPP) v23.05 | PDF | Archive NVIDIA 2D Image and Signal Processing Performance Primitives (NPP)  Indices and Search  Index Search Page Contents: What is NPP ? Files Header Files Library Files Library Organization Supported NVIDIA Hardware General Conventions Memory Management Scratch Buffer and Host Pointer Function Naming Integer Result Scaling Rounding Modes Rounding Mode Parameter Image Processing Conventions Function Naming Image Data Line Step Parameter Names for Image Data Passing Source-Image Data Source-Image Pointer Source-Batch-Images Pointer Source-Planar-Image Pointer Array Source-Planar-Image Pointer Source-Image Line Step Source-Planar-Image Line Step Array Source-Planar-Image Line Step Passing Destination-Image Data Destination-Image Pointer Destination-Batch-Images Pointer Destination-Planar-Image Pointer Array Destination-Planar-Image Pointer Destination-Image Line Step Destination-Planar-Image Line Step Passing In-Place Image Data In-Place Image Pointer In-Place-Image Line Step Passing Mask-Image Data Mask-Image Pointer Mask-Image Line Step Passing Channel-of-Interest Data Channel_of_Interest Number Image Data Alignment Requirements Image Data Related Error Codes Region-Of-Interest (ROI) ROI Related Error Codes Masked Operation Channel-of-Interest API Select-Channel Source-Image Pointer Select-Channel Source-Image Select-Channel Destination-Image Pointer Source-Image Sampling Point-Wise Operations Neighborhood Operations Mask-Size Parameter Anchor-Point Parameter Sampling Beyond Image Boundaries Signal Processing Conventions Signal Data Parameter Names for Signal Data Source Signal Pointer Destination Signal Pointer In-Place Signal Pointer Signal Data Alignment Requirements Signal Data Related Error Codes Signal Length Length Related Error Codes Data Types, Structs, Enums, and Constants Image Arithmetic And Logical Operations Arithmetic Operations Arithmetic Operations AddC MulC MulCScale SubC DivC AbsDiffC Add AddSquare AddProduct AddWeighted Mul MulScale Sub Div Div_Round Abs AbsDiff Sqr Sqrt Ln Exp Logical Operations Logical Operations AndC OrC XorC RShiftC LShiftC And Or Xor Not Image Alpha Composition Operations AlphaCompC AlphaComp Image Color Conversion Functions Color Processing Functions Color To Gray Conversion Color Debayer Color Gamma Correction Complement Color Key ColorTwist ColorTwistBatch ColorLUT ColorLUTLinear ColorLUTCubic ColorLUTTrilinear ColorLUTPalette Color Sampling Format Conversion Functions YCbCr420ToYCbCr411 YCbCr422ToYCbCr422 YCbCr422ToYCrCb422 YCbCr422ToCbYCr422 CbYCr422ToYCbCr411 YCbCr422ToYCbCr420 YCrCb420ToYCbCr422 YCbCr422ToYCrCb420 YCbCr422ToYCbCr411 YCrCb422ToYCbCr422 YCrCb422ToYCbCr420 YCrCb422ToYCbCr411 CbYCr422ToYCbCr422 CbYCr422ToYCbCr420 CbYCr422ToYCrCb420 YCbCr420ToYCbCr420 YCbCr420ToYCbCr422 YCbCr420ToCbYCr422 YCbCr420ToYCrCb420 YCrCb420ToCbYCr422 YCrCb420ToYCbYCr420 YCrCb420ToYCbYCr411 YCbCr411ToYCbCr411 YCbCr411ToYCbCr422 YCbCr411ToYCrCb422 YCbCr411ToYCbCr420 YCbCr411ToYCrCb420 NV12ToYUV420 Color Model Conversion Functions RGBToYUV BGRToYUV YUVToRGB YUVToRGBBatch YUVToRGBBatchAdvanced YUVToBGR YUVToBGRBatch YUVToBGRBatchAdvanced RGBToYUV422 YUV422ToRGB YUV422ToRGBBatch YUV422ToRGBBatchAdvanced YUV422ToBGRBatch YUV422ToBGRBatchAdvanced RGBToYUV420 YUV420ToRGB YUV420ToRGBBatch YUV420ToRGBBatchAdvanced NV12ToRGB NV21ToRGB BGRToYUV420 YUV420ToBGR YUV420ToBGRBatch YUV420ToBGRBatchAdvanced NV12ToBGR NV21ToBGR RGBToYCbCr YCbCrToRGB YCbCrToRGBBatch YCbCrToRGBBatchAdvanced YCbCrToBGR YCbCrToBGRBatch YCbCrToBGRBatchAdvanced YCbCrToBGR709CSC RGBToYCbCr422 YCbCr422ToRGB YCbCr422ToRGBBatch YCbCr422ToRGBBatchAdvanced RGBToYCrCb422 YCrCb422ToRGB YCbCr422ToBGR YCbCr422ToBGRBatch YCbCr422ToBGRBatchAdvanced RGBToCbYCr422 CbYCr422ToRGB BGRToCbYCr422 BGRToCbYCr422 709HDTV CbYCr422ToBGR CbYCr422ToBGR 709HDTV RGBToYCbCr420 YCbCr420ToRGB YCbCr420ToRGBBatch YCbCr420ToRGBBatchAdvanced RGBToYCrCb420 YCrCb420ToRGB BGRToYCbCr420 BGRToYCbCr420 709CSC BGRToYCbCr420 709HDTV BGRToYCrCb420 709CSC YCbCr420ToBGR YCbCr420ToBGRBatch YCbCr420ToBGRBatchAdvanced YCbCr420ToBGR 709CSC YCbCr420ToBGR 709HDTV BGRToYCrCb420 BGRToYCbCr411 BGRToYCbCr YCbCr411ToBGR YCbCr411ToRGB RGBToXYZ XYZToRGB RGBToLUV LUVToRGB BGRToLab LabToBGR RGBToYCC YCCToRGB YCCKToCMYK_JPEG CMYKOrYCCKJPEGToRGB YCCKJPEGOrCMYKToBGR RGBToHLS HLSToRGB BGRToHLS HLSToBGR RBGToHSV HSVToRGB JPEG Color Conversion Image Data Exchange And Initialization Functions Set Common parameters for nppiSet functions: Masked Set Common parameters for nppiSet_CXM functions: Channel Set Common parameters for nppiSet_CXC functions: Copy Common parameters for nppiCopy functions: Masked Copy Common parameters for nppiCopy_CXM functions: Channel Copy Common parameters for nppiCopy_CXC functions: Extract Channel Copy Common parameters for nppiCopy_CXC1 functions: Insert Channel Copy Common parameters for nppiCopy_C1CX functions: Packed To Planar Channel Copy Common parameters for nppiCopy_CXPX functions: Planar To Packed Channel Copy Common parameters for nppiCopy_PXCX functions: Copy Constant Border Common parameters for nppiCopyConstBorder functions: Copy Replicate Border Common parameters for nppiCopyReplicateBorder functions: Copy Wrap Border Common parameters for nppiCopyWrapBorder functions: Copy Sub-Pixel Common parameters for nppiCopySubPix functions: Convert Bit Depth Convert To Increased Bit Depth Common parameters for nppiConvert to increased bit depth functions: Convert To Decreased Bit Depth Common parameters for nppiConvert to decreased bit depth functions: Scale Bit Depth Scale To Higher Bit Depth Common parameters for nppiScale to higher bit depth functions: Scale To Lower Bit Depth Common parameters for nppiScale to lower bit depth functions: Duplicate Channel Common parameters for nppiDup functions: Transpose Common parameters for nppiTranspose functions: Swap Channels Image Filtering Functions Image 1D Linear Filters 1DLinearFilter Image Filter Column FilterColumn Image Filter Column Border FilterColumnBorder Image Filter Column 32f FilterColumn32f Image Filter Column Border 32f FilterColumnBorder32f Image Filter Row FilterRow Image Filter Row Border FilterRowBorder Image Filter Row 32f FilterRow32f Image Filter Row Border 32f FilterRowBorder32f Image Filter 1D Window Sum 1D Window Sum Image Filter 1D Window Column Sum 1D Window Column Sum Image Filter 1D Window Row Sum 1D Window Row Sum Image Filter 1D Window Sum Border 1D Window Sum with Border Control Image Filter 1D Window Column Sum Border 1D Window Column Sum Border Image Filter 1D Window Row Sum Border 1D Window Row Sum Border Image Convolution Convolution Image Filter Filter Image Filter 32f Filter32f Image Filter Border FilterBorder Image Filter Border 32f FilterBorder32f 2D Fixed Linear Filters 2D Fixed Linear Filters Image Filter Box FilterBox Image Filter Box Border FilterBoxBorder Image Filter Box Border Advanced FilterBoxBorderAdvanced Image Filter Threshold Adaptive Box Border FilterThresholdAdaptiveBoxBorder Rank Filters Rank Filters Image Filter Max FilterMax Image Filter Max Border FilterMaxBorder Image Filter Min FilterMin Image Filter Min Border FilterMinBorder Image Filter Median FilterMedian Image Filter Median Border FilterMedianBorder Fixed Filters Fixed Filters Image Filter Prewitt FilterPrewitt Image Filter Prewitt Border FilterPrewittBorder Image Filter Scharr FilterScharr Image Filter Scharr Border FilterScharrBorder Image Filter Sobel FilterSobel Image Filter Sobel Border FilterSobelBorder Image Filter Roberts FilterRoberts Image Filter Roberts Border FilterRobertsBorder Image Filter Laplace FilterLaplace Image Filter Laplace Border FilterLaplaceBorder Image Filter Gauss FilterGauss Image Filter Gauss Advanced FilterGaussAdvanced Image Filter Gauss Border FilterGaussBorder Image Filter Advanced Gauss Border FilterGaussAdvancedBorder Image Filter Gauss Pyramid Layer Down Border FilterGaussPyramidLayerDownBorder Image Filter Gauss Pyramid Layer Up Border FilterGaussPyramidLayerUpBorder Image Filter Bilateral Gauss Border FilterBilateralGaussBorder Image Filter  High Pass FilterHighPass Image Filter High Pass Border FilterHighPassBorder Image Filter Low Pass FilterLowPass Image Filter Low Pass Border FilterLowPassBorder Image Filter Sharpen FilterSharpen Image Filter Sharpen Border FilterSharpenBorder Image Filter Unsharp Border FilterUnsharpBorder Image Filter Wiener Border FilterWienerBorder Image Filter Gradient Vector Prewitt Border GradientVectorPrewittBorder Image Filter Gradient Vector Scharr Border GradientVectorScharrBorder Image Filter Gradient Vector Sobel Border GradientVectorSobelBorder Computer Vision Filtering Functions Computer Vision Image Filter Distance Transform FilterDistanceTransform Image Filter Harris Corners Border FilterHarrisCornersBorder Image Filter Hough Line FilterHoughLine Image Filter Histogram Of Oriented Gradients Border HistogramOfOrientedGradientsBorder Image Filter Flood Fill FloodFill Flood Fill FloodFill Flood Fill Boundary FloodFillBoundary Flood Fill Range FloodFillRange Flood Fill Range Boundary FloodFillRangeBoundary Flood Fill Gradient FloodFillGradient Flood Fill Gradient Boundary FloodFillGradientBoundary Label Markers LabelMarkers Label MarkersUF LabelMarkersUF Label MarkersUF Batch LabelMarkersUFBatch Label MarkersUF Batch Advanced LabelMarkersUFBatchAdvanced Image Filter Compress Marker Labels Image Filter Compressed Marker Labels Info Image Filter Contour Pixel Interpolation ContourPixelInterpolation Bound Segments Watershed Segmentation WatershedSegmentation Image Geometry Transforms Functions Geometric Transform API Specifics Geometric Transforms and ROIs Pixel Interpolation Resize Error Codes ResizeSqrPixel Resize ResizeBatch Common parameters for nppiResizeBatch functions: Common parameters for nppiResizeBatchAdvanced functions: Remap Error Codes Common parameters for nppiRemap packed pixel functions: Common parameters for nppiRemap planar pixel functions: Rotate Rotate Error Codes Rotate Utility Functions Mirror Mirror Error Codes Common parameters for nppiMirror functions: Affine Transforms Affine Transforms Affine Transform Error Codes Affine Transform Utility Functions Affine Transform Common parameters for nppiWarpAffine packed pixel functions: Common parameters for nppiWarpAffine planar pixel functions: Affine Transform Batch Codes Common parameters for nppiWarpAffineBatch functions: Backwards Affine Transform Common parameters for nppiWarpAffineBack packed pixel functions: Common parameters for nppiWarpAffineBack planar pixel functions: Quad-Based Affine Transform Common parameters for nppiWarpAffineQuad packed pixel functions: Common parameters for nppiWarpAffineQuad planar pixel functions: Perspective Transforms Perspective Transform Perspective Transform Error Codes Perspective Transform Common parameters for nppiWarpPerspective packed pixel functions: Common parameters for nppiWarpPerspective planar pixel functions: Perspective Transform Batch Common parameters for nppiWarpPerspectiveBatch functions: Backwards Perspective Transform Common parameters for nppiWarpPerspectiveBack packed pixel functions: Common parameters for nppiWarpPerspectiveBack planar pixel functions: Quad-Based Perspective Transform Common parameters for nppiWarpPerspectiveQuad packed pixel functions: Common parameters for nppiWarpPerspectiveQuad planar pixel functions: Image Linear Transforms Functions Fourier Transforms Image Morphological Operations Dilation Functions Image Dilate Dilation Image Dilate Border Dilation with border control Image Dilate 3x3 Dilate3x3 Image Dilate 3x3 Border Dilate3x3Border Erosion Functions Image Erode Erode Image Erode Border Erosion with border control Image Erode 3x3 Erode3x3 Image Erode 3x3 Border Erode3x3Border Image Complex Morphphological Operations Image Morph ComplexImageMorphology Image Morph Get Buffer Size MorphGetBufferSize Image Morph Close Border MorphCloseBorder Image Morph Open Border MorphOpenBorder Image Morph Top Hat Border MorphToHatBorder Image Morph Black Hat Border MorphBlackHatBorder Image Morph Gradient Border MorphGradientBorder Image Statistics Functions CommonGetBufferHostSizeParameters Image Sum Sum Image Min Min Image Min Index MinIndx Image Max Max Common parameters for nppiMax functions include: Image Max Index MaxIndx Image MinMax MinMax Image Mean Mean Image Mean StdDev Mean_StdDev Image Norms Image Norms Common parameters for nppiNorm functions include: Image Norm Inf Norm_Inf Image Norm L1 Norm_L1 Image Norm L2 Norm_L2 Image NormDiff Inf NormDiff_Inf Image NormDiff L1 NormDiff_L1 Image NormDiff L2 NormDiff_L2 Image NormRel Inf NormRel_Inf Image NormRel L1 NormRel_L1 Image NormRel L2 NormRel_L2 Image DotProd DotProd Image Count In Range CountInRange. Image MaxEvery MaxEvery Image MinEvery MinEvery Image Integral Integral Image Square Integral SqrIntegral Image RectStdDev RectStdDev Image Histogram Even HistogramEven Image Histogram Range HistogramRange Image Proximity Image Proximity General Introduction Categorizations Image Square Distance Full Norm SqrDistanceFull_Norm Image Square Distance Same Norm SqrDistanceSame_Norm Image Square Distance Valid Norm SqrDistanceValid_Norm Image Cross Correlation Full Norm CrossCorrFull_Norm Image Cross Correlation Same Norm CrossCorrSame_Norm Image Cross Correlation Valid Norm CrossCorrValid_Norm Image Cross Correlation Valid CrossCorrValid Image Cross Correlation Full Norm Level CrossCorrFull_NormLevel Image Cross Correlation Same Norm Level CrossCorrSame_NormLevel Image Cross Correlation Valid Norm Level CrossCorrValid_NormLevel Image Cross Correlation Full Norm Level Advanced CrossCorrFull_NormLevelAdvanced Image Cross Correlation Same Norm Level Advanced CrossCorrSame_NormLevelAdvanced Image Cross Correlation Valid Norm Level Advanced CrossCorrValid_NormLevelAdvanced Image Quality Index Image Quality Index Image Maximum Error MaximumError Common parameters for nppiMaximumError functions include: Image Average Error AverageError Image Maximum Relative Error MaximumRelativeError Image Average Relative Error AverageRelativeError Image Quality Assessment IQA IQA Image Batch Quality Assessment IQABatch Image Advanced Batch Quality Assessment IQABatchAdvanced Image Threshold And Compare Operations Image Threshold Operations Threshold Operations Common parameters for nppiThreshold non-inplace and inplace functions: Image Threshold Greater Than Operations Threshold Greater Than Operations Image Threshold Less Than Operations Threshold Less Than Operations Image Threshold Value Operations Threshold Value Operations Image Threshold Greater Than Value Operations Threshold Greater Than Value Operations Image Threshold Less Than Value Operations Threshold Less Than Value Operations Image Fused AbsDiff Threshold Greater Than Value Operations Fused AbsDiff Threshold Greater Than Value Operations Image Threshold Less Than Value Greater Than Value Operations Threshold Less Than Value Or Greater Than Value Operations Image Comparison Operations Comparison Operations Compare Images Operations Compare Images Operations Compare Image With Constant Operations Compare Image With Constant Operations Compare Image Differences With Epsilon Operations Compare Image Differences With Epsilon Operations Compare Image Difference To Constant With Epsilon Operations Compare Image Difference With Constant Within Epsilon Operations Image Memory Management Functions Signal Arithmetic And Logical Operations Signal Arithmetic Functions Arithmetic Operations Signal AddC AddC Signal AddProductC AddProductC Signal MulC MulC Signal SubC SubC Signal SubCRev SubCRev Signal DivC DivC Signal DivCRev DivCRev Signal Add Add Signal AddProduct AddProduct Signal Mul Mul Signal Sub Sub Signal Div Div Signal Div Round Div_Round Signal Abs Abs Signal Square Sqr Signal Square Root Sqrt Signal Cube Root Cubrt Signal Exp Exp Signal Ln Ln Signal 10Log10 10Log10 Signal SumLn SumLn Signal ArcTan Arctan Signal Normalize Normalize Signal Cauchy, CouchyD, And CouchyDD2 Cauchy, CauchyD, and CauchyDD2 Logical And Shift Operations Logical And Shift Operations Signal AndC AndC Signal And And Signal OrC OrC Signal Or Or Signal XorC XorC Signal Xor Xor Signal Not Not Signal LShiftC LShiftC Signal RShiftC RShiftC Signal Conversion Functions Signal Convert Convert Signal Threshold Threshold Signal Filtering Functions Integral Signal Initialization Functions Signal Set Set Signal Zero Zero Signal Copy Copy Signal Statistical Functions Signal Min Every Or Max Every MinEvery And MaxEvery Functions Signal Sum Sum Signal Maximum Maximum Signal Minimum Minimum Signal Mean Mean Signal StdDev Standard Deviation Signal Mean And StdDev Mean And Standard Deviation Signal MinMax Minimum Maximum Signal Norms Signal Norm Inf Infinity Norm Signal Norm L1 L1 Norm Signal Norm L2 L2 Norm Signal Norm Inf NormDiff Infinity Norm Diff Signal Norm L1 NormDiff L1 Norm Diff Signal Norm L2 NormDiff L2 Norm Diff Signal Dot Product Dot Product Signal Count In Range Count In Range Signal Count Zero Crossings Count Zero Crossings Signal Maximum Error MaximumError Signal Average Error AverageError Signal Maximum Relative Error MaximumRelativeError Signal Average Relative Error AverageRelativeError Signal Memory Management Functions Malloc Free Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cufft/index.html", "parent_url": "https://docs.nvidia.com/cuda/cufft/index.html", "content_type": "text/html", "text": "cuFFT 1. Introduction 2. Using the cuFFT API 2.1. Accessing cuFFT 2.2. Fourier Transform Setup 2.2.1. Free Memory Requirement 2.2.2. Plan Initialization Time 2.3. Fourier Transform Types 2.3.1. Half-precision cuFFT Transforms 2.3.2. Bfloat16-precision cuFFT Transforms 2.4. Data Layout 2.5. Multidimensional Transforms 2.6. Advanced Data Layout 2.7. Streamed cuFFT Transforms 2.8. Multiple GPU cuFFT Transforms 2.8.1. Plan Specification and Work Areas 2.8.2. Helper Functions 2.8.3. Multiple GPU 2D and 3D Transforms on Permuted Input 2.8.4. Supported Functionality 2.9. cuFFT Callback Routines 2.9.1. Overview of the cuFFT Callback Routine Feature 2.9.2. Specifying Load and Store Callback Routines 2.9.3. Callback Routine Function Details 2.9.4. Coding Considerations for the cuFFT Callback Routine Feature 2.9.4.1. No Ordering Guarantees Within a Kernel 2.10. Thread Safety 2.11. CUDA Graphs Support 2.12. Static Library and Callback Support 2.12.1. Static library without callback support 2.13. Accuracy and Performance 2.14. Caller Allocated Work Area Support 2.15. cuFFT Link-Time Optimized Kernels 2.15.1. Overview of the cuFFT Callback Routine Feature 3. cuFFT API Reference 3.1. Return value cufftResult 3.2. cuFFT Basic Plans 3.2.1. cufftPlan1d() 3.2.2. cufftPlan2d() 3.2.3. cufftPlan3d() 3.2.4. cufftPlanMany() 3.3. cuFFT Extensible Plans 3.3.1. cufftCreate() 3.3.2. cufftDestroy() 3.3.3. cufftMakePlan1d() 3.3.4. cufftMakePlan2d() 3.3.5. cufftMakePlan3d() 3.3.6. cufftMakePlanMany() 3.3.7. cufftMakePlanMany64() 3.3.8. cufftXtMakePlanMany() 3.4. cuFFT Plan Properties 3.4.1. cufftSetPlanPropertyInt64() 3.4.2. cufftGetPlanPropertyInt64() 3.4.3. cufftResetPlanProperty() 3.5. cuFFT Estimated Size of Work Area 3.5.1. cufftEstimate1d() 3.5.2. cufftEstimate2d() 3.5.3. cufftEstimate3d() 3.5.4. cufftEstimateMany() 3.6. cuFFT Refined Estimated Size of Work Area 3.6.1. cufftGetSize1d() 3.6.2. cufftGetSize2d() 3.6.3. cufftGetSize3d() 3.6.4. cufftGetSizeMany() 3.6.5. cufftGetSizeMany64() 3.6.6. cufftXtGetSizeMany() 3.7. cufftGetSize() 3.8. cuFFT Caller Allocated Work Area Support 3.8.1. cufftSetAutoAllocation() 3.8.2. cufftSetWorkArea() 3.8.3. cufftXtSetWorkAreaPolicy() 3.9. cuFFT Execution 3.9.1. cufftExecC2C() and cufftExecZ2Z() 3.9.2. cufftExecR2C() and cufftExecD2Z() 3.9.3. cufftExecC2R() and cufftExecZ2D() 3.9.4. cufftXtExec() 3.9.5. cufftXtExecDescriptor() 3.10. cuFFT and Multiple GPUs 3.10.1. cufftXtSetGPUs() 3.10.2. cufftXtSetWorkArea() 3.10.3. cuFFT Multiple GPU Execution 3.10.3.1. cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z() 3.10.3.2. cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z() 3.10.3.3. cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D() 3.10.4. Memory Allocation and Data Movement Functions 3.10.4.1. cufftXtMalloc() 3.10.4.1.1. Parameter cufftXtSubFormat 3.10.4.2. cufftXtFree() 3.10.4.3. cufftXtMemcpy() 3.10.4.3.1. Parameter cufftXtCopyType 3.10.5. General Multiple GPU Descriptor Types 3.10.5.1. cudaXtDesc 3.10.5.2. cudaLibXtDesc 3.11. cuFFT Callbacks 3.11.1. cufftXtSetCallback() 3.11.2. cufftXtClearCallback() 3.11.3. cufftXtSetCallbackSharedSize() 3.12. cufftSetStream() 3.13. cufftGetVersion() 3.14. cufftGetProperty() 3.15. cuFFT Types 3.15.1. Parameter cufftType 3.15.2. Parameters for Transform Direction 3.15.3. Type definitions for callbacks 3.15.4. Other cuFFT Types 3.15.4.1. cufftHandle 3.15.4.2. cufftReal 3.15.4.3. cufftDoubleReal 3.15.4.4. cufftComplex 3.15.4.5. cufftDoubleComplex 3.16. Common types 3.16.1. cudaDataType 3.16.2. libraryPropertyType 4. cuFFT Code Examples 5. Multiple GPU Data Organization 5.1. Multiple GPU Data Organization for Batched Transforms 5.2. Multiple GPU Data Organization for Single 2D and 3D Transforms 5.3. Multiple-GPU Data Organization for Single 1D Transforms 6. FFTW Conversion Guide 7. FFTW Interface to cuFFT 8. Deprecated Functionality 9. Notices 9.1. Notice 9.2. OpenCL 9.3. Trademarks cuFFT » 1. Introduction v12.5 | PDF | Archive cuFFT API Reference The API reference guide for cuFFT, the CUDA Fast Fourier Transform library. 1. Introduction  This document describes cuFFT, the NVIDIA® CUDA® Fast Fourier Transform (FFT) product. It consists of two separate libraries: cuFFT and cuFFTW. The cuFFT library is designed to provide high performance on NVIDIA GPUs. The cuFFTW library is provided as a porting tool to enable users of FFTW to start using NVIDIA GPUs with a minimum amount of effort. The FFT is a divide-and-conquer algorithm for efficiently computing discrete Fourier transforms of complex or real-valued data sets. It is one of the most important and widely used numerical algorithms in computational physics and general signal processing. The cuFFT library provides a simple interface for computing FFTs on an NVIDIA GPU, which allows users to quickly leverage the floating-point power and parallelism of the GPU in a highly optimized and tested FFT library. The cuFFT product supports a wide range of FFT inputs and options efficiently on NVIDIA GPUs. This version of the cuFFT library supports the following features: Algorithms highly optimized for input sizes that can be written in the form \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) . In general the smaller the prime factor, the better the performance, i.e., powers of two are fastest. An \\(O\\left( n\\log n \\right)\\) algorithm for every input data size Half-precision (16-bit floating point), single-precision (32-bit floating point) and double-precision (64-bit floating point). Transforms of lower precision have higher performance. Complex and real-valued input and output. Real valued input or output require less computations and data than complex values and often have faster time to solution. Types supported are: C2C - Complex input to complex output R2C - Real input to complex output C2R - Symmetric complex input to real output 1D, 2D and 3D transforms Execution of multiple 1D, 2D and 3D transforms simultaneously. These batched transforms have higher performance than single transforms. In-place and out-of-place transforms Arbitrary intra- and inter-dimension element strides (strided layout) FFTW compatible data layout Execution of transforms across multiple GPUs Streamed execution, enabling asynchronous computation and data movement The cuFFTW library provides the FFTW3 API to facilitate porting of existing FFTW applications. Please note that starting from CUDA 11.0, the minimum supported GPU architecture is SM35. See Deprecated Functionality . 2. Using the cuFFT API  This chapter provides a general overview of the cuFFT library API. For more complete information on specific functions, see cuFFT API Reference . Users are encouraged to read this chapter before continuing with more detailed descriptions. The Discrete Fourier transform (DFT) maps a complex-valued vector \\(x_{k}\\) ( time domain ) into its frequency domain representation given by: \\(X_{k} = \\sum\\limits_{n = 0}^{N - 1}x_{n}e^{-2\\pi i\\frac{kn}{N}}\\) where \\(X_{k}\\) is a complex-valued vector of the same size. This is known as a forward DFT. If the sign on the exponent of e is changed to be positive, the transform is an inverse transform. Depending on \\(N\\) , different algorithms are deployed for the best performance. The cuFFT API is modeled after FFTW , which is one of the most popular and efficient CPU-based FFT libraries. cuFFT provides a simple configuration mechanism called a plan that uses internal building blocks to optimize the transform for the given configuration and the particular GPU hardware selected. Then, when the execution function is called, the actual transform takes place following the plan of execution. The advantage of this approach is that once the user creates a plan, the library retains whatever state is needed to execute the plan multiple times without recalculation of the configuration. This model works well for cuFFT because different kinds of FFTs require different thread configurations and GPU resources, and the plan interface provides a simple way of reusing configurations. Computing a number BATCH of one-dimensional DFTs of size NX using cuFFT will typically look like this: #define NX 256 #define BATCH 10 #define RANK 1 ... { cufftHandle plan ; cufftComplex * data ; ... cudaMalloc (( void ** ) & data , sizeof ( cufftComplex ) * NX * BATCH ); cufftPlanMany ( & plan , RANK , NX , & iembed , istride , idist , & oembed , ostride , odist , CUFFT_C2C , BATCH ); ... cufftExecC2C ( plan , data , data , CUFFT_FORWARD ); cudaDeviceSynchronize (); ... cufftDestroy ( plan ); cudaFree ( data ); } 2.1. Accessing cuFFT  The cuFFT and cuFFTW libraries are available as shared libraries. They consist of compiled programs ready for users to incorporate into applications with the compiler and linker. cuFFT can be downloaded from https://developer.nvidia.com/cufft . By selecting Download CUDA Production Release users are all able to install the package containing the CUDA Toolkit, SDK code samples and development drivers. The CUDA Toolkit contains cuFFT and the samples include simplecuFFT . The Linux release for simplecuFFT assumes that the root install directory is /usr/local/cuda and that the locations of the products are contained there as follows. Modify the Makefile as appropriate for your system. Product Location and name Include file nvcc compiler /bin/nvcc cuFFT library {lib, lib64}/libcufft.so inc/cufft.h cuFFT library with Xt functionality {lib, lib64}/libcufft.so inc/cufftXt.h cuFFTW library {lib, lib64}/libcufftw.so inc/cufftw.h The most common case is for developers to modify an existing CUDA routine (for example, filename.cu ) to call cuFFT routines. In this case the include file cufft.h or cufftXt.h should be inserted into filename.cu file and the library included in the link line. A single compile and link line might appear as /usr/local/cuda/bin/nvcc [options] filename.cu … -I/usr/local/cuda/inc -L/usr/local/cuda/lib -lcufft Of course there will typically be many compile lines and the compiler g++ may be used for linking so long as the library path is set correctly. Users of the FFTW interface (see FFTW Interface to cuFFT ) should include cufftw.h and link with both cuFFT and cuFFTW libraries. Functions in the cuFFT and cuFFTW library assume that the data is in GPU visible memory. This means any memory allocated by cudaMalloc , cudaMallocHost and cudaMallocManaged or registered with cudaHostRegister can be used as input, output or plan work area with cuFFT and cuFFTW functions. For the best performance input data, output data and plan work area should reside in device memory. cuFFTW library also supports input data and output data that is not GPU visible. 2.2. Fourier Transform Setup  The first step in using the cuFFT Library is to create a plan using one of the following: cufftPlan1D() / cufftPlan2D() / cufftPlan3D() - Create a simple plan for a 1D/2D/3D transform respectively. cufftPlanMany() - Creates a plan supporting batched input and strided data layouts. cufftXtMakePlanMany() - Creates a plan supporting batched input and strided data layouts for any supported precision. Among the plan creation functions, cufftPlanMany() allows use of more complicated data layouts and batched executions. Execution of a transform of a particular size and type may take several stages of processing. When a plan for the transform is generated, cuFFT derives the internal steps that need to be taken. These steps may include multiple kernel launches, memory copies, and so on. In addition, all the intermediate buffer allocations (on CPU/GPU memory) take place during planning. These buffers are released when the plan is destroyed. In the worst case, the cuFFT Library allocates space for 8*batch*n[0]*..*n[rank-1] cufftComplex or cufftDoubleComplex elements (where batch denotes the number of transforms that will be executed in parallel, rank is the number of dimensions of the input data (see Multidimensional Transforms ) and n[] is the array of transform dimensions) for single and double-precision transforms respectively. Depending on the configuration of the plan, less memory may be used. In some specific cases, the temporary space allocations can be as low as 1*batch*n[0]*..*n[rank-1] cufftComplex or cufftDoubleComplex elements. This temporary space is allocated separately for each individual plan when it is created (i.e., temporary space is not shared between the plans). The next step in using the library is to call an execution function such as cufftExecC2C() (see Parameter cufftType ) which will perform the transform with the specifications defined at planning. One can create a cuFFT plan and perform multiple transforms on different data sets by providing different input and output pointers. Once the plan is no longer needed, the cufftDestroy() function should be called to release the resources allocated for the plan. 2.2.1. Free Memory Requirement  The first program call to any cuFFT function causes the initialization of the cuFFT kernels. This can fail if there is not enough free memory on the GPU. It is advisable to initialize cufft first (e.g. by creating a plan) and then allocating memory. 2.2.2. Plan Initialization Time  During plan initialization, cuFFT conducts a series of steps, including heuristics to determine which kernels to be used as well as kernel module loads. Starting from CUDA 12.0, cuFFT delivers a larger portion of kernels using the CUDA Parallel Thread eXecution assembly form (PTX code), instead of the binary form (cubin object). The PTX code of cuFFT kernels are loaded and compiled further to the binary code by the CUDA device driver at runtime when a cuFFT plan is initialized. This is called just-in-time (JIT) compilation . JIT compilation slightly increases cuFFT plan initialization time, depending on the transform size and the speed of the host CPU (see Module load driver API ) . But the JIT overhead occurs only when a binary code is generated for the first time during plan initialization using one of the plan creation functions . The device driver automatically caches a copy of the generated binary code to avoid repeating the compilation in subsequent invocations. If necessary, CUDA_CACHE_PATH or CUDA_CACHE_MAXSIZE can be customized to set the cache folder and max size (see detail in CUDA Environmental Variables ), but the default settings are fine in general. 2.3. Fourier Transform Types  Apart from the general complex-to-complex (C2C) transform, cuFFT implements efficiently two other types: real-to-complex (R2C) and complex-to-real (C2R). In many practical applications the input vector is real-valued. It can be easily shown that in this case the output satisfies Hermitian symmetry ( \\(X_{k} = X_{N - k}^{\\ast}\\) , where the star denotes complex conjugation). The converse is also true: for complex-Hermitian input the inverse transform will be purely real-valued. cuFFT takes advantage of this redundancy and works only on the first half of the Hermitian vector. Transform execution functions for single and double-precision are defined separately as: cufftExecC2C() / cufftExecZ2Z() - complex-to-complex transforms for single/double precision. cufftExecR2C() / cufftExecD2Z() - real-to-complex forward transform for single/double precision. cufftExecC2R() / cufftExecZ2D() - complex-to-real inverse transform for single/double precision. Each of those functions demands different input data layout (see Data Layout for details). Note Complex-to-real (C2R) transforms accept complex-Hermitian input. For one-dimensional signals, this requires the 0th element (and the \\(\\frac{N}{2}\\) th input if N is even) to be real-valued, i.e. its imaginary part should be zero.\nFor d-dimension signals, this means \\(x_{(n_{1},n_{2},\\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\\ldots,N_{d} - n_{d})}^{\\ast}\\) .\nOtherwise, the behavior of the transform is undefined. Also see Multidimensional Transforms . Functions cufftXtExec() and cufftXtExecDescriptor() can perform transforms on any of the supported types. 2.3.1. Half-precision cuFFT Transforms  Half-precision transforms have the following limitations: Minimum GPU architecture is SM_53 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details. The CUDA Toolkit provides the cuda_fp16.h header with types and intrinsic functions for handling half-precision arithmetic. 2.3.2. Bfloat16-precision cuFFT Transforms  cuFFT supports bfloat16 precision using the nv_bfloat16 data type. Please note that cuFFT utilizes a combination of single- and bfloat16-precision arithmetic operations when computing the FFT in bfloat16 precision. Bfloat16-precision transforms have similar limitations to half-precision transforms: Minimum GPU architecture is SM_80 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details. The CUDA Toolkit provides the cuda_bf16.h header with types and intrinsic functions for handling bfloat16-precision arithmetic. 2.4. Data Layout  In the cuFFT Library, data layout depends strictly on the configuration and the transform type. In the case of general complex-to-complex transform both the input and output data shall be a cufftComplex / cufftDoubleComplex array in single- and double-precision modes respectively. In C2R mode an input array \\((x_{1},x_{2},\\ldots,x_{\\lfloor\\frac{N}{2}\\rfloor + 1})\\) of only non-redundant complex elements is required. The output array \\((X_{1},X_{2},\\ldots,X_{N})\\) consists of cufftReal / cufftDouble elements in this mode. Finally, R2C demands an input array \\((X_{1},X_{2},\\ldots,X_{N})\\) of real values and returns an array \\((x_{1},x_{2},\\ldots,x_{\\lfloor\\frac{N}{2}\\rfloor + 1})\\) of non-redundant complex elements. In real-to-complex and complex-to-real transforms the size of input data and the size of output data differ. For out-of-place transforms a separate array of appropriate size is created. For in-place transforms the user should use padded data layout. This layout is FFTW compatibile. In the padded layout output signals begin at the same memory addresses as the input data. Therefore input data for real-to-complex and output data for complex-to-real must be padded. Expected sizes of input/output data for 1-d transforms are summarized in the table below: FFT type input data size output data size C2C \\(x\\) cufftComplex \\(x\\) cufftComplex C2R \\(\\left\\lfloor \\frac{x}{2} \\right\\rfloor + 1\\) cufftComplex \\(x\\) cufftReal R2C* \\(x\\) cufftReal \\(\\left\\lfloor \\frac{x}{2} \\right\\rfloor + 1\\) cufftComplex The real-to-complex transform is implicitly a forward transform. For an in-place real-to-complex transform where FFTW compatible output is desired, the input size must be padded to \\(\\left( {\\lfloor\\frac{N}{2}\\rfloor + 1} \\right)\\) complex elements. For out-of-place transforms, input and output sizes match the logical transform size \\(N\\) and the non-redundant size \\(\\lfloor\\frac{N}{2}\\rfloor + 1\\) , respectively. The complex-to-real transform is implicitly inverse. For in-place complex-to-real FFTs where FFTW compatible output is selected (default padding mode), the input size is assumed to be \\(\\lfloor\\frac{N}{2}\\rfloor + 1\\) cufftComplex elements. Note that in-place complex-to-real FFTs may overwrite arbitrary imaginary input point values when non-unit input and output strides are chosen. Out-of-place complex-to-real FFT will always overwrite input buffer. For out-of-place transforms, input and output sizes match the logical transform non-redundant size \\(\\lfloor\\frac{N}{2}\\rfloor + 1\\) and size \\(N\\) , respectively. 2.5. Multidimensional Transforms  Multidimensional DFT map a \\(d\\) -dimensional array \\(x_{\\mathbf{n}}\\) , where \\(\\mathbf{n} = (n_{1},n_{2},\\ldots,n_{d})\\) into its frequency domain array given by: \\(X_{\\mathbf{k}} = \\sum\\limits_{n = 0}^{N - 1}x_{\\mathbf{n}}e^{-2\\pi i\\frac{\\mathbf{k}\\mathbf{n}}{\\mathbf{N}}}\\) where \\(\\frac{\\mathbf{n}}{\\mathbf{N}} = (\\frac{n_{1}}{N_{1}},\\frac{n_{2}}{N_{2}},\\ldots,\\frac{n_{d}}{N_{d}})\\) , and the summation denotes the set of nested summations \\(\\sum\\limits_{n_{1} = 0}^{N_{1} - 1}\\sum\\limits_{n_{2} = 0}^{N_{2} - 1}\\ldots\\sum\\limits_{n_{d} = 0}^{N_{d} - 1}\\) cuFFT supports one-dimensional, two-dimensional and three-dimensional transforms, which can all be called by the same cufftExec* functions (see Fourier Transform Types ). Similar to the one-dimensional case, the frequency domain representation of real-valued input data satisfies Hermitian symmetry, defined as: \\(x_{(n_{1},n_{2},\\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\\ldots,N_{d} - n_{d})}^{\\ast}\\) . C2R and R2C algorithms take advantage of this fact by operating only on half of the elements of signal array, namely on: \\(x_{\\mathbf{n}}\\) for \\(\\mathbf{n} \\in \\{ 1,\\ldots,N_{1}\\} \\times \\ldots \\times \\{ 1,\\ldots,N_{d - 1}\\} \\times \\{ 1,\\ldots,\\lfloor\\frac{N_{d}}{2}\\rfloor + 1\\}\\) . The general rules of data alignment described in Data Layout apply to higher-dimensional transforms. The following table summarizes input and output data sizes for multidimensional DFTs: Dims FFT type Input data size Output data size 1D C2C \\(\\mathbf{N}_{1}\\) cufftComplex \\(\\mathbf{N}_{1}\\) cufftComplex 1D C2R \\(\\lfloor\\frac{\\mathbf{N}_{1}}{2}\\rfloor + 1\\) cufftComplex \\(\\mathbf{N}_{1}\\) cufftReal 1D R2C \\(\\mathbf{N}_{1}\\) cufftReal \\(\\lfloor\\frac{\\mathbf{N}_{1}}{2}\\rfloor + 1\\) cufftComplex 2D C2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftComplex 2D C2R \\(\\mathbf{N}_{1}(\\lfloor\\frac{\\mathbf{N}_{2}}{2}\\rfloor + 1)\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftReal 2D R2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\) cufftReal \\(\\mathbf{N}_{1}(\\lfloor\\frac{\\mathbf{N}_{2}}{2}\\rfloor + 1)\\) cufftComplex 3D C2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftComplex 3D C2R \\(\\mathbf{N}_{1}\\mathbf{N}_{2}(\\lfloor\\frac{\\mathbf{N}_{3}}{2}\\rfloor + 1)\\) cufftComplex \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftReal 3D R2C \\(\\mathbf{N}_{1}\\mathbf{N}_{2}\\mathbf{N}_{3}\\) cufftReal \\(\\mathbf{N}_{1}\\mathbf{N}_{2}(\\lfloor\\frac{\\mathbf{N}_{3}}{2}\\rfloor + 1)\\) cufftComplex For example, static declaration of a three-dimensional array for the output of an out-of-place real-to-complex transform will look like this: cufftComplex odata [ N1 ][ N2 ][ N3 / 2 + 1 ]; 2.6. Advanced Data Layout  The advanced data layout feature allows transforming only a subset of an input array, or outputting to only a portion of a larger data structure. It can be set by calling function: cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ); Passing inembed or onembed set to NULL is a special case and is equivalent to passing n for each. This is same as the basic data layout and other advanced parameters such as istride are ignored. If the advanced parameters are to be used, then all of the advanced interface parameters must be specified correctly. Advanced parameters are defined in units of the relevant data type ( cufftReal , cufftDoubleReal , cufftComplex , or cufftDoubleComplex ). Advanced layout can be perceived as an additional layer of abstraction above the access to input/output data arrays. An element of coordinates [z][y][x] in signal number b in the batch will be associated with the following addresses in the memory: 1D input[ b * idist + x * istride ] output[ b * odist + x * ostride ] 2D input[ b * idist` + (x * inembed[1] + y) * istride ] output[ b * odist + (x * onembed[1] + y) * ostride ] 3D input[ b * idist + ((x * inembed[1] + y) * inembed[2] + z) * istride ] output[ b * odist + ((x * onembed[1] + y) * onembed[2] + z) * ostride ] The istride and ostride parameters denote the distance between two successive input and output elements in the least significant (that is, the innermost) dimension respectively. In a single 1D transform, if every input element is to be used in the transform, istride should be set to \\(1\\) ; if every other input element is to be used in the transform, then istride should be set to \\(2\\) . Similarly, in a single 1D transform, if it is desired to output final elements one after another compactly, ostride should be set to \\(1\\) ; if spacing is desired between the least significant dimension output data, ostride should be set to the distance between the elements. The inembed and onembed parameters define the number of elements in each dimension in the input array and the output array respectively. The inembed[rank-1] contains the number of elements in the least significant (innermost) dimension of the input data excluding the istride elements; the number of total elements in the least significant dimension of the input array is then istride*inembed[rank-1] . The inembed[0] or onembed[0] corresponds to the most significant (that is, the outermost) dimension and is effectively ignored since the idist or odist parameter provides this information instead. Note that the size of each dimension of the transform should be less than or equal to the inembed and onembed values for the corresponding dimension, that is n[i] ≤ inembed[i] , n[i] ≤ onembed[i] , where \\(i \\in \\{ 0,\\ldots,rank - 1\\}\\) . The idist and odist parameters indicate the distance between the first element of two consecutive batches in the input and output data. 2.7. Streamed cuFFT Transforms  Every cuFFT plan may be associated with a CUDA stream. Once so associated, all launches of the internal stages of that plan take place through the specified stream. Streaming of cuFFT execution allows for potential overlap between transforms and memory copies. (See the NVIDIA CUDA Programming Guide for more information on streams.) If no stream is associated with a plan, launches take place in stream(0) , the default CUDA stream. Note that many plan executions require multiple kernel launches. cuFFT uses private streams internally to sort operations, including event syncrhonization. cuFFT does not guarantee ordering of internal operations, and the order is only preserved with respect to the streams set by the user. As of CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported in multiple GPU cases. However, calls to cufftXtMemcpy() are still synchronous across multiple GPUs when using streams. In previous versions of cuFFT, cufftSetStream() returns an error in the multiple GPU case. Likewise, calling certain multi-GPU functions such as cufftXtSetCallback() after setting a stream with cufftSetStream() will result in an error (see API functions for more details). Please note that in order to overlap plans using single plan handle user needs to manage work area buffers. Each concurrent plan execution needs it’s exclusive work area. Work area can be set by cufftSetWorkArea function. 2.8. Multiple GPU cuFFT Transforms  cuFFT supports using up to sixteen GPUs connected to a CPU to perform Fourier Transforms whose calculations are distributed across the GPUs. An API has been defined to allow users to write new code or modify existing code to use this functionality. Some existing functions such as the creation of a plan using cufftCreate() also apply in the multiple GPU case. Multiple GPU routines contain Xt in their name. The memory on the GPUs is managed by helper functions cufftXtMalloc()/cufftXtFree() and cufftXtMemcpy() using the cudaLibXtDesc descriptor. Performance is a function of the bandwidth between the GPUs, the computational ability of the individual GPUs, and the type and number of FFT to be performed. The highest performance is obtained using NVLink interconnect ( https://www.nvidia.com/object/nvlink.html ). The second best option is using PCI Express 3.0 between the GPUs and ensuring that both GPUs are on the same switch. Note that multiple GPU execution is not guaranteed to solve a given size problem in a shorter time than single GPU execution. The multiple GPU extensions to cuFFT are built on the extensible cuFFT API. The general steps in defining and executing a transform with this API are: cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used Optional: cufftEstimate{1d,2d,3d,Many}() - estimate the sizes of the work areas required. These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. cufftMakePlan{1d,2d,3d,Many}() - create the plan. These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. Optional: cufftGetSize{1d,2d,3d,Many}() - refined estimate of the sizes of the work areas required. These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. Optional: cufftGetSize() - check workspace size. This is the same function used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used. Optional: cufftXtSetWorkArea() - do your own workspace allocation. cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - execute the plan cufftXtMemcpy() - copy data from the GPUs cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2.8.1. Plan Specification and Work Areas  In the single GPU case a plan is created by a call to cufftCreate() followed by a call to cufftMakePlan*() . For multiple GPUs, the GPUs to use for execution are identified by a call to cufftXtSetGPUs() and this must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() . Note that when cufftMakePlan*() is called for a single GPU, the work area is on that GPU. In a multiple GPU plan, the returned work area has multiple entries; one value per GPU. That is workSize points to a size_t array, one entry per GPU. Also the strides and batches apply to the entire plan across all GPUs associated with the plan. Once a plan is locked by a call to cufftMakePlan*() , different descriptors may be specified in calls to cufftXtExecDescriptor*() to execute the plan on different data sets, but the new descriptors must use the same GPUs in the same order. As in the single GPU case, cufftEstimateSize{Many,1d,2d,3d}() and cufftGetSize{Many,1d,2d,3d}() give estimates of the work area sizes required for a multiple GPU plan and in this case workSize points to a size_t array, one entry per GPU. Similarly the actual work size returned by cufftGetSize() is a size_t array, one entry per GPU in the multiple GPU case. 2.8.2. Helper Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution. cuFFT provides functions to assist users in manipulating data on multiple GPUs. These must be called after the call to cufftMakePlan*() . On a single GPU users may call cudaMalloc() and cudaFree() to allocate and free GPU memory. To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMalloc() and cufftXtFree() functions. The function cufftXtMalloc() returns a descriptor which specifies the location of these memories. On a single GPU users may call cudaMemcpy() to transfer data between host and GPU memory. To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMemcpy() which allows users to copy between host and multiple GPU memories or even between the GPU memories. All single GPU cuFFT FFTs return output the data in natural order, that is the ordering of the result is the same as if a DFT had been performed on the data. Some Fast Fourier Transforms produce intermediate results where the data is left in a permutation of the natural output. When batch is one, data is left in the GPU memory in a permutation of the natural output. When cufftXtMemcpy() is used to copy data from GPU memory back to host memory, the results are in natural order regardless of whether the data on the GPUs is in natural order or permuted. Using CUFFT_COPY_DEVICE_TO_DEVICE allows users to copy data from the permuted data format produced after a single transform to the natural order on GPUs. 2.8.3. Multiple GPU 2D and 3D Transforms on Permuted Input  For single 2D or 3D transforms on multiple GPUs, when cufftXtMemcpy() distributes the data to the GPUs, the array is divided on the X axis. E.G. for two GPUs half of the X dimenson points, for all Y (and Z) values, are copied to each of the GPUs. When the transform is computed, the data are permuted such that they are divided on the Y axis. I.E. half of the Y dimension points, for all X (and Z) values are on each of the GPUs. When cuFFT creates a 2D or 3D plan for a single transform on multiple GPUs, it actually creates two plans. One plan expects input to be divided on the X axis. The other plan expects data to be divided on the Y axis. This is done because many algorithms compute a forward FFT, then perform some point-wise operation on the result, and then compute the inverse FFT. A memory copy to restore the data to the original order would be expensive. To avoid this, cufftXtMemcpy and cufftXtExecDescriptor() keep track of the data ordering so that the correct operation is used. The ability of cuFFT to process data in either order makes the following sequence possible. cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used cufftMakePlan{1d,2d,3d,Many}() - create the plan. cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the forward FFT userFunction() - modify the data in the frequency domain cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the inverse FFT Note that it was not necessary to copy/permute the data between execute calls cufftXtMemcpy() - copy data to the host cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2.8.4. Supported Functionality  Starting with cuFFT version 7.0, a subset of single GPU functionality is supported for multiple GPU execution. Requirements and limitations: All GPUs must have the same CUDA architecture level and support Unified Virtual Address Space. On Windows, the GPU boards must be operating in Tesla Compute Cluster (TCC) mode. For an application that uses the CUDA Driver API, running cuFFT on multiple GPUs is only compatible with applications using the primary context on each GPU. Strided input and output are not supported. Running cuFFT on more than 8 GPUs (16 GPUs is max) is supported on machines with NVLink only. While transforms with batch count greater than one do not impose additional constraints, those with a single batch have some restrictions. Single-batch FFTs support only in-place mode, and have additional constraints depending on the FFT type. This behavior is summarized in the following table: batch=1 1D 2D 3D C2C / Z2Z 2,4,8,16 GPUs power of 2 sizes only Minimum size for 2-4 GPUs is 64 Minimum size for 8 GPUs is 128 Minimum size for 16 GPUs is 1024 2-16 GPUs One of the following conditions is met for each dimension: Dimension must factor into primes less than or equal to 127 Maximum dimension size is 4096 for single precision Maximum dimension size is 2048 for double precision Minimum size is 32 R2C / D2Z not supported 2-16 GPUs One of the following conditions is met for each dimension: Dimension must factor into primes less than or equal to 127 Maximum dimension size is 4096 for single precision Maximum dimension size is 2048 for double precision Minimum size is 32 Fastest changing dimension size needs to be even Supports only CUFFT_XT_FORMAT_INPLACE input descriptor format No callback support C2R / Z2D not supported 2-16 GPUs One of the following conditions is met for each dimension: Dimension must factor into primes less than or equal to 127 Maximum dimension size is 4096 for single precision Maximum dimension size is 2048 for double precision Minimum size is 32 Fastest changing dimension size needs to be even Supports only CUFFT_XT_FORMAT_INPLACE_SHUFFLED input descriptor format No callback support General guidelines are: Parameter whichGPUs of cufftXtSetGPUs() function determines ordering of the GPUs with respect to data decomposition (first data chunk is placed on GPU denoted by first element of whichGPUs ) The data for the entire transform must fit within the memory of the GPUs assigned to it. For batch size m on n GPUs : The first m % n GPUs execute \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor+\\ 1\\) transforms. The remaining GPUs execute \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor\\) transforms. Batch size output differences: Single GPU cuFFT results are always returned in natural order. When multiple GPUs are used to perform more than one transform, the results are also returned in natural order. When multiple GPUs are used to perform a single transform the results are returned in a permutation of the normal results to reduce communication time. This behavior is summarized in the following table: Number of GPUs Number of transforms Output Order on GPUs One One or multiple transforms Natural order Multiple One Permuted results Multiple Multiple Natural order To produce natural order results in GPU memory for multi-GPU runs in the 1D single transform case, requires calling cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE . 2D and 3D multi-GPU transforms support execution of a transform given permuted order results as input. After execution in this case, the output will be in natural order. It is also possible to use cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE to return 2D or 3D data to natural order. See the cuFFT Code Examples section for single GPU and multiple GPU examples. 2.9. cuFFT Callback Routines  Callback routines are user-supplied kernel routines that cuFFT will call when loading or storing data. They allow the user to do data pre- or post- processing without additional kernel calls. Note Starting from CUDA 11.4, support for callback functionality using separately compiled device code is deprecated on all GPU architectures. Callback functionality will continue to be supported for all GPU architectures. 2.9.1. Overview of the cuFFT Callback Routine Feature  cuFFT provides a set of APIs that allow the cuFFT user to provide CUDA functions that re-direct or manipulate the data as it is loaded prior to processing the FFT, or stored once the FFT has been done. For the load callback, cuFFT passes the callback routine the address of the input data and the offset to the value to be loaded from device memory, and the callback routine returns the value it wishes cuFFT to use instead. For the store callback, cuFFT passes the callback routine the value it has computed, along with the address of the output data and the offset to the value to be written to device memory, and the callback routine modifies the value and stores the modified result. In order to provide a callback to cuFFT, a plan is created and configured normally using the extensible plan APIs. After the call to cufftCreate and cufftMakePlan , the user may associate a load callback routine, or a store callback routine, or both, with the plan, by calling cufftXtSetCallback . The caller also has the option to specify a device pointer to an opaque structure they wish to associate with the plan. This pointer will be passed to the callback routine by the cuFFT library. The caller may use this structure to remember plan dimensions and strides, or have a pointer to auxiliary data, etc. With some restrictions, the callback routine is allowed to request shared memory for its own use. If the requested amount of shared memory is available, cufft will pass a pointer to it when it calls the callback routine. CUFFT allows for 8 types of callback routine, one for each possible combination of: load or store, real or complex, single precision or double. It is the caller’s responsibility to provide a routine that matches the function prototype for the type of routine specified. If there is already a callback of the specified type associated with the plan, the set callback function will replace it with the new one. The callback routine extensions to cuFFT are built on the extensible cuFFT API. The general steps in defining and executing a transform with callbacks are: cufftCreate() - create an empty plan, as in the single GPU case cufftMakePlan{1d,2d,3d,Many}() - create the plan. These are the same functions used in the single GPU case. cufftXtSetCallback() - called for load and/or store callback for this plan cufftExecC2C() etc. - execute the plan cufftDestroy() - free cuFFT plan resources Callback functions are not supported on transforms with a dimension size that does not factor into primes smaller than 127. Callback functions on plans whose dimensions’ prime factors are limited to 2, 3, 5, and 7 can safely call __syncthreads() . On other plans, results are not defined. Note The callback API is available in the statically linked cuFFT library only, and only on 64 bit LINUX operating systems. 2.9.2. Specifying Load and Store Callback Routines  In order to associate a callback routine with a plan, it is necessary to obtain a device pointer to the callback routine. As an example, if the user wants to specify a load callback for an R2C transform, they would write the device code for the callback function, and define a global device variable that contains a pointer to the function: __device__ cufftReal myOwnCallback ( void * dataIn , size_t offset , void * callerInfo , void * sharedPtr ) { cufftReal ret ; // use offset, dataIn, and optionally callerInfo to // compute the return value return ret ; } __device__ cufftCallbackLoadR myOwnCallbackPtr = myOwnCallback ; From the host side, the user then has to get the address of the callback routine, which is stored in myOwnCallbackPtr . This is done with cudaMemcpyFromSymbol , as follows: cufftCallbackLoadR hostCopyOfCallbackPtr ; cudaMemcpyFromSymbol ( & hostCopyOfCallbackPtr , myOwnCallbackPtr , sizeof ( hostCopyOfCallbackPtr )); hostCopyOfCallbackPtr then contains the device address of the callback routine, that should be passed to cufftXtSetCallback . Note that, for multi-GPU transforms, hostCopyOfCallbackPtr will need to be an array of pointers, and the cudaMemcpyFromSymbol will have to be invoked for each GPU. Please note that __managed__ variables are not suitable to pass to cufftSetCallback due to restrictions on variable usage (See the NVIDIA CUDA Programming Guide for more information about __managed__ variables). 2.9.3. Callback Routine Function Details  Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to load data prior to the transform. typedef cufftComplex ( * cufftCallbackLoadC )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleComplex ( * cufftCallbackLoadZ )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftReal ( * cufftCallbackLoadR )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleReal ( * cufftCallbackLoadD )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); Parameters for all of the load callbacks are defined as below: offset : offset of the input element from the start of output data. This is not a byte offset, rather it is the number of elements from start of data. dataIn : device pointer to the start of the input array that was passed in the cufftExecute call. callerInfo : device pointer to the optional caller specified data passed in the cufftXtSetCallback call. sharedPointer : pointer to shared memory, valid only if the user has called cufftXtSetCallbackSharedSize() . Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to store data after completion of the transform. Note that the store callback functions do not return a value. This is because a store callback function is responsible not only for transforming the data as desired, but also for writing the data to the desired location. This allows the store callback to rearrange the data, for example to shift the zero frequency result to the center of the ouput. typedef void ( * cufftCallbackStoreC )( void * dataOut , size_t offset , cufftComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreZ )( void * dataOut , size_t offset , cufftDoubleComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreR )( void * dataOut , size_t offset , cufftReal element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreD )( void * dataOut , size_t offset , cufftDoubleReal element , void * callerInfo , void * sharedPointer ); Parameters for all of the store callbacks are defined as below: offset : offset of the output element from the start of output data. This is not a byte offset, rather it is the number of elements from start of data. dataOut : device pointer to the start of the output array that was passed in the cufftExecute call. element : the real or complex result computed by CUFFT for the element specified by the offset argument. callerInfo : device pointer to the optional caller specified data passed in the cufftXtSetCallback call. sharedPointer : pointer to shared memory, valid only if the user has called cufftXtSetCallbackSharedSize() . 2.9.4. Coding Considerations for the cuFFT Callback Routine Feature  cuFFT supports callbacks on all types of transforms, dimension, batch, stride between elements or number of GPUs. Callbacks are supported for transforms of single and double precision. cuFFT supports a wide range of parameters, and based on those for a given plan, it attempts to optimize performance. The number of kernels launched, and for each of those, the number of blocks launched and the number of threads per block, will vary depending on how cuFFT decomposes the transform. For some configurations, cuFFT will load or store (and process) multiple inputs or outputs per thread. For some configurations, threads may load or store inputs or outputs in any order, and cuFFT does not guarantee that the inputs or outputs handled by a given thread will be contiguous. These characteristics may vary with transform size, transform type (e.g. C2C vs C2R), number of dimensions, and GPU architecture. These variations may also change from one library version to the next. cuFFT will call the load callback routine, for each point in the input, once and only once. Similarly it will call the store callback routine, for each point in the output, once and only once. If the transform is being done in-place (i.e. the input and output data are in the same memory location) the store callback for a given element cannot overwrite other elements. It can either overwrite the given element, or write in a completely distinct output buffer. When more than one kernel are used to implement a transform, the thread and block structure of the first kernel (the one that does the load) is often different from the thread and block structure of the last kernel (the one that does the store). One common use of callbacks is to reduce the amount of data read or written to memory, either by selective filtering or via type conversions. When more than one kernel are used to implement a transform, cuFFT alternates using the workspace and the output buffer to write intermediate results. This means that the output buffer must always be large enough to accommodate the entire transform. For multi-GPU transforms, the index passed to the callback routine is the element index from the start of data on that GPU , not from the start of the entire input or output data array. For transforms whose dimensions can be factored into powers of 2, 3, 5, or 7, cuFFT guarantees that it will call the load and store callback routines from points in the kernel that is safe to call __syncthreads function from within callback routine. Caller is responsible for guaranteeing that the callback routine is at a point where the callback code has converged, to avoid deadlock. For plans whose dimensions are factored into higher primes, results of a callback routine calling __syncthreads are not defined. 2.9.4.1. No Ordering Guarantees Within a Kernel  Note that there are no guarantees on the relative order of execution of blocks within a grid. As such, callbacks should not rely on any particular ordering within a kernel. For instance, reordering data (such as an FFT-shift) could rely on the order of execution of the blocks. Results in this case would be undefined. 2.10. Thread Safety  cuFFT APIs are thread safe as long as different host threads execute FFTs using different plans and the output data are disjoint. 2.11. CUDA Graphs Support  Using CUDA Graphs with cuFFT is supported on single GPU plans. It is also supported on multiple GPU plans starting with cuFFT version 10.4.0. The stream associated with a cuFFT plan must meet the requirements stated in Creating a Graph Using Stream Capture . Note Starting from CUDA 11.8 (including CUDA 12.0 onward), CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. 2.12. Static Library and Callback Support  Starting with release 6.5, the cuFFT libraries are also delivered in a static form as libcufft_static.a and libcufftw_static.a on Linux and Mac. Static libraries are not supported on Windows. The static cufft and cufftw libraries depend on thread abstraction layer library libculibos.a . For example, on linux, to compile a small application using cuFFT against the dynamic library, the following command can be used: nvcc mCufftApp.c  -lcufft  -o myCufftApp For cufftw on Linux, to compile a small application against the dynamic library, the following command can be used: nvcc mCufftwApp.c  -lcufftw  -lcufft  -o myCufftwApp Whereas to compile against the static cuFFT library, extra steps need to be taken. The library needs to be device linked. It may happen during building and linking of a simple program, or as a separate step. The entire process is described in Using Separarate Compilation in CUDA . For cuFFT and cufftw in version 9.0 or later any supported architecture can be used to do the device linking: Static cuFFT compilation command: nvcc mCufftApp.c  -lcufft_static   -lculibos -o myCufftApp Static cufftw compilation command: nvcc mCufftwApp.c   -lcufftw_static  -lcufft_static   -lculibos  -o myCufftwApp Prior to version 9.0 proper linking required specifying a subset of supported architectures, as shown in the following commands: Static cuFFT compilation command: nvcc mCufftApp.c  -lcufft_static   -lculibos -o myCufftApp\\\n    -gencode arch=compute_20,\\\"code=sm_20\\\"\\\n    -gencode arch=compute_30,\\\"code=sm_30\\\"\\\n    -gencode arch=compute_35,\\\"code=sm_35\\\"\\\n    -gencode arch=compute_50,\\\"code=sm_50\\\"\\\n    -gencode arch=compute_60,\\\"code=sm_60\\\"\\\n    -gencode arch=compute_60,\\\"code=compute_60\\\" Static cufftw compilation command: nvcc mCufftwApp.c    -lcufftw_static  -lcufft_static   -lculibos  -o myCufftwApp\\\n    -gencode arch=compute_20,\\\"code=sm_20\\\"\\\n    -gencode arch=compute_30,\\\"code=sm_30\\\"\\\n    -gencode arch=compute_35,\\\"code=sm_35\\\"\\\n    -gencode arch=compute_50,\\\"code=sm_50\\\"\\\n    -gencode arch=compute_60,\\\"code=sm_60\\\"\\\n    -gencode arch=compute_60,\\\"code=compute_60\\\" Please note that the cuFFT library might not contain code for certain architectures as long as there is code for a lower architecture that is binary compatibile (e.g. SM37, SM52, SM61). This is reflected in link commands above and significant when using versions prior r9.0. To determine if a specific SM is included in the cuFFT library, one may use cuobjdump utility. For example, if you wish to know if SM_50 is included, the command to run is cuobjdump -arch sm_50 libcufft_static.a . Some kernels are built only on select architectures (e.g. kernels with half precision arithmetics are present only for SM53 and above). This can cause warnings at link time that architectures are missing from these kernels. These warnings can be safely ignored. It is also possible to use the native Host C++ compiler and perform device link as a separate step. Please consult NVCC documentation for more details. Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line. Note that in this case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. The cuFFT static library supports user supplied callback routines. The callback routines are CUDA device code, and must be separately compiled with NVCC and linked with the cuFFT library. Please refer to the NVCC documentation regarding separate compilation for details. If you specify an SM when compiling your callback functions, you must specify one of the SM’s cuFFT includes. 2.12.1. Static library without callback support  Starting with cuFFT version 9.2, a new variant of the cuFTT static library, libcufft_static_nocallback.a , was added. This new version does not contain callback functionality and can be linked using the host compiler only. 2.13. Accuracy and Performance  A DFT can be implemented as a matrix vector multiplication that requires \\(O(N^{2})\\) operations. However, the cuFFT Library employs the Cooley-Tukey algorithm to reduce the number of required operations to optimize the performance of particular transform sizes. This algorithm expresses the DFT matrix as a product of sparse building block matrices. The cuFFT Library implements the following building blocks: radix-2, radix-3, radix-5, and radix-7. Hence the performance of any transform size that can be factored as \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) (where a , b , c , and d are non-negative integers) is optimized in the cuFFT library. There are also radix-m building blocks for other primes, m, whose value is < 128. When the length cannot be decomposed as multiples of powers of primes from 2 to 127, Bluestein’s algorithm is used. Since the Bluestein implementation requires more computations per output point than the Cooley-Tukey implementation, the accuracy of the Cooley-Tukey algorithm is better. The pure Cooley-Tukey implementation has excellent accuracy, with the relative error growing proportionally to \\(\\log_{2}(N)\\) , where \\(N\\) is the transform size in points. For sizes handled by the Cooley-Tukey code path, the most efficient implementation is obtained by applying the following constraints (listed in order from the most generic to the most specialized constraint, with each subsequent constraint providing the potential of an additional performance improvement). Half precision transforms might not be suitable for all kinds of problems due to limited range represented by half precision floating point arithmetics. Please note that the first element of FFT result is the sum of all input elements and it is likely to overflow for certain inputs. Results produced by the cuFFT library are deterministic (ie, bitwise reproducible) as long as the following are kept constant between runs: plan input parameters, cuFFT version, and GPU model. cuFFT batched plans require that input data includes valid signal for all batches. Performance optimizations in batched mode can combine signal from different batches for processing. Optimizations used in cuFFT can vary from version to version. Applies to Recommendation Comment All Use single precision transforms. Single precision transforms require less bandwidth per computation than double precision transforms. All Restrict the size along all dimensions to be representable as \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) . The cuFFT library has highly optimized kernels for transforms whose dimensions have these prime factors. In general the best performance occurs when using powers of 2, followed by powers of 3, then 5, 7. All Restrict the size along each dimension to use fewer distinct prime factors. A transform of size \\(2^{n}\\) or \\(3^{n}\\) will usually be faster than one of size \\(2^{i} \\times 3^{j}\\) even if the latter is slightly smaller, due to the composition of specialized paths. All Restrict the data to be contiguous in memory when performing a single transform. When performing multiple transforms make the individual datasets contiguous The cuFFT library has been optimized for this data layout. All Perform multiple (i.e., batched) transforms. Additional optimizations are performed in batched mode. real-to-complex transforms or complex-to-real transforms Ensure problem size of x dimension is a multiple of 4. This scheme uses more efficient kernels to implement conjugate symmetry property. real-to-complex transforms or complex-to-real transforms Use out-of-place mode. This scheme uses more efficient kernels than in-place mode. Multiple GPU transforms Use PCI Express 3.0 between GPUs and ensure the GPUs are on the same switch. The faster the interconnect between the GPUs, the faster the performance. 2.14. Caller Allocated Work Area Support  cuFFT plans may use additional memory to store intermediate results. The cuFFT library offers several functions to manage this temporary memory utilization behavior: cufftSetAutoAllocation cufftEstimate1d , cufftEstimate2d , cufftEstimate3d and cufftEstimateMany cufftGetSize cufftXtSetWorkAreaPolicy The first two functions manage allocation and ownership of temporary memory. By default cuFFT always allocates its own work area in GPU memory. Each cuFFT handle allocates data separately. If multiple cuFFT plans are to be launched sequentially it is possible to assign the same memory chunk as work area to all those plans and reduce memory overhead. The memory assigned as work area needs to be GPU visible. In addition to the regular memory acquired with cudaMalloc , usage of CUDA Unified Virtual Addressing enables cuFFT to use the following types of memory as work area memory: pinned host memory, managed memory, memory on GPU other than the one performing the calculations. While this provides flexibility, it comes with a performance penalty whose magnitude depends on the available memory bandwidth. The cufftEstimateNd , cufftEstimateMany , and cufftGetSize functions provide information about the required memory size for cases where the user is allocating the work space buffer. In version 9.2 cuFFT also introduced the cufftXtSetWorkAreaPolicy function. This function allows fine tuning of work area memory usage. cuFFT 9.2 version supports only the CUFFT_WORKAREA_MINIMAL policy, which instructs cuFFT to re-plan the existing plan without the need to use work area memory. Also as of cuFFT 9.2, supported FFT transforms that allow for CUFFT_WORKAREA_MINIMAL policy are as follows: Transforms of type C2C are supported with sizes up to 4096 in any dimension. Transforms of type Z2Z are supported with sizes up to 2048 in any dimension. Only single GPU transforms are supported. Depending on the FFT transform size, a different FFT algorithm may be used when the CUFFT_WORKAREA_MINIMAL policy is set. 2.15. cuFFT Link-Time Optimized Kernels  Starting from CUDA 12.4, cuFFT ships Link-Time Optimized (LTO) kernels. These kernels are linked and finalized at runtime as part of the cuFFT planning routines. This enables the cuFFT library to generate kernels optimized for the underlying architecture and the specific problem to solve. The current LTO kernel coverage includes: Kernels for 64-bit addressing (with FFTs spanning addresses greater than 2^(32)-1 elements). Some single- and double-precision R2C and C2R sizes. The number and coverage of LTO kernels will grow with future releases of cuFFT. We encourage our users to test whether LTO kernels improve the performance for their use case. Users can opt-in into LTO kernels by setting the NVFFT_PLAN_PROPERTY_INT64_PATIENT_JIT plan property using the cufftSetPlanProperty routine. In order to finalize LTO kernels, cuFFT relies on the nvJitLink library that ships as part of the CUDA Toolkit. Finalizing the kernels at runtime can cause an increase in planning time (which could be in the order of hundreds of milliseconds, depending on the cuFFT plan and hardware characteristics of the host system), in exchange for faster execution time of the optimized kernels. Note that nvJitLink caches kernels linked at runtime to speed-up subsequent kernel finalizations in repeated planning routines. If for any reason the runtime linking of the kernel fails, cuFFT will fall back to offline-compiled kernels to compute the FFT. Note cuFFT LTO kernels for a given toolkit version require using the nvJitLink library from the same toolkit or greater, but within the same toolkit major. For example, cuFFT in 12.4 requires nvJitLink to be from a CUDA Toolkit 12.X, with X >= 4 . The nvJitLink library is loaded dynamically, and should be present in the system’s dynamic linking path (e.g. LD_LIBRARY_PATH on Unix systems, or PATH on Windows systems). 2.15.1. Overview of the cuFFT Callback Routine Feature  cuFFT provides a set of APIs that allow the cuFFT user to provide CUDA functions that re-direct or manipulate the data as it is loaded prior to processing the FFT, or stored once the FFT has been done. For the load callback, cuFFT passes the callback routine the address of the input data and the offset to the value to be loaded from device memory, and the callback routine returns the value it wishes cuFFT to use instead. For the store callback, cuFFT passes the callback routine the value it has computed, along with the address of the output data and the offset to the value to be written to device memory, and the callback routine modifies the value and stores the modified result. 3. cuFFT API Reference  This chapter specifies the behavior of the cuFFT library functions by describing their input/output parameters, data types, and error codes. The cuFFT library is initialized upon the first invocation of an API function, and cuFFT shuts down automatically when all user-created FFT plans are destroyed. 3.1. Return value cufftResult  All cuFFT Library return values except for CUFFT_SUCCESS indicate that the current API call failed and the user should reconfigure to correct the problem. The possible return values are defined as follows: typedef enum cufftResult_t { CUFFT_SUCCESS = 0 , //  The cuFFT operation was successful CUFFT_INVALID_PLAN = 1 , //  cuFFT was passed an invalid plan handle CUFFT_ALLOC_FAILED = 2 , //  cuFFT failed to allocate GPU or CPU memory CUFFT_INVALID_TYPE = 3 , //  No longer used CUFFT_INVALID_VALUE = 4 , //  User specified an invalid pointer or parameter CUFFT_INTERNAL_ERROR = 5 , //  Driver or internal cuFFT library error CUFFT_EXEC_FAILED = 6 , //  Failed to execute an FFT on the GPU CUFFT_SETUP_FAILED = 7 , //  The cuFFT library failed to initialize CUFFT_INVALID_SIZE = 8 , //  User specified an invalid transform size CUFFT_UNALIGNED_DATA = 9 , //  No longer used CUFFT_INCOMPLETE_PARAMETER_LIST = 10 , //  Missing parameters in call CUFFT_INVALID_DEVICE = 11 , //  Execution of a plan was on different GPU than plan creation CUFFT_PARSE_ERROR = 12 , //  Internal plan database error CUFFT_NO_WORKSPACE = 13 //  No workspace has been provided prior to plan execution CUFFT_NOT_IMPLEMENTED = 14 , // Function does not implement functionality for parameters given. CUFFT_LICENSE_ERROR = 15 , // Used in previous versions. CUFFT_NOT_SUPPORTED = 16 // Operation is not supported for parameters given. } cufftResult ; Users are encouraged to check return values from cuFFT functions for errors as shown in cuFFT Code Examples . 3.2. cuFFT Basic Plans  3.2.1. cufftPlan1d()  cufftResult cufftPlan1d ( cufftHandle * plan , int nx , cufftType type , int batch ) ;  Creates a 1D FFT plan configuration for a specified signal size and data type. The batch input parameter tells cuFFT how many 1D transforms to configure. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. nx[In] – The transform size (e.g. 256 for a 256-point FFT). type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). batch[In] – Number of transforms of size nx . Please consider using cufftPlanMany for multiple transforms. plan[Out] – Contains a cuFFT 1D plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx or batch parameter is not a supported size. 3.2.2. cufftPlan2d()  cufftResult cufftPlan2d ( cufftHandle * plan , int nx , int ny , cufftType type ) ;  Creates a 2D FFT plan configuration according to specified signal sizes and data type. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. nx[In] – The transform size in the x dimension This is slowest changing dimension of a transform (strided in memory). ny[In] – The transform size in the y dimension. This is fastest changing dimension of a transform (contiguous in memory). type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). plan[Out] – Contains a cuFFT 2D plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.2.3. cufftPlan3d()  cufftResult cufftPlan3d ( cufftHandle * plan , int nx , int ny , int nz , cufftType type ) ;  Creates a 3D FFT plan configuration according to specified signal sizes and data type. This function is the same as cufftPlan2d() except that it takes a third size parameter nz . This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. nx[In] – The transform size in the x dimension. This is slowest changing dimension of a transform (strided in memory). ny[In] – The transform size in the y dimension. nz[In] – The transform size in the z dimension. This is fastest changing dimension of a transform (contiguous in memory). type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). plan[Out] – Contains a cuFFT 3D plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.2.4. cufftPlanMany()  cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ) ;  Creates a FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. The cufftPlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. All arrays are assumed to be in CPU memory. Please note that behavior of cufftPlanMany function when inembed and onembed is NULL is different than corresponding function in FFTW library fftw_plan_many_dft . This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. Parameters plan[In] – Pointer to a cufftHandle object. rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. plan[Out] – Contains a cuFFT plan handle. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3. cuFFT Extensible Plans  This API separates handle creation from plan generation. This makes it possible to change plan settings, which may alter the outcome of the plan generation phase, before the plan is actually generated. 3.3.1. cufftCreate()  cufftResult cufftCreate ( cufftHandle * plan )  Creates only an opaque handle, and allocates small data structures on the host. The cufftMakePlan*() calls actually do the plan generation. Parameters plan[In] – Pointer to a cufftHandle object. plan[Out] – Contains a cuFFT plan handle value. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_ALLOC_FAILED – The allocation of resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.3.2. cufftDestroy()  cufftResult cufftDestroy ( cufftHandle plan )  Frees all GPU resources associated with a cuFFT plan and destroys the internal plan data structure. This function should be called once a plan is no longer needed, to avoid wasting GPU memory.\nIn the case of multi-GPU plans, the plan created first should be destroyed last. Parameters plan[In] – The cufftHandle object of the plan to be destroyed. Return values CUFFT_SUCCESS – cuFFT successfully destroyed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. 3.3.3. cufftMakePlan1d()  cufftResult cufftMakePlan1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a 1D FFT plan configuration for a specified signal size and data type. The batch input parameter tells cuFFT how many 1D transforms to configure. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size (e.g. 256 for a 256-point FFT). For multiple GPUs, this must be a power of 2. type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). For multiple GPUs this must be a complex to complex transform. batch[In] – Number of transforms of size nx . Please consider using cufftMakePlanMany for multiple transforms. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked or multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED` – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx or batch parameter is not a supported size. 3.3.4. cufftMakePlan2d()  cufftResult cufftMakePlan2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 2D FFT plan configuration according to specified signal sizes and data type. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension. This is slowest changing dimension of a transform (strided in memory). For multiple GPUs, this must be factorable into primes less than or equal to 127. ny[In] – The transform size in the y dimension. This is fastest changing dimension of a transform (contiguous in memory). For 2 GPUs, this must be factorable into primes less than or equal to 127. type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.3.5. cufftMakePlan3d()  cufftResult cufftMakePlan3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 3D FFT plan configuration according to specified signal sizes and data type. This function is the same as cufftPlan2d() except that it takes a third size parameter nz . This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension. This is slowest changing dimension of a transform (strided in memory). For multiple GPUs, this must be factorable into primes less than or equal to 127. ny[In] – The transform size in the y dimension. For multiple GPUs, this must be factorable into primes less than or equal to 127. nz[In] – The transform size in the z dimension. This is fastest changing dimension of a transform (contiguous in memory). For multiple GPUs, this must be factorable into primes less than or equal to 127. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work area(s). Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.3.6. cufftMakePlanMany()  cufftResult cufftMakePlanMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. The cufftPlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. All arrays are assumed to be in CPU memory. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3) n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform. For multiple GPUs and rank equal to 1, the sizes must be a power of 2. For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory, inembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). For 2 GPUs this must be a complex to complex transform. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked or multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3.7. cufftMakePlanMany64()  cufftResult cufftMakePlanMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. This API is identical to cufftMakePlanMany except that the arguments specifying sizes and strides are 64 bit integers. This API makes very large transforms possible. cuFFT includes kernels that use 32 bit indexes, and kernels that use 64 bit indexes. cuFFT planning selects 32 bit kernels whenever possible to avoid any overhead due to 64 bit arithmetic. All sizes and types of transform are supported by this interface, with two exceptions. For transforms whose size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127. For real to complex and complex to real transforms whose size exceeds 4G elements, the fastest changing dimension must be even. The cufftPlanMany64() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. This call can only be used once for a given handle. It will fail and return CUFFT_INVALID_PLAN if the plan is locked, i.e. the handle was previously used with a different cufftPlan or cufftMakePlan call. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. All arrays are assumed to be in CPU memory. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. For multiple GPUs and rank equal to 1, the sizes must be a power of 2. For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). For 2 GPUs this must be a complex to complex transform. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when the plan is locked or multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3.8. cufftXtMakePlanMany()  cufftResult cufftXtMakePlanMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  Following a call to cufftCreate() makes an FFT plan configuration of dimension rank , with sizes specified in the array n . The batch input parameter tells cuFFT how many transforms to configure. With this function, batched plans of 1, 2, or 3 dimensions may be created. Type specifiers inputtype , outputtype and executiontype dictate type and precision of transform to be performed. Not all combinations of parameters are supported. Currently all three parameters need to match precision. Parameters inputtype and outputtype need to match transform type complex-to-complex, real-to-complex or complex-to-real. Parameter executiontype needs to match precision and be of a complex type. Example: for a half-precision real-to-complex transform, parameters inputtype , outputtype and executiontype would have values of CUDA_R_16F , CUDA_C_16F and CUDA_C_16F respectively. Similarly, a bfloat16 complex-to-real transform would use CUDA_C_16BF for inputtype and executiontype , and CUDA_R_16BF for outputtype . The cufftXtMakePlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used. The default assumes contiguous data arrays. If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes. See sections on multiple GPUs for more details. All arrays are assumed to be in CPU memory. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform. For multiple GPUs and rank equal to 1, the sizes must be a power of 2. For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory, inembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. inputtype[In] – Type of input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. outputtype[In] – Type of output data. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. executiontype[In] – Type of data to be used for computations. *workSize[Out] – Pointer to the size(s) of the work areas. Return values CUFFT_SUCCESS – cuFFT successfully created the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. Handle is not valid when multi-GPU restrictions are not met. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.4. cuFFT Plan Properties  Users can further customize cuFFT plans using plan properties. These properties can be set, queried and reset on a per-plan basis as needed, using the routines listed in this section. The current supported properties are listed below: Property Underlying Type Description Behavior NVFFT_PLAN_PROPERTY_INT64_PATIENT_JIT long long int Runtime LTO kernels are enabled when set to not-zero value. See Link-Time Optimized Kernels Runtime LTO kernles are disabled when set to zero (default) Can be set / reset before planning Cannot be set / reset after planning 3.4.1. cufftSetPlanPropertyInt64()  cufftResult cufftSetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , const long long int propertyValueInt64 ) ;  Associates a cuFFT plan with a property identified by the key property . The value for the property is given by value propertyValueInt64 , which is a signed long long integer. Parameters plan[In] – cufftHandle returned by cufftCreate . property[In] – The property identifier, of type cufftPlanProperty . propertyValueInt64[In] – Value to set for the property, a long long signed integer. Return values CUFFT_SUCCESS – cuFFT successfully set the property. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_NOT_SUPPORTED – The property is not supported, or it cannot be set at the time (e.g. some properties cannot be set after calling a planning routine for the plan, see cuFFT Plan Properties ). CUFFT_INVALID_VALUE – Invalid property or value with which to set the property 3.4.2. cufftGetPlanPropertyInt64()  cufftResult cufftGetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , long long int * propertyValueInt64 ) ;  Retrieves the property value identified by the key property associated with the cuFFT plan plan . The value for the property, which is a signed long long integer, is set in the address space pointed by propertyValueInt64 . Parameters plan[In] – cufftHandle returned by cufftCreate . property[In] – The property identifier, of type cufftPlanProperty . propertyValueInt64[In] – Pointer to the value to be set with the value of the property. Return values CUFFT_SUCCESS – cuFFT successfully retrieved the property value. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_NOT_SUPPORTED – The property is not supported. CUFFT_INVALID_VALUE – Invalid property, or pointer propertyValueInt64 is null 3.4.3. cufftResetPlanProperty()  cufftResult cufftResetPlanProperty ( cufftHandle plan , cufftProperty property ) ;  Resets the value of the property identified by the key property , associated with the cuFFT plan plan , to its default value. Parameters plan[In] – cufftHandle returned by cufftCreate . property[In] – The property identifier, of type cufftPlanProperty . Return values CUFFT_SUCCESS – cuFFT successfully reset the property value. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_NOT_SUPPORTED – The property is not supported for plan , or cannot be reset at present time (see Behavior column on cuFFT Plan Properties ). CUFFT_INVALID_VALUE – Invalid property 3.5. cuFFT Estimated Size of Work Area  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. The cufftEstimate*() calls return an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Some problem sizes require much more storage than others. In particular powers of 2 are very efficient in terms of temporary storage. Large prime numbers, however, use different algorithms and may need up to the eight times that of a similarly sized power of 2. These routines return estimated workSize values which may still be smaller than the actual values needed especially for values of n that are not multiples of powers of 2, 3, 5 and 7. More refined values are given by the cufftGetSize*() routines, but these values may still be conservative. 3.5.1. cufftEstimate1d()  cufftResult cufftEstimate1d ( int nx , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Parameters nx[In] – The transform size (e.g. 256 for a 256-point FFT). type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). batch[In] – Number of transforms of size nx . Please consider using cufftEstimateMany for multiple transforms. *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx parameter is not a supported size. 3.5.2. cufftEstimate2d()  cufftResult cufftEstimate2d ( int nx , int ny , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Parameters nx[In] – The transform size in the x dimension (number of rows). ny[In] – The transform size in the y dimension (number of columns). type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.5.3. cufftEstimate3d()  cufftResult cufftEstimate3d ( int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. Parameters nx[In] – The transform size in the x dimension. ny[In] – The transform size in the y dimension. nz[In] – The transform size in the z dimension. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.5.4. cufftEstimateMany()  cufftResult cufftEstimateMany ( int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results. This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings. The cufftEstimateMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist . All arrays are assumed to be in CPU memory. Parameters rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size, in bytes, of the work space. *workSize[Out] – Pointer to the size of the work space Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.6. cuFFT Refined Estimated Size of Work Area  The cufftGetSize*() routines give a more accurate estimate of the work area size required for a plan than the cufftEstimate*() routines as they take into account any plan settings that may have been made. As discussed in the section cuFFT Estimated Size of Work Area , the workSize value(s) returned may be conservative especially for values of n that are not multiples of powers of 2, 3, 5 and 7. 3.6.1. cufftGetSize1d()  cufftResult cufftGetSize1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate1d() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size (e.g. 256 for a 256-point FFT). type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex). batch[In] – Number of transforms of size nx . Please consider using cufftGetSizeMany for multiple transforms. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – The nx parameter is not a supported size. 3.6.2. cufftGetSize2d()  cufftResult cufftGetSize2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate2d() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension (number of rows). ny[In] – The transform size in the y dimension (number of columns). type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real). *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.6.3. cufftGetSize3d()  cufftResult cufftGetSize3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate3d() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . nx[In] – The transform size in the x dimension. ny[In] – The transform size in the y dimension. nz[In] – The transform size in the z dimension. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work space. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.6.4. cufftGetSizeMany()  cufftResult cufftGetSizeMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.6.5. cufftGetSizeMany64()  cufftResult cufftGetSizeMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made. This API is identical to cufftMakePlanMany except that the arguments specifying sizes and strides are 64 bit integers. This API makes very large transforms possible. cuFFT includes kernels that use 32 bit indexes, and kernels that use 64 bit indexes. cuFFT planning selects 32 bit kernels whenever possible to avoid any overhead due to 64 bit arithmetic. All sizes and types of transform are supported by this interface, with two exceptions. For transforms whose total size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127. For real to complex and complex to real transforms whose total size exceeds 4G elements, the fastest changing dimension must be even. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex). batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.6.6. cufftXtGetSizeMany()  cufftResult cufftXtGetSizeMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters that match signature of cufftXtMakePlanMany function, and taking into account any plan settings that may have been made. For more information about valid combinations of inputtype , outputtype and executiontype parameters please refer to documentation of cufftXtMakePlanMany function. Parameters plan[In] – cufftHandle returned by cufftCreate . rank[In] – Dimensionality of the transform (1, 2, or 3). n[In] – Array of size rank , describing the size of each dimension. inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. If set to NULL all other advanced data layout parameters are ignored. istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension. idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data. inputtype[In] ( cudaDataType ) – Type of input data. onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory. If set to NULL all other advanced data layout parameters are ignored. ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension. odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data. outputtype[In] ( cudaDataType ) – Type of output data. batch[In] – Batch size for this transform. *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. executiontype[In] ( cudaDataType ) – Type of data to be used for computations. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.7. cufftGetSize()  cufftResult cufftGetSize ( cufftHandle plan , size_t * workSize ) ;  Once plan generation has been done, either with the original API or the extensible API, this call returns the actual size of the work area required to support the plan. Callers who choose to manage work area allocation within their application must use this call after plan generation, and after any cufftSet*() calls subsequent to plan generation, if those calls might alter the required work space size. Parameters plan[In] – cufftHandle returned by cufftCreate . *workSize[In] – Pointer to the size(s), in bytes, of the work areas. For example for two GPUs worksize must be declared to have two elements. *workSize[Out] – Pointer to the size of the work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.8. cuFFT Caller Allocated Work Area Support  3.8.1. cufftSetAutoAllocation()  cufftResult cufftSetAutoAllocation ( cufftHandle plan , int autoAllocate ) ;  cufftSetAutoAllocation() indicates that the caller intends to allocate and manage work areas for plans that have been generated. cuFFT default behavior is to allocate the work area at plan generation time. If cufftSetAutoAllocation() has been called with autoAllocate set to 0 (“false”) prior to one of the cufftMakePlan*() calls, cuFFT does not allocate the work area. This is the preferred sequence for callers wishing to manage work area allocation. Parameters plan[In] – cufftHandle returned by cufftCreate . autoAllocate[In] – Indicates whether to allocate work area. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.8.2. cufftSetWorkArea()  cufftResult cufftSetWorkArea ( cufftHandle plan , void * workArea ) ;  cufftSetWorkArea() overrides the work area pointer associated with a plan. If the work area was auto-allocated, cuFFT frees the auto-allocated space. The cufftExecute*() calls assume that the work area pointer is valid and that it points to a contiguous region in device memory that does not overlap with any other work area. If this is not the case, results are indeterminate. Parameters plan[In] – cufftHandle returned by cufftCreate . *workArea[In] – Pointer to workArea . For multiple GPUs, multiple work area pointers must be given. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.8.3. cufftXtSetWorkAreaPolicy()  cufftResult cufftXtSetWorkAreaPolicy ( cufftHandle plan , cufftXtWorkAreaPolicy policy , size_t * workSize ) ;  cufftXtSetWorkAreaPolicy() indicates that the caller intends to change work area size for a given plan handle. cuFFT’s default behavior is to allocate the work area at plan generation time with a default size that depends on the plan type and other parameters. If cufftXtSetWorkAreaPolicy() has been called with the policy parameter set to CUFFT_WORKAREA_MINIMAL , cuFFT will attempt to re-plan the handle to use zero bytes of work area memory. If the cufftXtSetWorkAreaPolicy() call is successful the auto-allocated work area memory is released. Currently the policies CUFFT_WORKAREA_PERFORMANCE , CUFFT_WORKAREA_USER and the workSize parameter are not supported and reserved for use in future cuFFT releases. This function can be called once per lifetime of a plan handle. Parameters plan[In] – cufftHandle returned by cufftCreate . policy[In] – Type of work area policy to apply. *workSize[In] – Reserved for future use. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_SIZE – FFT size does not allow use of the selected policy. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9. cuFFT Execution  3.9.1. cufftExecC2C() and cufftExecZ2Z()  cufftResult cufftExecC2C ( cufftHandle plan , cufftComplex * idata , cufftComplex * odata , int direction ) ;  cufftResult cufftExecZ2Z ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleComplex * odata , int direction ) ;  cufftExecC2C() ( cufftExecZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter. cuFFT uses the GPU memory pointed to by the idata parameter as input data. This function stores the Fourier coefficients in the odata array. If idata and odata are the same, this method does an in-place transform. Parameters plan[In] – cufftHandle returned by cufftCreate . idata[In] – Pointer to the complex input data (in GPU memory) to transform. odata[In] – Pointer to the complex output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . odata[Out] – ontains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata , odata , and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.2. cufftExecR2C() and cufftExecD2Z()  cufftResult cufftExecR2C ( cufftHandle plan , cufftReal * idata , cufftComplex * odata ) ;  cufftResult cufftExecD2Z ( cufftHandle plan , cufftDoubleReal * idata , cufftDoubleComplex * odata ) ;  cufftExecR2C() ( cufftExecD2Z() ) executes a single-precision (double-precision) real-to-complex, implicitly forward, cuFFT transform plan. cuFFT uses as input data the GPU memory pointed to by the idata parameter. This function stores the nonredundant Fourier coefficients in the odata array. Pointers to idata and odata are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex data type in double-precision transforms. If idata and odata are the same, this method does an in-place transform. Note the data layout differences between in-place and out-of-place transforms as described in Parameter cufftType . Parameters plan[In] – cufftHandle returned by cufftCreate . idata[In] – Pointer to the real input data (in GPU memory) to transform. odata[In] – Pointer to the complex output data (in GPU memory). odata[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata and odata is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.3. cufftExecC2R() and cufftExecZ2D()  cufftResult cufftExecC2R ( cufftHandle plan , cufftComplex * idata , cufftReal * odata ) ;  cufftResult cufftExecZ2D ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleReal * odata ) ;  cufftExecC2R() ( cufftExecZ2D() ) executes a single-precision (double-precision) complex-to-real, implicitly inverse, cuFFT transform plan. cuFFT uses as input data the GPU memory pointed to by the idata parameter. The input array holds only the nonredundant complex Fourier coefficients. This function stores the real output values in the odata array. and pointers are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex type in double-precision transforms. If idata and odata are the same, this method does an in-place transform. Parameters plan[In] – cufftHandle returned by cufftCreate . idata[In] – Pointer to the complex input data (in GPU memory) to transform. odata[In] – Pointer to the real output data (in GPU memory). odata[Out] – Contains the real output data. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata and odata is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.4. cufftXtExec()  cufftResult cufftXtExec ( cufftHandle plan , void * input , void * output , int direction ) ;  Function cufftXtExec executes any cuFFT transform regardless of precision and type. In case of complex-to-real and real-to-complex transforms direction parameter is ignored. cuFFT uses the GPU memory pointed to by the input parameter as input data. This function stores the Fourier coefficients in the output array. If input and output are the same, this method does an in-place transform. Parameters plan[In] – cufftHandle returned by cufftCreate . input[In] – Pointer to the input data (in GPU memory) to transform. output[In] – Pointer to the output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . Ignored for complex-to-real and real-to-complex transforms. output[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata , odata , and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.9.5. cufftXtExecDescriptor()  cufftResult cufftXtExecDescriptor ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  Function cufftXtExecDescriptor() executes any cuFFT transform regardless of precision and type. In case of complex-to-real and real-to-complex transforms direction parameter is ignored. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input descriptor as input data and cudaLibXtDesc *output as output data. Parameters plan[In] – cufftHandle returned by cufftCreate . input[In] – Pointer to the complex input data (in GPU memory) to transform. output[In] – Pointer to the complex output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . Ignored for complex-to-real and real-to-complex transforms. idata[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters idata and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10. cuFFT and Multiple GPUs  3.10.1. cufftXtSetGPUs()  cufftResult cufftXtSetGPUs ( cufftHandle plan , int nGPUs , int * whichGPUs ) ;  cufftXtSetGPUs() identifies which GPUs are to be used with the plan. As in the single GPU case cufftCreate() creates a plan and cufftMakePlan*() does the plan generation. In cuFFT prior to 10.4.0, this call will return an error if a non-default stream has been associated with the plan. Note that the call to cufftXtSetGPUs() must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() . Parameter whichGPUs of cufftXtSetGPUs() function determines ordering of the GPUs with respect to data decomposition (first data chunk is placed on GPU denoted by first element of whichGPUs ). Parameters plan[In] – cufftHandle returned by cufftCreate . nGPUs[In] – Number of GPUs to use. whichGPUs[In] – The GPUs to use. Return values CUFFT_SUCCESS – cuFFT successfully set the GPUs to use. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_VALUE – The requested number of GPUs was less than 2 or more than 8. CUFFT_INVALID_DEVICE – An invalid GPU index was specified. CUFFT_INVALID_SIZE – Transform size that plan was created for does not meet minimum size criteria. 3.10.2. cufftXtSetWorkArea()  cufftResult cufftXtSetWorkArea ( cufftHandle plan , void * * workArea ) ;  cufftXtSetWorkArea() overrides the work areas associated with a plan. If the work area was auto-allocated, cuFFT frees the auto-allocated space. The cufftXtExec*() calls assume that the work area is valid and that it points to a contiguous region in each device memory that does not overlap with any other work area. If this is not the case, results are indeterminate. Parameters plan[In] – cufftHandle returned by cufftCreate . workArea[In] – Pointer to the pointers to workArea. Return values CUFFT_SUCCESS – cuFFT successfully set the GPUs to use. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – A GPU associated with the plan could not be selected. 3.10.3. cuFFT Multiple GPU Execution  3.10.3.1. cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z()  cufftResult cufftXtExecDescriptorC2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftResult cufftXtExecDescriptorZ2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftXtExecDescriptorC2C() ( cufftXtExecDescriptorZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input as input data. Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays. Parameters plan[In] – cufftHandle returned by cufftCreate . *input[In] – Pointer to the complex input data (in GPU memory) to transform. *output[In] – Pointer to the complex output data (in GPU memory). direction[In] – The transform direction: CUFFT_FORWARD or CUFFT_INVERSE . input[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.3.2. cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z()  cufftResult cufftXtExecDescriptorR2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorD2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorR2C() ( cufftXtExecDescriptorD2Z() ) executes a single-precision (double-precision) real-to-complex transform plan. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input as input data. Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays. Parameters plan[In] – cufftHandle returned by cufftCreate . *input[In] – Pointer to the complex input data (in GPU memory) to transform. *output[In] – Pointer to the complex output data (in GPU memory). input[Out] – Contains the complex Fourier coefficients Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.3.3. cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D()  cufftResult cufftXtExecDescriptorC2R ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorZ2D ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorC2R() ( cufftXtExecDescriptorZ2D() ) executes a single-precision (double-precision) complex-to-real transform plan in the transform direction as specified by direction parameter. cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input as input data. Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays. Parameters plan[In] – cufftHandle returned by cufftCreate . *input[In] – Pointer to the complex input data (in GPU memory) to transform. *output[In] – Pointer to the complex output data (in GPU memory). input[Out] – Contains the complex Fourier coefficients. Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.4. Memory Allocation and Data Movement Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution. The following functions assist in allocation, setup and retrieval of the data. They must be called after the call to cufftMakePlan*() . 3.10.4.1. cufftXtMalloc()  cufftResult cufftXtMalloc ( cufftHandle plan , cudaLibXtDesc * * descriptor , cufftXtSubFormat format ) ;  cufftXtMalloc() allocates a descriptor, and all memory for data in GPUs associated with the plan, and returns a pointer to the descriptor. Note the descriptor contains an array of device pointers so that the application may preprocess or postprocess the data on the GPUs. The enumerated parameter cufftXtSubFormat_t indicates if the buffer will be used for input or output. Parameters plan[In] – cufftHandle returned by cufftCreate . **descriptor[In] – Pointer to a pointer to a cudaLibXtDesc object. format[In] – cufftXtSubFormat`` value. **descriptor[Out] – Pointer to a pointer to a cudaLibXtDesc object. Return values CUFFT_SUCCESS – cuFFT successfully allows user to allocate descriptor and GPU memory. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle or it is not a multiple GPU plan . CUFFT_ALLOC_FAILED – The allocation of GPU resources for the plan failed. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in the descriptor. 3.10.4.1.1. Parameter cufftXtSubFormat  cufftXtSubFormat_t is an enumerated type that indicates if the buffer will be used for input or output and the ordering of the data. typedef enum cufftXtSubFormat_t { CUFFT_XT_FORMAT_INPUT , //by default input is in linear order across GPUs CUFFT_XT_FORMAT_OUTPUT , //by default output is in scrambled order depending on transform CUFFT_XT_FORMAT_INPLACE , //by default inplace is input order, which is linear across GPUs CUFFT_XT_FORMAT_INPLACE_SHUFFLED , //shuffled output order after execution of the transform CUFFT_FORMAT_UNDEFINED } cufftXtSubFormat ; 3.10.4.2. cufftXtFree()  cufftResult cufftXtFree ( cudaLibXtDesc * descriptor ) ;  cufftXtFree() frees the descriptor and all memory associated with it. The descriptor and memory must have been returned by a previous call to cufftXtMalloc() . Parameters *descriptor[In] – Pointer to a cudaLibXtDesc object. Return values CUFFT_SUCCESS – cuFFT successfully allows user to free descriptor and associated GPU memory. CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.10.4.3. cufftXtMemcpy()  cufftResult cufftXtMemcpy ( cufftHandle plan , void * dstPointer , void * srcPointer , cufftXtCopyType type ) ;  cufftXtMemcpy() copies data between buffers on the host and GPUs or between GPUs. The enumerated parameter cufftXtCopyType_t indicates the type and direction of transfer. Calling cufftXtMemcpy function for multi-GPU batched FFT plans with CUFFT_COPY_DEVICE_TO_DEVICE transfer type is not supported. Note that starting from CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported on multi-GPU plans. When associating a stream with a plan, cufftXtMemcpy() remains synchronous across the multiple GPUs. Parameters plan[In] – cufftHandle returned by cufftCreate . dstPointer[In] – Pointer to the destination address(es). srcPointer[In] – Pointer to the source address(es). type[In] – cufftXtCopyType value. Return values CUFFT_SUCCESS – cuFFT successfully allows user to copy memory between host and GPUs or between GPUs. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle. CUFFT_INVALID_VALUE – One or more invalid parameters were passed to the API. CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10.4.3.1. Parameter cufftXtCopyType  cufftXtCopyType_t is an enumerated type for multiple GPU functions that specifies the type of copy for cufftXtMemcpy() . CUFFT_COPY_HOST_TO_DEVICE copies data from a contiguous host buffer to multiple device buffers, in the layout cuFFT requires for input data. dstPointer must point to a cudaLibXtDesc structure, and srcPointer must point to a host memory buffer. CUFFT_COPY_DEVICE_TO_HOST copies data from multiple device buffers, in the layout cuFFT produces for output data, to a contiguous host buffer. dstPointer must point to a host memory buffer, and srcPointer must point to a cudaLibXtDesc structure. CUFFT_COPY_DEVICE_TO_DEVICE copies data from multiple device buffers, in the layout cuFFT produces for output data, to multiple device buffers, in the layout cuFFT requires for input data. dstPointer and srcPointer must point to different cudaLibXtDesc structures (and therefore memory locations). That is, the copy cannot be in-place. Note that device_to_device cufftXtMemcpy() for 2D and 3D data is not currently supported. typedef enum cufftXtCopyType_t { CUFFT_COPY_HOST_TO_DEVICE , CUFFT_COPY_DEVICE_TO_HOST , CUFFT_COPY_DEVICE_TO_DEVICE } cufftXtCopyType ; 3.10.5. General Multiple GPU Descriptor Types  3.10.5.1. cudaXtDesc  A descriptor type used in multiple GPU routines that contains information about the GPUs and their memory locations. struct cudaXtDesc_t { int version ; //descriptor version int nGPUs ; //number of GPUs int GPUs [ MAX_CUDA_DESCRIPTOR_GPUS ]; //array of device IDs void * data [ MAX_CUDA_DESCRIPTOR_GPUS ]; //array of pointers to data, one per GPU size_t size [ MAX_CUDA_DESCRIPTOR_GPUS ]; //array of data sizes, one per GPU void * cudaXtState ; //opaque CUDA utility structure }; typedef struct cudaXtDesc_t cudaXtDesc ; 3.10.5.2. cudaLibXtDesc  A descriptor type used in multiple GPU routines that contains information about the library used. struct cudaLibXtDesc_t { int version ; //descriptor version cudaXtDesc * descriptor ; //multi-GPU memory descriptor libFormat library ; //which library recognizes the format int subFormat ; //library specific enumerator of sub formats void * libDescriptor ; //library specific descriptor e.g. FFT transform plan object }; typedef struct cudaLibXtDesc_t cudaLibXtDesc ; 3.11. cuFFT Callbacks  3.11.1. cufftXtSetCallback()  cufftResult cufftXtSetCallback ( cufftHandle plan , void * * callbackRoutine , cufftXtCallbackType type , void * * callerInfo )  cufftXtSetCallback() specifies a load or store callback to be used with the plan. This call is valid only after a call to cufftMakePlan*() , which does the plan generation. If there was already a callback of this type associated with the plan, this new callback routine replaces it. If the new callback requires shared memory, you must call cufftXtSetCallbackSharedSize with the amount of shared memory it needs. cuFFT will not retain the amount of shared memory associated with the previous callback. Parameters plan[In] – cufftHandle returned by cufftCreate . callbackRoutine[In] – Array of callback routine pointers, one per GPU. type[In] – Type of callback routine. callerInfo[In] – Optional array of device pointers to caller specific information, one per GPU. Return values CUFFT_SUCCESS – cuFFT successfully associated the callback function with the plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_SETUP_FAILED – The cuFFT library failed to initialize. 3.11.2. cufftXtClearCallback()  cufftResult cufftXtClearCallback ( cufftHandle plan , cufftXtCallbackType type )  cufftXtClearCallback() instructs cuFFT to stop invoking the specified callback type when executing the plan. Only the specified callback is cleared. If no callback of this type had been specified, the return code is CUFFT_SUCCESS . Parameters plan[In] – cufftHandle returned by cufftCreate . type[In] – Type of callback routine. Return values CUFFT_SUCCESS – cuFFT successfully disassociated the callback function with the plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_INTERNAL_ERROR – An internal driver error was detected. 3.11.3. cufftXtSetCallbackSharedSize()  cufftResult cufftXtSetCallbackSharedSize ( cufftHandle plan , cufftXtCallbackType type , size_t sharedSize )  cufftXtSetCallbackSharedSize() instructs cuFFT to dynamically allocate shared memory at launch time, for use by the callback. The maximum allowable amount of shared memory is 16K bytes. cuFFT passes a pointer to this shared memory to the callback routine at execution time. This shared memory is only valid for the life of the load or store callback operation. During execution, cuFFT may overwrite shared memory for its own purposes. Parameters plan[In] – cufftHandle returned by cufftCreate . type[In] – Type of callback routine. sharedSize[In] – Amount of shared memory requested. Return values CUFFT_SUCCESS – cuFFT will invoke the callback routine with a pointer to the requested amount of shared memory. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 . CUFFT_INTERNAL_ERROR – An internal driver error was detected. CUFFT_ALLOC_FAILED – cuFFT will not be able to allocate the requested amount of shared memory. 3.12. cufftSetStream()  cufftResult cufftSetStream ( cufftHandle plan , cudaStream_t stream ) ;  Associates a CUDA stream with a cuFFT plan. All kernel launches made during plan execution are now done through the associated stream, enabling overlap with activity in other streams (e.g. data copying). The association remains until the plan is destroyed or the stream is changed with another call to cufftSetStream() . Note that starting from CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported on multi-GPU plans. When associating a stream with a plan, cufftXtMemcpy() remains synchronous across the multiple GPUs. For previous versions of cuFFT, cufftSetStream() will return an error in multiple GPU plans. Note that starting from CUDA 12.2 (cuFFT 11.0.8), on multi-GPU plans, stream can be associated with any context on any GPU. However, repeated calls to cufftSetStream() with streams from different contexts incur a small time penalty. Optimal performance is obtained when repeated calls to cufftSetStream use streams from the same CUDA context. Parameters plan[In] – The cufftHandle object to associate with the stream. stream[In] – A valid CUDA stream created with cudaStreamCreate() ; 0 for the default stream. Return values CUFFT_SUCCESS – The stream was associated with the plan. CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or plan is multi-gpu in cuFFT version prior to 10.4.0. 3.13. cufftGetVersion()  cufftResult cufftGetVersion ( int * version ) ;  Returns the version number of cuFFT. Parameters *version[In] – Pointer to the version number. *version[Out] – Contains the version number. Return values CUFFT_SUCCESS – cuFFT successfully returned the version number. 3.14. cufftGetProperty()  cufftResult cufftGetProperty ( libraryPropertyType type , int * value ) ;  Return in *value the number for the property described by type of the dynamically linked CUFFT library. Parameters type[In] – CUDA library property. value[Out] – Contains the integer value for the requested property. Return values CUFFT_SUCCESS – The property value was successfully returned. CUFFT_INVALID_TYPE – The property type is not recognized. CUFFT_INVALID_VALUE – value is NULL . 3.15. cuFFT Types  3.15.1. Parameter cufftType  The cuFFT library supports complex- and real-data transforms. The cufftType data type is an enumeration of the types of transform data supported by cuFFT. typedef enum cufftType_t { CUFFT_R2C = 0x2a , // Real to complex (interleaved) CUFFT_C2R = 0x2c , // Complex (interleaved) to real CUFFT_C2C = 0x29 , // Complex to complex (interleaved) CUFFT_D2Z = 0x6a , // Double to double-complex (interleaved) CUFFT_Z2D = 0x6c , // Double-complex (interleaved) to double CUFFT_Z2Z = 0x69 // Double-complex to double-complex (interleaved) } cufftType ; 3.15.2. Parameters for Transform Direction  The cuFFT library defines forward and inverse Fast Fourier Transforms according to the sign of the complex exponential term. #define cuFFTFORWARD -1 #define cuFFTINVERSE 1 cuFFT performs un-normalized FFTs; that is, performing a forward FFT on an input data set followed by an inverse FFT on the resulting set yields data that is equal to the input, scaled by the number of elements. Scaling either transform by the reciprocal of the size of the data set is left for the user to perform as seen fit. 3.15.3. Type definitions for callbacks  The cuFFT library supports callback funtions for all combinations of single or double precision, real or complex data, load or store. These are enumerated in the parameter cufftXtCallbackType . typedef enum cufftXtCallbackType_t { CUFFT_CB_LD_COMPLEX = 0x0 , CUFFT_CB_LD_COMPLEX_DOUBLE = 0x1 , CUFFT_CB_LD_REAL = 0x2 , CUFFT_CB_LD_REAL_DOUBLE = 0x3 , CUFFT_CB_ST_COMPLEX = 0x4 , CUFFT_CB_ST_COMPLEX_DOUBLE = 0x5 , CUFFT_CB_ST_REAL = 0x6 , CUFFT_CB_ST_REAL_DOUBLE = 0x7 , CUFFT_CB_UNDEFINED = 0x8 } cufftXtCallbackType ; The corresponding function prototypes and pointer type definitions are as follows: typedef cufftComplex ( * cufftCallbackLoadC )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleComplex ( * cufftCallbackLoadZ )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftReal ( * cufftCallbackLoadR )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleReal ( * cufftCallbackLoadD )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreC )( void * dataOut , size_t offset , cufftComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreZ )( void * dataOut , size_t offset , cufftDoubleComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreR )( void * dataOut , size_t offset , cufftReal element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreD )( void * dataOut , size_t offset , cufftDoubleReal element , void * callerInfo , void * sharedPointer ); 3.15.4. Other cuFFT Types  3.15.4.1. cufftHandle  type cufftHandle  A handle type used to store and access cuFFT plans. The user receives a handle after creating a cuFFT plan and uses this handle to execute the plan. typedef unsigned int cufftHandle ; 3.15.4.2. cufftReal  A single-precision, floating-point real data type. typedef float cufftReal ; 3.15.4.3. cufftDoubleReal  A double-precision, floating-point real data type. typedef double cufftDoubleReal ; 3.15.4.4. cufftComplex  A single-precision, floating-point complex data type that consists of interleaved real and imaginary components. typedef cuComplex cufftComplex ; 3.15.4.5. cufftDoubleComplex  A double-precision, floating-point complex data type that consists of interleaved real and imaginary components. typedef cuDoubleComplex cufftDoubleComplex ; 3.16. Common types  3.16.1. cudaDataType  The cudaDataType data type is an enumeration of the types supported by CUDA libraries. typedef enum cudaDataType_t { CUDA_R_16F = 2 , // 16 bit real CUDA_C_16F = 6 , // 16 bit complex CUDA_R_32F = 0 , // 32 bit real CUDA_C_32F = 4 , // 32 bit complex CUDA_R_64F = 1 , // 64 bit real CUDA_C_64F = 5 , // 64 bit complex CUDA_R_8I = 3 , // 8 bit real as a signed integer CUDA_C_8I = 7 , // 8 bit complex as a pair of signed integers CUDA_R_8U = 8 , // 8 bit real as an unsigned integer CUDA_C_8U = 9 // 8 bit complex as a pair of unsigned integers } cudaDataType ; 3.16.2. libraryPropertyType  The libraryPropertyType data type is an enumeration of library property types. (ie. CUDA version X.Y.Z would yield MAJOR_VERSION=X , MINOR_VERSION=Y , PATCH_LEVEL=Z ) typedef enum libraryPropertyType_t { MAJOR_VERSION , MINOR_VERSION , PATCH_LEVEL } libraryPropertyType ; 4. cuFFT Code Examples  For simple examples of complex and real 1D, 2D, and 3D transforms that use cuFFT to perform forward and inverse FFTs, refer to the cuFFT Library samples on GitHub . 5. Multiple GPU Data Organization  This chapter explains how data are distributed between the GPUs, before and after a multiple GPU transform. For simplicity, it is assumed in this chapter that the caller has specified GPU 0 and GPU 1 to perform the transform. 5.1. Multiple GPU Data Organization for Batched Transforms  For batches of transforms, each individual transform is executed on a single GPU. If possible the batches are evenly distributed among the GPUs. For a batch of size m performed on n GPUs, where m is not divisible by n , the first m % n GPUs will perform \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor+\\ 1\\) transforms. The remaining GPUs will perform \\(\\left\\lfloor \\frac{m}{n} \\right\\rfloor\\) transforms. For example, in a batch of 15 transforms performed on 4 GPUs, the first three GPUs would perform 4 transforms, and the last GPU would perform 3 transforms. This approach removes the need for data exchange between the GPUs, and results in nearly perfect scaling for cases where the batch size is divisible by the number of GPUs. 5.2. Multiple GPU Data Organization for Single 2D and 3D Transforms  Single transforms performed on multiple GPUs require the data to be divided between the GPUs. Then execution takes place in phases. For example with 2 GPUs, for 2D and 3D transforms with even sized dimensions, each GPU does half of the transform in (rank - 1) dimensions. Then data are exchanged between the GPUs so that the final dimension can be processed. Since 2D and 3D transforms support sizes other than powers of 2, it is possible that the data can not be evenly distributed among the GPUs. In general for the case of n GPUs, a dimension of size m that is not a multiple of n would be distributed such that the first m % n GPUs would get one extra row for 2D transforms, one extra plane for 3D transforms. Take for example, a 2D transform on 4 GPUs, using an array declared in C as data[x][y] , where x is 65 and y is 99. The surface is distributed prior to the transform such that GPU 0 receives a surface with dimensions [17][99] , and GPUs 1…3 receive surfaces with dimensions [16][99] . After the transform, each GPU again has a portion of the surface, but divided in the y dimension. GPUs 0…2 have surfaces with dimensions [65][25] . GPU 3 has a surface with dimensions [65][24] For a 3D transform on 4 GPUs consider an array declared in C as data[x][y][z] , where x is 103, y is 122, and z is 64. The volume is distributed prior to the transform such that each GPUs 0…2 receive volumes with dimensions [26][122][64] , and GPU 3 receives a volume with dimensions [25][122][64] . After the transform, each GPU again has a portion of the surface, but divided in the y dimension. GPUs 0 and 1 have a volumes with dimensions [103][31][64] , and GPUs 2 and 3 have volumes with dimensions [103][30][64] . 5.3. Multiple-GPU Data Organization for Single 1D Transforms  By default for 1D transforms, the initial distribution of data to the GPUs is similar to the 2D and 3D cases. For a transform of dimension x on two GPUs, GPU 0 receives data ranging from 0…(x/2-1). GPU 1 receives data ranging from (x/2)…(x-1). Similarly, with 4 GPUs, the data are evenly distributed among all 4 GPUs. Before computation can begin, data are redistributed among the GPUs. It is possible to perform this redistribution in the copy from host memory, in cases where the application does not need to pre-process the data prior to the transform. To do this, the application can create the data descriptor with cufftXtMalloc using the sub-format CUFFT_XT_FORMAT_1D_INPUT_SHUFFLED . This can significantly reduce the time it takes to execute the transform. cuFFT performs multiple GPU 1D transforms by decomposing the transform size into factors Factor1 and Factor2 , and treating the data as a grid of size Factor1 x Factor2 . The four steps done to calculate the 1D FFT are: Factor1 transforms of size Factor2 , data exchange between the GPUs, a pointwise twiddle multiplication, and Factor2 transforms of size Factor1 . To gain efficiency by overlapping computation with data exchange, cuFFT breaks the whole transform into independent segments or strings, which can be processed while others are in flight. A side effect of this algorithm is that the output of the transform is not in linear order. The output in GPU memory is in strings, each of which is composed of Factor2 substrings of equal size. Each substring contains contiguous results starting Factor1 elements subsequent to start of the previous substring. Each string starts substring size elements after the start of the previous string. The strings appear in order, the first half on GPU 0, and the second half on GPU 1. See the example below: transform size = 1024 number of strings = 8 Factor1 = 64 Factor2 = 16 substrings per string for output layout is Factor2 ( 16 ) string size = 1024 / 8 = 128 substring size = 128 / 16 = 8 stride between substrings = 1024 / 16 = Factor1 ( 64 ) On GPU 0 : string 0 has substrings with indices 0. . .7 64. . .71 128. . .135 ... 960. . .967 string 1 has substrings with indices 8. . .15 72. . .79 136. . .143 ... 968. . .975 ... On GPU 1 : string 4 has substrings with indices 32. . .39 96. . .103 160. . .167 ... 992. . .999 ... string 7 has substrings with indices 56. . .63 120. . .127 184. . .191 ... 1016. . .1023 The cufftXtQueryPlan API allows the caller to retrieve a structure containing the number of strings, the decomposition factors, and (in the case of power of 2 size) some useful mask and shift elements. The example below shows how cufftXtQueryPlan is invoked. It also shows how to translate from an index in the host input array to the corresponding index on the device, and vice versa. /* * These routines demonstrate the use of cufftXtQueryPlan to get the 1D * factorization and convert between permuted and linear indexes. */ /* * Set up a 1D plan that will execute on GPU 0 and GPU1, and query * the decomposition factors */ int main ( int argc , char ** argv ){ cufftHandle plan ; cufftResult stat ; int whichGPUs [ 2 ] = { 0 , 1 }; cufftXt1dFactors factors ; stat = cufftCreate ( & plan ); if ( stat != CUFFT_SUCCESS ) { printf ( \"Create error %d \\n \" , stat ); return 1 ; } stat = cufftXtSetGPUs ( plan , 2 , whichGPUs ); if ( stat != CUFFT_SUCCESS ) { printf ( \"SetGPU error %d \\n \" , stat ); return 1 ; } stat = cufftMakePlan1d ( plan , size , CUFFT_C2C , 1 , workSizes ); if ( stat != CUFFT_SUCCESS ) { printf ( \"MakePlan error %d \\n \" , stat ); return 1 ; } stat = cufftXtQueryPlan ( plan , ( void * ) & factors , CUFFT_QUERY_1D_FACTORS ); if ( stat != CUFFT_SUCCESS ) { printf ( \"QueryPlan error %d \\n \" , stat ); return 1 ; } printf ( \"Factor 1 %zd, Factor2 %zd \\n \" , factors . factor1 , factors . factor2 ); cufftDestroy ( plan ); return 0 ; } /* * Given an index into a permuted array, and the GPU index return the * corresponding linear index from the beginning of the input buffer. * * Parameters: *      factors     input:  pointer to cufftXt1dFactors as returned by *                          cufftXtQueryPlan *      permutedIx  input:  index of the desired element in the device output *                          array *      linearIx    output: index of the corresponding input element in the *                          host array *      GPUix       input:  index of the GPU containing the desired element */ cufftResult permuted2Linear ( cufftXt1dFactors * factors , size_t permutedIx , size_t * linearIx , int GPUIx ) { size_t indexInSubstring ; size_t whichString ; size_t whichSubstring ; // the low order bits of the permuted index match those of the linear index indexInSubstring = permutedIx & factors -> substringMask ; // the next higher bits are the substring index whichSubstring = ( permutedIx >> factors -> substringShift ) & factors -> factor2Mask ; // the next higher bits are the string index on this GPU whichString = ( permutedIx >> factors -> stringShift ) & factors -> stringMask ; // now adjust the index for the second GPU if ( GPUIx ) { whichString += factors -> stringCount / 2 ; } // linear index low order bits are the same // next higher linear index bits are the string index * linearIx = indexInSubstring + ( whichString << factors -> substringShift ); // next higher bits of linear address are the substring index * linearIx += whichSubstring << factors -> factor1Shift ; return CUFFT_SUCCESS ; } /* * Given a linear index into a 1D array, return the GPU containing the permuted * result, and index from the start of the data buffer for that element. * * Parameters: *      factors     input:  pointer to cufftXt1dFactors as returned by *                          cufftXtQueryPlan *      linearIx    input:  index of the desired element in the host input *                          array *      permutedIx  output: index of the corresponding result in the device *                          output array *      GPUix       output: index of the GPU containing the result */ cufftResult linear2Permuted ( cufftXt1dFactors * factors , size_t linearIx , size_t * permutedIx , int * GPUIx ) { size_t indexInSubstring ; size_t whichString ; size_t whichSubstring ; size_t whichStringMask ; int whichStringShift ; if ( linearIx >= factors -> size ) { return CUFFT_INVALID_VALUE ; } // get a useful additional mask and shift count whichStringMask = factors -> stringCount -1 ; whichStringShift = ( factors -> factor1Shift + factors -> factor2Shift ) - factors -> stringShift ; // the low order bits identify the index within the substring indexInSubstring = linearIx & factors -> substringMask ; // first determine which string has our linear index. // the low order bits indentify the index within the substring. // the next higher order bits identify which string. whichString = ( linearIx >> factors -> substringShift ) & whichStringMask ; // the first stringCount/2 strings are in the first GPU, // the rest are in the second. * GPUIx = whichString / ( factors -> stringCount / 2 ); // next determine which substring within the string has our index // the substring index is in the next higher order bits of the index whichSubstring = ( linearIx >> ( factors -> substringShift + whichStringShift )) & factors -> factor2Mask ; // now we can re-assemble the index * permutedIx = indexInSubstring ; * permutedIx += whichSubstring << factors -> substringShift ; if ( !* GPUIx ) { * permutedIx += whichString << factors -> stringShift ; } else { * permutedIx += ( whichString - ( factors -> stringCount / 2 ) ) << factors -> stringShift ; } return CUFFT_SUCCESS ; } 6. FFTW Conversion Guide  cuFFT differs from FFTW in that FFTW has many plans and a single execute function while cuFFT has fewer plans, but multiple execute functions. The cuFFT execute functions determine the precision (single or double) and whether the input is complex or real valued. The following table shows the relationship between the two interfaces. FFTW function cuFFT function fftw_plan_dft_1d(), fftw_plan_dft_r2c_1d(), fftw_plan_dft_c2r_1d() cufftPlan1d() fftw_plan_dft_2d(), fftw_plan_dft_r2c_2d(), fftw_plan_dft_c2r_2d() cufftPlan2d() fftw_plan_dft_3d(), fftw_plan_dft_r2c_3d(), fftw_plan_dft_c2r_3d() cufftPlan3d() fftw_plan_dft(), fftw_plan_dft_r2c(), fftw_plan_dft_c2r() cufftPlanMany() fftw_plan_many_dft(), fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() cufftPlanMany() fftw_execute() cufftExecC2C(), cufftExecZ2Z(), cufftExecR2C(), cufftExecD2Z(), cufftExecC2R(), cufftExecZ2D() fftw_destroy_plan() cufftDestroy() 7. FFTW Interface to cuFFT  NVIDIA provides FFTW3 interfaces to the cuFFT library. This allows applications using FFTW to use NVIDIA GPUs with minimal modifications to program source code. To use the interface first do the following two steps It is recommended that you replace the include file fftw3.h with cufftw.h Instead of linking with the double/single precision libraries such as fftw3/fftw3f libraries, link with both the cuFFT and cuFFTW libraries Ensure the search path includes the directory containing cuda_runtime_api.h After an application is working using the FFTW3 interface, users may want to modify their code to move data to and from the GPU and use the routines documented in the FFTW Conversion Guide for the best performance. The following tables show which components and functions of FFTW3 are supported in cuFFT. Section in FFTW manual Supported Unsupported Complex numbers fftw_complex, fftwf_complex types Precision double fftw3 , single fftwf3 long double fftw3l , quad precision fftw3q are not supported since CUDA functions operate on double and single precision floating-point quantities Memory Allocation fftw_malloc(), fftw_free(), fftw_alloc_real(), fftw_alloc_complex(), fftwf_alloc_real(), fftwf_alloc_complex() Multi-threaded FFTW fftw3_threads, fftw3_omp are not supported Distributed-memory FFTW with MPI fftw3_mpi,fftw3f_mpi are not supported Note that for each of the double precision functions below there is a corresponding single precision version with the letters fftw replaced by fftwf . Section in FFTW manual Supported Unsupported Using Plans fftw_execute(), fftw_destroy_plan(), fftw_cleanup() fftw_print_plan(), fftw_cost(), fftw_flops() exist but are not functional Basic Interface Complex DFTs fftw_plan_dft_1d(), fftw_plan_dft_2d(), fftw_plan_dft_3d(), fftw_plan_dft() Planner Flags Planner flags are ignored and the same plan is returned regardless Real-data DFTs fftw_plan_dft_r2c_1d(), fftw_plan_dft_r2c_2d(), fftw_plan_dft_r2c_3d(), fftw_plan_dft_r2c(), fftw_plan_dft_c2r_1d(), fftw_plan_dft_c2r_2d(), fftw_plan_dft_c2r_3d(), fftw_plan_dft_c2r() Read-data DFT Array Format Not supported Read-to-Real Transform Not supported Read-to-Real Transform Kinds Not supported Advanced Interface Advanced Complex DFTs fftw_plan_many_dft() with multiple 1D, 2D, 3D transforms fftw_plan_many_dft() with 4D or higher transforms or a 2D or higher batch of embedded transforms Advanced Real-data DFTs fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() with multiple 1D, 2D, 3D transforms fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() with 4D or higher transforms or a 2D or higher batch of embedded transforms Advanced Real-to-Real Transforms Not supported Guru Interface Interleaved and split arrays Interleaved format Split format Guru vector and transform sizes fftw_iodim struct Guru Complex DFTs fftw_plan_guru_dft(), fftw_plan_guru_dft_r2c(), fftw_plan_guru_dft_c2r() with multiple 1D, 2D, 3D transforms fftw_plan_guru_dft(), fftw_plan_guru_dft_r2c(), fftw_plan_guru_dft_c2r() with 4D or higher transforms or a 2D or higher batch of transforms Guru Real-data DFTs Not supported Guru Real-to-real Transforms Not supported 64-bit Guru Interface fftw_plan_guru64_dft(), fftw_plan_guru64_dft_r2c(), fftw_plan_guru64_dft_c2r() with multiple 1D, 2D, 3D transforms fftw_plan_guru64_dft(), fftw_plan_guru64_dft_r2c(), fftw_plan_guru64_dft_c2r() with 4D or higher transforms or a 2D or higher batch of transforms New-array Execute Functions fftw_execute_dft(), fftw_execute_dft_r2c(), fftw_execute_dft_c2r() with interleaved format Split format and real-to-real functions Wisdom fftw_export_wisdom_to_file(), fftw_import_wisdom_from_file() exist but are not functional. Other wisdom functions do not have entry points in the library. 8. Deprecated Functionality  Starting from CUDA 12.0: GPU architectures SM35 and SM37 are no longer supported. The minimum required architecture is SM50. Starting from CUDA 11.8: CUDA Graphs capture is no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. Starting from CUDA 11.4: Support for callback functionality using separately compiled device code is deprecated on all GPU architectures. Callback functionality will continue to be supported for all GPU architectures. Starting from CUDA 11.0: GPU architecture SM30 is no longer supported. The minimum required architecture is SM35. Support for GPU architectures SM35, SM37 (Kepler), and SM50, SM52 (Maxwell) is deprecated. Function cufftSetCompatibilityMode was removed in version 9.1. 9. Notices  9.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 9.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 9.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/curand/index.html", "parent_url": "https://docs.nvidia.com/cuda/curand/index.html", "content_type": "text/html", "text": "cuRAND :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 cuRAND Introduction 1. Compatibility and Versioning 2. Host API Overview 2.1. Generator Types 2.2. Generator Options 2.2.1. Seed 2.2.2. Offset 2.2.3. Order 2.3. Return Values 2.4. Generation Functions 2.5. Host API Example 2.6. Static Library support 2.7. Performance Notes 2.8. Thread Safety 3. Device API Overview 3.1. Pseudorandom Sequences 3.1.1. Bit Generation with XORWOW and MRG32k3a generators 3.1.2. Bit Generation with the MTGP32 generator 3.1.3. Bit Generation with Philox_4x32_10 generator 3.1.4. Distributions 3.2. Quasirandom Sequences 3.3. Skip-Ahead 3.4. Device API for discrete distributions 3.5. Performance Notes 3.6. Device API Examples 3.7. Thrust and cuRAND Example 3.8. Poisson API Example 4. Testing 5. Modules 5.1. Host API 5.2. Device API A. Bibliography B. Acknowledgements Search Results < Previous | Next > cuRAND\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback cuRAND The API reference guide for cuRAND, the CUDA random number generation library. Table of Contents Introduction 1. Compatibility and Versioning 2. Host API Overview 2.1. Generator Types 2.2. Generator Options 2.2.1. Seed 2.2.2. Offset 2.2.3. Order 2.3. Return Values 2.4. Generation Functions 2.5. Host API Example 2.6. Static Library support 2.7. Performance Notes 2.8. Thread Safety 3. Device API Overview 3.1. Pseudorandom Sequences 3.1.1. Bit Generation with XORWOW and MRG32k3a generators 3.1.2. Bit Generation with the MTGP32 generator 3.1.3. Bit Generation with Philox_4x32_10 generator 3.1.4. Distributions 3.2. Quasirandom Sequences 3.3. Skip-Ahead 3.4. Device API for discrete distributions 3.5. Performance Notes 3.6. Device API Examples 3.7. Thrust and cuRAND Example 3.8. Poisson API Example 4. Testing 5. Modules 5.1. Host API 5.2. Device API A. Bibliography B. Acknowledgements Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/nvjpeg/index.html", "parent_url": "https://docs.nvidia.com/cuda/nvjpeg/index.html", "content_type": "text/html", "text": "nvJPEG 1. Introduction 1.1. nvJPEG Decoder 1.2. nvJPEG Encoder 1.3. Thread Safety 1.4. Multi-GPU support 1.5. Hardware Acceleration 2. JPEG Decoding 2.1. Using JPEG Decoding 2.1.1. Single Image Decoding 2.1.2. Decode using Decoupled Phases 2.1.3. Batched Image Decoding 2.1.3.1. Single Phase 2.2. nvJPEG Type Declarations 2.2.1. nvJPEG Backend 2.2.2. nvJPEG Bitstream Handle 2.2.3. nvJPEG Decode Device Buffer Handle 2.2.4. nvJPEG Decode Parameter Handle 2.2.5. nvJPEG Decode Pinned Buffer Handle 2.2.6. nvJPEG Decoder Handle 2.2.7. nvJPEG Host Pinned Memory Allocator Interface 2.2.8. nvJPEG Extended Host Pinned Memory Allocator Interface 2.2.9. nvJPEG Image 2.2.10. nvJPEG Device Memory Allocator Interface 2.2.11. nvJPEG Extended Device Memory Allocator Interface 2.2.12. nvJPEG Opaque JPEG Decoding State Handle 2.2.13. nvJPEG Opaque Library Handle Struct 2.2.14. nvJPEG Output Pointer Struct 2.2.15. nvJPEG Jpeg Encoding 2.2.16. nvJPEG Scale Factor 2.2.17. nvJPEG Flags 2.2.18. nvJPEG Exif Orientation 2.3. nvJPEG API Reference 2.3.1. nvJPEG Helper API Reference 2.3.1.1. nvjpegGetProperty() 2.3.1.2. nvjpegGetCudartProperty() 2.3.1.3. nvjpegCreate() [DEPRECATED] 2.3.1.4. nvjpegCreateSimple() 2.3.1.5. nvjpegCreateEx() 2.3.1.6. nvjpegCreateExV2() 2.3.1.7. nvjpegDestroy() 2.3.1.8. nvjpegSetDeviceMemoryPadding() 2.3.1.9. nvjpegGetDeviceMemoryPadding() 2.3.1.10. nvjpegSetPinnedMemoryPadding() 2.3.1.11. nvjpegGetPinnedMemoryPadding() 2.3.1.12. nvjpegGetHardwareDecoderInfo() 2.3.1.13. nvjpegJpegStateCreate() 2.3.1.14. nvjpegJpegStateDestroy() 2.3.1.15. nvjpegDecoderCreate() 2.3.1.16. nvjpegDecoderDestroy() 2.3.1.17. nvjpegDecoderJpegSupported() 2.3.1.18. nvjpegDecoderStateCreate() 2.3.1.19. nvjpegJpegStreamCreate() 2.3.1.20. nvjpegJpegStreamDestroy() 2.3.1.21. nvjpegBufferPinnedCreate() 2.3.1.22. nvjpegBufferPinnedCreateV2() 2.3.1.23. nvjpegBufferPinnedDestroy() 2.3.1.24. nvjpegStateAttachPinnedBuffer() 2.3.1.25. nvjpegBufferPinnedRetrieve() 2.3.1.26. nvjpegBufferPinnedResize() 2.3.1.27. nvjpegBufferDeviceCreate() 2.3.1.28. nvjpegBufferDeviceCreateV2() 2.3.1.29. nvjpegBufferDeviceDestroy() 2.3.1.30. nvjpegStateAttachDeviceBuffer() 2.3.1.31. nvjpegBufferDeviceRetrieve() 2.3.1.32. nvjpegBufferDeviceResize() 2.3.1.33. nvjpegDecodeParamsCreate() 2.3.1.34. nvjpegDecodeParamsDestroy() 2.3.2. Retrieve Encoded Image Information API 2.3.2.1. nvjpegGetImageInfo() 2.3.2.2. nvJPEG Stream API 2.3.2.2.1. nvjpegJpegStreamParse() 2.3.2.2.2. nvjpegJpegStreamParseHeader() 2.3.2.2.3. nvjpegJpegStreamParseTables() 2.3.2.2.4. nvjpegJpegStreamGetFrameDimensions() 2.3.2.2.5. nvjpegJpegStreamGetComponentsNum() 2.3.2.2.6. nvjpegJpegStreamGetComponentDimensions() 2.3.2.2.7. nvjpegJpegStreamGetChromaSubsampling() 2.3.2.2.8. nvjpegJpegStreamGetJpegEncoding() 2.3.2.2.9. nvjpegJpegStreamGetExifOrientation() 2.3.2.2.10. nvjpegJpegStreamGetSamplePrecision() 2.3.3. Decode API—Single Phase 2.3.3.1. ​nvjpegDecode() 2.3.3.2. ​nvjpegDecodeBatchedInitialize() 2.3.3.3. ​nvjpegDecodeBatched() 2.3.3.4. nvjpegDecodeBatchedEx() 2.3.3.5. nvjpegDecodeBatchedSupported() 2.3.3.6. nvjpegDecodeBatchedSupportedEx() 2.3.3.7. nvjpegDecodeBatchedPreAllocate() 2.3.3.8. nvjpegDecodeBatchedParseJpegTables() 2.3.4. Decode API—Decoupled Decoding 2.3.4.1. nvjpegDecodeJpegHost() 2.3.4.2. nvjpegDecodeJpegTransferToDevice() 2.3.4.3. nvjpegDecodeJpegDevice() 2.3.4.4. nvjpegDecodeJpeg() 2.3.5. nvJPEG Decode Parameters 2.3.5.1. nvjpegDecodeParamsSetOutputFormat() 2.3.5.2. nvjpegDecodeParamsSetROI() 2.3.5.3. nvjpegDecodeParamsSetAllowCMYK() 2.3.5.4. nvjpegDecodeParamsSetScaleFactor() 2.3.5.5. nvjpegDecodeParamsSetExifOrientation() 2.3.6. nvJPEG API Return Codes 2.3.7. nvJPEG Chroma Subsampling 2.3.8. Reference Documents 2.4. Examples of nvJPEG 3. JPEG Encoding 3.1. Using the Encoder 3.1.1. Encoding the Parameters 3.1.2. Encoding the State 3.1.3. Encoding the Image 3.1.3.1. nvjpegEncodeYUV 3.1.3.2. nvjpegEncodeImage 3.1.4. Retrieving the Compressed Stream 3.1.5. JPEG Encoding Example 3.2. nvJPEG Encoder Type Declarations 3.2.1. nvjpegInputFormat_t 3.2.2. nvjpegEncoderState_t 3.2.3. nvjpegEncoderParams_t 3.3. nvJPEG Encoder Helper API Reference 3.3.1. nvjpegEncoderStateCreate() 3.3.2. nvjpegEncoderStateDestroy() 3.3.3. nvjpegEncoderParamsCreate() 3.3.4. nvjpegEncoderParamsDestroy() 3.3.5. nvjpegEncoderParamsSetEncoding() 3.3.6. nvjpegEncoderParamsSetQuality() 3.3.7. nvjpegEncoderParamsSetOptimizedHuffman() 3.3.8. nvjpegEncoderParamsSetSamplingFactors() 3.4. nvJPEG Encoder API Reference 3.4.1. nvjpegEncodeGetBufferSize() 3.4.2. nvjpegEncodeYUV() 3.4.3. nvjpegEncodeImage() 3.4.4. nvjpegEncodeRetrieveBitstream() 3.4.5. nvjpegEncodeRetrieveBitstreamDevice() 4. JPEG Transcoding 4.1. nvJPEG Transcoder Helper API Reference 4.1.1. nvjpegEncoderParamsCopyMetadata() 4.1.2. nvjpegEncoderParamsCopyQuantizationTables() 4.1.3. nvjpegEncoderParamsCopyHuffmanTables() [Deprecated] 4.2. JPEG Transcoding Example 5. List of Dropped APIs 6. Known Issues 7. Notices 7.1. Notice 7.2. OpenCL 7.3. Trademarks nvJPEG » 1. Introduction v12.5 | PDF | Archive nvJPEG A GPU accelerated JPEG codec library. 1. Introduction  1.1. nvJPEG Decoder  The nvJPEG library provides high-performance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. The library offers single and batched JPEG decoding capabilities which efficiently utilize the available GPU resources for optimum performance; and the flexibility for users to manage the memory allocation needed for decoding. The nvJPEG library enables the following functions: use the JPEG image data stream as input; retrieve the width and height of the image from the data stream, and use this retrieved information to manage the GPU memory allocation and the decoding. A dedicated API is provided for retrieving the image information from the raw JPEG image data stream. Note Throughout this document, the terms “CPU” and “Host” are used synonymously. Similarly, the terms “GPU” and “Device” are synonymous. The nvJPEG library supports the following: JPEG options: Baseline and Progressive JPEG decoding/encoding 8 bits per pixel Huffman bitstream decoding Upto 4 channel JPEG bitstreams 8- and 16-bit quantization tables The following chroma subsampling for the 3 color channels Y, Cb, Cr (Y, U, V): 4:4:4 4:2:2 4:2:0 4:4:0 4:1:1 4:1:0 Features: Hybrid decoding using both the CPU (i.e., host) and the GPU (i.e., device). Hardware acceleration for baseline JPEG decode on supported platforms . Input to the library is in the host memory, and the output is in the GPU memory. Single image and batched image decoding. Single phase and multiple phases decoding. Color space conversion. User-provided memory manager for the device and pinned host memory allocations. 1.2. nvJPEG Encoder  The encoding functions of the nvJPEG library perform GPU-accelerated compression of user’s image data to the JPEG bitstream. User can provide input data in a number of formats and colorspaces, and control the encoding process with parameters. Encoding functionality will allocate temporary buffers using user-provided memory allocator. Before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described in nvJPEG Encoder Helper API Reference . 1.3. Thread Safety  Not all nvJPEG types are thread safe. When using decoder APIs across multiple threads, the following decoder types should be instantiated separately for each thread: nvjpegJpegStream_t , nvjpegJpegState_t , nvjpegBufferDevice_t , nvjpegBufferPinned_t When using encoder APIs across multiple threads, nvjpegEncoderState_t should be instantiated separately for each thread. For user-provided allocators (inputs to nvJPEGCreateEx() ), the user needs to ensure thread safety. 1.4. Multi-GPU support  The nvJPEG states and handles are bound to the device that was set as current during their creation. Using these states and handles with another device set as current is undefined. The user is responsible of keeping track of the current device. 1.5. Hardware Acceleration  Hardware accelerated JPEG decode is available on the following GPUs - A100, A30, H100. Platforms which support hardware accelerated JPEG decode: Windows Linux (x86_64, PowerPC, ARM64) 2. JPEG Decoding  2.1. Using JPEG Decoding  ​The nvJPEG library provides functions for both the decoding of a single image, and batched decoding of multiple images. 2.1.1. Single Image Decoding  For single-image decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer. To use the nvJPEG library, start by calling the helper functions for initialization. Create nvJPEG library handle with one of the helper functions nvjpegCreateSimple() or nvjpegCreateEx() . Create JPEG state with the helper function nvjpegJpegStateCreate() . See nvJPEG Type Declarations and nvjpegJpegStateCreate() . The following helper functions are available in the nvJPEG library: nvjpegStatus_t nvjpegGetProperty(libraryPropertyType type, int *value); [DEPRECATED] nvjpegStatus_t nvjpegCreate(nvjpegBackend_t backend, nvjpegHandle_t *handle , nvjpeg_dev_allocator allocator); nvjpegStatus_t nvjpegCreateSimple(nvjpegHandle_t *handle); nvjpegStatus_t nvjpegCreateEx(nvjpegBackend_t backend, nvjpegDevAllocator_t *dev_allocator, nvjpegPinnedAllocator_t *pinned_allocator, unsigned int flags, nvjpegHandle_t *handle); nvjpegStatus_t nvjpegDestroy(nvjpegHandle_t handle); nvjpegStatus_t nvjpegJpegStateCreate(nvjpegHandle_t handle, nvjpegJpegState_t *jpeg_handle); nvjpegStatus_t nvjpegJpegStateDestroy(nvjpegJpegState handle); Other helper functions such as nvjpegSet*() and nvjpegGet*() can be used to configure the library functionality on per-handle basis. Refer to the helper API reference for more details. Retrieve the width and height information from the JPEG-encoded image by using the nvjpegGetImageInfo() function. Below is the signature of nvjpegGetImageInfo() function: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); For each image to be decoded, pass the JPEG data pointer and data length to the above function. The nvjpegGetImageInfo() function is thread safe. One of the outputs of the above nvjpegGetImageInfo() function is nvjpegChromaSubsampling_t . This parameter is an enum type, and its enumerator list is composed of the chroma subsampling property retrieved from the JPEG image. See nvJPEG Chroma Subsampling . Use the nvjpegDecode() function in the nvJPEG library to decode this single JPEG image. See the signature of this function below: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); In the above nvjpegDecode() function, the parameters nvjpegOutputFormat_t , nvjpegImage_t , and cudaStream_t can be used to set the output behavior of the nvjpegDecode() function. You provide the cudaStream_t parameter to indicate the stream to which your asynchronous tasks are submitted. The ``nvjpegOutputFormat_t`` parameter: The nvjpegOutputFormat_t parameter can be set to one of the output_format settings below: output_format Meaning NVJPEG_OUTPUT_UNCHANGED Return the decoded image planar format. NVJPEG_OUTPUT_RGB Convert to planar RGB. NVJPEG_OUTPUT_BGR Convert to planar BGR. NVJPEG_OUTPUT_RGBI Convert to interleaved RGB. NVJPEG_OUTPUT_BGRI Convert to interleaved BGR. NVJPEG_OUTPUT_Y Return the Y component only. NVJPEG_OUTPUT_YUV Return in the YUV planar format. NVJPEG_OUTPUT_UNCHANGEDI_U16 Return the decoded image interleaved format. For example, if output_format is set to NVJPEG_OUTPUT_Y or NVJPEG_OUTPUT_RGBI , or NVJPEG_OUTPUT_BGRI then the output is written only to channel[0] of nvjpegImage_t , and the other channels are not touched. Alternately, in the case of planar output, the data is written to the corresponding channels of the nvjpegImage_t destination structure. Finally, in the case of grayscale JPEG and RGB output, the luminance is used to create the grayscale RGB. The below table explains the combinations of the output formats and the number of channels supported by the library. No of Channels in bitstream 1 2 3 4 Output Format NVJPEG_OUTPUT_UNCHANGED Yes Yes Yes Yes NVJPEG_OUTPUT_YUV Only the first channel of the output is populated No Yes No NVJPEG_OUTPUT_Y Yes No Yes Yes (a) NVJPEG_OUTPUT_RGB Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGR Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_RGBI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGRI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_UNCHANGEDI_U16 Yes(c) Yes No No NOTES: Must be enabled using nvjpegDecodeParamsSetAllowCMYK() . Luminance is used to create the grayscale RGB. Supported only by NVJPEG_BACKEND_LOSSLESS_JPEG backend. As mentioned above, an important benefit of the nvjpegGetImageInfo() function is the ability to utilize the image information retrieved from the the input JPEG image to allocate proper GPU memory for your decoding operation. The nvjpegGetImageInfo() function returns the widths , heights and nComponents parameters. nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); You can use the retrieved parameters, widths , heights and nComponents , to calculate the required size for the output buffers, either for a single decoded JPEG, or for every decoded JPEG in a batch. To optimally set the destination parameter for the nvjpegDecode() function, use the following guidelines: For the output_format: NVJPEG_OUTPUT_Y destination.pitch[0] should be at least: width[0] destination.channel[0] should be at least of size: destination.pitch[0]*height[0] For the output_format destination.pitch[c] should be at least: destination.channel[c] should be at least of size: NVJPEG_OUTPUT_YUV width[c] for c = 0, 1, 2 destination.pitch[c]*height[c] for c = 0, 1, 2 NVJPEG_OUTPUT_RGB and NVJPEG_OUTPUT_BGR width[0] for c = 0, 1, 2 destination.pitch[0]*height[0] for c = 0, 1, 2 NVJPEG_OUTPUT_RGBI and NVJPEG_OUTPUT_BGRI width[0]*3 destination.pitch[0]*height[0] NVJPEG_OUTPUT_UNCHANGED width[c] for c = [ 0, nComponents - 1 ] destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] NVJPEG_OUTPUT_UNCHANGEDI_U16 width[c]* nComponents* sizeof(unsigned short) destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] Ensure that the nvjpegImage_t structure (or structures, in the case of batched decode) is filled with the pointers and pitches of allocated buffers. The nvjpegImage_t structure that holds the output pointers is defined as follows: typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; NVJPEG_MAX_COMPONENT is the maximum number of color components the nvJPEG library supports in the current release. For generic images, this is the maximum number of encoded channels that the library is able to decompress. Finally, when you call the nvjpegDecode() function with the parameters as described above, the nvjpegDecode() function fills the output buffers with the decoded data. 2.1.2. Decode using Decoupled Phases  The nvJPEG library allows further separation of the host and device phases of the decode process. The host phase of the decoding will not need to access to device resources. A few examples of decoupled APIs can be found under Decode API - Decoupled Decoding. Below is the sequence of API calls to decode a single image Initialize all the items that are used in the decoding process: Create the library handle using one of the library handle initialization routines. Choose decoder implementation nvjpegBackend_t , and create decoder using nvjpegDecoderCreate() . Create JPEG decoder state using nvjpegDecoderStateCreate() . Create JPEG stream using nvjpegJpegStreamCreate() . Create the pinned and device buffers used by the decoder using the below APIs respectively. These buffers are used to store intermediate decoding results. nvjpegBufferPinnedCreate() nvjpegBufferDeviceCreate() Link the buffers to the JPEG state using the following APIs respectively: nvjpegStateAttachPinnedBuffer() nvjpegStateAttachDeviceBuffer() Create decode parameters using the below API. This is used to set the output format, and enable ROI decode: nvjpegDecodeParamsCreate() Perform decoding: Parse the jpeg bit-stream using nvjpegJpegStreamParse() Encoded bitstream information, like channel dimensions, can be retrieved using the below API. This information is used to allocate the output pointers in nvjpegImage_t . nvjpegJpegStreamGetComponentsNum() nvjpegJpegStreamGetComponentDimensions() Call the decode API in the below sequence to decode the image: nvjpegDecodeJpegHost() nvjpegDecodeJpegTransferToDevice() nvjpegDecodeJpegDevice() 2.1.3. Batched Image Decoding  For the batched image decoding you provide pointers to multiple file data in the memory, and also provide the buffer sizes for each file data. The nvJPEG library will decode these multiple images, and will place the decoded data in the output buffers that you specified in the parameters. 2.1.3.1. Single Phase  For batched image decoding in single phase, follow these steps: Call nvjpegDecodeBatchedInitialize() function to initialize the batched decoder. Specify the batch size in the batch_size parameter. See nvjpegDecodeBatchedInitialize() . Next, call nvjpegDecodeBatched() for each new batch. Make sure to pass the parameters that are correct to the specific batch of images. If the size of the batch changes, or if the batch decoding fails, then call the nvjpegDecodeBatchedInitialize() function again. 2.2. nvJPEG Type Declarations  2.2.1. nvJPEG Backend  typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , NVJPEG_BACKEND_GPU_HYBRID = 2 , NVJPEG_BACKEND_HARDWARE = 3 , NVJPEG_BACKEND_GPU_HYBRID_DEVICE = 4 , NVJPEG_BACKEND_HARDWARE_DEVICE = 5 , NVJPEG_BACKEND_LOSSLESS_JPEG = 6 } nvjpegBackend_t ; The nvjpegBackend_t enum is used to select either default back-end by default, or use GPU decoding for baseline JPEG images, or use CPU for Huffman decoding. Member Description NVJPEG_BACKEND_DEFAULT Back-end is selected internally. NVJPEG_BACKEND_HYBRID Uses CPU for Huffman decoding. NVJPEG_BACKEND_GPU_HYBRID Uses GPU for Huffman decoding. nvjpegDecodeBatched will use GPU decoding for baseline JPEG images with interleaved scan when batch size is greater than 50. The decoupled APIs will use GPU assisted Huffman decoding. NVJPEG_BACKEND_HARDWARE Uses Hardware Acceleration for decode. Supports baseline JPEG images with single scan with 1 or 3 channels. 410 and 411 chroma subsamplings are not supported. NVJPEG_BACKEND_GPU_HYBRID_DEVICE Supports input bitstream on device memory. Can be used only with batched decode APIs for baseline JPEG images without restart intervals. NVJPEG_BACKEND_HARDWARE_DEVICE Supports input bitstream on device memory. Can be used only with batched decode APIs. Uses Hardware Acceleration for decode. Supports baseline JPEG images with single scan with 1 or 3 channels. 410 and 411 chroma subsamplings are not supported. NVJPEG_BACKEND_LOSSLESS_JPEG Supports lossless jpeg bitstreams as defined in the jpeg 92 standard. Bitstreams with up to 2 channels and prediction mode 1 are supported. 2.2.2. nvJPEG Bitstream Handle  struct nvjpegJpegStream ; typedef struct nvjpegJpegStream * nvjpegJpegStream_t ; This handle stores the bit-stream parameters on the host. This helps retrieve bitstream meta-data using APIs defined in nvJPEG Stream API . 2.2.3. nvJPEG Decode Device Buffer Handle  struct nvjpegBufferDevice ; typedef struct nvjpegBufferDevice * nvjpegBufferDevice_t ; This nvjpegBufferDevice_t is used by decoder states to store the intermediate information in device memory. 2.2.4. nvJPEG Decode Parameter Handle  struct nvjpegDecodeParams ; typedef struct nvjpegDecodeParams * nvjpegDecodeParams_t ; This decoder parameter handle stores the parameters like output format, and the ROI decode parameters that are set using APIs defined in nvJPEG Chroma Subsampling . 2.2.5. nvJPEG Decode Pinned Buffer Handle  struct nvjpegBufferPinned ; typedef struct nvjpegBufferPinned * nvjpegBufferPinned_t ; This nvjpegBufferPinned_t handle is used by decoder states to store the intermediate information on pinned memory. 2.2.6. nvJPEG Decoder Handle  struct nvjpegJpegDecoder ; typedef struct nvjpegJpegDecoder * nvjpegJpegDecoder_t ; This decoder handle stores the intermediate decoder data, which is shared across the decoding stages. This decoder handle is initialized for a given nvjpegBackend_t . It is used as input to the Decode API—Decoupled Decoding . 2.2.7. nvJPEG Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMalloc )( void ** , size_t , unsigned int flags ); typedef int ( * tPinnedFree )( void * ); typedef struct { tPinnedMalloc pinned_malloc ; tPinnedFree pinned_free ; } nvjpegPinnedAllocator_t ; When the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set as a pointer to the above nvjpegPinnedAllocator_t structure, then this structure will be used for allocating and releasing host pinned memory for copying data to/from device. The function prototypes for the memory allocation and memory freeing functions are similar to the cudaHostAlloc() and cudaFreeHost() functions. They will return 0 in case of success, and non-zero otherwise. However, if the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaHostAlloc() and cudaFreeHost() will be used. When using nvjpegCreate() or nvjpegCreateSimple() function to create library handle, the default host pinned memory allocator will be used. 2.2.8. nvJPEG Extended Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tPinnedFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tPinnedMallocV2 pinned_malloc ; tPinnedFreeV2 pinned_free ; void * pinned_ctx ; } nvjpegPinnedAllocatorV2_t ; Extended pinned allocators support stream ordered allocations along with user defined context information pinned_ctx . When invoking the allocators, nvJPEG will pass pinned_ctx as input to the extended pinned allocators. 2.2.9. nvJPEG Image  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t structure (or structures, in the case of batched decode) is used to fill with the pointers and pitches of allocated buffers. The nvjpegImage_t structure that holds the output pointers. Member Description NVJPEG_MAX_COMPONENT Maximum number of color components the nvJPEG library supports. For generic images, this is the maximum number of encoded channels that the library is able to decompress. 2.2.10. nvJPEG Device Memory Allocator Interface  typedef int ( * tDevMalloc )( void ** , size_t ); typedef int ( * tDevFree )( void * ); typedef struct { tDevMalloc dev_malloc ; tDevFree dev_free ; } nvjpegDevAllocator_t ; Users can tell the library to use their own device memory allocator. The function prototypes for the memory allocation and memory freeing functions are similar to the cudaMalloc() and cudaFree() functions. They should return 0 in case of success, and non-zero otherwise. A pointer to the nvjpegDevAllocator_t structure, with properly filled fields, should be provided to the nvjpegCreate() function. NULL is accepted, in which case the default memory allocation functions cudaMalloc() and cudaFree() is used. When the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set as a pointer to the above nvjpegDevAllocator_t structure, then this structure is used for allocating and releasing the device memory. The function prototypes for the memory allocation and memory freeing functions are similar to the cudaMalloc() and cudaFree() functions. They should return 0 in case of success, and non-zero otherwise. However, if the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaMalloc() and cudaFree() will be used. When using nvjpegCreateSimple() function to create library handle the default device memory allocator will be used. 2.2.11. nvJPEG Extended Device Memory Allocator Interface  typedef int ( * tDevMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tDevFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tDevMallocV2 dev_malloc ; tDevFreeV2 dev_free ; void * dev_ctx ; } nvjpegDevAllocatorV2_t ; Extended device allocators support stream ordered allocations along with user defined context information dev_ctx . When invoking the allocators, nvJPEG will pass dev_ctx as input to the extended device allocators. 2.2.12. nvJPEG Opaque JPEG Decoding State Handle  struct nvjpegJpegState ; typedef struct nvjpegJpegState * nvjpegJpegState_t ; The nvjpegJpegState structure stores the temporary JPEG information. It should be initialized before any usage. This JPEG state handle can be reused after being used in another decoding. The same JPEG handle should be used across the decoding phases for the same image or batch. Multiple threads are allowed to share the JPEG state handle only when processing same batch during first phase ( nvjpegDecodePhaseOne ) . 2.2.13. nvJPEG Opaque Library Handle Struct  struct nvjpegHandle ; typedef struct nvjpegHandle * nvjpegHandle_t ; The library handle is used in any consecutive nvJPEG library calls, and should be initialized first. The library handle is thread safe, and can be used by multiple threads simultaneously. 2.2.14. nvJPEG Output Pointer Struct  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t struct holds the pointers to the output buffers, and holds the corresponding strides of those buffers for the image decoding. See Single Image Decoding on how to set up the nvjpegImage_t struct. 2.2.15. nvJPEG Jpeg Encoding  typedef enum { NVJPEG_ENCODING_UNKNOWN = 0x0 , NVJPEG_ENCODING_BASELINE_DCT = 0xc0 , NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN = 0xc1 , NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN = 0xc2 , NVJPEG_ENCODING_LOSSLESS_HUFFMAN = 0xc3 } nvjpegJpegEncoding_t ; The nvjpegJpegEncoding_t enum lists the JPEG encoding types that are supported by the nvJPEG library The enum values are based on the markers defined in the JPEG specification Member Description NVJPEG_ENCODING_UNKNOWN This value is returned for all the JPEG markers not supported by the nvJPEG library. NVJPEG_ENCODING_BASELINE_DCT Corresponds to the JPEG marker 0xc0, refer to the JPEG spec for more details. NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN Corresponds to the JPEG marker 0xc1, refer to the JPEG spec for more details. NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN Corresponds to the JPEG marker 0xc2, refer to the JPEG spec for more details. NVJPEG_ENCODING_LOSSLESS_HUFFMAN Corresponds to the JPEG marker 0xc3, refer to the JPEG spec for more details. 2.2.16. nvJPEG Scale Factor  typedef enum { NVJPEG_SCALE_NONE = 0 , NVJPEG_SCALE_1_BY_2 = 1 , NVJPEG_SCALE_1_BY_4 = 2 , NVJPEG_SCALE_1_BY_8 = 3 } nvjpegScaleFactor_t ; The nvjpegScaleFactor_t enum lists all the scale factors supported by the library. This feature is supported when nvjpeg handles are intstaniated using NVJPEG_BACKEND_HARDWARE Member Description NVJPEG_SCALE_NONE Decoded output is not scaled NVJPEG_SCALE_1_BY_2 Decoded output width and height are scaled by a factor of 1/2 NVJPEG_SCALE_1_BY_4 Decoded output width and height are scaled by a factor of 1/4 NVJPEG_SCALE_1_BY_8 Decoded output width and height are scaled by a factor of 1/8 2.2.17. nvJPEG Flags  #define NVJPEG_FLAGS_DEFAULT 0 #define NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE 1 #define NVJPEG_FLAGS_ENABLE_MEMORY_POOLS   2 #define NVJPEG_FLAGS_BITSTREAM_STRICT      4 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE            8 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY 16 #define NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION   32 nvJPEG flags provide additional controls when initializing the library using nvJPEGCreateEx() or nvJPEGCreateExV2() . It is possible to combine the flags as they are bit fields. Member Description NVJPEG_FLAGS_DEFAULT Corresponds to default library behavior. NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE To be used when the library is initialized with NVJPEG_BACKEND_HARDWARE. It will be ignored for other back-ends. nvjpeg in batched decode mode buffers additional images to achieve optimal performance. Use this flag to disable buffering of additional images. NVJPEG_FLAGS_ENABLE_MEMORY_POOLS [Deprecated] Starting with CUDA 11.1 this flag will be ignored. NVJPEG_FLAGS_BITSTREAM_STRICT nvJPEG library will try to decode a bitstream even if it doesn’t strictly follow the JPEG specification. Using this flag will return an error in such cases. NVJPEG_FLAGS_REDUCED_MEMORY_DECODE When using NVJPEG_BACKEND_HYBRID or NVJPEG_BACKEND_GPU_HYBRID backends, enabling this flag will reduce the memory usage of the decoding whenever possible. NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY Using this flag enables zero-copy memory when feasible on supported platforms. NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION Using this flag enables the decoder to use interpolation when performing chroma upsampling during the YCbCr to RGB conversion stage. 2.2.18. nvJPEG Exif Orientation  typedef enum { NVJPEG_ORIENTATION_UNKNOWN = 0 , NVJPEG_ORIENTATION_NORMAL = 1 , NVJPEG_ORIENTATION_FLIP_HORIZONTAL = 2 , NVJPEG_ORIENTATION_ROTATE_180 = 3 , NVJPEG_ORIENTATION_FLIP_VERTICAL = 4 , NVJPEG_ORIENTATION_TRANSPOSE = 5 , NVJPEG_ORIENTATION_ROTATE_90 = 6 , NVJPEG_ORIENTATION_TRANSVERSE = 7 , NVJPEG_ORIENTATION_ROTATE_270 = 8 } nvjpegExifOrientation_t ; The nvjpegExifOrientation_t enum represents the exif orientation in a jfif(jpeg) file. Exif orientation information is typically used to denote the digital camera sensor orientation at the time of image capture. Member Description NVJPEG_ORIENTATION_UNKNOWN Exif orientation information is not available in the bitstream. NVJPEG_ORIENTATION_NORMAL Decode output remains unchanged. NVJPEG_ORIENTATION_FLIP_HORIZONTAL Decoded output should be mirrored/flipped horizontally. NVJPEG_ORIENTATION_ROTATE_180 Decoded output should be rotated 180 degrees. NVJPEG_ORIENTATION_FLIP_VERTICAL Decoded output should be mirrored/flipped vertically. NVJPEG_ORIENTATION_TRANSPOSE Decoded output should be flipped/mirrored horizontally followed by a 90 degrees counter-clockwise rotation. NVJPEG_ORIENTATION_ROTATE_90 Decoded output should be rotated 90 degrees counter-clockwise. NVJPEG_ORIENTATION_TRANSVERSE Decoded output should be flipped/mirrored horizontally followed by a 270 degrees counter-clockwise rotation. NVJPEG_ORIENTATION_ROTATE_270 Decoded output should be rotated 270 degrees counter-clockwise. 2.3. nvJPEG API Reference  This section describes the nvJPEG decoder API. 2.3.1. nvJPEG Helper API Reference  2.3.1.1. nvjpegGetProperty()  Gets the numeric value for the major or minor version, or the patch level, of the nvJPEG library. Signature: nvjpegStatus_t nvjpegGetProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. int *value Output Host The numeric value corresponding to the specific libraryPropertyType requested. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.2. nvjpegGetCudartProperty()  Gets the numeric value for the major version, minor version, or the patch level of the CUDA toolkit that was used to build nvJPEG library. For the same information on the nvJPEG library itself, see nvjpegGetProperty() . Signature: nvjpegStatus_t nvjpegGetCudartProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. int *value Output Host The numeric value corresponding to the specific libraryPropertyType requested. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.3. nvjpegCreate() [DEPRECATED]  Allocates and initializes the library handle. Note This function is deprecated. Use either nvjpegCreateSimple() or nvjpegCreateEx() functions to create the library handle. Signature: nvjpegStatus_t nvjpegCreate ( nvjpegBackend_t backend , nvjpegDevAllocator_t * allocator , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocator_t *allocator Input Host Device memory allocator. See nvjpegDevAllocator_t structure description. If NULL is provided, then the default CUDA runtime cudaMalloc() and cudaFree() functions will be used. nvjpegHandle_t *handle Input/Output Host The library handle. The nvjpegBackend_t parameter is an enum type, with the below enumerated list values: typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , } nvjpegBackend_t ; Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.4. nvjpegCreateSimple()  Allocates and initializes the library handle, with default codec implementations selected by library and default memory allocators. Signature: nvjpegStatus_t nvjpegCreateSimple ( nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t *handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.5. nvjpegCreateEx()  Allocates and initializes the library handle using the provided arguments. Signature: nvjpegStatus_t nvjpegCreateEx ( nvjpegBackend_t backend , nvjpegDevAllocator_t * dev_allocator , nvjpegPinnedAllocator_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocator_t *dev_allocator Input Host Device memory allocator. See nvjpegDevAllocator_t structure description. If NULL is provided, then the default CUDA runtime functions cudaMalloc() and cudaFree() will be used. nvjpegPinnedAllocator_t *pinned_allocator Input Host Pinned host memory allocator. See nvjpegPinnedAllocator_t structure description. If NULL is provided, then the default CUDA runtime functions cudaHostAlloc() and cudaFreeHost() will be used. unsigned int flags Input Host Refer to nvJPEG Flags for details. nvjpegHandle_t *handle Input/Output Host The library handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.6. nvjpegCreateExV2()  Allocates and initializes the library handle using the provided arguments. Signature: nvjpegStatus_t nvjpegCreateExV2 ( nvjpegBackend_t backend , nvjpegDevAllocatorV2_t * dev_allocator , nvjpegPinnedAllocatorV2_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocatorV2_t *dev_allocator Input Host Extended device memory allocator. See nvjpegDevAllocatorV2_t_t structure description. Cannot be NULL. nvjpegPinnedAllocatorV2_t *pinned_allocator Input Host Extended pinned memory allocator. See nvjpegPinnedAllocatorV2_t structure description. Cannot be NULL. unsigned int flags Input Host Refer to nvJPEG Flags for details. nvjpegHandle_t *handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.7. nvjpegDestroy()  Releases the library handle. Signature: nvjpegStatus_t nvjpegDestroy ( nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input/Output Host The library handle to release. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.8. nvjpegSetDeviceMemoryPadding()  Use the provided padding for all device memory allocations with specified library handle. A large number will help to amortize the need for device memory reallocations when needed. Signature: nvjpegStatus_t nvjpegSetDeviceMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Device memory padding to use for all further device memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.9. nvjpegGetDeviceMemoryPadding()  Retrieve the device memory padding that is currently used for the specified library handle. Signature: nvjpegStatus_t nvjpegGetDeviceMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Device memory padding that is currently used for device memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.10. nvjpegSetPinnedMemoryPadding()  Use the provided padding for all pinned host memory allocations with specified library handle. A large number will help to amortize the need for pinned host memory reallocations when needed. Signature: nvjpegStatus_t nvjpegSetPinnedMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Pinned host memory padding to use for all further pinned host memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.11. nvjpegGetPinnedMemoryPadding()  Retrieve the pinned host memory padding that is currently used for specified library handle. Signature: nvjpegStatus_t nvjpegGetPinnedMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Pinned host memory padding that is currently used for pinned host memory allocations. nvjpegHandle_t handle Input/Output Host The library handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.12. nvjpegGetHardwareDecoderInfo()  Retrieve hardware decoder details such as number of engines and number of cores available in each engine. Signature: nvjpegStatus_t nvjpegGetHardwareDecoderInfo ( nvjpegHandle_t handle , unsigned int * num_engines , unsigned int * num_cores_per_engine ); Parameters: nvjpegHandle_t handle Input Host The library handle. unsigned int* num_engines Input/Output Host Retrieves number of engines available for decode. Return value of 0 indicates that hardware decoder is not available. unsigned int* num_cores_per_engine Input/Output Host Retrieves number of cores per engine. Return value of 0 indicates that hardware decoder is not available. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.13. nvjpegJpegStateCreate()  Allocates and initializes the internal structure required for the JPEG processing. Signature: nvjpegStatus_t nvjpegJpegStateCreate ( nvjpegHandle_t handle , nvjpegJpegState_t * jpeg_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t *jpeg_handle Input/Output Host The image state handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.14. nvjpegJpegStateDestroy()  Releases the image internal structure. Signature: nvjpegStatus_t nvjpegJpegStateDestroy ( nvjpegJpegState handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState handle Input/Output Host The image state handle. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.15. nvjpegDecoderCreate()  Creates a decoder handle. Signature: nvjpegStatus_t nvjpegDecoderCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegBackend_t implementation , nvjpegJpegDecoder_t * decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle. nvjpegBackend_t backend Input Host Backend parameter for the decoder_handle.The back end applies to all the functions under the decoupled API , when called with this handle. nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder state handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.16. nvjpegDecoderDestroy()  Destroys the decoder handle. Signature: nvjpegStatus_t nvjpegDecoderDestroy ( nvjpegJpegDecoder_t decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.17. nvjpegDecoderJpegSupported()  Determines whether the decoder_handle is able to handle the bit-stream stored in jpeg_stream . Signature: nvjpegStatus_t nvjpegDecoderJpegSupported ( nvjpegJpegDecoder_t decoder_handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input Host Decoder state handle nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data nvjpegDecodeParams_t decode_params Input Host Decoder output configuration int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.18. nvjpegDecoderStateCreate()  Creates the decoder_state internal structure. The decoder_state is associated with the nvjpegBackend_t implementation that was used to create the decoder_handle . Signature: nvjpegStatus_t nvjpegDecoderStateCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegJpegDecoder_t decoder_handle , nvjpegJpegState_t * decoder_state ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle. nvjpegJpegDecoder_t decoder_handle Input Host Decoder handle. nvjpegJpegState_t* decoder_state Input/Output Host nvJPEG Image State Handle. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.19. nvjpegJpegStreamCreate()  Creates jpeg_stream that is used to parse the JPEG bitstream and store bitstream parameters. Signature: nvjpegStatus_t nvjpegJpegStreamCreate ( nvjpegHandle_t handle , nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.20. nvjpegJpegStreamDestroy()  Destroys the jpeg_stream structure. Signature: nvjpegStatus_t nvjpegJpegStreamDestroy ( nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.21. nvjpegBufferPinnedCreate()  Creates a pinned buffer handle. Signature: nvjpegStatus_t nvjpegBufferPinnedCreate ( nvjpegHandle_t handle , nvjpegPinnedAllocator_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegPinnedAllocator_t* pinned_allocator Input Host Pinned host memory allocator. See nvjpegPinnedAllocator_t structure description. nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.22. nvjpegBufferPinnedCreateV2()  Creates a pinned buffer handle using extended allocators. Signature: nvjpegStatus_t nvjpegBufferPinnedCreateV2 ( nvjpegHandle_t handle , nvjpegPinnedAllocatorV2_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegPinnedAllocatorV2_t* pinned_allocator Input Host Extended pinned host memory allocator. See nvjpegPinnedAllocatorV2_t structure description. nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.23. nvjpegBufferPinnedDestroy()  Destroys a pinned buffer handle. Signature: nvjpegStatus_t nvjpegBufferPinnedDestroy ( nvjpegBufferPinned_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.24. nvjpegStateAttachPinnedBuffer()  Link the nvJPEG pinned buffer handle to decoder_state . The pinned_buffer is used by the decoder to store the intermediate information that is used across the decoding stages. Pinned buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory. Signature: nvjpegStatus_t nvjpegStateAttachPinnedBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferPinned_t pinned_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state. nvjpegBufferPinned_t pinned_buffer Input Host nvJPEG pinned buffer container. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.25. nvjpegBufferPinnedRetrieve()  Retrieves the pinned memory pointer and size from the nvJPEG pinned buffer handle. Allows the application to re-use the memory once the decode is complete. Signature: nvjpegStatus_t nvjpegBufferPinnedRetrieve ( nvjpegBufferPinned_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container. size_t* size Input/Output Host Size in bytes of the pinned buffer. void** ptr Input/Output Host Pointer to the pinned buffer. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.26. nvjpegBufferPinnedResize()  Resize the pinned buffer to the specified size in bytes. This API can be used to pre-allocate the pinned buffer\nto a large value and avoid allocator calls during decode. Signature: nvjpegStatus_t nvjpegBufferPinnedResize ( nvjpegBufferPinned_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container. size_t* size Input Host Size in bytes of the pinned buffer. cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferPinned_t buffer is initialized using stream ordered allocators. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.27. nvjpegBufferDeviceCreate()  Creates the device buffer handle. Signature: nvjpegStatus_t nvjpegBufferDeviceCreate ( nvjpegHandle_t handle , nvjpegDevAllocator_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDevAllocator_t* device_allocator Input Host Device memory allocator. See the `nvjpegDevAllocator_t <index.html#nvjpeg-memory-allocator-interface>`__ structure description. nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.28. nvjpegBufferDeviceCreateV2()  Creates the device buffer handle using extended allocators. Signature: nvjpegStatus_t nvjpegBufferDeviceCreateV2 ( nvjpegHandle_t handle , nvjpegDevAllocatorV2_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDevAllocatorV2_t* device_allocator Input Host Extended device memory allocator. See nvjpegDevAllocatorV2_t_t structure description. nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container. Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.29. nvjpegBufferDeviceDestroy()  Destroys the device buffer handle. Signature: nvjpegStatus_t nvjpegBufferDeviceDestroy ( nvjpegBufferDevice_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host/Device nvJPEG device buffer container. Device pointers are stored within the host structures. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.30. nvjpegStateAttachDeviceBuffer()  Link the nvJPEG device buffer handle to the decoder_state . The device_buffer is used by the decoder to store the intermediate information that is used across the decoding stages. Device buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory. Signature: nvjpegStatus_t nvjpegStateAttachDeviceBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferDevice_t device_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state. nvjpegBufferDevice_t device buffer Input Host/Device nvJPEG device buffer container. Device pointers are stored within the host structures. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.31. nvjpegBufferDeviceRetrieve()  Retrieve the device memory pointer and size from the nvJPEG device buffer handle. Allows the application to re-use the memory after the decode is complete. Signature: nvjpegStatus_t nvjpegBufferDeviceRetrieve ( nvjpegBufferDevice_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container. size_t* size Input/Output Host Device buffer size in bytes. void** ptr Input/Output Host Pointer to the device buffer. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.32. nvjpegBufferDeviceResize()  Resize the device buffer to the specified size in bytes. This API can be used to pre-allocate the device buffer\nto a large value and avoid allocator calls during decode. Signature: nvjpegStatus_t nvjpegBufferDeviceResize ( nvjpegBufferDevice_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container. size_t* size Input Host Size in bytes of the device buffer. cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferDevice_t buffer is initialized using stream ordered allocators. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.33. nvjpegDecodeParamsCreate()  Creates a handle for the parameters. The parameters that can be programmed include: output format, ROI decode, CMYK to RGB conversion. Signature: nvjpegStatus_t nvjpegDecodeParamsCreate ( nvjpegHandle_t handle , nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.34. nvjpegDecodeParamsDestroy()  Destroys the decode_params handle. Signature: nvjpegStatus_t nvjpegDecodeParamsDestroy ( nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2. Retrieve Encoded Image Information API  The helper functions for retrieving the encoded image information. 2.3.2.1. nvjpegGetImageInfo()  Decodes the JPEG header and retrieves the basic information about the image. Signature: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the encoded data. size_t length Input Host Size of the encoded data in bytes. int *nComponents Output Host Chroma subsampling for the 1- or 3- channel encoding. int *widths Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the width of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. If the channel is not encoded, then the corresponding value would be zero. int *heights Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the height of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. If the channel is not encoded, then the corresponding value would be zero. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2. nvJPEG Stream API  These functions store the parsed bit-stream data on the host. 2.3.2.2.1. nvjpegJpegStreamParse()  Parses the bitstream and stores the metadata in the jpeg_stream struct. Signature: nvjpegStatus_t nvjpegJpegStreamParse ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int save_metadata , int save_stream , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the bit-stream. size_t length Input Host Bit-stream size. int save_metadata Input Host (Not enabled. Marked for future use). If not 0, then the JPEG stream metadata (headers, app markers, etc.) will be saved in the internal JpegStream structure for future usage.\nIf 0, then the meta data (headers, app markerms etc.) will be discarded. int save_stream Input Host If not 0, then the whole jpeg stream will be copied to the internal JpegStream structure, and the pointer to the JPEG file data will not be needed after this call.\nIf 0, then JpegStream will just save the pointers (to JPEG file data), and these pointers will be used later during the image decoding. nvjpegJpegStream_t jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.2. nvjpegJpegStreamParseHeader()  Parses only the header of the bit-stream and stores the header information in the jpeg_stream struct. Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the bit-stream. size_t length Input Host Bit-stream size. nvjpegJpegStream_t jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.3. nvjpegJpegStreamParseTables()  To be used when decoding TIFF files with JPEG compression. Parses the JPEG tables bitstream and stores the jpeg tables in jpeg_stream Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. const unsigned char *data Input Host Pointer to the JPEG tables bitstream. Can be set to NULL to reset the JPEG tables. size_t length Input Host JPEG tables bitstream size. nvjpegJpegStream_t jpeg_stream Input/Output Host The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.4. nvjpegJpegStreamGetFrameDimensions()  Extracts the JPEG frame dimensions from the bitstream. Signature: nvjpegStatus_t nvjpegJpegStreamGetFrameDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int * width , unsigned int * height ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int* width Output Host Frame height. unsigned int* height Output Host Frame width. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.5. nvjpegJpegStreamGetComponentsNum()  Extracts the JPEG frame dimensions from the bitstream. Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentsNum ( nvjpegJpegStream_t jpeg_stream , unsigned int * components_num ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int* components_num Output Host Number of encoded channels in the input. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.6. nvjpegJpegStreamGetComponentDimensions()  Extracts the component dimensions from the bitstream. Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int component , unsigned int * width , unsigned int * height ) Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int component Input Host Component index. unsigned int* width Output Host Component height. unsigned int* height Output Host Component width. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.7. nvjpegJpegStreamGetChromaSubsampling()  Gets the chroma subsampling from the jpeg_stream . For grayscale (single channel) images it returns NVJPEG_CSS_GRAY. For 3-channel images it tries to assign one of the known chroma sub-sampling values based on the sampling information present in the bitstream, else it returns NVJPEG_CSS_UNKNOWN. If the number of channels is 2 or 4, then it returns NVJPEG_CSS_UNKNOWN. Signature: nvjpegStatus_t nvjpegJpegStreamGetChromaSubsampling ( nvjpegJpegStream_t jpeg_stream , nvjpegChromaSubsampling_t * chroma_subsampling ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. nvjpegChromaSubsampling_t* chroma_subsampling Output Host Chroma subsampling for the 1- or 3- channel encoding. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.8. nvjpegJpegStreamGetJpegEncoding()  This function obtains the JPEG encoding type from the jpeg_stream . For baseline images it returns NVJPEG_ENCODING_BASELINE_DCT. For progressive images it returns NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN. Signature: nvjpegStatus_t nvjpegJpegStreamGetJpegEncoding ( nvjpegJpegStream_t jpeg_stream , nvjpegJpegEncoding_t * jpeg_encoding ); Parameters: Parameter Input / Output Memory Description jpeg_stream In Host Input bitstream handle. jpeg_encoding Out Host Encoding type obtained—baseline or progressive. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.9. nvjpegJpegStreamGetExifOrientation()  Extracts the exif orientation from the bitstream. Returns NVJPEG_ORIENTATION_UNKNOWN if the exif marker/orientation information is not present. Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetExifOrientation ( nvjpegJpegStream_t jpeg_stream , nvjpegExifOrientation_t * orientation_flag ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. nvjpegExifOrientation_t *orientation_flag Output Host Exif orientation in JPEG stream. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.2.2.10. nvjpegJpegStreamGetSamplePrecision()  Extracts the sample precision(bit depth) from the bitstream. Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetSamplePrecision ( nvjpegJpegStream_t jpeg_stream , unsigned int * precision ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle. unsigned int *precision Output Host Sample precision value. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3. Decode API—Single Phase  Functions for decoding single image or batched images in a single phase. 2.3.3.1. ​nvjpegDecode()  Decodes a single image, and writes the decoded image in the desired format to the output buffers. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. From CUDA 11 onwards, nvjpegDecode() picks the best available back-end for a given image, user no longer has control on this. If there is a need to select the back-end, then consider using nvjpegDecodeJpeg . This is a new API added in CUDA 11 which allows user to control the back-end. Signature: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. const unsigned char *data Input Host Pointer to the encoded data. size_t length Input Host Size of the encoded data in bytes. nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved. nvjpegImage_t *destination Input/Output Host/Device Pointer to the structure that describes the output destination. This structure should be on the host (CPU), but the pointers in this structure should be pointing to the device (i.e., GPU) memory. See nvjpegImage_t. cudaStream_t stream Input Host The CUDA stream where all of the GPU work will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.2. ​nvjpegDecodeBatchedInitialize()  This function initializes the batched decoder state. The initialization parameters include the batch size, the maximum number of CPU threads, and the specific output format in which the decoded image will be saved. This function should be called once, prior to decoding the batches of images. Any currently running batched decoding should be finished before calling this function. Signature: nvjpegStatus_t nvjpegDecodeBatchedInitialize ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int max_cpu_threads , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. int batch_size Input Host Batch size. int max_cpu_threads Input Host This parameter is no longer used by the library. nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.3. ​nvjpegDecodeBatched()  Decodes the batch of images, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. Signature: nvjpegStatus_t nvjpegDecodeBatched ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. const unsigned char *const *data Input Host Pointer to the first element of array of the input data. The size of the array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() batch initialization function. const size_t *lengths Input Host Pointer to the first element of array of input sizes. Size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function. nvjpegImage_t *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors. The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize(), the batch initialization function. See also nvjpegImage_t . cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.4. nvjpegDecodeBatchedEx()  This API helps to Decodes the batch of images with ROI, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. Signature: nvjpegStatus_t nvjpegDecodeBatchedEx ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , nvjpegDecodeParams_t * decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. const unsigned char *const *data Input Host Pointer to the first element of array of the input data. The size of the array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() batch initialization function. const size_t *lengths Input Host Pointer to the first element of array of input sizes. nvjpegImage_t *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors. The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function. See also nvjpegImage_t . nvjpegDecodeParams_t *decode_params Input Host Setting ROI Decode parameters cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.5. nvjpegDecodeBatchedSupported()  This API helps determine whether an image can be decoded by nvjpegDecodeBatched . User can parse the bitstream header using nvjpegJpegStreamParseHeader and then call this API to determine whether the image can be decoded. Signature: nvjpegStatus_t nvjpegDecodeBatchedSupported ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle. nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data. int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.6. nvjpegDecodeBatchedSupportedEx()  This API helps determine whether an image can be decoded by nvjpegDecodeBatchedEx . User can parse the bitstream header using nvjpegJpegStreamParseHeader and set the ROI in the decode params then call this API to determine whether the image can be decoded. Signature: nvjpegStatus_t nvjpegDecodeBatchedSupportedEx ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle. nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data. nvjpegDecodeParams_t decode_params Input Host Setting ROI Decode parameters. int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , a non zero value indicates that the bitstream is not supported. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.7. nvjpegDecodeBatchedPreAllocate()  This is an experimental API that can be used with nvjpegDecodeBatched . When decoding images with varying sizes and chroma subsampling, performance is limited by the repeated cuda calls made by the library to free/allocate device memory. This API attempts to avoid this problem by allocating device memory prior to the actual decoding. Users have the option to call this API with values that are unlikely to be exceeded when nvjpegDecodeBatched is called. Note Note:\nThis functionality is available only when the nvjpegHandle_t is instantiated using NVJPEG_BACKEND_HARDWARE. It is currently a No Op for other backends. This API only provides a hint for initial allocation. If the image dimensions at the time of decode exceed what was provided, then the library will resize the device buffers. If the images being decoded have different chroma subsamplings, then the chroma_subsampling field should be set to NVJPEG_CSS_444 to ensure that the device memory can be reused. Signature: nvjpegStatus_t nvjpegDecodeBatchedPreAllocate ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int width , int height , nvjpegChromaSubsampling_t chroma_subsampling , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input Host The image state handle. int batch_size Input Host Batch size. int width Input Host Maximum width of image that will be decoded. int height Input Host Maximum height of image that will be decoded. nvjpegChromaSubsampling_t chroma_subsampling Input Host Chroma-subsampling of the images. nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.3.8. nvjpegDecodeBatchedParseJpegTables()  To be used along with batched decode APIs when decoding JPEG bitstreams from a TIFF file. This function parses the JPEG tables bitstream to extract the JPEG tables. The external Huffman and quantization tables will be applied to all the JPEG bitstreams in the batch. Signature: nvjpegStatus_t nvjpegDecodeBatchedParseJpegTables ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , const size_t length ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegState_t jpeg_handle Input/Output Host/Device The image state handle. const unsigned char *data Input Host Pointer to the JPEG tables bitstream. Can be set to NULL to reset the jpeg tables. size_t length Input Host JPEG tables bitstream size. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4. Decode API—Decoupled Decoding  This set of decoding API works with the bitstream handles, decode parameter handles, pinned and device buffers handles as input, thus decoupling JPEG bitstream parse, buffer management and setting up decoder parameters from the decode process itself. Currently only multiphase decoding is available. Multiphase decoupled single image decoding consists of three phases: Host Mixed Device Each of the above decodings is carried on according to its individual semantics. Phases on different images can be carried out with different decoding state handles simultaneously, while sharing of some helper objects is possible. See the details of semantics in the individual phases descriptions. Below are a couple of examples of using decoupled API. The following snippet explains how to use the API to prefetch the host stage of the processing: first do all of the host work on the host, and then submit the rest of decoding work to the device. #define BATCH_SIZE 2 nvjpegHandle_t nvjpeg_handle ; nvjpegJpegState_t nvjpeg_decoder_state [ BATCH_SIZE ]; nvjpegBufferPinned_t nvjpeg_pinned_buffer [ BATCH_SIZE ]; nvjpegBufferDevice_t nvjpeg_device_buffer ; nvjpegJpegStream_t nvjpeg_jpeg_stream [ BATCH_SIZE ]; nvjpegDecodeParams_t nvjpeg_decode_params ; nvjpegJpegDecoder_t nvjpeg_decoder ; nvjpegBackend_t impl = NVJPEG_BACKEND_DEFAULT ; unsigned char * bitstream [ BATCH_SIZE ] // pointers jpeg bitstreams size_t length [ BATCH_SIZE ]; // bitstream sizes nvjpegImage_t output_images [ BATCH_SIZE ]; // all the images in the batch will be decoded as RGBI nvjpegDecodeParamsSetOutputFormat ( nvjpeg_decode_params , NVJPEG_OUTPUT_RGBI ); // call host phase for two bitstreams for ( int i = 0 ; i < BATCH_SIZE ; i ++ ) { nvjpegJpegStreamParse ( nvjpeg_handle , bitstream [ i ], length [ i ], 0 , 0 , nvjpeg_jpeg_stream [ i ]); nvjpegStateAttachPinnedBuffer ( nvjpeg_decoder_state [ i ], nvjpeg_pinned_buffer [ i ]); nvjpegDecodeJpegHost ( nvjpeg_handle , nvjpeg_decoder , nvjpeg_decoder_state [ i ], nvjpeg_decode_params , nvjpeg_jpeg_stream [ i ]) } for ( int i = 0 ; i < BATCH_SIZE ; i ++ ) { // same device buffer being used for decoding bitstreams nvjpegStateAttachDeviceBuffer ( nvjpeg_decoder_state [ i ], nvjpeg_device_buffer ); // cuda stream set to NULL nvjpegDecodeJpegTransferToDevice ( nvjpeg_handle , nvjpeg_decoder , nvjpeg_decoder_state [ i ], nvjpeg_jpeg_stream [ i ], NULL ); // cuda stream set to NULL nvjpegDecodeJpegDevice ( nvjpeg_handle , nvjpeg_decoder , nvjpeg_decoder_state [ i ], & output_images [ i ], NULL ); cudaDeviceSynchronize (); } The following snippet explains how pinned and device buffers can be shared across two instances of nvJPEG Decoder Handle . #define BATCH_SIZE 4 nvjpegHandle_t nvjpeg_handle ; nvjpegJpegDecoder_t nvjpeg_decoder_impl1 ; nvjpegJpegDecoder_t nvjpeg_decoder_impl2 ; nvjpegJpegState_t nvjpeg_decoder_state_impl1 ; nvjpegJpegState_t nvjpeg_decoder_state_impl2 ; nvjpegBufferPinned_t nvjpeg_pinned_buffer ; nvjpegBufferDevice_t nvjpeg_device_buffer ; nvjpegJpegStream_t nvjpeg_jpeg_stream ; nvjpegDecodeParams_t nvjpeg_decode_params ; unsigned char * bitstream [ BATCH_SIZE ] // pointers jpeg bitstreams size_t length [ BATCH_SIZE ]; // bitstream sizes // populate bitstream and length correctly for this code to work nvjpegImage_t output_images [ BATCH_SIZE ]; // allocate device memory for output images, for this snippet to work nvjpegStateAttachPinnedBuffer ( nvjpeg_decoder_state_impl1 , nvjpeg_pinned_buffer ); nvjpegStateAttachPinnedBuffer ( nvjpeg_decoder_state_impl2 , nvjpeg_pinned_buffer ); nvjpegStateAttachDeviceBuffer ( nvjpeg_decoder_state_impl1 , nvjpeg_device_buffer ); nvjpegStateAttachDeviceBuffer ( nvjpeg_decoder_state_impl2 , nvjpeg_device_buffer ); // all the images in the batch will be decoded as RGBI nvjpegDecodeParamsSetOutputFormat ( nvjpeg_decode_params , NVJPEG_OUTPUT_RGBI ); for ( int i = 0 ; i < BATCH_SIZE ; i ++ ) { nvjpegJpegStreamParse ( nvjpeg_handle , bitstream [ i ], length [ i ], 0 , 0 , nvjpeg_jpeg_stream ); // decide which implementation to use, based on image size unsigned int frame_width ; unsigned int frame_height ; nvjpegJpegStreamGetFrameDimensions ( nvjpeg_jpeg_stream , & frame_width , & frame_height )); nvjpegJpegDecoder_t & decoder = ( frame_height * frame_width > 1024 * 768 ) ? nvjpeg_decoder_impl2 : nvjpeg_decoder_impl1 ; nvjpegJpegState_t & decoder_state = ( frame_height * frame_width > 1024 * 768 ) ? nvjpeg_decoder_state_impl2 : nvjpeg_decoder_state_impl1 ; nvjpegDecodeJpegHost ( nvjpeg_handle , decoder , decoder_state , nvjpeg_decode_params , nvjpeg_jpeg_stream ); // cuda stream set to NULL nvjpegDecodeJpegTransferToDevice ( nvjpeg_handle , decoder , decoder_state , nvjpeg_jpeg_stream , NULL ); // cuda stream set to NULL nvjpegDecodeJpegDevice ( nvjpeg_handle , nvjpeg_decoder , decoder_state , & output_images , NULL ); cudaDeviceSynchronize (); } 2.3.4.1. nvjpegDecodeJpegHost()  This is the first stage of the decoupled decoding process. It is done entirely on the host, hence it is synchronous with respect to the host. If a pinned buffer is attached to the decoder state, then the pinned buffer object will be used to allocate the pinned memory required for the host decoding phase. There wouldn’t be allocation if the pinned buffer object already handles the required amount of pinned memory. If a pinned buffer object is not attached, then the state will use heap host memory to allocate the memory required for the host processing. In this phase, device is not participating. Hence the device selection, device initialization, and device memory initialization can be done later in the decoding process. This function works on a parsed stream. The parsed stream handle that is available after calling the nvjpegJpegStreamParse() function should be provided to this function. Signature: nnvjpegStatus_t nvjpegDecodeJpegHost ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegDecodeParams_t decode_params , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegDecodeParams_t decode_params Input Host Handle to decode the output properties. nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4.2. nvjpegDecodeJpegTransferToDevice()  This phase contains both host and device operations. Hence it is a mix of synchronous and asynchronous operations with respect to the host. All the device operations will be submitted to the provided stream. This phase should be called only after the host phase with the same decoder handle, decoder state handle and parsed jpeg stream handle. Device should be initialized and device buffer should be attached to decoder_state handle using nvjpegStateAttachDeviceBuffer() prior to calling this API. This device buffer object will be resized to the required amount of memory if needed. For the host memory buffer, this phase will use whatever was used in the host phase: either the attached pinned buffer or the state’s host memory buffer. Signature: nvjpegStatus_t nvjpegDecodeJpegTransferToDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4.3. nvjpegDecodeJpegDevice()  This phase consists of decode operations that take place mainly on the device (no significant host side computation is done). Hence this phase is asynchronous with respect to the host. This phase should be called after nvjpegDecodeJpegTransferToDevice() for a given decoder_state handle and decoder handle. In this function call, the host memory buffers are not used, so if the pinned buffer was attached to the state, then it can be reused somewhere else. Note that at this point the Jpeg stream handle is not needed anymore, since parts that are needed for device decoding will be copied to the device memory in the previous phase. Signature: nvjpegStatus_t nvjpegDecodeJpegDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegImage_t *destination Input/Output Host/Device Pointer to a structure that describes the output destination. This structure should be on host, but the pointers in this structure should be pointing to the device memory. See nvJPEG Image for details. cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.4.4. nvjpegDecodeJpeg()  This is a single phase API with the flexibility to select nvJPEG back-end when creating an nvjpegJpegDecoder_t object. The user has the option to call this API instead of making three separate calls to nvjpegDecodeJpegHost() , nvjpegDecodeJpegTransferToDevice() , and nvjpegDecodeJpegDevice() . It is required to atttach the device buffer to the decoder state before calling this API. The pinned buffer is optional. If the pinned buffer is not attached, then heap memory will be used for host processing. This function works on a parsed stream. The parsed stream handle that is available after calling the nvjpegJpegStreamParse() function should be provided to this function. Signature: nvjpegStatus_t nvjpegDecodeJpeg ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_bitstream , nvjpegImage_t * destination , nvjpegDecodeParams_t decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. nvjpegJpegDecoder_t decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t decoder_state Input Host The nvJPEG decoder state handle. nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. nvjpegImage_t *destination Input/Output Host/Device Pointer to a structure that describes the output destination. This structure should be on the host, but the pointers in this structure should be pointing to the device memory. See nvJPEG Image for details. nvjpegDecodeParams_t decode_params Input Host The handle which stores the decode output properties. cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5. nvJPEG Decode Parameters  This category of APIs is used to set the decoding parameters. These APIs should be used with the decode APIs defined in Decode API—Decoupled Decoding . 2.3.5.1. nvjpegDecodeParamsSetOutputFormat()  This function is used to set the decode output format. See nvjpegOutputFormat_t described in step 6 of Single Image Decoding . The output parameter of nvjpegOutputFormat_t defaults to NVJPEG_OUTPUT_UNCHANGED if not set using this API. Signature: nvjpegStatus_t nvjpegDecodeParamsSetOutputFormat ( nvjpegDecodeParams_t decode_params , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. nvjpegOutputFormat_t output_format Input Host See step 6 of Single Image Decoding . Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.2. nvjpegDecodeParamsSetROI()  This function enables the region of interest-only (ROI-only) decode. To disable the ROI-only, i.e., to decode the whole image, set: offset_x = 0, offset_y = 0, roi_width = -1, and roi_height = -1. Note ROI decode is disabled by default. It is not supported when the nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE. The ROI window cannot go out of image bounds. That is: offset_x cannot be lower than zero, or offset_x + roi_width cannot be larger than the JPEG image width. If the output format is NVJPEG_OUTPUT_YUV or NVJPEG_OUTPUT_UNCHANGED, then the offset_x and offset_y values have to be multiples of the maximum subsampling factor, as defined in the JPEG standard. Signature: nvjpegStatus_t nvjpegDecodeParamsSetROI ( nvjpegDecodeParams_t decode_params , int offset_x , int offset_y , int roi_width , int roi_height ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host The decode output parameter handle. int offset_x Input Host Image offset along the horizontal direction relative to the top left corner. int offset_y Input Host Image offset along the vertical direction relative to the top left corner. int roi_width Input Host Image width relative to offset_x . int roi_height Input Host Image height relative to offset_y . Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.3. nvjpegDecodeParamsSetAllowCMYK()  If enabled, the nvJPEG library assumes that the JPEG with 4 encoded color components is in CMYK colorspace, and enables the conversion to RGB/YUV colorspace. The CMYK-to-RGB conversion is disabled by default. The conversion is based on the subtractive scheme—this behavior matches OpenCV’s handling of 4-component JPEGs. Signature: nvjpegStatus_t nvjpegDecodeParamsSetAllowCMYK ( nvjpegDecodeParams_t decode_params , int allow_cmyk ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. int allow_cmyk Input Host Enable CMYK to RGB conversion. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.4. nvjpegDecodeParamsSetScaleFactor()  Allows the user to scale decode output. Note This feature is currently supported only when nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE. Signature: nvjpegStatus_t nvjpegDecodeParamsSetScaleFactor ( nvjpegDecodeParams_t decode_params , nvjpegScaleFactor_t scale_factor ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. nvjpegScaleFactor_t scale_factor Input Host Set the scaling factor for the decode output. The scale factor is set to NVJPEG_SCALE_NONE by default. The supported values are listed here . When setting a scale factor value, the recommended allocation of the destination parameters is as follows: Use nvjpegGetImageInfo() , or nvjpegJpegStreamGetFrameDimensions() to extract the dimensions of each channel. Let height[NVJPEG_MAX_COMPONENT] and width[NVJPEG_MAX_COMPONENT] be 2 arrays which store the height and width. The index to these arrays correspond to the channel id. For a channel c, the scaled dimensions are calculated as follows: scaled_height[c] = (height[c] + rounding_factor - 1)/rounding_factor scaled_width[c] = (width[c] + rounding_factor - 1)/rounding_factor when scale_factor = NVJPEG_SCALE_NONE, rounding_factor = 1 when scale_factor = NVJPEG_SCALE_1_BY_2, rounding_factor = 2 when scale_factor = NVJPEG_SCALE_1_BY_4, rounding_factor = 4 when scale_factor = NVJPEG_SCALE_1_BY_8, rounding_factor = 8 For the output_format: NVJPEG_OUTPUT_Y destination.pitch[0] should be at least: width[0] destination.channel[0] should be at least of size: destination.pitch[0]*height[0] For the output_format destination.pitch[c] should be at least: destination.channel[c] should be at least of size: NVJPEG_OUTPUT_YUV width[c] for c = 0, 1, 2 destination.pitch[c]*height[c] for c = 0, 1, 2 NVJPEG_OUTPUT_RGB and NVJPEG_OUTPUT_BGR width[0] for c = 0, 1, 2 destination.pitch[0]*height[0] for c = 0, 1, 2 NVJPEG_OUTPUT_RGBI and NVJPEG_OUTPUT_BGRI width[0]*3 destination.pitch[0]*height[0] NVJPEG_OUTPUT_UNCHANGED width[c] for c = [ 0, nComponents - 1 ] destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.5.5. nvjpegDecodeParamsSetExifOrientation()  This function is used to generate the decoded output based on the exif orientation parameter. When ExifOrientation is enabled, the output buffers should be allocated based on the rotated dimensions. If the orientation is set as NVJPEG_ORIENTATION_UNKNOWN , the library will default to NVJPEG_ORIENTATION_HORIZONTAL . ROI Decode and EXIF rotation Exif rotation and ROI Decode can be enabled together. The ROI coordinates should be in the rotated space. Signature: nvjpegStatus_t nvjpegDecodeParamsSetExifOrientation ( nvjpegDecodeParams_t decode_params , nvjpegExifOrientation_t orientation ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle. nvjpegExifOrientation_t orientation Input Host Set the exif orientation for the decode output. Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.6. nvJPEG API Return Codes  The nvJPEG API adheres to the following return codes and their indicators: typedef enum { NVJPEG_STATUS_SUCCESS = 0 , NVJPEG_STATUS_NOT_INITIALIZED = 1 , NVJPEG_STATUS_INVALID_PARAMETER = 2 , NVJPEG_STATUS_BAD_JPEG = 3 , NVJPEG_STATUS_JPEG_NOT_SUPPORTED = 4 , NVJPEG_STATUS_ALLOCATOR_FAILURE = 5 , NVJPEG_STATUS_EXECUTION_FAILED = 6 , NVJPEG_STATUS_ARCH_MISMATCH = 7 , NVJPEG_STATUS_INTERNAL_ERROR = 8 , NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED = 9 } nvjpegStatus_t ; Description of the returned error codes: Returned Error (Returned Code) Description NVJPEG_STATUS_SUCCESS (0) The API call has finished successfully. Note that many of the calls are asynchronous and some of the errors may be seen only after synchronization. NVJPEG_STATUS_NOT_INITIALIZED (1) The library handle was not initialized. A call to nvjpegCreate() is required to initialize the handle. NVJPEG_STATUS_INVALID_PARAMETER (2) Wrong parameter was passed. For example, a null pointer as input data, or an image index not in the allowed range. NVJPEG_STATUS_BAD_JPEG (3) Cannot parse the JPEG stream. Check that the encoded JPEG stream and its size parameters are correct. NVJPEG_STATUS_JPEG_NOT_SUPPORTED (4) Attempting to decode a JPEG stream that is not supported by the nvJPEG library. NVJPEG_STATUS_ALLOCATOR_FAILURE (5) The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code. NVJPEG_STATUS_EXECUTION_FAILED (6) Error during the execution of the device tasks. NVJPEG_STATUS_ARCH_MISMATCH (7) The device capabilities are not enough for the set of input parameters provided (input parameters such as backend, encoded stream parameters, output format). NVJPEG_STATUS_INTERNAL_ERROR (8) Error during the execution of the device tasks. NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED (9) Not supported. NVJPEG_STATUS_INCOMPLETE_BITSTREAM (10) Bitstream input data incomplete 2.3.7. nvJPEG Chroma Subsampling  One of the outputs of the nvjpegGetImageInfo() API is nvjpegChromaSubsampling_t . This parameter is an enum type, and its enumerator list comprises of the chroma subsampling property retrieved from the encoded JPEG image. The nvjpegGetImageInfo() function currently supports the following chroma subsampling types: typedef enum { NVJPEG_CSS_444 , NVJPEG_CSS_422 , NVJPEG_CSS_420 , NVJPEG_CSS_440 , NVJPEG_CSS_411 , NVJPEG_CSS_410 , NVJPEG_CSS_GRAY , NVJPEG_CSS_410V , NVJPEG_CSS_UNKNOWN } nvjpegChromaSubsampling_t ; 2.3.8. Reference Documents  Refer to the JPEG standard: https://jpeg.org/jpeg/ 2.4. Examples of nvJPEG  nvJPEG Decode sample can be found here: https://github.com/NVIDIA/CUDALibrarySamples/tree/master/nvJPEG/nvJPEG-Decoder 3. JPEG Encoding  This section describes the encoding functions of the nvJPEG Library. 3.1. Using the Encoder  The user should perform the below prerequisite steps before calling the nvJPEG encoding functions. See also nvJPEG Encoder Helper API Reference . 3.1.1. Encoding the Parameters  The user should create an encoding parameters structure with nvjpegEncoderParamsCreate() function. The function will be initialized with default parameters. User can use an appropriate nvjpegEncoderParamsSet*() function to set a specific parameter. The quality parameter can be set, using the nvjpegEncoderParamsSetQuality() function, to an integer value between 1 and 100, and this quality parameter will be used as a base for generating the JPEG quantization tables. Note Occasionally, when encoding high entropy input data, such as random images, the encoding can fail if the quality parameter is set too high. This is due to the fact that the compressed bitstream would be larger than the input image. We recommend restarting the encoding with slightly lower quality factor or using a real-world images if possible. The parameters structure should be passed to compression functions. Note The encoding parameters structure can be reused to compress multiple images simultaneously, but no changes to the parameters should be made during the ongoing encoding, or the encoding result will be undefined. 3.1.2. Encoding the State  The user should create the encoding state structure using nvjpegEncoderStateCreate() function. This function will hold intermediate buffers for the encoding process. This state should be passed to the compression functions. Note The encoding state structure can be reused to encode a series of images, but no encoding should be performed on multiple images with the same encoding state at the same time—otherwise the result of the encodings will be undefined. 3.1.3. Encoding the Image  The nvJPEG library provides a few interfaces for compressing the image in different formats and colorspaces. See below. 3.1.3.1. nvjpegEncodeYUV  Input for this function is an image in YUV colorspace. See nvjpegEncodeYUV() . The source argument should be filled with the corresponding YUV planar data. The chroma_subsampling argument should have the chroma subsampling of the input data. If the chroma subsampling in the encoding parameters is the same as input chroma subsampling, then the user’s input data will be directly used in the JPEG compression. Otherwise chroma will be resampled to match the chroma subsampling of the encoding parameters. Input data should be provided with respect to the subsampling factors. That is, the chrominance image planes should have sizes aligned to the corresponding subsamplings. For example: Image dimensions: 123x321 Input chroma subsampling: NVJPEG_CSS_410 Chroma subsampling factor for this chroma subsampling: 4x2 Given the above, the encoder library expects the user to provide: Y plane with size: 123 x 321 Cb and Cr plane with size: 31 x 161 3.1.3.2. nvjpegEncodeImage  See nvjpegEncodeImage() . Input for this function, i.e., how data should be provided in the source argument, is determined by the input_format argument. For the interleaved formats (ending with I ) only the first channel is used. For the non-interleaved formats, all the channels in the input format are used. For example, if the user has interleaved the RGB image of size W x H , stored continuously, and the pointer to it is pImage , then source should be: source.channel[0] = pImage source.pitch[0] = W*3 When the same image is stored in planar format, with image planes pointers stored continuously in the array pImage[3] , then source should be: source.channel[0] = pImage[0] source.channel[1] = pImage[1] source.channel[2] = pImage[2] The pitch values for each channel in the source parameter should be set accordingly to the data layout. The nvJPEG library will perform the color transformation to the YCbCr, and will compress the result. 3.1.4. Retrieving the Compressed Stream  Often it is not feasible to accurately predict the final compressed data size of the final JPEG stream for any input data and parameters. The nvJPEG library, while encoding, will calculate the size of the final stream, allocate temporary buffer in the encoder state and save the compressed data in the encoding state’s buffer. In order to get final compressed JPEG stream, the user should provide the memory buffer large enough to store this compressed data. There are two options for how to do this: Use the upper bound on compressed JPEG stream size for the given parameters and image dimensions: Use the nvjpegEncodeRetrieveBitstream() function to retrieve the maximum possible JPEG stream size at any given time. Allocate the memory buffer at any given time. Encode the image using one of the encoding functions. Retrieve the compressed JPEG stream from the encoder state after successful encoding, using the nvjpegEncodeRetrieveBitstream() and the allocated buffer. Wait for the encoding to complete, and retrieve the exact size of required buffer, as below: Encode the image using one of the encoding functions. Use the nvjpegEncodeRetrieveBitstream() function to retrieve the size in bytes of the compressed JPEG stream. Allocate the memory buffer of at least this size. Use the nvjpegEncodeRetrieveBitstream() function to populate your buffer with the compressed JPEG stream. Note As the same encoding image state can be reused to compress a series of images, the nvjpegEncodeRetrieveBitstream() function will return the result for the last compressed image. 3.1.5. JPEG Encoding Example  See below the example code, and the block diagram shown in Figure 1 , for encoding with nvJPEG Encoder. JPEG Encoding Using nvJPEG Encoder  nvjpegHandle_t nv_handle ; nvjpegEncoderState_t nv_enc_state ; nvjpegEncoderParams_t nv_enc_params ; cudaStream_t stream ; // initialize nvjpeg structures nvjpegCreateSimple ( & nv_handle ); nvjpegEncoderStateCreate ( nv_handle , & nv_enc_state , stream ); nvjpegEncoderParamsCreate ( nv_handle , & nv_enc_params , stream ); nvjpegImage_t nv_image ; // Fill nv_image with image data, let's say 640x480 image in RGB format // Compress image nvjpegEncodeImage ( nv_handle , nv_enc_state , nv_enc_params , & nv_image , NVJPEG_INPUT_RGB , 640 , 480 , stream ); // get compressed stream size size_t length ; nvjpegEncodeRetrieveBitstream ( nv_handle , nv_enc_state , NULL , & length , stream ); // get stream itself cudaStreamSynchronize ( stream ); std :: vector < char > jpeg ( length ); nvjpegEncodeRetrieveBitstream ( nv_handle , nv_enc_state , jpeg . data (), & length , 0 ); // write stream to file cudaStreamSynchronize ( stream ); std :: ofstream output_file ( \"test.jpg\" , std :: ios :: out | std :: ios :: binary ); output_file . write ( jpeg . data (), length ); output_file . close (); 3.2. nvJPEG Encoder Type Declarations  This section describes the nvJPEG Encoder Type Declarations. 3.2.1. nvjpegInputFormat_t  typedef enum { NVJPEG_INPUT_RGB = 3 , NVJPEG_INPUT_BGR = 4 , NVJPEG_INPUT_RGBI = 5 , NVJPEG_INPUT_BGRI = 6 } nvjpegInputFormat_t ; The nvjpegInputFormat_t enum is used to select the color model and pixel format of the input image. It is used for conversion to YCbCr during encoding. Member Description NVJPEG_INPUT_RGB Input image is in RGB color model. Pixel format is RGB. NVJPEG_INPUT_BGR Input image is in RGB color model. Pixel format is BGR. NVJPEG_INPUT_RGBI Input image is in RGB color model. Pixel format is interleaved RGB. NVJPEG_INPUT_BGRI Input image is in RGB color model. Pixel format is interleaved BGR. 3.2.2. nvjpegEncoderState_t  The nvjpegEncoderState_t structure stores intermediate buffers and variables used for compression. 3.2.3. nvjpegEncoderParams_t  The nvjpegEncoderParams_t structure stores JPEG encode parameters. 3.3. nvJPEG Encoder Helper API Reference  The nvJPEG Encoder helper functions are used for initializing. 3.3.1. nvjpegEncoderStateCreate()  Creates encoder state that stores intermediate buffers used in compression. Signature: nvjpegStatus_t nvjpegEncoderStateCreate ( nvjpegHandle_t handle , nvjpegEncoderState_t * encoder_state , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_state Output Host Pointer to the encoder state structure, where the new state will be placed. stream Inputt Host CUDA stream where all the required device operations will be placed. 3.3.2. nvjpegEncoderStateDestroy()  Destroys the encoder state. Signature: nvjpegStatus_t nvjpegEncoderStateDestroy ( nvjpegEncoderState_t encoder_state ); Parameters: Parameter Input / Output Memory Description encoder_state Input/Output Host Encoder state structure that will be released. 3.3.3. nvjpegEncoderParamsCreate()  Creates the structure that holds the compression parameters. Signature: nvjpegStatus_t nvjpegEncoderParamsCreate ( nvjpegHandle_t handle , nvjpegEncoderParams_t * encoder_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_params Output Host Pointer to the location where the new parameters structure will be placed. stream Inputt Host CUDA stream where all the required device operations will be placed. 3.3.4. nvjpegEncoderParamsDestroy()  Destroys the encoder parameters structure. Signature: nvjpegEncoderParamsDestroy ( nvjpegEncoderParams_t encoder_params ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder params structure that will be released. 3.3.5. nvjpegEncoderParamsSetEncoding()  Sets the parameter quality in the encoder parameters structure. Signature: nvjpegStatus_t nvjpegEncoderParamsSetEncoding ( nvjpegEncoderParams_t encoder_params , nvjpegJpegEncoding_t etype , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. etype Input Host Encoding type selection (Baseline/Progressive). Default is Baseline. stream Input Host CUDA stream where all the required device operations will be placed. 3.3.6. nvjpegEncoderParamsSetQuality()  Sets the parameter quality in the encoder parameters structure. Signature: nvjpegStatus_t nvjpegEncoderParamsSetQuality ( nvjpegEncoderParams_t encoder_params , const int quality , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameterss structure handle. quality Input Host Integer value of quality between 1 and 100, where 100 is the highest quality. Default value is 70. stream Input Host CUDA stream where all the required device operations will be placed. 3.3.7. nvjpegEncoderParamsSetOptimizedHuffman()  Sets whether or not to use optimized Huffman. Using optimized Huffman produces smaller JPEG bitstream sizes with the same quality, but with slower performance. Signature: nvjpegStatus_t nvjpegEncoderParamsSetOptimizedHuffman ( nvjpegEncoderParams_t encoder_params , const int optimized , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. optimized Input Host If this value is 0 then non-optimized Huffman will be used. Otherwise optimized version will be used. Default value is 0. stream Input Host CUDA stream where all the required device operations will be placed. 3.3.8. nvjpegEncoderParamsSetSamplingFactors()  Sets which chroma subsampling will be used for JPEG compression. Signature: nvjpegStatus_t nvjpegEncoderParamsSetSamplingFactors ( nvjpegEncoderParams_t encoder_params , const nvjpegChromaSubsampling_t chroma_subsampling , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. chroma_subsampling Input Host Chroma subsampling that will be used for JPEG compression. If the input is in YUV color model and chroma_subsampling is different from the subsampling factors of source image, then the NVJPEG library will convert subsampling to the value of chroma_subsampling . Default value is 4:4:4. stream Input Host CUDA stream where all the required device operations will be placed. 3.4. nvJPEG Encoder API Reference  This section describes the nvJPEG Encoder API. 3.4.1. nvjpegEncodeGetBufferSize()  Returns the maximum possible buffer size that is needed to store the compressed JPEG stream, for the given input parameters. Signature: nvjpegStatus_t nvjpegEncodeGetBufferSize ( nvjpegHandle_t handle , const nvjpegEncoderParams_t encoder_params , int image_width , int image_height , size_t * max_stream_length ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_params Input/Output Host Encoder parameters structure handle. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.2. nvjpegEncodeYUV()  Compresses the image in YUV colorspace to JPEG stream using the provided parameters, and stores it in the state structure. Signature: nvjpegStatus_t nvjpegEncodeYUV ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegChromaSubsampling_t chroma_subsampling , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream. encoder_params Input Host Encoder parameters structure handle. source Input Host Pointer to the nvjpeg structure that holds the device pointers to the Y, U(Cb) and V(Cr) image planes and the respective strides. chroma_subsampling Input Host Chroma subsampling of the input data. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.3. nvjpegEncodeImage()  Compresses the image in the provided format to the JPEG stream using the provided parameters, and stores it in the state structure. Signature: nvjpegStatus_t nvjpegEncodeImage ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegInputFormat_t input_format , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream. encoder_params Input Host Encoder parameters structure handle. source Input Host Pointer to the nvjpeg structure that holds the device pointers to the Y, U(Cb) and V(Cr) image planes and the respective strides. input_format Input Host Value of nvjpegInputFormat_t type that describes the input data. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.4. nvjpegEncodeRetrieveBitstream()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. If data parameter is NULL then the encoder will return compressed stream size in the length parameter. If data is not NULL then the provided length parameter should contain the data buffer size. If the provided length is less than compressed stream size, then an error will be returned. Otherwise the compressed stream will be stored in the data buffer and the actual compressed buffer size will be stored in the length parameter. Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstream ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host The encoder_state that was previously used in one of the encoder functions. data Input/Output Host Pointer to the buffer in the host memory where the compressed stream will be stored. Can be NULL (see description). length Input/Output Host Pointer to the input buffer size. On return the NVJPEG library will store the actual compressed stream size in this parameter. stream Input Host CUDA stream where all the required device operations will be placed. 3.4.5. nvjpegEncodeRetrieveBitstreamDevice()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. data parameter should be on device memory If data parameter is NULL then the encoder will return compressed stream size in the length parameter. If data is not NULL then the provided length parameter should contain the data buffer size. If the provided length is less than compressed stream size, then an error will be returned. Otherwise the compressed stream will be stored in the data buffer and the actual compressed buffer size will be stored in the length parameter. Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstreamDevice ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host The encoder_state that was previously used in one of the encoder functions. data Input/Output Device Pointer to the buffer in the device memory where the compressed stream will be stored. Can be NULL (see description). length Input/Output Host Pointer to the input buffer size. On return the NVJPEG library will store the actual compressed stream size in this parameter. stream Input Host CUDA stream where all the required device operations will be placed. 4. JPEG Transcoding  This section describes the transcoding functions of the nvJPEG Library. 4.1. nvJPEG Transcoder Helper API Reference  This section describes the nvJPEG Transcoder helper API. 4.1.1. nvjpegEncoderParamsCopyMetadata()  Copies the metadata (JFIF, APP, EXT, and COM markers) from the parsed stream. Signature: nvjpegStatus_t nvjpegEncoderParamsCopyMetadata ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. 4.1.2. nvjpegEncoderParamsCopyQuantizationTables()  Copies the quantization tables from the parsed stream. Signature: nvjpegStatus_t nvjpegEncoderParamsCopyQuantizationTables ( nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. 4.1.3. nvjpegEncoderParamsCopyHuffmanTables() [Deprecated]  nvjpegEncoderParamsCopyHuffmanTables() is now deprecated. Due to precision differences in the JPEG encode/decode process, the input huffman tables may no longer be valid for the image being encoded and may result in corrupt bitstream. Signature: nvjpegStatus_t nvjpegEncoderParamsCopyHuffmanTables ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. 4.2. JPEG Transcoding Example  See below the example code. cudaStream_t stream ; // create library handle nvjpegHandle_t handle ; nvjpegCreateSimple ( & handle ); /////////////////////////////////// nvJPEG decoding //////////////////////////////////////// // create bitstream object nvjpegJpegStream_t jpeg_stream ; nvjpegJpegStreamCreate ( handle , & jpeg_stream ); // parse jpeg stream nvjpegJpegStreamParse ( handle , data_ptr , data_size , 1 , // save metadata in the jpegStream structure 0 , jpeg_stream ); // create decoder and decoder state nvjpegJpegDecoder_t decoder ; nvjpegJpegState_t decoder_state ; nvjpegDecoderCreate ( handle , NVJPEG_BACKEND_DEFAULT , & decoder ); nvjpegDecoderStateCreate ( handle , decoder , & decoder_state ); // create and set up decoder parameters nvjpegDecodeParams_t decode_params ; nvjpegDecodeParamsCreate ( handle , & decode_params ); nvjpegDecodeParamsSetOutputFormat ( decode_params , NVJPEG_OUTPUT_RGBI ); // decode image nvjpegImage_t output_image ; nvjpegDecodeJpeg ( handle , decoder , decode_params , jpeg_stream , decoder_state , & output_image , stream ); /////////////////////////////////// nvJPEG Transcode and encode API /////////////////////////////////// nvjpegEncoderState_t encoder_state ; nvjpegEncoderParams_t encode_params ; // get encoding from the jpeg stream and copy it to the encode parameters nvjpegJpegEncoding_t jpeg_encoding ; nvjpegJpegStreamGetJpegEncoding ( jpeg_stream , & jpeg_encoding ); nvjpegEncoderParamsSetEncoding ( encode_params , jpeg_encoding ); // copies according data to the encode parameters nvjpegEncoderParamsCopyMetadata ( encode_params , jpeg_stream , stream ); nvjpegEncoderParamsCopyQuantizationTables ( encode_params , jpeg_stream , stream ); nvjpegEncoderParamsCopyHuffmanTables ( encode_params , jpeg_stream , stream ); // retrieve frame dimensions unsigned width , height ; nvjpegJpegStreamGetFrameDimensions ( jpeg_stream , & width , & height ); // encode using encode parameters nvjpegEncodeImage ( nvjpeg_handle , encoder_state , encode_params , & output_image , input_format , width , height , stream ); // get compressed stream size size_t length ; nvjpegEncodeRetrieveBitstream ( nvjpeg_handle , encoder_state , NULL , & length , stream ); // get stream itself cudaStreamSynchronize ( stream ); std :: vector < char > jpeg ( length ); nvjpegEncodeRetrieveBitstream ( nvjpeg_handle , encoder_state , jpeg . data (), & length , 0 ); 5. List of Dropped APIs  The following APIs are dropped starting CUDA 11.0 nvjpegStatus_t nvjpegDecodePhaseOne ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodePhaseTwo ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodePhaseThree ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , nvjpegImage_t * destination , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodeBatchedPhaseOne ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , int image_idx , int thread_idx , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodeBatchedPhaseTwo ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , cudaStream_t stream ); nvjpegStatus_t nvjpegDecodeBatchedPhaseThree ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , nvjpegImage_t * destinations , cudaStream_t stream ); 6. Known Issues  Decoupled APIs, when initialized with NVJPEG_BACKEND_GPU_HYBRID , may not be able to correctly decode jpeg bitstreams which have out of bound run length codes. 7. Notices  7.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cudla-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cudla-api/index.html", "content_type": "text/html", "text": "cuDLA API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 cuDLA API 1. Modules 1.1. Data types used by cuDLA driver 1.2. ModulesData StructuresData FieldsNotices 2. Data Structures 2.1. cudlaDevAttribute 2.2. cudlaExternalMemoryHandleDesc_t 2.3. cudlaExternalSemaphoreHandleDesc_t 2.4. CudlaFence 2.5. cudlaModuleAttribute 2.6. cudlaModuleTensorDescriptor 2.7. cudlaSignalEvents 2.8. cudlaTask 2.9. cudlaWaitEvents 3. Data Fields Search Results cuDLA API\n                  ( PDF )\n                  -\n                   \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback 1. Modules Here is a list of all modules: Data types used by cuDLA driver cuDLA API 1.1. Data types used by cuDLA driver Classes struct CudlaFence union cudlaDevAttribute struct cudlaExternalMemoryHandleDesc_t struct cudlaExternalSemaphoreHandleDesc_t union cudlaModuleAttribute struct cudlaModuleTensorDescriptor struct cudlaSignalEvents struct cudlaTask struct cudlaWaitEvents Typedefs typedef cudlaDevHandle_t * cudlaDevHandle typedef cudlaModule_t * cudlaModule Enumerations enum cudlaAccessPermissionFlags enum cudlaDevAttributeType enum cudlaFenceType enum cudlaMode enum cudlaModuleAttributeType enum cudlaModuleLoadFlags enum cudlaNvSciSyncAttributes enum cudlaStatus enum cudlaSubmissionFlags Typedefs typedef cudlaDevHandle_t *  cudlaDevHandle cuDLA Device Handle typedef cudlaModule_t *  cudlaModule cuDLA Module Handle Enumerations enum cudlaAccessPermissionFlags Access permission flags for importing NvSciBuffers Values CUDLA_READ_WRITE_PERM = 0 Flag to import memory with read-write permission CUDLA_READ_ONLY_PERM = 1 Flag to import memory with read-only permission CUDLA_TASK_STATISTICS = 1<<1 Flag to indicate buffer as layerwise statistics buffer. enum cudlaDevAttributeType Device attribute type. Values CUDLA_UNIFIED_ADDRESSING = 0 Flag to check for support for UVA. CUDLA_DEVICE_VERSION = 1 Flag to check for DLA HW version. enum cudlaFenceType Supported fence types. Values CUDLA_NVSCISYNC_FENCE = 1 NvSciSync fence type for EOF. CUDLA_NVSCISYNC_FENCE_SOF = 2 enum cudlaMode Device creation modes. Values CUDLA_CUDA_DLA = 0 Hyrbid mode. CUDLA_STANDALONE = 1 Standalone mode. enum cudlaModuleAttributeType Module attribute types. Values CUDLA_NUM_INPUT_TENSORS = 0 Flag to retrieve number of input tensors. CUDLA_NUM_OUTPUT_TENSORS = 1 Flag to retrieve number of output tensors. CUDLA_INPUT_TENSOR_DESCRIPTORS = 2 Flag to retrieve all the input tensor descriptors. CUDLA_OUTPUT_TENSOR_DESCRIPTORS = 3 Flag to retrieve all the output tensor descriptors. CUDLA_NUM_OUTPUT_TASK_STATISTICS = 4 Flag to retrieve total number of output task statistics buffer. CUDLA_OUTPUT_TASK_STATISTICS_DESCRIPTORS = 5 Flag to retrieve all the output task statistics descriptors. enum cudlaModuleLoadFlags Module load flags for cudlaModuleLoadFromMemory . Values CUDLA_MODULE_DEFAULT = 0 Default flag. CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS = 1 Flag to load a module that is used to perform permanent fault diagnostics for DLA HW. enum cudlaNvSciSyncAttributes cuDLA NvSciSync attributes. Values CUDLA_NVSCISYNC_ATTR_WAIT = 1 Wait attribute. CUDLA_NVSCISYNC_ATTR_SIGNAL = 2 Signal attribute. enum cudlaStatus Error codes. Values cudlaSuccess = 0 The API call returned with no errors. cudlaErrorInvalidParam = 1 This indicates that one or more parameters passed to the API is/are incorrect. cudlaErrorOutOfResources = 2 This indicates that the API call failed due to lack of underlying resources. cudlaErrorCreationFailed = 3 This indicates that an internal error occurred during creation of device handle. cudlaErrorInvalidAddress = 4 This indicates that the memory object being passed in the API call has not been registered before. cudlaErrorOs = 5 This indicates that an OS error occurred. cudlaErrorCuda = 6 This indicates that there was an error in a CUDA operation as part of the API call. cudlaErrorUmd = 7 This indicates that there was an error in the DLA runtime for the API call. cudlaErrorInvalidDevice = 8 This indicates that the device handle passed to the API call is invalid. cudlaErrorInvalidAttribute = 9 This indicates that an invalid attribute is being requested. cudlaErrorIncompatibleDlaSWVersion = 10 This indicates that the underlying DLA runtime is incompatible with the current cuDLA version. cudlaErrorMemoryRegistered = 11 This indicates that the memory object is already registered. cudlaErrorInvalidModule = 12 This indicates that the module being passed is invalid. cudlaErrorUnsupportedOperation = 13 This indicates that the operation being requested by the API call is unsupported. cudlaErrorNvSci = 14 This indicates that the NvSci operation requested by the API call failed. cudlaErrorDlaErrInvalidInput = 0x40000001 DLA HW Error. cudlaErrorDlaErrInvalidPreAction = 0x40000002 DLA HW Error. cudlaErrorDlaErrNoMem = 0x40000003 DLA HW Error. cudlaErrorDlaErrProcessorBusy = 0x40000004 DLA HW Error. cudlaErrorDlaErrTaskStatusMismatch = 0x40000005 DLA HW Error. cudlaErrorDlaErrEngineTimeout = 0x40000006 DLA HW Error. cudlaErrorDlaErrDataMismatch = 0x40000007 DLA HW Error. cudlaErrorUnknown = 0x7fffffff This indicates that an unknown error has occurred. enum cudlaSubmissionFlags Task submission flags for cudlaSubmitTask . Values CUDLA_SUBMIT_NOOP = 1 Flag to specify that the submitted task must be bypassed for execution. CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE = 1<<1 Flag to specify that the global lock acquire must be skipped. CUDLA_SUBMIT_DIAGNOSTICS_TASK = 1<<2 Flag to specify that the submitted task is to run permanent fault diagnostics for DLA HW. 2. Data Structures Here are the data structures with brief descriptions: cudlaDevAttribute cudlaExternalMemoryHandleDesc cudlaExternalSemaphoreHandleDesc CudlaFence cudlaModuleAttribute cudlaModuleTensorDescriptor cudlaSignalEvents cudlaTask cudlaWaitEvents 2.1. cudlaDevAttribute Union Reference [ Data types used by cuDLA driver ] Device attribute. Public Variables uint32_t deviceVersion uint8_t unifiedAddressingSupported Variables uint32_t cudlaDevAttribute :: deviceVersion [inherited] DLA device version. Xavier has 1.0 and Orin has 2.0. uint8_t cudlaDevAttribute :: unifiedAddressingSupported [inherited] Returns 0 if unified addressing is not supported. 2.2. cudlaExternalMemoryHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External memory handle descriptor. Public Variables const \n                              void \n                              * extBufObject unsigned long long size Variables const \n                                 \n                                 void \n                                 * cudlaExternalMemoryHandleDesc_t :: extBufObject [inherited] A handle representing an external memory object. unsigned long long cudlaExternalMemoryHandleDesc_t :: size [inherited] Size of the memory allocation 2.3. cudlaExternalSemaphoreHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External semaphore handle descriptor. Public Variables const \n                              void \n                              * extSyncObject Variables const \n                                 \n                                 void \n                                 * cudlaExternalSemaphoreHandleDesc_t :: extSyncObject [inherited] A handle representing an external synchronization object. 2.4. CudlaFence Struct Reference [ Data types used by cuDLA driver ] Fence description. Public Variables void \n                              * fence cudlaFenceType type Variables void \n                                 * CudlaFence :: fence [inherited] Fence. cudlaFenceType CudlaFence :: type [inherited] Fence type. 2.5. cudlaModuleAttribute Union Reference [ Data types used by cuDLA driver ] Module attribute. Public Variables cudlaModuleTensorDescriptor * inputTensorDesc uint32_t numInputTensors uint32_t numOutputTensors cudlaModuleTensorDescriptor * outputTensorDesc Variables cudlaModuleTensorDescriptor * cudlaModuleAttribute :: inputTensorDesc [inherited] Returns an array of input tensor descriptors. uint32_t cudlaModuleAttribute :: numInputTensors [inherited] Returns the number of input tensors. uint32_t cudlaModuleAttribute :: numOutputTensors [inherited] Returns the number of output tensors. cudlaModuleTensorDescriptor * cudlaModuleAttribute :: outputTensorDesc [inherited] Returns an array of output tensor descriptors. 2.6. cudlaModuleTensorDescriptor Struct Reference [ Data types used by cuDLA driver ] Tensor descriptor. 2.7. cudlaSignalEvents Struct Reference [ Data types used by cuDLA driver ] Signal events for cudlaSubmitTask Public Variables const \n                              \n                              \n                              * devPtrs CudlaFence * eofFences uint32_t numEvents Variables const \n                                 \n                                 \n                                 \n                                 * cudlaSignalEvents :: devPtrs [inherited] Array of registered synchronization objects (via cudlaImportExternalSemaphore ). CudlaFence * cudlaSignalEvents :: eofFences [inherited] Array of fences pointers for all the signal events corresponding to the synchronization objects. uint32_t cudlaSignalEvents :: numEvents [inherited] Total number of signal events. 2.8. cudlaTask Struct Reference [ Data types used by cuDLA driver ] Structure of Task. Public Variables const \n                              \n                              \n                              * inputTensor cudlaModule moduleHandle uint32_t numInputTensors uint32_t numOutputTensors const \n                              \n                              \n                              * outputTensor cudlaSignalEvents * signalEvents const cudlaWaitEvents * waitEvents Variables const \n                                 \n                                 \n                                 \n                                 * cudlaTask :: inputTensor [inherited] Array of input tensors. cudlaModule cudlaTask :: moduleHandle [inherited] cuDLA module handle. uint32_t cudlaTask :: numInputTensors [inherited] Number of input tensors. uint32_t cudlaTask :: numOutputTensors [inherited] Number of output tensors. const \n                                 \n                                 \n                                 \n                                 * cudlaTask :: outputTensor [inherited] Array of output tensors. cudlaSignalEvents * cudlaTask :: signalEvents [inherited] Signal events. const cudlaWaitEvents * cudlaTask :: waitEvents [inherited] Wait events. 2.9. cudlaWaitEvents Struct Reference [ Data types used by cuDLA driver ] Wait events for cudlaSubmitTask . Public Variables uint32_t numEvents const CudlaFence * preFences Variables uint32_t cudlaWaitEvents :: numEvents [inherited] Total number of wait events. const CudlaFence * cudlaWaitEvents :: preFences [inherited] Array of fence pointers for all the wait events. 3. Data Fields Here is a list of all documented struct and union fields with links to the struct/union documentation for each field: deviceVersion cudlaDevAttribute devPtrs cudlaSignalEvents eofFences cudlaSignalEvents extBufObject cudlaExternalMemoryHandleDesc extSyncObject cudlaExternalSemaphoreHandleDesc fence CudlaFence inputTensor cudlaTask inputTensorDesc cudlaModuleAttribute moduleHandle cudlaTask numEvents cudlaWaitEvents cudlaSignalEvents numInputTensors cudlaTask cudlaModuleAttribute numOutputTensors cudlaTask cudlaModuleAttribute outputTensor cudlaTask outputTensorDesc cudlaModuleAttribute preFences cudlaWaitEvents signalEvents cudlaTask size cudlaExternalMemoryHandleDesc type CudlaFence unifiedAddressingSupported cudlaDevAttribute waitEvents cudlaTask Notices Notice This document is provided for information\n                              purposes only and shall not be regarded as a warranty of a\n                              certain functionality, condition, or quality of a product.\n                              NVIDIA Corporation (“NVIDIA”) makes no representations or\n                              warranties, expressed or implied, as to the accuracy or\n                              completeness of the information contained in this document\n                              and assumes no responsibility for any errors contained\n                              herein. NVIDIA shall have no liability for the consequences\n                              or use of such information or for any infringement of\n                              patents or other rights of third parties that may result\n                              from its use. This document is not a commitment to develop,\n                              release, or deliver any Material (defined below), code, or\n                              functionality. NVIDIA reserves the right to make corrections, modifications,\n                              enhancements, improvements, and any other changes to this\n                              document, at any time without notice. Customer should obtain the latest relevant information before\n                              placing orders and should verify that such information is\n                              current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and\n                              conditions of sale supplied at the time of order\n                              acknowledgement, unless otherwise agreed in an individual\n                              sales agreement signed by authorized representatives of\n                              NVIDIA and customer (“Terms of Sale”). NVIDIA hereby\n                              expressly objects to applying any customer general terms and\n                              conditions with regards to the purchase of the NVIDIA\n                              product referenced in this document. No contractual\n                              obligations are formed either directly or indirectly by this\n                              document. NVIDIA products are not designed, authorized, or warranted to be\n                              suitable for use in medical, military, aircraft, space, or\n                              life support equipment, nor in applications where failure or\n                              malfunction of the NVIDIA product can reasonably be expected\n                              to result in personal injury, death, or property or\n                              environmental damage. NVIDIA accepts no liability for\n                              inclusion and/or use of NVIDIA products in such equipment or\n                              applications and therefore such inclusion and/or use is at\n                              customer’s own risk. NVIDIA makes no representation or warranty that products based on\n                              this document will be suitable for any specified use.\n                              Testing of all parameters of each product is not necessarily\n                              performed by NVIDIA. It is customer’s sole responsibility to\n                              evaluate and determine the applicability of any information\n                              contained in this document, ensure the product is suitable\n                              and fit for the application planned by customer, and perform\n                              the necessary testing for the application in order to avoid\n                              a default of the application or the product. Weaknesses in\n                              customer’s product designs may affect the quality and\n                              reliability of the NVIDIA product and may result in\n                              additional or different conditions and/or requirements\n                              beyond those contained in this document. NVIDIA accepts no\n                              liability related to any default, damage, costs, or problem\n                              which may be based on or attributable to: (i) the use of the\n                              NVIDIA product in any manner that is contrary to this\n                              document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA\n                              patent right, copyright, or other NVIDIA intellectual\n                              property right under this document. Information published by\n                              NVIDIA regarding third-party products or services does not\n                              constitute a license from NVIDIA to use such products or\n                              services or a warranty or endorsement thereof. Use of such\n                              information may require a license from a third party under\n                              the patents or other intellectual property rights of the\n                              third party, or a license from NVIDIA under the patents or\n                              other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if\n                              approved in advance by NVIDIA in writing, reproduced without\n                              alteration and in full compliance with all applicable export\n                              laws and regulations, and accompanied by all associated\n                              conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE\n                              BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER\n                              DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING\n                              PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED,\n                              IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE\n                              MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF\n                              NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A\n                              PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN\n                              NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING\n                              WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL,\n                              INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER\n                              CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING\n                              OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN\n                              ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding\n                              any damages that customer might incur for any reason\n                              whatsoever, NVIDIA’s aggregate and cumulative liability\n                              towards customer for the products described herein shall be\n                              limited in accordance with the Terms of Sale for the\n                              product. OpenCL OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. Trademarks NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation\n                              in the U.S. and other countries.  Other company and product names may be trademarks of\n                              the respective companies with which they are associated. Copyright © 2021 - 2024 NVIDIA Corporation &\n                              affiliates. All rights reserved. This product includes software developed by the Syncro Soft SRL (http://www.sync.ro/). Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2021-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html", "parent_url": "https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html", "content_type": "text/html;charset=utf-8", "text": "cuFile API Reference Guide - NVIDIA Docs Submit Search NVIDIA Developer Blog Forums Join Submit Search NVIDIA Developer Blog Forums Join Menu cuFile API Reference Guide Submit Search Submit Search NVIDIA Docs Hub NVIDIA GPUDirect Storage (GDS) NVIDIA GPUDirect Storage cuFile API Reference Guide NVIDIA GPUDirect Storage (GDS) (Latest Release) Download PDF GDS cuFile API Reference The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the cuFile API reference that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. 1. Introduction NVIDIA® Magnum IO GPUDirect® Storage (GDS) is part of the GPUDirect family. GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU. This document provides information about the cuFile APIs that are used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs which are part of the GDS technology. Note: The APIs and descriptions are subject to change without notice. 2. Usage This section describes the operation of the cuFile APIs. Because the functionality is part of the CUDA Driver C API, the APIs use the cuFile prefix and camel case motif of the CUDA Driver. All APIs are thread-safe. The fork system call should not be used after the library is initialized. The behavior of the APIs after the fork system call is undefined in the child process. The APIs with GPU buffers should be called in a valid CUDA context and stream if applicable. All APIs are issued from the CPU, not the GPU. Note: Starting from CUDA toolkit 12.2 (GDS version 1.7.x) release cuFile APIs support memory allocated on GPU device as well as host memory. peer to peer transfer using GPUDirect™ is supported to and from device memory on supported file system and hardware configurations. The APIs will refer to this memory address as buffer pointer unless the API specifically applies to a particular type of memory. 2.1. Dynamic Interactions The following describes the dynamic interactions between the cuFile APIs. Some of the cuFile APIs are optional. If they are not called proactively, their actions will occur reactively: If cuFile{DriverOpen, HandleRegister, BufRegister} is called on a driver, file, or buffer, respectively that has been opened or registered by a previous cuFile * API call, this will result in an error. Calling cuFile{BufDeregister, HandleDeregister, DriverClose} on a buffer, file, or driver, respectively that has never been opened or registered by a previous cuFile * API call results in an error. For these errors, the output parameters of the APIs are left in an undefined state, and there are no other side effects. cuFileDriverOpen explicitly causes driver initialization. Its use is optional. If it is not used, driver initialization happens implicitly at the first use of the cuFile{HandleRegister, Read, Write, BufRegister} APIs. (Mandatory) cuFileHandleRegister turns an OS-specific file descriptor into a CUfileHandle_t and performs checking on the GDS supportability based on the mount point and the way that the file was opened. cuFileBufRegister explicitly registers a memory buffer. If this API is not called, an internal registered memory is used if required on the first time the buffer is used, for example, in cuFile{Read, Write} . cuFile{BufDeregister, HandleDeregister} explicitly frees a buffer and file resources, respectively. If this API is not called, the buffer and resources are implicitly freed when the driver is closed using cuFileDriverClose . cuFileDriverClose explicitly frees driver resources. If this API is not called, the driver resources are implicitly freed when dlclose() is performed on the library handle or when the process is terminated. 2.2. Driver, File, and Buffer Management This section describes the overall workflow to manage the driver, the file, and buffer management: Call cuFileDriverOpen() to initialize the state of the critical performance path. Allocate GPU memory with cudaMalloc, cudaMallocManaged , cuMem* APIs or host memory using cudaMallocHost , malloc or mmap . To register the buffer, call cuFileBufRegister to initialize the buffer state of the critical performance path. Complete the following IO workflow: For Linux, open a file with POSIX open. Call cuFileHandleRegister to wrap an existing file descriptor in an OS-agnostic CUfileHandle_t . This step evaluates the suitability of the file state and the file mount for GDS and initializes the file state of the critical performance path. Call IO APIs such as cuFileRead / cuFileWrite on an existing cuFile handle and existing buffer. If the cuFileBufRegister has not been previously called on the buffer pointer, cuFileRead/cuFileWrite will use internal registered buffers when required. Not using cuFileBufRegister might not be performant for small IO sizes. Refer to the GPUDirect Best Practices Guide for more information. Unless an error condition is returned, the IO is performed successfully. Call cuFileBufDeregister to free the buffer-specific cuFile state. Call cuFileHandleDeregister to free the file-specific cuFile state. Call cuFileDriverClose to free up the cuFile state. Note: Not using the cuFileDeregister and cuFileDriverClose APIs (steps 5, 6, and 7) might unnecessarily consume resources, as shown by tools such as valgrind. The best practice is to always call these APIs in the application cleanup paths. 2.3. cuFile Compatibility Mode Use Cases cuFile APIs can be used in different scenarios: Developers building GPUDirect Storage applications with cuFile APIs, but don’t have the supported hardware configurations. Developers building applications running on GPU cards that have CUDA compute capability > 6, but don’t have BAR space exposed. Deployments where nvidia-fs.ko is not loaded or cannot be loaded. Deployments where the Linux distribution does not support GPUDirect Storage. Deployments where the filesystem may be not supported with GPUDirect Storage. Deployments where the network links are not enabled with RDMA support. Deployment where the configuration is not optimal for GPUDirect Storage. Behavior The cuFile library provides a mechanism for cuFile reads and writes to use compatibility mode using POSIX pread , pwrite , and aio_submit APIS respectively to host memory and copying to GPU memory when applicable. The behavior of compatibility mode with cuFile APIs is determined by the following configuration parameters. Configuration Option (default) cuFile IO Behavior \"allow_compat_mode\": true If true , falls back to using compatibility mode when the library detects that the buffer file descriptor opened cannot use GPUDirect Storage. \"force_compat_mode\": false If true , this option can be used to force all IO to use compatibility mode. Alternatively the admin can unload the nvidia_fs.ko or not expose the character devices in the docker container environment. \"gds_rdma_write_support\": true If false , forces compatibility mode to be used for writes even when the underlying file system is capable of performing GPUDirect Storage writes. Note: If the option is “false”, this option will override and disable any filesystem-specific option to enable RDMA writes. \"posix_unaligned_writes\" : false If true , forces compatibility mode to be used for writes where the file offset and/or IO size is not aligned to Page Boundary (4KB). “lustre:posix_gds_min_kb\" : 0 For a lustre filesystem, if greater than 0 , compatibility mode is used for IO sizes between [1 - posix_gds_min_kb ] specified in kB. Note: This option will force posix mode even if “ allow_compat_mode ” is set to “false” . \"weka:rdma_write_support\" : false If this option is false , all writes to WekaFS will use compatibility mode. Note: If the option is set to “false” , cuFile library will use the posix path even if the allow_compat_mode option is true or false . \"gpfs:gds_write_support\" : false If this option is false, all writes to IBM Spectrum Scale will use compatibility mode. Note: If the option is set to “false” , cuFile library will use the posix path even if the allow_compat_mode option is true or false. \"rdma_dynamic_routing\": false, \"rdma_dynamic_routing_order\": [ \" \"SYS_MEM\" ] If rdma_dynamic_routing is set to true and rdma_dynamic_routing_order is set to [“SYS_MEM”] , then all IO for DFS will use compatibility mode. In addition to the above configuration options, compatibility mode will be used as a fallback option for following use cases. Use Case cuFile IO Behavior No BAR1 memory in GPU. Use compatibility mode. For wekaFS or IBM Spectrum Scale mounts: If there are no rdma_dev_addr_list specified, or failure to register MR with ib device. Use compatibility mode. Bounce buffers cannot be allocated in GPU memory. Use compatibility mode. For WekaFS and IBM Spectrum Scale: If the kernel returns -ENOTSUP for GPUDirect Storage read/write. Retry the IO operation internally using compatibility mode. cuFile Stream and cuFile Batch APIs on IBM Spectrum Scale or WekaFS All Async and batch operations will internally use compatibility mode IO. The nvidia_fs.ko driver is not loaded. All IO operations will use compatibility mode. Limitations Compatible mode does not work in cases where the GPUs have CUDA compute capability less than 6. GDS Compat mode has been tested and works with GDS enabled file systems and environments. It has not been tested to work on all other filesystems. 3. cuFile API Specification This section provides information about the cuFile APIs that are used from the CPU to enable applications and frameworks. 3.1. Data Types 3.1.1. Declarations and Definitions Here are the relevant cuFile enums and their descriptions. Copy Copied! typedef struct CUfileError {\n        CUfileOpError err; // cufile error\n        enum CUresult cu_err; // for CUDA-specific errors\n} CUfileError_t;\n\n/**\n * error macros to inspect error status of type CUfileOpError\n */\n \n#define IS_CUFILE_ERR(err) \\\n        (abs((err)) > CUFILEOP_BASE_ERR)\n \n#define CUFILE_ERRSTR(err) \\\n        cufileop_status_error(static_cast<CUfileOpError>(abs((err))))\n \n#define IS_CUDA_ERR(status) \\\n        ((status).err == CU_FILE_CUDA_DRIVER_ERROR)\n \n#define CU_FILE_CUDA_ERR(status) ((status).cu_ The following enum and two structures enable broader cross-OS support: Copy Copied! enum CUfileFileHandleType { \n    CU_FILE_HANDLE_TYPE_OPAQUE_FD = 1, /* linux based fd    */\n    CU_FILE_HANDLE_TYPE_OPAQUE_WIN32 = 2, /* windows based handle */\nCU_FILE_HANDLE_TYPE_USERSPACE_FS  = 3, /* userspace based FS */\n}; \n \ntypedef struct CUfileDescr_t {\nCUfileFileHandleType type; /* type of file being registered */\nunion { \nint fd;             /* Linux   */\nvoid *handle;         /* Windows */\n} handle;\nconst CUfileFSOps_t *fs_ops;     /* file system operation table */ \n}CUfileDescr_t;\n \n/* cuFile handle type */\ntypedef void*  CUfileHandle_t;\n \ntypedef struct cufileRDMAInfo\n{\n        int version;\n        int desc_len;\n        const char *desc_str;\n}cufileRDMAInfo_t;\n \ntypedef struct CUfileFSOps {\n      /* NULL means discover using fstat */\n      const char* (*fs_type) (void *handle);\n \n      /* list of host addresses to use,  NULL means no restriction */\n      int (*getRDMADeviceList)(void *handle, sockaddr_t **hostaddrs);\n \n      /* -1 no pref */\n      int (*getRDMADevicePriority)(void *handle, char*, size_t,\n                                loff_t, sockaddr_t* hostaddr);\n \n      /* NULL means try VFS */\n      ssize_t (*read) (void *handle, char*, size_t, loff_t, cufileRDMAInfo_t*);\n      ssize_t (*write) (void *handle, const char *, size_t, loff_t , cufileRDMAInfo_t*);\n}CUfileFSOps_t;\n\ntypedef enum CUfileDriverStatusFlags {\n        CU_FILE_LUSTRE_SUPPORTED = 0,        /*!< Support for DDN LUSTRE */\n        CU_FILE_WEKAFS_SUPPORTED = 1,        /*!< Support for WEKAFS */\n        CU_FILE_NFS_SUPPORTED = 2,           /*!< Support for NFS */\n        CU_FILE_GPFS_SUPPORTED = 3,          /*! < Support for GPFS */\n        CU_FILE_NVME_SUPPORTED = 4,          /*!< Support for NVMe */\n        CU_FILE_NVMEOF_SUPPORTED = 5,        /*!< Support for NVMeOF */\n        CU_FILE_SCSI_SUPPORTED = 6,          /*!< Support for SCSI */\n        CU_FILE_SCALEFLUX_CSD_SUPPORTED = 7, /*!< Support for Scaleflux CSD*/\n        CU_FILE_NVMESH_SUPPORTED = 8,        /*!< Support for NVMesh Block Dev*/\n        CU_FILE_BEEGFS_SUPPORTED = 9,        /*!< Support for BeeGFS */\n}CUfileDriverStatusFlags_t;\n\n \nenum CUfileDriverControlFlags {\n      CU_FILE_USE_POLL_MODE = 0, /*!< use POLL mode. properties.use_poll_mode*/\n      CU_FILE_ALLOW_COMPAT_MODE = 1 /*!< allow COMPATIBILITY mode. properties.allow_compat_mode*/\n};\n \ntypedef enum CUfileFeatureFlags {\n    CU_FILE_DYN_ROUTING_SUPPORTED =0,\n    CU_FILE_BATCH_IO_SUPPORTED = 1,\n    CU_FILE_STREAMS_SUPPORTED = 2\n} CUfileFeatureFlags_t;;\n \n/* cuFileDriverGetProperties describes this structure’s members */\ntypedef struct CUfileDrvProps {\n   struct {\n     unsigned int major_version;\n     unsigned int minor_version;\n     size_t poll_thresh_size;\n     size_t max_direct_io_size;\n     unsigned int dstatusflags;\n     unsigned int dcontrolflags;\n   } nvfs;\n   CUfileFeatureFlags_t fflags;\n   unsigned int max_device_cache_size;\n   unsigned int per_buffer_cache_size;\n   unsigned int max_pinned_memory_size;\n   unsigned int max_batch_io_timeout_msecs;\n}CUfileDrvProps_t;\n\n/* Parameter block for async cuFile IO */ \n/* Batch APIs use an array of these    */\n/* Status must be CU_FILE_WAITING when submitted, and is\n   updated when enqueued and when complete, so this user-allocated\n   structure is live until the operation completes.    */\ntypedef enum CUFILEStatus_enum {\n        CUFILE_WAITING = 0x000001,  /* required value prior to submission */\n        CUFILE_PENDING = 0x000002,  /* once enqueued */\n        CUFILE_INVALID = 0x000004,  /* request was ill-formed or could not be enqueued */\n        CUFILE_CANCELED = 0x000008, /* request successfully canceled */\n        CUFILE_COMPLETE = 0x0000010, /* request successfully completed */\n        CUFILE_TIMEOUT = 0x0000020,  /* request timed out */\n        CUFILE_FAILED  = 0x0000040  /* unable to complete */\n}CUfileStatus_t;\n\ntypedef enum cufileBatchMode {\n        CUFILE_BATCH = 1,\n} CUfileBatchMode_t;\n \ntypedef struct CUfileIOParams {\n        CUfileBatchMode_t mode; // Must be the very first field.\n        union {\n                struct  {\n                        void *devPtr_base;\n                        off_t file_offset;\n                        off_t devPtr_offset;\n                        size_t size;\n                }batch;\n        }u;\n        CUfileHandle_t fh;\n        CUfileOpcode_t opcode;\n        void *cookie;\n}CUfileIOParams_t;\n \ntypedef struct CUfileIOEvents {\n        void *cookie;\n        CUfileStatus_t   status;      /* status of the operation */\n        size_t ret;       /* -ve error or amount of I/O done. */\n}CUfileIOEvents_t; 3.1.2. Typedefs cuFile typedefs: Copy Copied! typedef struct CUfileDescr CUfileDesr_t\ntypedef struct CUfileError CUfileError_t\ntypedef struct CUfileDrvProps CUfileDrvProps_t\ntypedef enum CUfileFeatureFlags CUfileFeatureFlags_t\ntypedef enum CUfileDriverStatusFlags_enum CUfileDriverStatusFlags_t\ntypedef enum CUfileDriverControlFlags_enum CUfileDriverControlFlags_t\ntypedef struct CUfileIOParams CUfileIOParams_t\ntypedef enum CUfileBatchOpcode CUfileBatchOpcode_t 3.1.3. Enumerations cuFile enums: enum CUfileOpcode_enum This is the cuFile operation code for batch mode. OpCode Value Description CU_FILE_READ 0 Batch Read CU_FILE_WRITE 1 Batch Write Copy Copied! /* cuFile Batch IO operation kind */\nenum CUfileOpcode { \n     CU_FILE_READ,\n     CU_FILE_WRITE,\n}; enum CUfileStatus The cuFile Status codes for batch mode. Status Value Description CUFILE_WAITING 0x01 The initial value. CUFILE_PENDING 0x02 Set once enqueued into the driver. CUFILE_INVALID 0x04 Invalid parameters. CUFILE_CANCELED 0x08 Request successfully canceled. CUFILE_COMPLETE 0x10 Successfully completed. CUFILE_TIMEOUT 0x20 The operation has timed out. CUFILE_FAILED 0x40 IO has failed. enum CUfileOpError The cuFile Operation error types. All error code values, other than CU_FILE_SUCCESS , are considered failures that might leave the output and input parameter values of APIs in an undefined state. These values cannot have any side effects on the file system, the application process, and the larger system. Note: cuFile-specific errors will be greater than CUFILEOP_BASE_ERR to enable users to distinguish between POSIX errors and cuFile errors. Copy Copied! #define CUFILEOP_BASE_ERR 5000 Error Code Value Description CU_FILE_SUCCESS 0 The cufile is successful. CU_FILE_DRIVER_NOT_INITIALIZED 5001 The nvidia-fs driver is not loaded. CU_FILE_DRIVER_INVALID_PROPS 5002 An invalid property. CU_FILE_DRIVER_UNSUPPORTED_LIMIT 5003 A property range error. CU_FILE_DRIVER_VERSION_MISMATCH 5004 An nvidia-fs driver version mismatch. CU_FILE_DRIVER_VERSION_READ_ERROR 5005 An nvidia-fs driver version read error. CU_FILE_DRIVER_CLOSING 5006 Driver shutdown in progress. CU_FILE_PLATFORM_NOT_SUPPORTED 5007 GDS is not supported on the current platform. CU_FILE_IO_NOT_SUPPORTED 5008 GDS is not supported on the current file. CU_FILE_DEVICE_NOT_SUPPORTED 5009 GDS is not supported on the current GPU. CU_FILE_NVFS_DRIVER_ERROR 5010 An nvidia-fs driver ioctl error. CU_FILE_CUDA_DRIVER_ERROR 5011 A CUDA Driver API error. This error indicates a CUDA driver-api error. If this is set, a CUDA-specific error code is set in the cu_err field for cuFileError. CU_FILE_CUDA_POINTER_INVALID 5012 An invalid device pointer. CU_FILE_CUDA_MEMORY_TYPE_INVALID 5013 An invalid pointer memory type. CU_FILE_CUDA_POINTER_RANGE_ERROR 5014 The pointer range exceeds the allocated address range. CU_FILE_CUDA_CONTEXT_MISMATCH 5015 A CUDA context mismatch. CU_FILE_INVALID_MAPPING_SIZE 5016 Access beyond the maximum pinned memory size. CU_FILE_INVALID_MAPPING_RANGE 5017 Access beyond the mapped size. CU_FILE_INVALID_FILE_TYPE 5018 An unsupported file type. CU_FILE_INVALID_FILE_OPEN_FLAG 5019 Unsupported file open flags. CU_FILE_DIO_NOT_SET 5020 The fd direct IO is not set. CU_FILE_INVALID_VALUE 5022 Invalid API arguments. CU_FILE_MEMORY_ALREADY_REGISTERED 5023 Device pointer is already registered. CU_FILE_MEMORY_NOT_REGISTERED 5024 A device pointer lookup failure has occurred. CU_FILE_PERMISSION_DENIED 5025 A driver or file access error. CU_FILE_DRIVER_ALREADY_OPEN 5026 The driver is already open. CU_FILE_HANDLE_NOT_REGISTERED 5027 The file descriptor is not registered. CU_FILE_HANDLE_ALREADY_REGISTERED 5028 The file descriptor is already registered. CU_FILE_DEVICE_NOT_FOUND 5029 The GPU device cannot be not found. CU_FILE_INTERNAL_ERROR 5030 An internal error has occurred. Refer to cufile.log for more details. CU_FILE_GETNEWFD_FAILED 5031 Failed to obtain a new file descriptor. CU_FILE_NVFS_SETUP_ERROR 5033 An NVFS driver initialization error has occurred. CU_FILE_IO_DISABLED 5034 GDS is disabled by config on the current file. CU_FILE_BATCH_SUBMIT_FAILED 5035 Failed to submit a batch operation. CU_FILE_GPU_MEMORY_PINNING_FAILED 5036 Failed to allocate pinned GPU memory. CU_FILE_BATCH_FULL 5037 Queue full for batch operation. CU_FILE_ASYNC_NOT_SUPPORTED 5038 cuFile stream operation is not supported. Note: Data path errors are captured via standard error codes by using errno. The APIs will return -1 on error. 3.2. cuFile Driver APIs The following cuFile APIs that are used to initialize, finalize, query, and tune settings for the cuFile system. Copy Copied! /* Initialize the cuFile infrastructure */\nCUfileError_t cuFileDriverOpen();  \n\n/* Finalize the cuFile system */\nCUfileError_t cuFileDriverClose();\n\n/* Query capabilities based on current versions, installed functionality */\nCUfileError_t cuFileGetDriverProperties(CUfileDrvProps_t *props);\n\n/*API to set whether the Read/Write APIs use polling to do IO operations */\nCUfileError_t cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size);\n\n/*API to set max IO size(KB) used by the library to talk to nvidia-fs driver */\nCUfileError_t cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size);\n\n/* API to set maximum GPU memory reserved per device by the library for internal buffering */\nCUfileError_t cuFileDriverSetMaxCacheSize(size_t max_cache_size);\n\n/* Sets maximum buffer space that is pinned in KB for use by  cuFileBufRegister\nCUfileError_t cuFileDriverSetMaxPinnedMemSize(size_t\n   max_pinned_memory_size); Note: Refer to sample_007 for usage. 3.3. cuFile Synchronous IO APIs The core of the cuFile IO APIs are the read and write functions. Copy Copied! ssize_t cuFileRead(CUFileHandle_t fh, void *bufPtr_base, size_t size, off_t file_offset, off_t devPtr_offset);\nssize_t cuFileWrite(CUFileHandle_t fh, const void *bufPtr_base, size_t size, off_t file_offset, off_t devPtr_offset); The starting offset of the buffer on the device or host is determined by a base ( bufPtr_base ) and offset ( bufPtr_offset ). This offset is distinct from the offset in the file. Note: To use the registered buffer, the bufPtr_base must be the buffer pointer used to register during cuFileBufRegister . Otherwise cuFileRead and cuFileWrite APIs may use internal memory buffers for GPUDirect Storage peer to peer operations. Note: The default behavior for all paths where GDS is not supported is for the cuFile IO API to attempt IO using file system supported posix mode APIs when properties.allow_compat_mode is set to true. In order to disable cuFile APIs falling back to posix APIs for unsupported GDS paths, properties.allow_compat_mode in the /etc/cufile.json file should be set to false. Note: Refer to sample sample_003 for usage. 3.4. cuFile File Handle APIs Here is some information about the cuFile Handle APIs. The cuFileHandleRegister API makes a file descriptor or handle that is known to the cuFile subsystem by using an OS-agnostic interface. The API returns an opaque handle that is owned by the cuFile subsystem. To conserve memory, the cuFileHandleDeregister API is used to release cuFile-related memory objects. Using only the POSIX close will not clean up resources that were used by cuFile. Additionally, the clean up of cuFile objects associated with the files that were operated on in the cuFile context will occur at cuFileDriverClose . Copy Copied! CUfileError_t cuFileHandleRegister(CUFileHandle_t *fh, CUFileDescr_t *descr);\nvoid cuFileHandleDeregister(CUFileHandle_t fh); Note: Refer to sample_003 for usage. 3.5. cuFile Buffer APIs The cuFileBufRegister API incurs a significant performance cost, so registration costs should be amortized where possible. Developers must ensure that buffers are registered up front and off the critical path. The cuFileBufRegister API is optional. If this is not used, instead of pinning the user’s memory, cuFile-managed and internally pinned buffers are used. The cuFileBufDeregister API is used to optimally clean up cuFile-related memory objects, but CUDA currently has no analog to cuFileBufDeregister . The cleaning up of objects associated with the buffers operated on in the cuFile context occurs at cuFileDriverClose . If explicit APIs are used, the incurred errors are reported immediately, but if the operations of these explicit APIs are performed implicitly, error reporting and handling are less clear. Copy Copied! CUfileError_t cuFileBufRegister(const void *devPtr_base, size_t size, int flags);\nCUfileError_t cuFileBufDeregister(const void *devPtr_base); Note: Refer to sample_005 for usage. 3.6. cuFile Stream APIs Operations that are enqueued with cuFile Stream APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing with the next action in the stream. Copy Copied! CUfileError_t cuFileReadAsync(CUFileHandle_t fh, void *bufPtr_base, \n                  size_t *size_p, off_t *file_offset_p, off_t *bufPtr_offset_p,\n                  ssize_t *bytes_read_p, CUStream stream);\nCUfileError_t cuFileWriteAsync(CUFileHandle_t fh, void *bufPtr_base, \n                  size_t *size_p, off_t *file_offset_p, off_t *bufPtr_offse_pt,\n                  ssize_t *bytes_written_p, CUstream stream); Note: Refer to samples sample_031 , sample_032 , sample_033 , and sample_034 for usage. 3.7. cuFile Batch APIs Batch APIs are submitted synchronously, but executed asynchronously with respect to host thread. These operations can be submitted on different files, different locations in the same file, or a mix. Completion of IO can be checked asynchronously using a status API in the same host thread or in a different thread. The cuFileBatchIOGetStatus API takes an array of CUfileIOEvents_t and minimum number of elements to poll for. which describes the IO action, status, errors, and bytes transacted for each instance. The bytes transacted field is valid only when the status indicates a successful completion. Note: Refer to samples sample_019, sample_020 , sample_021 , and sample_022 for usage. 4. cuFile API Functional Specification This section provides information about the cuFile API functional specification. See the GPUDirect Storage Overview Guide for a high-level analysis of the set of functions and their relation to each other. We anticipate adding additional return codes for some of these functions. All cuFile APIs are called from the host code. 4.1. cuFileDriver API Functional Specification This section provides information about the cuFileDriver API functional specification. 4.1.1. cuFileDriverOpen Copy Copied! CUfileError_t cuFileDriverOpen(); Opens the Driver session to support GDS IO operations. Parameters None Returns CU_FILE_SUCCESS on a successful open, or if the driver is already open. CU_FILE_DRIVER_NOT_INITIALIZED on a failure to open the driver. CU_FILE_PERMISSION_DENIED on a failure to open. This can happen when the character device (/dev/nvidia_fs[0-15] ) is restricted to certain users by an administrator, for example, admin, where /dev is not exposed with read permissions in the container. CU_FILE_DRIVER_VERSION_MISMATCH , when there is a mismatch between the cuFile library and its kernel driver. CU_FILE_CUDA_DRIVER_ERROR if the CUDA driver failed to initialize. CU_FILE_PLATFORM_NOT_SUPPORTED if the current platform is not supported by GDS. CU_FILE_NVFS_SETUP_ERROR for a cuFile-specific internal error. Refer to the cufile.log file for more information. Description This API opens the session with the NVFS kernel driver to communicate from userspace to kernel space and calls the GDS driver to set up the resources required to support GDS IO operations. The API checks whether the current platform supports GDS and initializes the cuFile library. This API loads the cuFile settings from a JSON configuration file in /etc/cufile.JSON . If the JSON configuration file does not exist, the API loads the default library settings. To modify this default config file, administrative privileges are needed. The administrator can modify it to grant cuFile access to the specified devices and mount paths and also tune IO parameters (in KB, 4K aligned) that are based on the type of workload. Refer to the default config file (/etc/cufile.json) for more information. 4.1.2. cuFileDriverClose Copy Copied! CUfileError_t cuFileDriverClose(); Closes the driver session and frees any associated resources for GDS. This happens implicitly upon process exit. The driver can be reopened once it is closed. Parameters None Returns CU_FILE_SUCCESS on a successful close. CU_FILE_DRIVER_NOT_INITIALIZED on failure. Description Close the GDS session and any associated memory resources. If there are buffers registered by using cuFileBufRegister , which are not unregistered, a cuFileDriverClose implicitly unregisters those buffers. Any in-flight IO when cuFileDriverClose is in-progress will receive an error. 4.1.3. cuFileDriverGetProperties The cuFileDrvProps_t structure can be queried with cuFileDriverGetProperties and selectively modified with cuFileDriverSetProperties . The structure is self-describing, and its fields are consistent with the major and minor API version parameters. Copy Copied! CUfileError_t cuFileDriverGetProperties(cuFileDrvProps_t *props); Gets the Driver session properties for GDS functionality. Parameters props Pointer to the cuFile Driver properties. Returns CU_FILE_SUCCESS on a successful completion. CU_FILE_DRIVER_NOT_INITIALIZED on failure. CU_FILE_DRIVER_VERSION_MISMATCH on a driver version mismatch. CU_FILE_INVALID_VALUE if input is invalid. Description This API is used to get current GDS properties and nvidia-fs driver properties and functionality, such as support for SCSI, NVMe, and NVMe-OF. This API is used to get the current nvidia-fs drivers-specific properties such as the following: major_version : the cuFile major version minor_version : the cuFile minor version props.nvfs.dstatusflags , which are bit flags that indicate support for the following driver features: CU_FILE_EXASCALER_SUPPORTED , a bit to check whether the DDN EXAScaler parallel filesystem solutions (based on the Lustre filesystem) client supports GDS. CU_FILE_WEKAFS_SUPPORTED , a bit to check whether WekaFS supports GDS. Props.nvfs.dcontrolflags , which are bit flags that indicate the current activation for driver features: CU_FILE_USE_POLL_MODE , when bit is set, IO uses polling mode. CU_FILE_ALLOW_COMPAT_MODE , if the value is 1 compatible mode is set. Otherwise, the compatible mode is disabled. Props.fflags , which are bit flags that indicate whether the following library features are supported: CU_FILE_STREAMS_SUPPORTED , an attribute that checks whether CUDA-streams are supported. CU_FILE_DYN_ROUTING_SUPPORTED , an attribute that checks whether dynamic routing feature is supported. Props.nvfs.poll_thresh_size , a maximum IO size, in KB and must be 4K-aligned, that is used for the POLLING mode. Props.nvfs.max_direct_io_size , a maximum GDS IO size, in KB and must be 4K-aligned, that is requested by the nvidia-fs driver to the underlying filesystem. Props.max_device_cache_size , a maximum GPU buffer space per device, in KB and must be 4K-aligned. Used internally, for example, to handle unaligned IO and optimal IO path routing. This value might be rounded down to the nearest GPU page size. Props.max_device_pinned_mem_size , a maximum buffer space, in KB and must be 4K-aligned, that is pinned and mapped to the GPU BAR space. This might be rounded down to the nearest GPU page size. Props.per_buffer_cache_size , a GPU bounce buffer size, in KB, used for internal pools. Additional Information See the following for more information: cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size) cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size) cuFileDriverSetMaxCacheSize(size_t max_cache_size) cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size) 4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size) cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size) API Copy Copied! CUfileError_t cuFileDriverSetPollMode(bool poll,\n                                       size_t poll_threshold_size); Sets whether the Read/Write APIs use polling to complete IO operations. If poll mode is enabled, an IO size less than or equal to the threshold value is used for polling. The poll_threshold_size must be 4K aligned. Parameters poll Boolean to indicate whether to use the poll mode. poll_threshold_size IO size to use for POLLING mode in KB. The default value is 4KB. Returns CU_FILE_SUCCESS on a successful completion. CU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. CU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid threshold size Description This API is used in conjunction with cuFileGetDriverProperties . This API is used to set whether the library should use polling and the maximum IO threshold size less than or equal to which it will poll. This API overrides the default value that may be set through the JSON configuration file using the config keys properties.poll_mode and properties.poll_max_size_kb for the current process.  \n         \n          See the following for more information: cuFileDriverGetProperties 4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size) Copy Copied! CUfileError_t cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size); Sets the max IO size, in KB. This parameter is used by the nvidia-fs driver as the maximum IO chunk size in which IO is issued to the underlying filesystem. In compatible mode, this is the maximum IO chunk size that the library uses to issue POSIX read/writes. The max direct IO size must be 4K aligned. Parameters max_direct_io_size The maximum allowed direct IO size in KB. The default value is 16384KB. This is because typically parallel-file systems perform better with bulk read/writes. Returns CU_FILE_SUCCESS on successful completion. CU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. CU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid size. Description This API is used with cuFileGetDriverProperties and is used to set the maximum direct IO size used by the library to specify the nvidia-fs kernel driver the maximum chunk size in which the latter can issue IO to the underlying filesystem. In compatible mode, this is the maximum IO chunk size which the library uses for issuing POSIX read/writes. This parameter is dependent on the underlying GPU hardware and system memory. This API overrides the default value that might be set through the JSON configuration file by using the properties.max_direct_io_size_kb config key for the current process.  \n         \n          Refer to the following for more information: cuFileDriverGetProperties 4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size) Copy Copied! CUfileError_t cuFileDriverSetMaxCacheSize(size_t max_cache_size); Sets the maximum GPU buffer space, in KB, per device and is used for internal use, for example, to handle unaligned IO and optimal IO path routing. This value might be rounded down to the nearest GPU page size. The max cache size must be 4K aligned. This API overrides the default value that might be set through the JSON configuration file using the properties.max_device_cache_size_kb config key for the current process. Parameters max_cache_size The maximum GPU buffer space, in KB, per device used for internal use, for example, to handle unaligned IO and optimal IO path routing. This value might be rounded down to the nearest GPU page size. The default value is 131072KB. Returns CU_FILE_SUCCESS on successful completion. CU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. CU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid IO size Description This API is used with cuFileGetDriverProperties and is used to set the upper limit on the cache size per device for internal use by the library. See cuFileDriverGetProperties for more information. 4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size) Copy Copied! CUfileError_t cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_mem_size); Sets the maximum GPU buffer space, in KB, that is pinned and mapped. This value might be rounded down to the nearest GPU page size. The max pinned size must be 4K aligned. The default value corresponds to the maximum PinnedMemory or the physical memory size of the device. This API overrides the default value that may be set by the properties.max_device_pinned_mem_size_kb JSON config key for the current process. Parameters max_pinned_memory_size The maximum buffer space, in KB, that is pinned and mapped to the GPU BAR space. This value might be rounded down to the nearest GPU page size. The maximum limit may be set to UINT64_MAX, which is equivalent to no enforced limit. It may be set to something smaller than the size of the GPU’s physical memory. Returns CU_FILE_SUCCESS on successful completion. CU_FILE_DRIVER_NOT_INITIALIZED on failure to load driver. CU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid size. Description This API is used with cuFileGetDriverProperties and is used to set an upper limit on the maximum size of GPU memory that can be pinned and mapped and is dependent on the underlying GPU hardware and system memory. This API is related to cuFileBufRegister , which is used to register GPU device memory. See cuFileDriverGetProperties for more information. 4.2. cuFile IO API Functional Specification This section provides information about the cuFile IO API function specification. The device pointer addresses referred to in these APIs pertain to the current context for the caller. Unlike the non-async version of cuMemcpy , the cuFileHandleRegister , cuFileHandleDeregister , cuFileRead , and cuFileWrite APIs do not have the semantic of being ordered with respect to other work in the null stream. 4.2.1. cuFileHandleRegister Copy Copied! CUfileError_t cuFileHandleRegister(CUFileHandle_t *fh, CUfileDescr_t *descr); Register an open file. cuFileHandleRegister is required and performs extra checking that is memoized to provide increased performance on later cuFile operations. This API is OS agnostic. Note: CUDA toolkit 12.2 (GDS version 1.7.x) supports non O_DIRECT open flags as well as O_DIRECT. Application is allowed to open a file in non O_DIRECT mode in compat mode and also with nvidia-fs.ko installed. In the latter case, an O_DIRECT path between GPU and Storage will be used if such a path exists. Parameters fh Valid pointer to the OS-neutral cuFile handle structure supplied by the user but populated and maintained by the cuFile runtime. desc Valid pointer to the OS-neutral file descriptor supplied by the user carrying details regarding the file to be opened such as fd for Linux-based files. Returns CU_FILE_SUCCESS on successful completion. CU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. CU_FILE_IO_NOT_SUPPORTED , if the filesystem is not supported. CU_FILE_INVALID_VALUE if there are null or bad API arguments. CU_FILE_INVALID_FILE_OPEN_FLAG , if the file is opened with unsupported modes such as no O_APPEND , O_NOCTTY , O_NONBLOCK , O_DIRECTORY , O_NOFOLLOW , O_NOATIME , and O_TMPFILE . CU_FILE_INVALID_FILE_TYPE , if the file path is not valid, not a regular file, not a symbolic link, or not a device file. CU_FILE_HANDLE_ALREADY_REGISTERED if the file is already registered using the same file-descriptor. Description Given a file-descriptor will populate and return the CUfileHandle_t needed for issuing IO with cuFile APIs. A return value of anything other than CU_FILE_SUCCESS leaves fh in an undefined state but has no other side effects. By default this API accepts whether the file descriptor is opened with O_DIRECT mode or non O_DIRECT mode. Refer to the following for more information: cuFileRead cuFileWrite cuFileReadAsync cuFileWriteAsync cuFileHandleDeregister cuFileHandleDeregister Copy Copied! CUfileError_t cuFileHandleDeregister(CUFileHandle_t *fh); Parameters fh The file handle obtained from cuFileHandleRegister. Returns None Note: This API only logs an ERROR level message in the cufile.log file for valid inputs. Description The API is used to release resources that are claimed by cuFileHandleRegister . This API should be invoked only after the application ensures there are no outstanding IO operations with the handle. If cuFileHandleDeregister is called while IO on the file is in progress might result in undefined behavior. The user is still expected to close the file descriptor outside the cuFile subsystem after calling this API using close system call. Closing a file handle without calling cuFileHandleDeregister does not release the resources that are held in the cuFile library. If this API is not called, the cuFile subsystem releases the resources lazily or when the application exits. See the following for more information: cuFileRead cuFileWrite cuFileHandleDeregister 4.2.3. cuFileRead Copy Copied! ssize_t cuFileRead(CUfileHandle_tfh, void *bufPtr_base, size_t size, off_t file_offset, off_t bufPtr_offset); Reads specified bytes from the file descriptor into the device memory or the host memory. Parameters fh File descriptor for the file. bufPtr_base Base address of buffer in device memory or host memory. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call. size Size in bytes to read. file_offset Offset in the file to read from. bufPtr_offset Offset relative to the bufPtr_base pointer to read into. This parameter should be used only with registered buffers. Returns Size of bytes that were successfully read. -1 on an error, so errno is set to indicate filesystem errors. All other errors return a negative integer value of the CUfileOpError enum value. Description This API reads the data from a specified file handle at a specified offset and size bytes into the GPU memory by using GDS functionality or into the host memory based on the type of memory pointer. The API works correctly for unaligned offsets and any data size, although the performance might not match the performance of aligned reads.This is a synchronous call and blocks until the IO is complete. Note: For the bufPtr_offset , if data will be read starting exactly from the bufPtr_base that is registered with cuFileBufRegister , bufPtr_offset should be set to 0. To read starting from an offset in the registered buffer range, the relative offset should be specified in the bufPtr_offset, and the bufPtr_base must remain set to the base address that was used in the cuFileBufRegister call. See the following for more information: cuFileWrite cuFileReadAsync cuFileWriteAsync 4.2.4. cuFileWrite Copy Copied! ssize_t cuFileWrite(CUfileHandle_t fh, const void *bufPtr_base, size_t size, off_t file_offset, off_t bufPtr_offset); Writes specified bytes from the device memory into the file descriptor using GDS. Parameters fh File descriptor for the file bufPtr_base Base address of buffer in device memory or host memory. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call. size Size in bytes to which to write. file_offset Offset in the file to which to write. bufPtr_offset Offset relative to the bufPtr_base pointer from which to write. This parameter should be used only with registered buffers. Returns Size of bytes that were successfully written. -1 on an error, so errno is set to indicate filesystem errors. All other errors return a negative integer value of the CUfileOpError enum value. Description This API writes the data from the GPU memory or the host memory to a file specified by the file handle at a specified offset and size bytes by using GDS functionality. The API works correctly for unaligned offset and data sizes, although the performance is not on-par with aligned writes.This is a synchronous call and will block until the IO is complete. Note: GDS functionality modified the standard file system metadata in SysMem. However, GDS functionality does not take any special responsibility for writing that metadata back to permanent storage. The data is not guaranteed to be present after a system crash unless the application uses an explicit fsync(2) call. If the file is opened with an O_SYNC flag, the metadata will be written to the disk before the call is complete. Refer to the note in cuFileRead for more information about bufPtr_offset: .  \n         \n          Refer to the following for more information: cuFileWrite cuFileReadAsync cuFileWriteAsync 4.3. cuFile Memory Management Functional Specification The device pointer addresses that are mentioned in the APIs in this section pertain to the current context for the caller. cuFile relies on users to complete their own allocation before using the cuFileBufRegister API and free after using the cuFileBufDeregister API. 4.3.1. cuFileBufRegister Copy Copied! CUfileError_t cuFileBufRegister(const void *bufPtr_base,\n                                size_t size, int flags); Based on the memory type, this API registers existing cuMemAlloc’d (pinned) memory for GDS IO operations or host memory for IO operations. Parameters bufPtr_base Address of device pointer. cuFileRead and cuFileWrite must use this bufPtr_base as the base address. size Size in bytes from the start of memory to map. flags Reserved for future use, must be 0. Returns CU_FILE_SUCCESS on a successful registration. CU_FILE_NVFS_DRIVER_ERROR if the nvidia-fs driver cannot handle the request. CU_FILE_INVALID_VALUE on a failure. CU_FILE_CUDA_DRIVER_ERROR on CUDA-specific errors. CUresult code can be obtained using CU_FILE_CUDA_ERR (err). CU_FILE_MEMORY_ALREADY_REGISTERED , if memory is already registered. CU_FILE_INTERNAL_ERROR , an internal library-specific error. CU_FILE_CUDA_MEMORY_TYPE_INVALID , for device memory that is not allocated via cudaMalloc or cuMemAlloc . CU_FILE_CUDA_POINTER_RANGE_ERROR , if the size exceeds the bounds of the allocated memory. CU_FILE_INVALID_MAPPING_SIZE , if the size exceeds the GPU resource limits. CU_FILE_GPU_MEMORY_PINNING_FAILED , if not enough pinned memory is available. Description Based on the memory type, this API either registers the specified GPU address or host memory address and size for use with the cuFileRead and cuFileWrite operations. The user must call cuFileBufDeregister to release the pinned memory mappings for GPU memory if needed.  \n         \n          See the following for more information: cuFileBufDeregister 4.3.2. cuFileBufDeregister Copy Copied! CUfileError_t cuFileBufDeregister(const void *bufPtr_base); Based on the memory type, this API either deregisters CUDA memory or the host memory registered using the cuFileBufRegister API. Parameters bufPtr_base Address of device pointer to release the mappings that were provided to cuFileBufRegister Returns CU_FILE_SUCCESS on a successful deregistration. CU_FILE_MEMORY_NOT_REGISTERED , if bufPtr_base was not registered. CU_FILE_ERROR_INVALID_VALUE on failure to find the registration for the specified memory. CU_FILE_INTERNAL_ERROR , an internal library-specific error. Description This API deregisters memory mappings that were registered by cuFileBufRegister . Refer to cuFileBufRegister for more information. 4.4. cuFile Stream API Functional Specification This section provides information about the cuFile stream API functional specification. The stream APIs are similar to Read and Write, but they take a stream parameter to support asynchronous operations and execute in the CUDA stream order. 4.4.1. cuFileStreamRegister Copy Copied! CUfileError_t cuFileStreamRegister(CUStream_t stream, unsigned flags); Defines the input behavior for stream I/O APIs. Parameters stream CUDA stream in which to enqueue the operation. If NULL, make this operation in the default CUDA stream. flags The following are valid values: Value Description 0x0 All the I/O parameters are valid only at the time of execution. 0x1 Buffer offset value is valid at submission time. 0x2 File offset value is valid at submission time. 0x4 Size is valid at submission time. 0x8 All inputs i.e. buffer offset, file offset and size are 4K aligned. 0xf All inputs are aligned and known at submission time. Note: Using the flag ‘0XF’ will perform best as the workflow can be optimized during submission time. Description This optional API registers the stream with the cuFile subsystem. This API will allocate resources to handle stream operations for cuFile. The API will synchronize on the stream before allocating resources. The stream pointer is expected to be a valid pointer. Returns CU_FILE_SUCCESS on a successful submission. CU_FILE_ERROR_INVALID_VALUE on a invalid stream specification. CU_FILE_DRIVER_ERROR if the NVIDIA-fs driver cannot handle the request. CU_FILE_PLATFORM_NOT_SUPPORTED on unsupported platforms. 4.4.2. cuFileStreamDeregister Copy Copied! CUfileError_t cuFileStreamDeregister(CUStream_t stream); Parameters stream CUDA stream in which to enqueue the operation. If NULL, make this operation in the default CUDA stream. flags Reserved for future use. Description This optional API deregisters the stream with the cuFile subsystem. This API will free allocated cuFile resources associated with the stream. The API will synchronize on the stream before releasing resources. The stream pointer is expected to be a valid pointer. The stream will be automatically deregistered as part of cuFileDriverClose . Returns CU_FILE_SUCCESS on a successful submission. CU_FILE_ERROR_INVALID_VALUE on a invalid stream specification. CU_FILE_PLATFORM_NOT_SUPPORTED on unsupported platforms. 4.4.3. cuFileReadAsync Copy Copied! CUfileError_t cuFileReadAsync(CUFileHandle_t fh,\n                        void *bufPtr_base, \n                        size_t *size_p,\n                        off_t *file_offset_p, \n                        off_t *bufPtr_offset_p,\n                        int *bytes_read_p,\n                        CUstream stream); Enqueues a read operation for the specified bytes from the cuFile handle into the device memory by using GDS functionality or to the host memory based on the type of memory pointer. If non-NULL, the action is ordered in the stream. The current context of the caller is assumed. Parameters fh The cuFile handle for the file. bufPtr_base The base address of the buffer in the memory into which to read. The buffer can be allocated using either cudaMemory / cudaMallocHost / malloc / mmap . For registered buffers, bufPtr_base must remain set to the base address used in cuFileBufRegister call. size_p Pointer to size in bytes to read. If the exact size is not known at the time of I/O submission, then you must set it to the maximum possible I/O size for that stream I/O. file_offset_p Pointer to offset in the file from which to read. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time. bufPtr_offset_p Pointer to the offset relative to the bufPtr_base pointer from which to write. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time. bytes_read_p Pointer to the bytes read from the specified filehandle. This pointer should be a non NULL value and *bytes_read_p set to 0. After successful execution of the operation in the stream, the value *bytes_read_p will contain either: The number of bytes successfully read. -1 on IO errors. All other errors return a negative integer value of the CUfileOpError enum value. stream CUDA stream in which to enqueue the operation. If NULL, make this operation synchronous. Returns CU_FILE_SUCCESS on a successful submission. CU_FILE_DRIVER_ERROR , if the nvidia-fs driver cannot handle the request. CU_FILE_ERROR_INVALID_VALUE on an input failure. CU_FILE_CUDA_ERROR on CUDA-specific errors. CUresult code can be obtained by using CU_FILE_CUDA_ERR(err) . Description This API reads the data from the specified file handle at the specified offset and size bytes into the GPU memory using GDS functionality. This is an asynchronous call and enqueues the operation into the specified CUDA stream and will not block the host thread for IO completion. The operation can be waited upon using cuStreamSynchronize(stream) . The bytes_read_p memory should be allocated with cuMemHostAlloc/malloc/mmap or registered with cuMemHostRegister . The pointer to access that memory from the device can be obtained by using cuMemHostGetDevicePointer . Operations that are enqueued with cuFile Stream APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing to the next action in the stream. Unless otherwise specified through cuFileStreamRegister API, file offset, buffer offset or size parameter will not be evaluated until execution time. In these scenarios, size parameters should be set to the maximum possible I/O size at the time of submission and can be set to the actual size prior to the stream I/O execution. Refer to the following for more information: cuFileRead cuFileWrite cuFileWriteAsync 4.4.4. cuFileWriteAsync Copy Copied! CUfileError_t cuFileWriteAsync(CUFileHandle_t fh,\n                        void *bufPtr_base, \n                        size_t *size_p,\n                        off_t file_offset_p, \n                        off_t bufPtr_offset_p,\n                        int *bytes_written_p,\n                        CUstream_t stream); Queues Write operation for the specified bytes from the device memory into the cuFile handle by using GDS. Parameters fh The cuFile handle for the file. bufPtr_base The base address of the buffer in the memory from which to write. The buffer can be allocated using either cudaMemory/cudaMallocHost/malloc/mmap . For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call. size_p Pointer to the size in bytes to write. If the exact size is not known at the time of I/O submission, then you must set it to the maximum possible I/O size for that stream I/O. file_offset_p Pointer to the offset in the file from which to write. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time. bufPtr_offset_p Pointer to the offset relative to the bufPtr_base pointer from which to write. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time. bytes_written_p Pointer to the bytes written to the specified filehandle.This pointer should be a non NULL value and *bytes_written_p set to 0. After successful execution of the operation in the stream, the value *bytes_written_p will contain either: The number of bytes successfully written. -1 on IO errors. All other errors will return a negative integer value of the CUfileOpError enum value. stream The CUDA stream to enqueue the operation. Returns CU_FILE_SUCCESS on a successful submission. CU_FILE_DRIVER_ERROR , if the nvidia-fs driver cannot handle the request. CU_FILE_ERROR_INVALID_VALUE on an input failure. CU_FILE_CUDA_ERROR on CUDA-specific errors. The CUresult code can be obtained by using CU_FILE_CUDA_ERR(err) . Description This API writes the data from the GPU memory to a file specified by the file handle at a specified offset and size bytes by using GDS functionality. This is an asynchronous call and enqueues the operation into the specified CUDA stream and will not block the host thread for IO completion. The operation can be waited upon by using cuStreamSynchronize(stream) . The bytes_written pointer should be allocated with cuMemHostAlloc or registered with cuMemHostRegister , and the pointer to access that memory from the device can be obtained by using cuMemHostGetDevicePointer . Operations that are enqueued with cuFile Stream APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing to the next action in the stream. Unless otherwise specified through cuFileStreamRegister API, file offset, buffer offset or size parameter will not be evaluated until execution time. In these scenarios, size parameters should be set to the maximum possible I/O size at the time of submission and can be set to the actual size prior to the stream I/O execution. See the following for more information: cuFileRead cuFileWrite cuFileReadAsync 4.5. cuFile Batch API Functional Specification This section provides information about the cuFile Batch API functional specification. 4.5.1. cuFileBatchIOSetUp Copy Copied! CUfileError_t\ncuFileBatchIOSetUp(CUfileBatchHandle_t *batch_idp, int max_nr); Parameters max_nr (Input) The maximum number of events this batch will hold. Note: The number should be between 1 - “ properties.io_batch_size ” batch_idp (Output) Will be used in subsequent batch IO calls. Returns CU_FILE_SUCCESS on success. CU_FILE_INTERNAL_ERROR on on any failures. Description This interface should be the first call in the sequence of batch I/O operation. This takes the maximum number of batch entries the caller intends to use and returns a CUFileBatchHandle_t which should be used by the caller for subsequent batch I/O calls.  \n         \n          See the following for more information: cuFileRead cuFileWrite cuFileReadAsync cuFileWriteAsync cuFileBatchIOGetStatus cuFileBatchIOCancel cuFileBatchIODestroy 4.5.2. cuFileBatchIOSubmit Copy Copied! CUfileError_t cuFileBatchIOSubmit(CUfileBatchHandle_t batch_idp,\n                                 unsigned nr, \n                                 CUfileIOParams_t *iocbp,\n                                 unsigned int flags) Parameters batch_idp The address of the output parameter for the newly created batch ID, which was obtained from a cuFileBatchSetup call. nr The number of requests for the batch request. The value must be greater than 0 and less than or equal to max_nr specified in cuFileBatchIOSetup . iocbp The pointer contains the CUfileIOParams_t array structures of the length nr array. flags Reserved for future use. Should be set to 0. Returns CU_FILE_SUCCESS on success. CU_FILE_INTERNAL_ERROR on any failures. Description This API will need to be used to submit a read/write operation on an array of GPU/CPU data pointers from their respective file handle, offset, and size bytes. Based on the type of memory pointer, the data is transferred to/from the GPU memory by using GDS or the data is transferred to/from the CPU memory. This is an asynchronous call and will enqueue the operation on a batch_id provided by the cuFileIOSetup API. The operation can be monitored when using this batch_id through cuFileBatchIOGetStatus . The operation can be canceled by calling cuFileBatchIOCancel or destroyed by cuFileBatchIODestroy . The entries in the CUfileIOParams_t array describe individual IOs. The bytes transacted field is valid only when the status indicates a completion. Operations that are enqueued with cuFile Batch APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing to the next action in the stream. Operations in each batch might be reordered with respect to each other. The status field of individual IO operations via CUfileIOParams_t entries will have undefined values before the entire batch is complete. This definition is subject to change. See the following for more information: cuFileRead cuFileWrite cuFileReadAsync cuFileWriteAsync cuFileBatchIOGetStatus cuFileBatchIOCancel cuFileBatchIODestroy 4.5.3. cuFileBatchIOGetStatus Copy Copied! CUfileError_t cuFileBatchIOGetStatus(CUfileBatchHandle_t batch_idp, \n                                     unsigned min_nr,\n                                     unsigned *nr,\n                                     CUfileIOEvents_t *iocbp,\n                                     struct timespec* timeout)); Parameters batch_idp Obtained during setup. min_nr The minimum number of IO entries for which status is requested. The min_nr should be greater than or equal to zero and less than or equal to *nr . nr This is a pointer to max requested IO entries to poll for completion and is used as an Input/Output parameter. As an input *nr must be set to pass the maximum number of IO requests to poll for. As an output, *nr returns the number of completed I/Os. iocbp CUFileIOEvents_t array containing the status of completed I/Os in that batch. timeout This parameter is used to specify the amount of time to wait for in this API, even if the minimum number of requests have not completed. If the timeout hits, it is possible that the number of returned IOs can be less than min_nr . Returns CU_FILE_SUCCESS on success. The success here refers to the completion of the API. Individual IO status and error can be obtained by examining the returned status and error in the array iocbp. CU_FILE_ERROR_INVALID_VALUE for an invalid batch ID. Description This is a batch API to monitor the status of batch IO operations by using the batch_id that was returned by cuFileBatchIOSubmit . The operation will be canceled automatically if cuFileBatchIOCancel is called and the status will reflect CU_FILE_CANCELED for all canceled IO operations. The status of each member of the batch is queried, which would not be possible with one CUEvent . The status field of individual IO operations via CUfileIOParams_t entries will have undefined values before the entire batch is completed. This definition is subject to change. See the following for more information: cuFileBatchIOSubmit cuFileBatchIODestroy 4.5.4. cuFileBatchIOCancel Copy Copied! CUfileError_t cuFileBatchIOCancel(CUfileBatchHandle_t batch_idp) Parameters batch_idp The batch ID to cancel. Returns CU_FILE_SUCCESS on success. CU_FILE_ERROR_INVALID_VALUE on any failures. Description This is a batch API to cancel an ongoing IO batch operation by using the batch_id that was returned by cuFileBatchIOSubmit . This API tries to cancel an individual IO operation in the batch if possible and provides no guarantee about canceling an ongoing operation. Refer to the following for more information: cuFileBatchIOGetStatus cuFileBatchIOSubmit cuFileBatchIODestroy 4.5.5. cuFileBatchIODestroy Copy Copied! void cuFileBatchIODestroy(CUfileBatchHandle_t batch_idp) Parameters batch_idp The batch handle to be destroyed. Returns void Description This is a batch API that destroys a batch context and the resources that are allocated with cuFileBatchIOSetup .  \n         \n          Refer to the following for more information: cuFileBatchIOGetStatus cuFileBatchIOSubmit cuFileBatchIOCancel 5. Sample Program with cuFile APIs The following sample program uses the cuFile APIs: Copy Copied! // To compile this sample code:\n//\n// nvcc gds_helloworld.cxx -o gds_helloworld -lcufile\n//\n// Set the environment variable TESTFILE\n// to specify the name of the file on a GDS enabled filesystem\n//\n// Ex:   TESTFILE=/mnt/gds/gds_test ./gds_helloworld\n//\n//\n#include <fcntl.h>\n#include <errno.h>\n#include <unistd.h>\n\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include <cuda_runtime.h>\n#include \"cufile.h\"\n\n//#include \"cufile_sample_utils.h\"\nusing namespace std;\n\nint main(void) {\n        int fd;\n        ssize_t ret;\n        void *devPtr_base;\n        off_t file_offset = 0x2000;\n        off_t devPtr_offset = 0x1000;\n        ssize_t IO_size = 1UL << 24;\n        size_t buff_size = IO_size + 0x1000;\n        CUfileError_t status;\n        // CUResult cuda_result;\n        int cuda_result;\n        CUfileDescr_t cf_descr;\n        CUfileHandle_t cf_handle;\n        char *testfn;\n        \n        testfn=getenv(\"TESTFILE\");\n        if (testfn==NULL) {\n            std::cerr << \"No testfile defined via TESTFILE.  Exiting.\" << std::endl;\n            return -1;\n        } \n       \n        cout << std::endl; \n        cout << \"Opening File \" << testfn << std::endl;\n\n        fd = open(testfn, O_CREAT|O_WRONLY|O_DIRECT, 0644);\n        if(fd < 0) {\n                std::cerr << \"file open \" << testfn << \"errno \" << errno << std::endl;\n                return -1;\n        }\n\n        // the above fd could also have been opened without O_DIRECT starting CUDA toolkit 12.2\n        // (gds 1.7.x version) as follows\n        // fd = open(testfn, O_CREAT|O_WRONLY, 0644);\n\n        cout << \"Opening cuFileDriver.\" << std::endl;\n        status = cuFileDriverOpen();\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \" cuFile driver failed to open \" << std::endl;\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Registering cuFile handle to \" << testfn << \".\" << std::endl;\n\n        memset((void *)&cf_descr, 0, sizeof(CUfileDescr_t));\n        cf_descr.handle.fd = fd;\n        cf_descr.type = CU_FILE_HANDLE_TYPE_OPAQUE_FD;\n        status = cuFileHandleRegister(&cf_handle, &cf_descr);\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \"cuFileHandleRegister fd \" << fd << \" status \" << status.err << std::endl;\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Allocating CUDA buffer of \" << buff_size << \" bytes.\" << std::endl;\n\n        cuda_result = cudaMalloc(&devPtr_base, buff_size);\n        if (cuda_result != CUDA_SUCCESS) {\n                std::cerr << \"buffer allocation failed \" << cuda_result << std::endl;\n                cuFileHandleDeregister(cf_handle);\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Registering Buffer of \" << buff_size << \" bytes.\" << std::endl;\n        status = cuFileBufRegister(devPtr_base, buff_size, 0);\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \"buffer registration failed \" << status.err << std::endl;\n                cuFileHandleDeregister(cf_handle);\n                close(fd);\n                cudaFree(devPtr_base);\n                return -1;\n        }\n\n        // fill a pattern\n        cout << \"Filling memory.\" << std::endl;\n\n        cudaMemset((void *) devPtr_base, 0xab, buff_size);\n        cuStreamSynchronize(0);\n\n        // perform write operation directly from GPU mem to file\n        cout << \"Writing buffer to file.\" << std::endl;\n        ret = cuFileWrite(cf_handle, devPtr_base, IO_size, file_offset, devPtr_offset);\n\n        if (ret < 0 || ret != IO_size) {\n                std::cerr << \"cuFileWrite failed \" << ret << std::endl;\n        }\n\n        // release the GPU memory pinning\n        cout << \"Releasing cuFile buffer.\" << std::endl;\n        status = cuFileBufDeregister(devPtr_base);\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \"buffer deregister failed\" << std::endl;\n                cudaFree(devPtr_base);\n                cuFileHandleDeregister(cf_handle);\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Freeing CUDA buffer.\" << std::endl;\n        cudaFree(devPtr_base);\n        // deregister the handle from cuFile\n        cout << \"Releasing file handle. \" << std::endl;\n        (void) cuFileHandleDeregister(cf_handle);\n        close(fd);\n\n        // release all cuFile resources\n        cout << \"Closing File Driver.\" << std::endl;\n        (void) cuFileDriverClose();\n\n        cout << std::endl; \n\n        return 0;\n} 6. Known Limitations of cuFile Batch APIs This section provides information about the known limitations of cuFile Batch APIs in this release of GDS. Batch I/Os will be supported mainly by either the local file systems which are hosted on NVMe or NVMeOF devices or by the native file system that supports Linux AIO. Following table provides an overview of the cuFile batch API support with respect to different file systems. The following table provides an overview of cuFile batch API support with respect to distributed file systems: File System GDS Batch Mode Comments Ext4/XFS Read/Write support DDN EXAScaler Read/Write support NFS Read/Write support IBM Spectrum Scale Not available Will work in compat mode Weka Not available Will work in compat mode BeeGFS Not available Will work in compat mode Notices Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. OpenCL OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. Trademarks NVIDIA, the NVIDIA logo, DGX, DGX-1, DGX-2, DGX-A100, Tesla, and Quadro are trademarks and/or registered trademarks of NVIDIA Corporation in the United States and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. © 2020-2024 NVIDIA Corporation and affiliates. All rights reserved. Last updated on Jun 14, 2024. Topics NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage Design Guide 1. Introduction 2. Data Transfer Issues for GPU and Storage 3. GPUDirect Storage Benefits 4. Application Suitability 4.1. Transfers To and From the GPU 4.2. Understanding IO Bottlenecks 4.3. Explicit GDS APIs 4.4. Pinned Memory for DMA Transfers 4.5. cuFile APIs 5. Platform Performance Suitability 5.1. Bandwidth from Storage 5.2. Paths from Storage to GPUs 5.3. GPU BAR1 Size 6. Call to Action NVIDIA GPUDirect Storage Overview Guide 1. Introduction 1.1. Related Documents 1.2. Benefits for a Developer 1.3. Intended Uses 2. Functional Overview 2.1. Explicit and Direct 2.2. Performance Optimizations 2.2.1. Implementation Performance Enhancements 2.2.2. Concurrency Across Threads 2.2.3. Asynchrony 2.2.4. Batching 2.2.5. Use of CUDA Streams in cuFile 2.3. Compatibility and Generality 2.4. Monitoring 2.5. Scope of the Solutions in GDS 2.6. Dynamic Routing 2.6.1. cuFile Configuration for Dynamic Routing 2.6.2. cuFile Configuration for DFS Mount 2.6.3. cuFile Configuration Validation for Dynamic Routing 3. Software Architecture 3.1. Software Components 3.2. Primary Components 3.2.1. Workflows for GDS Functionality 3.2.2. Workflow 1 3.2.3. Workflow 2 3.3. Aligning with Other Linux Initiatives 4. Deployment 4.1. Software Components for Deployment 4.2. Using GPUDirect Storage in Containers cuFile API Reference Guide 1. Introduction 2. Usage 2.1. Dynamic Interactions 2.2. Driver, File, and Buffer Management 2.3. cuFile Compatibility Mode 3. cuFile API Specification 3.1. Data Types 3.1.1. Declarations and Definitions 3.1.2. Typedefs 3.1.3. Enumerations 3.2. cuFile Driver APIs 3.3. cuFile Synchronous IO APIs 3.4. cuFile File Handle APIs 3.5. cuFile Buffer APIs 3.6. cuFile Stream APIs 3.7. cuFile Batch APIs 4. cuFile API Functional Specification 4.1. cuFileDriver API Functional Specification 4.1.1. cuFileDriverOpen 4.1.2. cuFileDriverClose 4.1.3. cuFileDriverGetProperties 4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size) 4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size) 4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size) 4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size) 4.2. cuFile IO API Functional Specification 4.2.1. cuFileHandleRegister 4.2.2. cuFileHandleDeregister 4.2.3. cuFileRead 4.2.4. cuFileWrite 4.3. cuFile Memory Management Functional Specification 4.3.1. cuFileBufRegister 4.3.2. cuFileBufDeregister 4.4. cuFile Stream API Functional Specification 4.4.1. cuFileStreamRegister 4.4.2. cuFileStreamDeregister 4.4.3. cuFileReadAsync 4.4.4. cuFileWriteAsync 4.5. cuFile Batch API Functional Specification 4.5.1. cuFileBatchIOSetUp 4.5.2. cuFileBatchIOSubmit 4.5.3. cuFileBatchIOGetStatus 4.5.4. cuFileBatchIOCancel 4.5.5. cuFileBatchIODestroy 5. Sample Program with cuFile APIs 6. Known Limitations of cuFile Batch APIs NVIDIA GPUDirect Storage Release Notes 1. Introduction 2. New Features and Changes 3. MLNX_OFED and Filesystem Requirements 4. Support Matrix 5. GDS Enabled Libraries/Frameworks 6. Included Packages 7. Minor Updates and Bug Fixes 8. Known Issues 9. Known Limitations Getting Started with NVIDIA GPUDirect Storage 1. Introduction 2. If you are a system administrator or a performance engineer 3. If you are a developer 4. If you are OEM, ODM, CSP 5. Troubleshooting GDS issues Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide 1. Introduction 2. Software Settings 2.1. System Settings 2.2. Use of CUDA Context in GPU Kernels and Storage IO 2.3. cuFile Configuration Settings 3. API Usage 3.1. cuFileDriverOpen 3.2. cuFileHandleRegister 3.3. cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister 3.3.1. IO Pattern 1 3.3.2. IO Pattern 2 3.3.3. IO Pattern 3 3.3.4. IO Pattern 4 3.3.5. IO Pattern 5 3.3.6. IO Pattern 6 3.3.7. IO Pattern 7 3.4. cuFileHandleDeregister 3.5. cuFileBufDeregister 3.6. cuFileStreamRegister 3.7. cuFileStreamDeregister 3.8. cuFileDriverClose NVIDIA GPUDirect Storage Benchmarking and Configuration Guide 1. Introduction 2. About this Guide 3. Benchmarking GPUDirect Storage 3.1. Determining PCIe Device Affinity 3.2. GPUDirect Storage Configuration Parameters 3.2.1. System Parameters 3.2.2. GPUDirect Storage Parameters 3.3. GPUDirect Storage Benchmarking Tools 3.3.1. gdsio Utility 3.3.2. gds-stats Tool 4. GPUDirect Storage Benchmarking on Direct Attached Storage 4.1. GPUDirect Storage Performance on DGX-2 System 4.2. GPUDirect Storage Performance on a DGX A100 System 5. GPUDirect Storage Benchmarking on Network Attached Storage 5.1. GPUDirect Storage Benchmarking on NFS 6. Summary A. Benchmarking and Performance A.1. The Language of Performance A.2. Benchmarking Storage Performance NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 1. Introduction 2. Installing GPUDirect Storage 2.1. Before You Install GDS 2.2. Installing GDS 2.2.1. Configuring File System Settings for GDS 2.2.2. Verifying a Successful GDS Installation 2.3. Installed GDS Libraries and Tools 2.4. Uninstalling GPUDirect Storage 2.5. Environment Variables Used by GPUDirect Storage 2.6. JSON Config Parameters Used by GPUDirect Storage 2.7. GDS Configuration File Changes to Support Dynamic Routing 2.8. Determining Which Version of GDS is Installed 2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems 3. API Errors 3.1. CU_FILE_DRIVER_NOT_INITIALIZED 3.2. CU_FILE_DEVICE_NOT_SUPPORTED 3.3. CU_FILE_IO_NOT_SUPPORTED 3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID 4. Basic Troubleshooting 4.1. Log Files for the GDS Library 4.2. Enabling a Different cufile.log File for Each Application 4.3. Enabling Tracing GDS Library API Calls 4.4. cuFileHandleRegister Error 4.5. Troubleshooting Applications that Return cuFile Errors 4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics 4.7. CUDA Runtime and Driver Mismatch with Error Code 35 4.8. CUDA API Errors when Running the cuFile-* APIs 4.9. Finding GDS Driver Statistics 4.10. Tracking IO Activity that Goes Through the GDS Driver 4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats 4.12. Tracking Registration and Deregistration of GPU Buffers 4.13. Enabling RDMA-specific Logging for Userspace File Systems 4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation 4.15. Adding udev Rules for RAID Volumes 4.16. When You Observe \"Incomplete write\" on NVME Drives 4.17. CUFILE async I/O is failing 5. Advanced Troubleshooting 5.1. Resolving Hung cuFile* APIs with No Response 5.2. Sending Relevant Data to Customer Support 5.3. Resolving an IO Failure with EIO and Stack Trace Warning 5.4. Controlling GPU BAR Memory Usage 5.5. Determining the Amount of Cache to Set Aside 5.6. Monitoring BAR Memory Usage 5.7. Resolving an ENOMEM Error Code 5.8. GDS and Compatibility Mode 5.9. Enabling Compatibility Mode 5.10. Tracking the IO After Enabling Compatibility Mode 5.11. Bypassing GPUDirect Storage 5.12. GDS Does Not Work for a Mount 5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File 5.14. Running Data Verification Tests Using GPUDirect Storage NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 6. Troubleshooting Performance 6.1. Running Performance Benchmarks with GDS 6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache 6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance 6.4. Using GPUDirect Statistics to Monitor CPU Activity 6.5. Monitoring Performance and Tracing with cuFile-* APIs 6.6. Example: Using Linux Tracing Tools 6.7. Tracing the cuFile-* APIs 6.8. Improving Performance using Dynamic Routing 7. Troubleshooting IO Activity 7.1. Managing Coherency of Data in the Page Cache and on Disk 8. EXAScaler Filesystem LNet Troubleshooting 8.1. Determining the EXAScaler Filesystem Client Module Version 8.2. Checking the LNet Network Setup on a Client 8.3. Checking the Health of the Peers 8.4. Checking for Multi-Rail Support 8.5. Checking GDS Peer Affinity 8.6. Checking for LNet-Level Errors 8.7. Resolving LNet NIDs Health Degradation from Timeouts 8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection 9. Understanding EXAScaler Filesystem Performance 9.1. osc Tuning Performance Parameters 9.2. Miscellaneous Commands for osc, mdc, and stripesize 9.3. Getting the Number of Configured Object-Based Disks 9.4. Getting Additional Statistics related to the EXAScaler Filesystem 9.5. Getting Metadata Statistics 9.6. Checking for an Existing Mount 9.7. Unmounting an EXAScaler Filesystem Cluster 9.8. Getting a Summary of EXAScaler Filesystem Statistics 9.9. Using GPUDirect Storage in Poll Mode 10. Troubleshooting and FAQ for the WekaIO Filesystem 10.1. Downloading the WekaIO Client Package 10.2. Determining Whether the WekaIO Version is Ready for GDS 10.3. Mounting a WekaIO File System Cluster 10.4. Resolving a Failing Mount 10.5. Resolving 100% Usage for WekaIO for Two Cores 10.6. Checking for an Existing Mount in the Weka File System 10.7. Checking for a Summary of the WekaIO Filesystem Status 10.8. Displaying the Summary of the WekaIO Filesystem Statistics 10.9. Why WekaIO Writes Go Through POSIX 10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct 10.11. Checking Memory Peer Direct Stats 10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem 10.13. Conducting a Basic WekaIO Filesystem Test 10.14. Unmounting a WekaIO File System Cluster 10.15. Verify the Installed Libraries for the WekaIO Filesystem 10.16. GDS Configuration File Changes to Support the WekaIO Filesystem 10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem 10.18. Check for WekaFS Support 11. Enabling IBM Spectrum Scale Support with GDS 11.1. IBM Spectrum Scale Limitations with GDS 11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect 11.3. Verifying Installed Libraries for IBM Spectrum Scale 11.4. Checking PeerDirect Stats 11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale 11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process 11.7. GDS Configuration to Support IBM Spectrum Scale 11.8. Scenarios for Falling Back to Compatibility Mode 11.9. GDS Limitations with IBM Spectrum Scale 12. NetApp E-series BeeGFS with GDS Solution Deployment 12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements 12.2. BeeGFS Client Configuration for GDS 12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server 12.4. Verify the Setup 12.4.1. List the Management Node 12.4.2. List the Metadata Nodes 12.4.3. List the Storage Nodes 12.4.4. List the Client Nodes 12.4.5. Display Client Connections 12.4.6. Verify Connectivity to the Different Services 12.4.7. List Storage Pools 12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets 12.5. Testing 12.5.1. Verifying Integration is Working 12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test 13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath) 13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages 13.1.1. Client Software Requirements 13.1.2. Install the VAST Multipath Package 13.2. Set Up the Networking 13.2.1. VAST Network Configuration 13.2.2. Client Network Configuration 13.2.3. Verify Network Connectivity 13.3. Mount VAST NFS 13.4. Debugging and Monitoring VAST Data 14. Troubleshooting and FAQ for NVMe and NVMeOF Support 14.1. MLNX_OFED Requirements and Installation 14.2. Determining Whether the NVMe device is Supported for GDS 14.3. RAID Support in GDS 14.4. Mounting a Local Filesystem for GDS 14.5. Check for an Existing EXT4 Mount 14.6. Check for IO Statistics with Block Device Mount 14.7. RAID Group Configuration for GPU Affinity 14.8. Conduct a Basic EXT4 Filesystem Test 14.9. Unmount a EXT4 Filesystem 14.10. Udev Device Naming for a Block Device 14.11. BATCH I/O Performance 15. Displaying GDS NVIDIA FS Driver Statistics 15.1. nvidia-fs Statistics 15.2. Analyze Statistics for each GPU 15.3. Resetting the nvidia-fs Statistics 15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers 15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers 15.6. Display the GPU-to-Peer Distance Table 15.7. The GDSIO Tool 15.8. Tabulated Fields 15.9. The GDSCHECK Tool 15.10. NFS Support with GPUDirect Storage 15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later 15.10.2. Install GPUDirect Storage Support for the NFS Client 15.11. NFS GPUDirect Storage Statistics and Debugging 15.12. GPUDirect Storage IO Behavior 15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO 15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite) 15.12.3. GPU to NIC Peer Affinity 15.12.4. Compatible Mode with Unregistered Buffers 15.12.5. Unaligned writes with Non-Registered Buffers 15.12.6. Process Hang with NFS 15.12.7. Tools Support Limitations for CUDA 9 and Earlier 15.13. GDS Statistics for Dynamic Routing 15.13.1. Peer Affinity Dynamic Routing 15.13.2. cuFile Log Related to Dynamic Routing 16. GDS Library Tracing 16.1. Example: Display Tracepoints 16.1.1. Example: Tracepoint Arguments 16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite 16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS 16.4. Understand the IO Pattern of a Process 16.5. IO Pattern of a Process with the File Descriptor on Different GPUs 16.6. Determine the IOPS and Bandwidth for a Process in a GPU 16.7. Display the Frequency of Reads by Processes that Issue cuFileRead 16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms 16.9. Displaying the Latency of cuFileRead for Each Process 16.10. Example: Tracking the Processes that Issue cuFileBufRegister 16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister 16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer 16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure 16.14. Example: User-Space Statistics for Each GDS Process 16.15. Example: Viewing GDS User-Level Statistics for a Process 16.16. Example: Displaying Sample User-Level Statistics for each GDS Process 17. User-Space Counters in GPUDirect Storage 17.1. Distribution of IO Usage in Each GPU 17.2. User-space Statistics for Dynamic Routing 18. User-Space RDMA Counters in GPUDirect Storage 18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS) 18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS) 19. Cheat Sheet for Diagnosing Problems NVIDIA GPUDirect Storage O_DIRECT Requirements Guide 1. Introduction 1.1. Related Documents 2. GPUDirect Storage Requirements 2.1. Summary of Basic Requirements 2.2. Client and Server 2.3. Cases Where O_DIRECT is Not a Fit 2.3.1. Buffered IO 2.3.2. Inline Files 2.3.3. Block Allocation For Writes 2.3.4. Examining or Transforming User Data 2.3.5. Summary Corporate Info NVIDIA.com Home About NVIDIA ‎NVIDIA Developer Developer Home Blog Resources Contact Us Developer Program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/cuda-math-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-math-api/index.html", "content_type": "text/html", "text": "CUDA Math API Reference Manual 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices CUDA Math API Reference Manual » CUDA Math API Reference Manual v12.5 | PDF | Archive CUDA Math API Reference Manual  CUDA mathematical functions are always available in device code. Host implementations of the common mathematical functions are mapped in a platform-specific way to standard math library functions, provided by the host compiler and respective host libm where available. Some functions, not available with the host compilers, are implemented in crt/math_functions.hpp header file. For example, see erfinv() . Other, less common functions, like rhypot() , cyl_bessel_i0() are only available in device code. CUDA Math device functions are no-throw for well-formed CUDA programs. Note that many floating-point and integer functions names are overloaded for different argument types. For example, the log() function has the following prototypes: double log ( double x ); float log ( float x ); float logf ( float x ); Note also that due to implementation constraints, certain math functions from std:: namespace may be callable in device code even via explicitly qualified std:: names. However, such use is discouraged, since this capability is unsupported, unverified, undocumented, not portable, and may change without notice. 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-driver-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-driver-api/index.html", "content_type": "text/html", "text": "CUDA Driver API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 CUDA Driver API 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Data types used by CUDA driver 6.2. Error Handling 6.3. Initialization 6.4. Version Management 6.5. Device Management 6.6. Device Management [DEPRECATED] 6.7. Primary Context Management 6.8. Context Management 6.9. Context Management [DEPRECATED] 6.10. Module Management 6.11. Module Management [DEPRECATED] 6.12. Library Management 6.13. Memory Management 6.14. Virtual Memory Management 6.15. Stream Ordered Memory Allocator 6.16. Multicast Object Management 6.17. Unified Addressing 6.18. Stream Management 6.19. Event Management 6.20. External Resource Interoperability 6.21. Stream Memory Operations 6.22. Execution Control 6.23. Execution Control [DEPRECATED] 6.24. Graph Management 6.25. Occupancy 6.26. Texture Reference Management [DEPRECATED] 6.27. Surface Reference Management [DEPRECATED] 6.28. Texture Object Management 6.29. Surface Object Management 6.30. Tensor Map Object Managment 6.31. Peer Context Memory Access 6.32. Graphics Interoperability 6.33. Driver Entry Point Access 6.34. Coredump Attributes Control API 6.35. Green Contexts 6.36. Profiler Control [DEPRECATED] 6.37. Profiler Control 6.38. OpenGL Interoperability 6.38.1. OpenGL Interoperability [DEPRECATED] 6.39. Direct3D 9 Interoperability 6.39.1. Direct3D 9 Interoperability [DEPRECATED] 6.40. Direct3D 10 Interoperability 6.40.1. Direct3D 10 Interoperability [DEPRECATED] 6.41. Direct3D 11 Interoperability 6.41.1. Direct3D 11 Interoperability [DEPRECATED] 6.42. VDPAU Interoperability 6.43. EGL Interoperability 7. Data Structures 7.1. CUaccessPolicyWindow_v1 7.2. CUarrayMapInfo_v1 7.3. CUasyncNotificationInfo 7.4. CUctxCigParam 7.5. CUctxCreateParams 7.6. CUDA_ARRAY3D_DESCRIPTOR_v2 7.7. CUDA_ARRAY_DESCRIPTOR_v2 7.8. CUDA_ARRAY_MEMORY_REQUIREMENTS_v1 7.9. CUDA_ARRAY_SPARSE_PROPERTIES_v1 7.10. 7.11. CUDA_CHILD_GRAPH_NODE_PARAMS 7.12. CUDA_CONDITIONAL_NODE_PARAMS 7.13. CUDA_EVENT_RECORD_NODE_PARAMS 7.14. CUDA_EVENT_WAIT_NODE_PARAMS 7.15. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1 7.16. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2 7.17. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1 7.18. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2 7.19. CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1 7.20. CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1 7.21. CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1 7.22. CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1 7.23. CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1 7.24. CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1 7.25. CUDA_GRAPH_INSTANTIATE_PARAMS 7.26. CUDA_HOST_NODE_PARAMS_v1 7.27. CUDA_HOST_NODE_PARAMS_v2 7.28. CUDA_KERNEL_NODE_PARAMS_v1 7.29. CUDA_KERNEL_NODE_PARAMS_v2 7.30. CUDA_KERNEL_NODE_PARAMS_v3 7.31. CUDA_LAUNCH_PARAMS_v1 7.32. CUDA_MEM_ALLOC_NODE_PARAMS_v1 7.33. CUDA_MEM_ALLOC_NODE_PARAMS_v2 7.34. CUDA_MEM_FREE_NODE_PARAMS 7.35. CUDA_MEMCPY2D_v2 7.36. CUDA_MEMCPY3D_PEER_v1 7.37. CUDA_MEMCPY3D_v2 7.38. CUDA_MEMCPY_NODE_PARAMS 7.39. CUDA_MEMSET_NODE_PARAMS_v1 7.40. CUDA_MEMSET_NODE_PARAMS_v2 7.41. CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1 7.42. CUDA_RESOURCE_DESC_v1 7.43. CUDA_RESOURCE_VIEW_DESC_v1 7.44. CUDA_TEXTURE_DESC_v1 7.45. CUdevprop_v1 7.46. CUdevResource 7.47. CUdevSmResource 7.48. CUeglFrame_v1 7.49. CUexecAffinityParam_v1 7.50. CUexecAffinitySmCount_v1 7.51. CUgraphEdgeData 7.52. CUgraphExecUpdateResultInfo_v1 7.53. CUgraphNodeParams 7.54. CUipcEventHandle_v1 7.55. CUipcMemHandle_v1 7.56. CUlaunchAttribute 7.57. CUlaunchAttributeValue 7.58. CUlaunchConfig 7.59. CUlaunchMemSyncDomainMap 7.60. CUmemAccessDesc_v1 7.61. CUmemAllocationProp_v1 7.62. CUmemFabricHandle_v1 7.63. CUmemLocation_v1 7.64. CUmemPoolProps_v1 7.65. CUmemPoolPtrExportData_v1 7.66. CUmulticastObjectProp_v1 7.67. CUstreamBatchMemOpParams_v1 7.68. CUtensorMap 8. Data Fields 9. Deprecated List Search Results CUDA Driver API\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback Table of Contents 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Data types used by CUDA driver 6.2. Error Handling 6.3. Initialization 6.4. Version Management 6.5. Device Management 6.6. Device Management [DEPRECATED] 6.7. Primary Context Management 6.8. Context Management 6.9. Context Management [DEPRECATED] 6.10. Module Management 6.11. Module Management [DEPRECATED] 6.12. Library Management 6.13. Memory Management 6.14. Virtual Memory Management 6.15. Stream Ordered Memory Allocator 6.16. Multicast Object Management 6.17. Unified Addressing 6.18. Stream Management 6.19. Event Management 6.20. External Resource Interoperability 6.21. Stream Memory Operations 6.22. Execution Control 6.23. Execution Control [DEPRECATED] 6.24. Graph Management 6.25. Occupancy 6.26. Texture Reference Management [DEPRECATED] 6.27. Surface Reference Management [DEPRECATED] 6.28. Texture Object Management 6.29. Surface Object Management 6.30. Tensor Map Object Managment 6.31. Peer Context Memory Access 6.32. Graphics Interoperability 6.33. Driver Entry Point Access 6.34. Coredump Attributes Control API 6.35. Green Contexts 6.36. Profiler Control [DEPRECATED] 6.37. Profiler Control 6.38. OpenGL Interoperability 6.38.1. OpenGL Interoperability [DEPRECATED] 6.39. Direct3D 9 Interoperability 6.39.1. Direct3D 9 Interoperability [DEPRECATED] 6.40. Direct3D 10 Interoperability 6.40.1. Direct3D 10 Interoperability [DEPRECATED] 6.41. Direct3D 11 Interoperability 6.41.1. Direct3D 11 Interoperability [DEPRECATED] 6.42. VDPAU Interoperability 6.43. EGL Interoperability 7. Data Structures 7.1. CUaccessPolicyWindow_v1 7.2. CUarrayMapInfo_v1 7.3. CUasyncNotificationInfo 7.4. CUctxCigParam 7.5. CUctxCreateParams 7.6. CUDA_ARRAY3D_DESCRIPTOR_v2 7.7. CUDA_ARRAY_DESCRIPTOR_v2 7.8. CUDA_ARRAY_MEMORY_REQUIREMENTS_v1 7.9. CUDA_ARRAY_SPARSE_PROPERTIES_v1 7.10. 7.11. CUDA_CHILD_GRAPH_NODE_PARAMS 7.12. CUDA_CONDITIONAL_NODE_PARAMS 7.13. CUDA_EVENT_RECORD_NODE_PARAMS 7.14. CUDA_EVENT_WAIT_NODE_PARAMS 7.15. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1 7.16. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2 7.17. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1 7.18. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2 7.19. CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1 7.20. CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1 7.21. CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1 7.22. CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1 7.23. CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1 7.24. CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1 7.25. CUDA_GRAPH_INSTANTIATE_PARAMS 7.26. CUDA_HOST_NODE_PARAMS_v1 7.27. CUDA_HOST_NODE_PARAMS_v2 7.28. CUDA_KERNEL_NODE_PARAMS_v1 7.29. CUDA_KERNEL_NODE_PARAMS_v2 7.30. CUDA_KERNEL_NODE_PARAMS_v3 7.31. CUDA_LAUNCH_PARAMS_v1 7.32. CUDA_MEM_ALLOC_NODE_PARAMS_v1 7.33. CUDA_MEM_ALLOC_NODE_PARAMS_v2 7.34. CUDA_MEM_FREE_NODE_PARAMS 7.35. CUDA_MEMCPY2D_v2 7.36. CUDA_MEMCPY3D_PEER_v1 7.37. CUDA_MEMCPY3D_v2 7.38. CUDA_MEMCPY_NODE_PARAMS 7.39. CUDA_MEMSET_NODE_PARAMS_v1 7.40. CUDA_MEMSET_NODE_PARAMS_v2 7.41. CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1 7.42. CUDA_RESOURCE_DESC_v1 7.43. CUDA_RESOURCE_VIEW_DESC_v1 7.44. CUDA_TEXTURE_DESC_v1 7.45. CUdevprop_v1 7.46. CUdevResource 7.47. CUdevSmResource 7.48. CUeglFrame_v1 7.49. CUexecAffinityParam_v1 7.50. CUexecAffinitySmCount_v1 7.51. CUgraphEdgeData 7.52. CUgraphExecUpdateResultInfo_v1 7.53. CUgraphNodeParams 7.54. CUipcEventHandle_v1 7.55. CUipcMemHandle_v1 7.56. CUlaunchAttribute 7.57. CUlaunchAttributeValue 7.58. CUlaunchConfig 7.59. CUlaunchMemSyncDomainMap 7.60. CUmemAccessDesc_v1 7.61. CUmemAllocationProp_v1 7.62. CUmemFabricHandle_v1 7.63. CUmemLocation_v1 7.64. CUmemPoolProps_v1 7.65. CUmemPoolPtrExportData_v1 7.66. CUmulticastObjectProp_v1 7.67. CUstreamBatchMemOpParams_v1 7.68. CUtensorMap 8. Data Fields 9. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/cuda-runtime-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-runtime-api/index.html", "content_type": "text/html", "text": "CUDA Runtime API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 CUDA Runtime API 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Device Management 6.2. Device Management [DEPRECATED] 6.3. Thread Management [DEPRECATED] 6.4. Error Handling 6.5. Stream Management 6.6. Event Management 6.7. External Resource Interoperability 6.8. Execution Control 6.9. Execution Control [DEPRECATED] 6.10. Occupancy 6.11. Memory Management 6.12. Memory Management [DEPRECATED] 6.13. Stream Ordered Memory Allocator 6.14. Unified Addressing 6.15. Peer Device Memory Access 6.16. OpenGL Interoperability 6.17. OpenGL Interoperability [DEPRECATED] 6.18. Direct3D 9 Interoperability 6.19. Direct3D 9 Interoperability [DEPRECATED] 6.20. Direct3D 10 Interoperability 6.21. Direct3D 10 Interoperability [DEPRECATED] 6.22. Direct3D 11 Interoperability 6.23. Direct3D 11 Interoperability [DEPRECATED] 6.24. VDPAU Interoperability 6.25. EGL Interoperability 6.26. Graphics Interoperability 6.27. Texture Object Management 6.28. Surface Object Management 6.29. Version Management 6.30. Graph Management 6.31. Driver Entry Point Access 6.32. C++ API Routines 6.33. Interactions with the CUDA Driver API 6.34. Profiler Control 6.35. Data types used by CUDA Runtime 7. Data Structures 7.1. __cudaOccupancyB2DHelper 7.2. cudaAccessPolicyWindow 7.3. cudaArrayMemoryRequirements 7.4. cudaArraySparseProperties 7.5. cudaAsyncNotificationInfo_t 7.6. cudaChannelFormatDesc 7.7. cudaChildGraphNodeParams 7.8. cudaConditionalNodeParams 7.9. cudaDeviceProp 7.10. cudaEglFrame 7.11. cudaEglPlaneDesc 7.12. cudaEventRecordNodeParams 7.13. cudaEventWaitNodeParams 7.14. cudaExtent 7.15. cudaExternalMemoryBufferDesc 7.16. cudaExternalMemoryHandleDesc 7.17. cudaExternalMemoryMipmappedArrayDesc 7.18. cudaExternalSemaphoreHandleDesc 7.19. cudaExternalSemaphoreSignalNodeParams 7.20. cudaExternalSemaphoreSignalNodeParamsV2 7.21. cudaExternalSemaphoreSignalParams 7.22. cudaExternalSemaphoreSignalParams_v1 7.23. cudaExternalSemaphoreWaitNodeParams 7.24. cudaExternalSemaphoreWaitNodeParamsV2 7.25. cudaExternalSemaphoreWaitParams 7.26. cudaExternalSemaphoreWaitParams_v1 7.27. cudaFuncAttributes 7.28. cudaGraphEdgeData 7.29. cudaGraphExecUpdateResultInfo 7.30. cudaGraphInstantiateParams 7.31. cudaGraphKernelNodeUpdate 7.32. cudaGraphNodeParams 7.33. cudaHostNodeParams 7.34. cudaHostNodeParamsV2 7.35. cudaIpcEventHandle_t 7.36. cudaIpcMemHandle_t 7.37. cudaKernelNodeParams 7.38. cudaKernelNodeParamsV2 7.39. cudaLaunchAttribute 7.40. cudaLaunchAttributeValue 7.41. cudaLaunchConfig_t 7.42. cudaLaunchMemSyncDomainMap 7.43. cudaLaunchParams 7.44. cudaMemAccessDesc 7.45. cudaMemAllocNodeParams 7.46. cudaMemAllocNodeParamsV2 7.47. cudaMemcpy3DParms 7.48. cudaMemcpy3DPeerParms 7.49. cudaMemcpyNodeParams 7.50. cudaMemFreeNodeParams 7.51. cudaMemLocation 7.52. cudaMemPoolProps 7.53. cudaMemPoolPtrExportData 7.54. cudaMemsetParams 7.55. cudaMemsetParamsV2 7.56. cudaPitchedPtr 7.57. cudaPointerAttributes 7.58. cudaPos 7.59. cudaResourceDesc 7.60. cudaResourceViewDesc 7.61. cudaTextureDesc 7.62. CUuuid_st 8. Data Fields 9. Deprecated List Search Results CUDA Runtime API\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2024\n                  - Send Feedback Table of Contents 1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Device Management 6.2. Device Management [DEPRECATED] 6.3. Thread Management [DEPRECATED] 6.4. Error Handling 6.5. Stream Management 6.6. Event Management 6.7. External Resource Interoperability 6.8. Execution Control 6.9. Execution Control [DEPRECATED] 6.10. Occupancy 6.11. Memory Management 6.12. Memory Management [DEPRECATED] 6.13. Stream Ordered Memory Allocator 6.14. Unified Addressing 6.15. Peer Device Memory Access 6.16. OpenGL Interoperability 6.17. OpenGL Interoperability [DEPRECATED] 6.18. Direct3D 9 Interoperability 6.19. Direct3D 9 Interoperability [DEPRECATED] 6.20. Direct3D 10 Interoperability 6.21. Direct3D 10 Interoperability [DEPRECATED] 6.22. Direct3D 11 Interoperability 6.23. Direct3D 11 Interoperability [DEPRECATED] 6.24. VDPAU Interoperability 6.25. EGL Interoperability 6.26. Graphics Interoperability 6.27. Texture Object Management 6.28. Surface Object Management 6.29. Version Management 6.30. Graph Management 6.31. Driver Entry Point Access 6.32. C++ API Routines 6.33. Interactions with the CUDA Driver API 6.34. Profiler Control 6.35. Data types used by CUDA Runtime 7. Data Structures 7.1. __cudaOccupancyB2DHelper 7.2. cudaAccessPolicyWindow 7.3. cudaArrayMemoryRequirements 7.4. cudaArraySparseProperties 7.5. cudaAsyncNotificationInfo_t 7.6. cudaChannelFormatDesc 7.7. cudaChildGraphNodeParams 7.8. cudaConditionalNodeParams 7.9. cudaDeviceProp 7.10. cudaEglFrame 7.11. cudaEglPlaneDesc 7.12. cudaEventRecordNodeParams 7.13. cudaEventWaitNodeParams 7.14. cudaExtent 7.15. cudaExternalMemoryBufferDesc 7.16. cudaExternalMemoryHandleDesc 7.17. cudaExternalMemoryMipmappedArrayDesc 7.18. cudaExternalSemaphoreHandleDesc 7.19. cudaExternalSemaphoreSignalNodeParams 7.20. cudaExternalSemaphoreSignalNodeParamsV2 7.21. cudaExternalSemaphoreSignalParams 7.22. cudaExternalSemaphoreSignalParams_v1 7.23. cudaExternalSemaphoreWaitNodeParams 7.24. cudaExternalSemaphoreWaitNodeParamsV2 7.25. cudaExternalSemaphoreWaitParams 7.26. cudaExternalSemaphoreWaitParams_v1 7.27. cudaFuncAttributes 7.28. cudaGraphEdgeData 7.29. cudaGraphExecUpdateResultInfo 7.30. cudaGraphInstantiateParams 7.31. cudaGraphKernelNodeUpdate 7.32. cudaGraphNodeParams 7.33. cudaHostNodeParams 7.34. cudaHostNodeParamsV2 7.35. cudaIpcEventHandle_t 7.36. cudaIpcMemHandle_t 7.37. cudaKernelNodeParams 7.38. cudaKernelNodeParamsV2 7.39. cudaLaunchAttribute 7.40. cudaLaunchAttributeValue 7.41. cudaLaunchConfig_t 7.42. cudaLaunchMemSyncDomainMap 7.43. cudaLaunchParams 7.44. cudaMemAccessDesc 7.45. cudaMemAllocNodeParams 7.46. cudaMemAllocNodeParamsV2 7.47. cudaMemcpy3DParms 7.48. cudaMemcpy3DPeerParms 7.49. cudaMemcpyNodeParams 7.50. cudaMemFreeNodeParams 7.51. cudaMemLocation 7.52. cudaMemPoolProps 7.53. cudaMemPoolPtrExportData 7.54. cudaMemsetParams 7.55. cudaMemsetParamsV2 7.56. cudaPitchedPtr 7.57. cudaPointerAttributes 7.58. cudaPos 7.59. cudaResourceDesc 7.60. cudaResourceViewDesc 7.61. cudaTextureDesc 7.62. CUuuid_st 8. Data Fields 9. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html", "parent_url": "https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html", "content_type": "text/html", "text": "PTX Interoperability 1. Introduction 2. Data Representation 2.1. Fundamental Types 2.2. Aggregates and Unions 2.3. Bit Fields 2.4. Texture, Sampler, and Surface Types 3. Function Calling Sequence 3.1. Registers 3.2. Stack Frame 3.3. Parameter Passing 4. System Calls 5. Debug Information 5.1. Generation of Debug Information 5.2. CUDA-Specific DWARF Definitions 6. Example 7. C++ 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks PTX Interoperability » 1. Introduction v12.5 | PDF | Archive PTX Writer’s Guide to Interoperability The guide to writing ABI-compliant PTX. 1. Introduction  This document defines the Application Binary Interface (ABI) for the CUDA ® architecture when generating PTX. By following the ABI, external developers can generate compliant PTX code that can be linked with other code. PTX is a low-level parallel-thread-execution virtual machine and ISA (Instruction Set Architecture). PTX can be output from multiple tools or written directly by developers. PTX is meant to be GPU-architecture independent, so that the same code can be reused for different GPU architectures. For more information on PTX, refer to the latest version of the PTX ISA reference document . There are multiple CUDA architecture families, each with their own ISA; e.g. SM 5.x is the Maxwell family, SM 6.x is the Pascal family. This document describes the high-level ABI for all architectures. Programs conforming to an ABI are expected to be executed on the appropriate architecture GPU, and can assume that instructions from that ISA are available. 2. Data Representation  2.1. Fundamental Types  The below table shows the native scalar PTX types that are supported. Any PTX producer must use these sizes and alignments in order for its PTX to be compatible with PTX generated by other producers. PTX also supports native vector types, which are discussed in Aggregates and Unions . The sizes of types are defined by the host. For example, pointer size and long int size are dictated by the hosts ABI. PTX has an .address_size directive that specifies the address size used throughout the PTX code. The size of pointers is 32 bits on a 32-bit host or 64 bits on a 64-bit host. However, addresses of the local and shared memory spaces are always 32 bits in size. During separate compilation we store info about the host platform in each object file. The linker will fail to link object files generated for incompatible host platforms. PTX Type Size (bytes) Align (bytes) Hardware Representation .b8 1 1 untyped byte .b16 2 2 untyped halfword .b32 4 4 untyped word .b64 8 8 untyped doubleword .s8 1 1 signed integral byte .s16 2 2 signed integral halfword .s32 4 4 signed integral word .s64 8 8 signed integral doubleword .u8 1 1 unsigned integral byte .u16 2 2 unsigned integral halfword .u32 4 4 unsigned integral word .u64 8 8 unsigned integral doubleword .f16 2 2 IEEE half precision .f32 4 4 IEEE single precision .f64 8 8 IEEE double precision 2.2. Aggregates and Unions  Beyond the scalar types, PTX also supports native-vector types of these scalar types, with both its vector syntax and its byte-array syntax. For scalar types with a size no greater than four bytes, vector types with 1, 2, 3, and 4 elements exist; for all other types, only 1 and 2 element vector types exist. All aggregates and unions can be supported in PTX with its byte-array syntax. The following are the size-and-alignment rules for all aggregates and unions. For a non-native-vector type, an entire aggregate or union is aligned on the same boundary as its most strictly aligned member. This rule is not followed if the alignments are defined by the input language. For example, in OpenCL built-in vector data types have their alignment set to the size of the built-in data type in bytes. For a native vector type – discussed at the start of this section – the alignment is defined as follows. (For the definitions below, the native vector has n elements and has an element type t.) For a vector with an odd number of elements, its alignment is the same as its member: alignof(t). For a vector with an even number of elements, its alignment is set to number of elements times the alignment of its member: n*alignof(t). Each member is assigned to the lowest available offset with the appropriate alignment. This may require internal padding, depending on the previous member. The size of an aggregate or union, if necessary, is increased to make it a multiple of the alignment of the aggregate or union. This may require tail padding, depending on the last member. 2.3. Bit Fields  C structure and union definitions may have bit fields that define integral objects with a specified number of bits. Bit Field Type Width w Range signed char 1 to 8 -2 w-1 to 2 w-1 - 1 unsigned char 1 to 8 0 to 2 w - 1 signed short 1 to 16 -2 w-1 to 2 w-1 - 1 unsigned short 1 to 16 0 to 2 w - 1 signed int 1 to 32 -2 w-1 to 2 w-1 - 1 unsigned int 1 to 32 0 to 2 w - 1 signed long long 1 to 64 -2 w-1 to 2 w-1 - 1 unsigned long long 1 to 64 0 to 2 w - 1 Current GPUs only support little-endian memory, so the below assumes little-endian layout. The following are rules that apply to bit fields. Plain bit fields (neither signed nor unsigned is specified) are treated as signed. When no type is provided (e.g., signed : 6 is specified), the type defaults to int. Bit fields obey the same size and alignment rules as other structure and union members, with the following modifications. Bit fields are allocated in memory from right to left (least to more significant) for little endian. A bit field must entirely reside in a storage unit appropriate for its declared type. A bit field should never cross its unit boundary. Bit fields may share a storage unit with other structure and union members, including members that are not bit fields, as long as there is enough space within the storage unit. Unnamed bit fields do not affect the alignment of a structure or union. Zero-length bit fields force the alignment of the following member of a structure to the next alignment boundary corresponding to the bit-field type. An unnamed, zero-length bit field will not force the external alignment of the structure to that boundary. If an unnamed, zero-length bit field has a stricter alignment than the external alignment, there is no guarantee that the stricter alignment will be maintained when the structure or union gets allocated to memory. The following figures contain examples of bit fields. Figure 1 shows the byte offsets (upper corners) and the bit numbers (lower corners) that are used in the examples. The remaining figures show different bit-field examples. Bit Numbering  Bit-field Allocation  Boundary Alignment  Storage Unit Sharing  Union Allocation  Unnamed Bit Fields  2.4. Texture, Sampler, and Surface Types  Texture, sampler and surface types are used to define references to texture and surface memory. The CUDA architecture provides hardware and instructions to efficiently read data from texture or surface memory as opposed to global memory. References to textures are bound through runtime functions to device read-only regions of memory, called a texture memory, before they can be used by a kernel. A texture reference has several attributes e.g. normalized mode, addressing mode, and texture filtering etc. A sampler reference can be used to sample a texture when read in a kernel. A surface reference is used to read or write data from and to the surface memory. It also has various attributes similar to a texture. At the PTX level objects that access texture or surface memory are referred to as opaque objects. Textures are expressed by either a .texref or .samplerref type and surfaces are expressed by the .surfref type. The data of opaque objects can be accessed by specific instructions (TEX for .texref/.samplerref and SULD/SUST for .surfref). The attributes of opaque objects are implemented by allocating a descriptor in memory which is populated by the driver. PTX TXQ/SUQ instructions get translated into memory reads of fields of the descriptor. The internal format of the descriptor varies with each architecture and should not be relied on by the user. The data and the attributes of an opaque object may be accessed directly if the texture or surface reference is known at compile time or indirectly. If the reference is not known during compile time all information required to read data and attributes is contained in a .b64 value called the handle. The handle can be used to pass and return oqaque object references to and from functions as well as to reference external textures, samplers and surfaces. 3. Function Calling Sequence  This section describes the PTX-level function calling sequence, including register usage, stack-frame layout, and parameter passing. The PTX-level function calling sequence describes what gets represented in PTX to enable function calls. There is an abstraction at this level. Most of the details associated with the function calling sequence are handled at the SASS level. PTX versions earlier than 2.0 do not conform to the ABI defined in this document, and cannot perform ABI compatible function calls. For the calling convention to work PTX version 2.0 or greater must be used. 3.1. Registers  At the PTX level, the registers that are specified are virtual. Register allocation occurs during PTX-to-SASS translation. The PTX-to-SASS translation also converts parameters and return values to physical registers or stack locations. 3.2. Stack Frame  The PTX level has no concept of the software stack. Manipulation of the stack is completely defined at the SASS level, and gets allocated during the PTX-to-SASS translation process. 3.3. Parameter Passing  At the PTX level, all parameters and return values present in a device function use the parameter state space (.param). The below table contains the rules for handling parameters and return values that are defined at the source level. For each source-level type, the corresponding PTX-level type that should be used is provided. Source Type Size in Bits PTX Type Integral types 8 to 32 (A) .u32 (if unsigned) or .s32 (if signed) Integral types 64 .u64 (if unsigned) or .s64 (if signed) Pointers (B) 32 .u32 Pointers (B) 64 .u64 Floating-point types (C) 32 .f32 Floating-point types (C) 64 .f64 Aggregates or unions Any size .align align .b8 name [ size ] Where align is overall aggregate-or-union alignment in bytes (D), name is variable name associated with aggregate or union, and size is the aggregate-or-union size in bytes. Handles (E) 64 .b64 (assigned from .texref, .sampleref, .surfref) NOTES: Values shorter than 32-bits are sign extended or zero extended, depending on whether they are signed or unsigned types. Unless the memory type is specified in the function declaration, all pointers passed at the PTX level must use a generic address. 16-bit floating-point types are only used for storage. Therefore, they cannot be used for parameters or return values. The alignment must be 1, 2, 4, 8, 16, 32, 64, or 128 bytes. The PTX built-in opaque types such as texture, sampler, and surface types are can be passed into functions as parameters and be returned by them through 64-bit handles. The handle contains the necessary information to access the actual data from the texture or surface memory as well as the attributes of the object stored in its type descriptor. See section Texture, Sampler, and Surface Types for more information on handles. 4. System Calls  System calls are calls into the driver operating system code. In PTX they look like regular calls, but the function definition is not given. A prototype must be provided in the PTX file, but the implementation of the function is provided by the driver. The prototype for the vprintf system call is: . extern . func (. param . s32 status ) vprintf (. param t1 format , . param t2 valist ) The following are the definitions for the vprintf parameters and return value. status : The status value that is returned by vprintf. format : A pointer to the format specifier input. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. valist : A pointer to the valist input. For 32-bit addresses, type t2 is .b32. For 64-bit addresses, type t2 is .b64. A call to vprintf using 32-bit addresses looks like: cvta . global . b32 % r2 , _fmt ; st . param . b32 [ param0 ], % r2 ; cvta . local . b32 % r3 , _valist_array ; st . param . b32 [ param1 ], % r3 ; call . uni ( _ ), vprintf , ( param0 , param1 ); For this code, _fmt is the format string in global memory, and _valist_array is the valist of arguments. Note that any pointers must be converted to generic space. The vprintf syscall is emitted as part of the printf function defined in “stdio.h”. The prototype for the malloc system call is: . extern . func (. param t1 ptr ) malloc (. param t2 size ) The following are the definitions for the malloc parameters and return value. ptr : The pointer to the memory that was allocated by malloc. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. size : The size of memory needed from malloc. This size is defined by the type size_t. When size_t is 32 bits, type t2 is .b32. When size_t is 64 bits, type t2 is .b64. The prototype for the free system call is: . extern . func free (. param t1 ptr ) The following is the definition for the free parameter. ptr : The pointer to the memory that should be freed. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. The malloc and free system calls are emitted as part of the malloc and free functions defined in “malloc.h”. In order to support assert, the PTX function call __assertfail is used whenever the assert expression produces a false value. The prototype for the __assertfail system call is: . extern . func __assertfail (. param t1 message , . param t1 file , . param . b32 line , . param t1 function , . param t2 charSize ) The following are the definitions for the __assertfail parameters. message : The pointer to the string that should be output. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. file : The pointer to the file name string associated with the assert. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. line : The line number associated with the assert. function : The pointer to the function name string associated with the assert. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. charSize : The size in bytes of the characters contained in the __assertfail parameter strings. The only supported character size is 1. The character size is defined by the type size_t. When size_t is 32 bits, type t2 is .b32. When size_t is 64 bits, type t2 is .b64. The __assertfail system call is emitted as part of the assert macro defined in “assert.h”. 5. Debug Information  Debug information is encoded in DWARF (Debug With Arbitrary Record Format). 5.1. Generation of Debug Information  The responsibility for generating debug information is split between the PTX producer and the PTX-to-SASS backend. The PTX producer is responsible for emitting binary DWARF into the PTX file, using the .section and .b8-.b16-.b32-and-.b64 directives in PTX. This should contain the .debug_info and .debug_abbrev sections, and possibly optional sections .debug_pubnames and .debug_aranges. These sections are standard DWARF2 sections that refer to labels and registers in the PTX. The PTX-to-SASS backend is responsible for generating the .debug_line section from the .file and .loc directives in the PTX file. This section maps source lines to SASS addresses. The backend also generates the .debug_frame section. 5.2. CUDA-Specific DWARF Definitions  In order to support debugging of multiple memory segments, address class codes are defined to reflect the memory space of variables. The address-class values are emitted as the DW_AT_address_class attribute for all variable and parameter Debugging Information Entries. The address class codes are defined in the below table. Code Value Description ADDR_code_space 1 Code storage ADDR_reg_space 2 Register storage ADDR_sreg_space 3 Special register storage ADDR_const_space 4 Constant storage ADDR_global_space 5 Global storage ADDR_local_space 6 Local storage ADDR_param_space 7 Parameter storage ADDR_shared_space 8 Shared storage ADDR_surf_space 9 Surface storage ADDR_tex_space 10 Texture storage ADDR_tex_sampler_space 11 Texture sampler storage ADDR_generic_space 12 Generic-address storage 6. Example  The following is example PTX with debug information for implementing the following program that makes a call: __device__ __noinline__ int foo ( int i , int j ) { return i + j ; } __global__ void test ( int * p ) { * p = foo ( 1 , 2 ); } The resulting PTX would be something like: . version 4.2 . target sm_20 , debug . address_size 64 . file 1 \"call_example.cu\" . visible . func (. param . b32 func_retval0 ) // return value _Z3fooii ( . param . b32 _Z3fooii_param_0 , // parameter \"i\" . param . b32 _Z3fooii_param_1 ) // parameter \"j\" { . reg . s32 % r < 4 > ; . loc 1 1 1 // following instructions are for line 1 func_begin0 : ld . param . u32 % r1 , [ _Z3fooii_param_0 ]; // load 1st param ld . param . u32 % r2 , [ _Z3fooii_param_1 ]; // load 2nd param . loc 1 3 1 // following instructions are for line 3 add . s32 % r3 , % r1 , % r2 ; st . param . b32 [ func_retval0 + 0 ], % r3 ; // store return value ret ; func_end0 : } . visible . entry _Z4testPi ( . param . u64 _Z4testPi_param_0 ) // parameter *p { . reg . s32 % r < 4 > ; . reg . s64 % rd < 2 > ; . loc 1 6 1 func_begin1 : ld . param . u64 % rd1 , [ _Z4testPi_param_0 ]; // load *p mov . u32 % r1 , 1 ; mov . u32 % r2 , 2 ; . loc 1 8 9 . param . b32 param0 ; st . param . b32 [ param0 + 0 ], % r1 ; // store 1 . param . b32 param1 ; st . param . b32 [ param1 + 0 ], % r2 ; // store 2 . param . b32 retval0 ; call . uni ( retval0 ), _Z3fooii , ( param0 , param1 ); // call foo ld . param . b32 % r3 , [ retval0 + 0 ]; // get return value st . u32 [ % rd1 ], % r3 ; // *p = return value . loc 1 9 2 ret ; func_end1 : } . section . debug_info { . b32 262 . b8 2 , 0 . b32 . debug_abbrev . b8 8 , 1 , 108 , 103 , 101 , 110 , 102 , 101 , 58 , 32 , 69 , 68 , 71 , 32 , 52 , 46 , 57 . b8 0 , 4 , 99 , 97 , 108 , 108 , 49 , 46 , 99 , 117 , 0 . b64 0 . b32 . debug_line // the .debug_line section will be created by ptxas from the .loc . b8 47 , 104 , 111 , 109 , 101 , 47 , 109 , 109 , 117 , 114 , 112 , 104 , 121 , 47 , 116 . b8 101 , 115 , 116 , 0 , 2 , 95 , 90 , 51 , 102 , 111 , 111 , 105 , 105 , 0 , 95 , 90 . b8 51 , 102 , 111 , 111 , 105 , 105 , 0 . b32 1 , 1 , 164 . b8 1 . b64 func_begin0 // start and end location of foo . b64 func_end0 . b8 1 , 156 , 3 , 105 , 0 . b32 1 , 1 , 164 . b8 5 , 144 , 177 , 228 , 149 , 1 , 2 , 3 , 106 , 0 . b32 1 , 1 , 164 . b8 5 , 144 , 178 , 228 , 149 , 1 , 2 , 0 , 4 , 105 , 110 , 116 , 0 , 5 . b32 4 . b8 2 , 95 , 90 , 52 , 116 , 101 , 115 , 116 , 80 , 105 , 0 , 95 , 90 , 52 , 116 , 101 . b8 115 , 116 , 80 , 105 , 0 . b32 1 , 6 , 253 . b8 1 . b64 func_begin1 // start and end location of test . b64 func_end1 . b8 1 , 156 , 3 , 112 , 0 . b32 1 , 6 , 259 . b8 9 , 3 . b64 _Z4testPi_param_0 . b8 7 , 0 , 5 , 118 , 111 , 105 , 100 , 0 , 6 . b32 164 . b8 12 , 0 } . section . debug_abbrev { . b8 1 , 17 , 1 , 37 , 8 , 19 , 11 , 3 , 8 , 17 , 1 , 16 , 6 , 27 , 8 , 0 , 0 , 2 , 46 , 1 , 135 . b8 64 , 8 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 63 , 12 , 17 , 1 , 18 , 1 , 64 , 10 , 0 , 0 . b8 3 , 5 , 0 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 2 , 10 , 51 , 11 , 0 , 0 , 4 , 36 , 0 , 3 . b8 8 , 62 , 11 , 11 , 6 , 0 , 0 , 5 , 59 , 0 , 3 , 8 , 0 , 0 , 6 , 15 , 0 , 73 , 19 , 51 , 11 . b8 0 , 0 , 0 } . section . debug_pubnames { . b32 41 . b8 2 , 0 . b32 . debug_info . b32 262 , 69 . b8 95 , 90 , 51 , 102 , 111 , 111 , 105 , 105 , 0 . b32 174 . b8 95 , 90 , 52 , 116 , 101 , 115 , 116 , 80 , 105 , 0 . b32 0 } 7. C++  The C++ implementation for device functions follows the Itanium C++ ABI. However, not everything in C++ is supported. In particular, the following are not supported in device code. Exceptions and try/catch blocks RTTI STL library Global constructors and destructors Virtual functions and classes across host and device (i.e., vtables cannot be used across host and device) There are also a few C features that are not currently supported: stdio other than printf 8. Notices  8.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html", "parent_url": "https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html", "content_type": "text/html", "text": "Inline PTX Assembly 1. Using Inline PTX Assembly in CUDA 1.1. Assembler (ASM) Statements 1.1.1. Parameters 1.1.2. Constraints 1.2. Pitfalls 1.2.1. Namespace Conflicts 1.2.2. Memory Space Conflicts 1.2.3. Incorrect Optimization 1.2.4. Incorrect PTX 1.3. Error Checking 2. Notices 2.1. Notice 2.2. OpenCL 2.3. Trademarks Inline PTX Assembly in CUDA » 1. Using Inline PTX Assembly in CUDA v12.5 | PDF | Archive Inline PTX Assembly in CUDA The reference guide for inlining PTX (parallel thread execution) assembly statements into CUDA. 1. Using Inline PTX Assembly in CUDA  The NVIDIA ® CUDA ® programming environment provides a parallel thread execution (PTX) instruction set architecture (ISA) for using the GPU as a data-parallel computing device. For more information on the PTX ISA, refer to the latest version of the PTX ISA reference document . This application note describes how to inline PTX assembly language statements into CUDA code. 1.1. Assembler (ASM) Statements  Assembler statements, asm() , provide a way to insert arbitrary PTX code into your CUDA program. A simple example is: asm ( \"membar.gl;\" ); This inserts a PTX membar.gl into your generated PTX code at the point of the asm() statement. 1.1.1. Parameters  An asm() statement becomes more complicated, and more useful, when we pass values in and out of the asm. The basic syntax is as follows: asm ( \"template-string\" : \"constraint\" ( output ) : \"constraint\" ( input )); where you can have multiple input or output operands separated by commas. The template string contains PTX instructions with references to the operands. Multiple PTX instructions can be given by separating them with semicolons. A simple example is as follows: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); Each %n in the template string is an index into the following list of operands, in text order. So %0 refers to the first operand, %1 to the second operand, and so on. Since the output operands are always listed ahead of the input operands, they are assigned the smallest indices. This example is conceptually equivalent to the following: add . s32 i , j , k ; Note that the numbered references in the string can be in arbitrary order. The following is equivalent to the above example: asm ( \"add.s32 %0, %2, %1;\" : \"=r\" ( i ) : \"r\" ( k ), \"r\" ( j )); You can also repeat a reference, e.g.: asm ( \"add.s32 %0, %1, %1;\" : \"=r\" ( i ) : \"r\" ( k )); is conceptually add . s32 i , k , k ; If there is no input operand, you can drop the final colon, e.g.: asm ( \"mov.s32 %0, 2;\" : \"=r\" ( i )); If there is no output operand, the colon separators are adjacent, e.g.: asm ( \"mov.s32 r1, %0;\" :: \"r\" ( i )); If you want the % in a ptx instruction, then you should escape it with double %% , e.g.: asm ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x )); The above was simplified to explain the ordering of the string % references. In reality, the operand values are passed via whatever mechanism the constraint specifies. The full list of constraints will be explained later, but the “r” constraint refers to a 32bit integer register. So the earlier example asm() statement: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); produces the following code sequence in the output generated by the compiler: ld . s32 r1 , [ j ]; ld . s32 r2 , [ k ]; add . s32 r3 , r1 , r2 ; st . s32 [ i ], r3 ; This is where the distinction between input and output operands becomes important. The input operands are loaded into registers before the asm() statement, then the result register is stored to the output operand. The “=” modifier in “=r” specifies that the register is written to. There is also available a “+” modifier that specifies the register is both read and written, e.g.: asm ( \"add.s32 %0, %0, %1;\" : \"+r\" ( i ) : \"r\" ( j )); Multiple instructions can be combined into a single asm() statement; basically, anything legal can be put into the asm string. Multiple instructions can be split across multiple lines by making use of C/C++’s implicit string concatenation. Both C++ style line end comments “//” and classical C-style comments “/**/” can be interspersed with these strings. To generate readable output in the PTX intermediate file it is best practice to terminate each instruction string except the last one with “nt”. For example, a cube routine could be written as: __device__ int cube ( int x ) { int y ; asm ( \".reg .u32 t1; \\n\\t \" // temp reg t1 \" mul.lo.u32 t1, %1, %1; \\n\\t \" // t1 = x * x \" mul.lo.u32 %0, t1, %1;\" // y = t1 * x : \"=r\" ( y ) : \"r\" ( x )); return y ; } If an output operand is conditionally updated by the asm instructions, then the “+” modifier should be used. There is an implicit use of the output operand in such a case. For example, __device__ int cond ( int x ) { int y = 0 ; asm ( \"{ \\n\\t \" \" .reg .pred %p; \\n\\t \" \" setp.eq.s32 %p, %1, 34; \\n\\t \" // x == 34? \" @%p mov.s32 %0, 1; \\n\\t \" // set y to 1 if true \"}\" // conceptually y = (x==34)?1:y : \"+r\" ( y ) : \"r\" ( x )); return y ; } 1.1.2. Constraints  There is a separate constraint letter for each PTX register type: \"h\" = . u16 reg \"r\" = . u32 reg \"l\" = . u64 reg \"f\" = . f32 reg \"d\" = . f64 reg Example: asm ( \"cvt.f32.s64 %0, %1;\" : \"=f\" ( x ) : \"l\" ( y )); generates: ld . s64 rd1 , [ y ]; cvt . f32 . s64 f1 , rd1 ; st . f32 [ x ], f1 ; The constraint \"n\" may be used for immediate integer operands with a known value. Example: asm ( \"add.u32 %0, %0, %1;\" : \"=r\" ( x ) : \"n\" ( 42 )); generates: add . u32 r1 , r1 , 42 ; The constraint \"C\" can be used for operand of type ‘array of const char’, where the array contents are known at compile time.\nIt is intended to allow customization of PTX instruction modes based on compile time computation (see examples). Here is the specification\nfor the \"C\" constraint: 'C' ( constant - expression ) The constant-expression is evaluated during compilation and shall generate the address of a variable V , where: V has static storage duration . V has type ‘array of const char’. V is constant-initialized . If V is a static class member, then V ’s initializing declaration is the declaration within the class. During translation, the compiler will replace a reference to the operand within the Assembler Template with the contents of V ’s initializer, except for the last trailing zero.\nNo constraint modifiers are allowed for this constraint. This constraint can only be used in device code. (terms in italics are C++ standard terms and/or terms from the GNU inline asm specification). Here’s an example of the use of C constraint to generate different PTX instruction modes based on compile time computation: constexpr int mode_rz = 0 ; constexpr int mode_rn = 1 ; template < int mode > struct helper ; template <> struct helper < mode_rz > { static constexpr const char mode [] = \".rz\" ; }; template <> struct helper < mode_rn > { static constexpr const char mode [] = \".rn\" ; }; template < int rounding_mode > __device__ float compute_add ( float a , float b ) { float result ; asm ( \"add.f32%1 %0,%2,%3;\" : \"=f\" ( result ) : \"C\" ( helper < rounding_mode >:: mode ), \"f\" ( a ), \"f\" ( b )); return result ; } __global__ void kern ( float * result , float a , float b ) { * result ++ = compute_add < mode_rn > ( a , b ); // generates add.f32.rn * result = compute_add < mode_rz > ( a , b ); // generates add.f32.rz } Other examples (compile in C++17 or later dialect): struct S1 { static constexpr char buf1 [] = \"Jumped\" ; static constexpr char buf2 [] = { 'O' , 'v' , 'e' , 'r' , 0 }; }; template < const char * p1 , const char * p2 , const char * p3 > __device__ void doit () { asm volatile ( \"%0 %1 %2\" : : \"C\" ( p1 ), \"C\" ( p2 ), \"C\" ( p3 )); } struct S2 { static const char buf []; }; const char S2 :: buf [] = \"this\" ; const char buf3 [] = \"Jumped\" ; extern const char buf4 []; __global__ void foo () { static const char v1 [] = \"The\" ; static constexpr char v2 [] = \"Quick\" ; static const char v3 [] = { 'B' , 'r' , 'o' , 'w' , 'n' , 0 }; static constexpr char v4 [] = { 'F' , 'o' , 'x' , 0 }; //OK: generates 'The Quick Brown Fox Jumped Over' in PTX asm volatile ( \"%0 %1 %2 %3 %4 %5\" : : \"C\" ( v1 ) , \"C\" ( v2 ), \"C\" ( v3 ), \"C\" ( v4 ), \"C\" ( S1 :: buf1 ), \"C\" ( S1 :: buf2 ) ); //OK: generates 'Brown Fox Jumped' in PTX doit < v3 , v4 , buf3 > (); //error cases const char n1 [] = \"hi\" ; //error: argument to \"C\" constraint is not a constant expression asm volatile ( \"%0\" :: \"C\" ( n1 )); //error: S2::buf was not initialized at point of declaration asm volatile ( \"%0\" :: \"C\" ( S2 :: buf )); //error: buf4 was not initialized asm volatile ( \"%0\" :: \"C\" ( buf4 )); } There is no constraint letter for 8-bit wide PTX registers. PTX instructions types accepting 8-bit wide types permit operands to be wider than the instruction-type size . Example: __device__ void copy_u8 ( char * in , char * out ) { int d ; asm ( \"ld.u8 %0, [%1];\" : \"=r\" ( d ) : \"l\" ( in )); * out = d ; } generates: ld . u8 r1 , [ rd1 ]; st . u8 [ rd2 ], r1 ; The behavior of using a constraint string that is not one of those specified above is undefined. 1.2. Pitfalls  Although asm() statements are very flexible and powerful, you may encounter some pitfalls—these are listed in this section. 1.2.1. Namespace Conflicts  If the cube function (described before) is called and inlined multiple times in the code, it generates an error about duplicate definitions of the temp register t1. To avoid this error you need to: not inline the cube function, or, nest the t1 use inside {} so that it has a separate scope for each invocation, e.g.: __device__ int cube ( int x ) { int y ; asm ( \"{ \\n\\t \" // use braces for local scope \" reg .u32 t1; \\n\\t \" // temp reg t1, \" mul.lo.u32 t1, %1, %1; \\n\\t \" // t1 = x * x \" mul.lo.u32 %0, t1, %1; \\n\\t \" // y = t1 * x \"}\" : \"=r\" ( y ) : \"r\" ( x )); return y ; } Note that you can similarly use braces for local labels inside the asm() statement. 1.2.2. Memory Space Conflicts  Since asm() statements have no way of knowing what memory space a register is in, the user must make sure that the appropriate PTX instruction is used. For sm_20 and greater, any pointer argument to an asm() statement is passed as a generic address. 1.2.3. Incorrect Optimization  The compiler assumes that an asm() statement has no side effects except to change the output operands. To ensure that the asm is not deleted or moved during generation of PTX, you should use the volatile keyword, e.g.: asm volatile ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x )); Normally any memory that is written to will be specified as an out operand, but if there is a hidden side effect on user memory (for example, indirect access of a memory location via an operand), or if you want to stop any memory optimizations around the asm() statement performed during generation of PTX, you can add a “memory” clobbers specification after a 3rd colon, e.g.: asm volatile ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x ) :: \"memory\" ); asm ( \"st.u32 [%0], %1;\" :: \"l\" ( p ), \"r\" ( x ) : \"memory\" ); 1.2.4. Incorrect PTX  The compiler front end does not parse the asm() statement template string and does not know what it means or even whether it is valid PTX input. So if there are any errors in the string it will not show up until ptxas . For example, if you pass a value with an “r” constraint but use it in an add.f64 you will get a parse error from ptxas. Similarly, operand modifiers are not supported. For example, in asm ( \"mov.u32 %0, %n1;\" : \"=r\" ( n ) : \"r\" ( 1 )); the ‘n’ modifier in “%n1” is not supported and will be passed to ptxas , where it can cause undefined behavior. Refer to the document nvcc.pdf for further compiler related details. 1.3. Error Checking  The following are some of the error checks that the compiler will do on inlinePTXasm: Multiple constraint letters for a single asm operand are not allowed, e.g.: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"rf\" ( j ), \"r\" ( k )); error: an asm operand may specify only one constraint letter in a __device__/__global__ function Only scalar variables are allowed as asm operands. Specifically aggregates like ‘struct’ type variables are not allowed, e.g. int4 i4 ; asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i4 ) : \"r\" ( j ), \"r\" ( k )); error: an asm operand must have scalar type The type and size implied by a PTX asm constraint must match that of the associated operand. Example where size does not match: For ‘char’ type variable “ci”, asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( ci ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(1) does not match type/size implied by constraint ‘r’ In order to use ‘char’ type variables “ci”, “cj”, and “ck” in the above asm statement, code segment similar to the following may be used, int temp = ci ; asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( temp ) : \"r\" (( int ) cj ), \"r\" (( int ) ck )); ci = temp ; Another example where type does not match: For ‘float’ type variable “fi”, asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( fi ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(4) does not match type/size implied by constraint ‘r’ 2. Notices  2.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cublas/index.html", "parent_url": "https://docs.nvidia.com/cuda/cublas/index.html", "content_type": "text/html", "text": "cuBLAS 1. Introduction 1.1. Data Layout 1.2. New and Legacy cuBLAS API 1.3. Example Code 2. Using the cuBLAS API 2.1. General Description 2.1.1. Error Status 2.1.2. cuBLAS Context 2.1.3. Thread Safety 2.1.4. Results Reproducibility 2.1.5. Scalar Parameters 2.1.6. Parallelism with Streams 2.1.7. Batching Kernels 2.1.8. Cache Configuration 2.1.9. Static Library Support 2.1.10. GEMM Algorithms Numerical Behavior 2.1.11. Tensor Core Usage 2.1.12. CUDA Graphs Support 2.1.13. 64-bit Integer Interface 2.2. cuBLAS Datatypes Reference 2.2.1. cublasHandle_t 2.2.2. cublasStatus_t 2.2.3. cublasOperation_t 2.2.4. cublasFillMode_t 2.2.5. cublasDiagType_t 2.2.6. cublasSideMode_t 2.2.7. cublasPointerMode_t 2.2.8. cublasAtomicsMode_t 2.2.9. cublasGemmAlgo_t 2.2.10. cublasMath_t 2.2.11. cublasComputeType_t 2.3. CUDA Datatypes Reference 2.3.1. cudaDataType_t 2.3.2. libraryPropertyType_t 2.4. cuBLAS Helper Function Reference 2.4.1. cublasCreate() 2.4.2. cublasDestroy() 2.4.3. cublasGetVersion() 2.4.4. cublasGetProperty() 2.4.5. cublasGetStatusName() 2.4.6. cublasGetStatusString() 2.4.7. cublasSetStream() 2.4.8. cublasSetWorkspace() 2.4.9. cublasGetStream() 2.4.10. cublasGetPointerMode() 2.4.11. cublasSetPointerMode() 2.4.12. cublasSetVector() 2.4.13. cublasGetVector() 2.4.14. cublasSetMatrix() 2.4.15. cublasGetMatrix() 2.4.16. cublasSetVectorAsync() 2.4.17. cublasGetVectorAsync() 2.4.18. cublasSetMatrixAsync() 2.4.19. cublasGetMatrixAsync() 2.4.20. cublasSetAtomicsMode() 2.4.21. cublasGetAtomicsMode() 2.4.22. cublasSetMathMode() 2.4.23. cublasGetMathMode() 2.4.24. cublasSetSmCountTarget() 2.4.25. cublasGetSmCountTarget() 2.4.26. cublasLoggerConfigure() 2.4.27. cublasGetLoggerCallback() 2.4.28. cublasSetLoggerCallback() 2.5. cuBLAS Level-1 Function Reference 2.5.1. cublasI<t>amax() 2.5.2. cublasI<t>amin() 2.5.3. cublas<t>asum() 2.5.4. cublas<t>axpy() 2.5.5. cublas<t>copy() 2.5.6. cublas<t>dot() 2.5.7. cublas<t>nrm2() 2.5.8. cublas<t>rot() 2.5.9. cublas<t>rotg() 2.5.10. cublas<t>rotm() 2.5.11. cublas<t>rotmg() 2.5.12. cublas<t>scal() 2.5.13. cublas<t>swap() 2.6. cuBLAS Level-2 Function Reference 2.6.1. cublas<t>gbmv() 2.6.2. cublas<t>gemv() 2.6.3. cublas<t>ger() 2.6.4. cublas<t>sbmv() 2.6.5. cublas<t>spmv() 2.6.6. cublas<t>spr() 2.6.7. cublas<t>spr2() 2.6.8. cublas<t>symv() 2.6.9. cublas<t>syr() 2.6.10. cublas<t>syr2() 2.6.11. cublas<t>tbmv() 2.6.12. cublas<t>tbsv() 2.6.13. cublas<t>tpmv() 2.6.14. cublas<t>tpsv() 2.6.15. cublas<t>trmv() 2.6.16. cublas<t>trsv() 2.6.17. cublas<t>hemv() 2.6.18. cublas<t>hbmv() 2.6.19. cublas<t>hpmv() 2.6.20. cublas<t>her() 2.6.21. cublas<t>her2() 2.6.22. cublas<t>hpr() 2.6.23. cublas<t>hpr2() 2.6.24. cublas<t>gemvBatched() 2.6.25. cublas<t>gemvStridedBatched() 2.7. cuBLAS Level-3 Function Reference 2.7.1. cublas<t>gemm() 2.7.2. cublas<t>gemm3m() 2.7.3. cublas<t>gemmBatched() 2.7.4. cublas<t>gemmStridedBatched() 2.7.5. cublas<t>gemmGroupedBatched() 2.7.6. cublas<t>symm() 2.7.7. cublas<t>syrk() 2.7.8. cublas<t>syr2k() 2.7.9. cublas<t>syrkx() 2.7.10. cublas<t>trmm() 2.7.11. cublas<t>trsm() 2.7.12. cublas<t>trsmBatched() 2.7.13. cublas<t>hemm() 2.7.14. cublas<t>herk() 2.7.15. cublas<t>her2k() 2.7.16. cublas<t>herkx() 2.8. BLAS-like Extension 2.8.1. cublas<t>geam() 2.8.2. cublas<t>dgmm() 2.8.3. cublas<t>getrfBatched() 2.8.4. cublas<t>getrsBatched() 2.8.5. cublas<t>getriBatched() 2.8.6. cublas<t>matinvBatched() 2.8.7. cublas<t>geqrfBatched() 2.8.8. cublas<t>gelsBatched() 2.8.9. cublas<t>tpttr() 2.8.10. cublas<t>trttp() 2.8.11. cublas<t>gemmEx() 2.8.12. cublasGemmEx() 2.8.13. cublasGemmBatchedEx() 2.8.14. cublasGemmStridedBatchedEx() 2.8.15. cublasGemmGroupedBatchedEx() 2.8.16. cublasCsyrkEx() 2.8.17. cublasCsyrk3mEx() 2.8.18. cublasCherkEx() 2.8.19. cublasCherk3mEx() 2.8.20. cublasNrm2Ex() 2.8.21. cublasAxpyEx() 2.8.22. cublasDotEx() 2.8.23. cublasRotEx() 2.8.24. cublasScalEx() 3. Using the cuBLASLt API 3.1. General Description 3.1.1. Problem Size Limitations 3.1.2. Heuristics Cache 3.1.3. cuBLASLt Logging 3.1.4. 8-bit Floating Point Data Types (FP8) Usage 3.1.5. Disabling CPU Instructions 3.1.6. Atomics Synchronization 3.2. cuBLASLt Code Examples 3.3. cuBLASLt Datatypes Reference 3.3.1. cublasLtClusterShape_t 3.3.2. cublasLtEpilogue_t 3.3.3. cublasLtHandle_t 3.3.4. cublasLtLoggerCallback_t 3.3.5. cublasLtMatmulAlgo_t 3.3.6. cublasLtMatmulAlgoCapAttributes_t 3.3.7. cublasLtMatmulAlgoConfigAttributes_t 3.3.8. cublasLtMatmulDesc_t 3.3.9. cublasLtMatmulDescAttributes_t 3.3.10. cublasLtMatmulHeuristicResult_t 3.3.11. cublasLtMatmulInnerShape_t 3.3.12. cublasLtMatmulPreference_t 3.3.13. cublasLtMatmulPreferenceAttributes_t 3.3.14. cublasLtMatmulSearch_t 3.3.15. cublasLtMatmulTile_t 3.3.16. cublasLtMatmulStages_t 3.3.17. cublasLtNumericalImplFlags_t 3.3.18. cublasLtMatrixLayout_t 3.3.19. cublasLtMatrixLayoutAttribute_t 3.3.20. cublasLtMatrixTransformDesc_t 3.3.21. cublasLtMatrixTransformDescAttributes_t 3.3.22. cublasLtOrder_t 3.3.23. cublasLtPointerMode_t 3.3.24. cublasLtPointerModeMask_t 3.3.25. cublasLtReductionScheme_t 3.4. cuBLASLt API Reference 3.4.1. cublasLtCreate() 3.4.2. cublasLtDestroy() 3.4.3. cublasLtDisableCpuInstructionsSetMask() 3.4.4. cublasLtGetCudartVersion() 3.4.5. cublasLtGetProperty() 3.4.6. cublasLtGetStatusName() 3.4.7. cublasLtGetStatusString() 3.4.8. cublasLtHeuristicsCacheGetCapacity() 3.4.9. cublasLtHeuristicsCacheSetCapacity() 3.4.10. cublasLtGetVersion() 3.4.11. cublasLtLoggerSetCallback() 3.4.12. cublasLtLoggerSetFile() 3.4.13. cublasLtLoggerOpenFile() 3.4.14. cublasLtLoggerSetLevel() 3.4.15. cublasLtLoggerSetMask() 3.4.16. cublasLtLoggerForceDisable() 3.4.17. cublasLtMatmul() 3.4.18. cublasLtMatmulAlgoCapGetAttribute() 3.4.19. cublasLtMatmulAlgoCheck() 3.4.20. cublasLtMatmulAlgoConfigGetAttribute() 3.4.21. cublasLtMatmulAlgoConfigSetAttribute() 3.4.22. cublasLtMatmulAlgoGetHeuristic() 3.4.23. cublasLtMatmulAlgoGetIds() 3.4.24. cublasLtMatmulAlgoInit() 3.4.25. cublasLtMatmulDescCreate() 3.4.26. cublasLtMatmulDescInit() 3.4.27. cublasLtMatmulDescDestroy() 3.4.28. cublasLtMatmulDescGetAttribute() 3.4.29. cublasLtMatmulDescSetAttribute() 3.4.30. cublasLtMatmulPreferenceCreate() 3.4.31. cublasLtMatmulPreferenceInit() 3.4.32. cublasLtMatmulPreferenceDestroy() 3.4.33. cublasLtMatmulPreferenceGetAttribute() 3.4.34. cublasLtMatmulPreferenceSetAttribute() 3.4.35. cublasLtMatrixLayoutCreate() 3.4.36. cublasLtMatrixLayoutInit() 3.4.37. cublasLtMatrixLayoutDestroy() 3.4.38. cublasLtMatrixLayoutGetAttribute() 3.4.39. cublasLtMatrixLayoutSetAttribute() 3.4.40. cublasLtMatrixTransform() 3.4.41. cublasLtMatrixTransformDescCreate() 3.4.42. cublasLtMatrixTransformDescInit() 3.4.43. cublasLtMatrixTransformDescDestroy() 3.4.44. cublasLtMatrixTransformDescGetAttribute() 3.4.45. cublasLtMatrixTransformDescSetAttribute() 4. Using the cuBLASXt API 4.1. General description 4.1.1. Tiling design approach 4.1.2. Hybrid CPU-GPU computation 4.1.3. Results reproducibility 4.2. cuBLASXt API Datatypes Reference 4.2.1. cublasXtHandle_t 4.2.2. cublasXtOpType_t 4.2.3. cublasXtBlasOp_t 4.2.4. cublasXtPinningMemMode_t 4.3. cuBLASXt API Helper Function Reference 4.3.1. cublasXtCreate() 4.3.2. cublasXtDestroy() 4.3.3. cublasXtDeviceSelect() 4.3.4. cublasXtSetBlockDim() 4.3.5. cublasXtGetBlockDim() 4.3.6. cublasXtSetCpuRoutine() 4.3.7. cublasXtSetCpuRatio() 4.3.8. cublasXtSetPinningMemMode() 4.3.9. cublasXtGetPinningMemMode() 4.4. cuBLASXt API Math Functions Reference 4.4.1. cublasXt<t>gemm() 4.4.2. cublasXt<t>hemm() 4.4.3. cublasXt<t>symm() 4.4.4. cublasXt<t>syrk() 4.4.5. cublasXt<t>syr2k() 4.4.6. cublasXt<t>syrkx() 4.4.7. cublasXt<t>herk() 4.4.8. cublasXt<t>her2k() 4.4.9. cublasXt<t>herkx() 4.4.10. cublasXt<t>trsm() 4.4.11. cublasXt<t>trmm() 4.4.12. cublasXt<t>spmm() 5. Using the cuBLASDx API 6. Using the cuBLAS Legacy API 6.1. Error Status 6.2. Initialization and Shutdown 6.3. Thread Safety 6.4. Memory Management 6.5. Scalar Parameters 6.6. Helper Functions 6.7. Level-1,2,3 Functions 6.8. Converting Legacy to the cuBLAS API 6.9. Examples 7. cuBLAS Fortran Bindings 8. Interaction with Other Libraries and Tools 8.1. nvprune 9. Acknowledgements 10. Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks cuBLAS » 1. Introduction v12.5 | PDF | Archive cuBLAS The API Reference guide for cuBLAS, the CUDA Basic Linear Algebra Subroutine library. 1. Introduction  The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA®CUDA™ runtime. It allows the user to access the computational resources of NVIDIA Graphics Processing Unit (GPU). The cuBLAS Library exposes four sets of APIs: The cuBLAS API , which is simply called cuBLAS API in this document (starting with CUDA 6.0), The cuBLASXt API (starting with CUDA 6.0), and The cuBLASLt API (starting with CUDA 10.1) The cuBLASDx API (not shipped with the CUDA Toolkit) To use the cuBLAS API, the application must allocate the required matrices and vectors in the GPU memory space, fill them with data, call the sequence of desired cuBLAS functions, and then upload the results from the GPU memory space back to the host. The cuBLAS API also provides helper functions for writing and retrieving data from the GPU. To use the cuBLASXt API, the application may have the data on the Host or any of the devices involved in the computation, and the Library will take care of dispatching the operation to, and transferring the data to, one or multiple GPUs present in the system, depending on the user request. The cuBLASLt is a lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API. This library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. After a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data. 1.1. Data Layout  For maximum compatibility with existing Fortran environments, the cuBLAS library uses column-major storage, and 1-based indexing. Since C and C++ use row-major storage, applications written in these languages can not use the native array semantics for two-dimensional arrays. Instead, macros or inline functions should be defined to implement matrices on top of one-dimensional arrays. For Fortran code ported to C in mechanical fashion, one may chose to retain 1-based indexing to avoid the need to transform loops. In this case, the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) Here, ld refers to the leading dimension of the matrix, which in the case of column-major storage is the number of rows of the allocated matrix (even if only a submatrix of it is being used). For natively written C and C++ code, one would most likely choose 0-based indexing, in which case the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2C(i,j,ld) (((j)*(ld))+(i)) 1.2. New and Legacy cuBLAS API  Starting with version 4.0, the cuBLAS Library provides a new API, in addition to the existing legacy API. This section discusses why a new API is provided, the advantages of using it, and the differences with the existing legacy API. Warning The legacy cuBLAS API is deprecated and will be removed in future release. The new cuBLAS library API can be used by including the header file cublas_v2.h . It has the following features that the legacy cuBLAS API does not have: The handle to the cuBLAS library context is initialized using the function and is explicitly passed to every subsequent library function call. This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs. This also allows the cuBLAS APIs to be reentrant. The scalars \\(\\alpha\\) and \\(\\beta\\) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host. This change allows library functions to execute asynchronously using streams even when \\(\\alpha\\) and \\(\\beta\\) are generated by a previous kernel. When a library routine returns a scalar result, it can be returned by reference on the host or the device, instead of only being allowed to be returned by value only on the host. This change allows library routines to be called asynchronously when the scalar result is generated and returned by reference on the device resulting in maximum parallelism. The error status cublasStatus_t is returned by all cuBLAS library function calls. This change facilitates debugging and simplifies software development. Note that cublasStatus was renamed cublasStatus_t to be more consistent with other types in the cuBLAS library. The cublasAlloc() and cublasFree() functions have been deprecated. This change removes these unnecessary wrappers around cudaMalloc() and cudaFree() , respectively. The function cublasSetKernelStream() was renamed cublasSetStream() to be more consistent with the other CUDA libraries. The legacy cuBLAS API, explained in more detail in Using the cuBLAS Legacy API , can be used by including the header file cublas.h . Since the legacy API is identical to the previously released cuBLAS library API, existing applications will work out of the box and automatically use this legacy API without any source code changes. The current and the legacy cuBLAS APIs cannot be used simultaneously in a single translation unit: including both cublas.h and cublas_v2.h header files will lead to compilation errors due to incompatible symbol redeclarations. In general, new applications should not use the legacy cuBLAS API, and existing applications should convert to using the new API if it requires sophisticated and optimal stream parallelism, or if it calls cuBLAS routines concurrently from multiple threads. For the rest of the document, the new cuBLAS Library API will simply be referred to as the cuBLAS Library API. As mentioned earlier the interfaces to the legacy and the cuBLAS library APIs are the header file cublas.h and cublas_v2.h , respectively. In addition, applications using the cuBLAS library need to link against: The DSO cublas.so for Linux, The DLL cublas.dll for Windows, or The dynamic library cublas.dylib for Mac OS X. Note The same dynamic library implements both the new and legacy cuBLAS APIs. 1.3. Example Code  For sample code references please see the two examples below. They show an application written in C using the cuBLAS library API with two indexing styles (Example 1. “Application Using C and cuBLAS: 1-based indexing” and Example 2. “Application Using C and cuBLAS: 0-based Indexing”). //Example 1. Application Using C and cuBLAS: 1-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include <cuda_runtime.h> #include \"cublas_v2.h\" #define M 6 #define N 5 #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) static __inline__ void modify ( cublasHandle_t handle , float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( handle , n - q + 1 , & alpha , & m [ IDX2F ( p , q , ldm )], ldm ); cublasSscal ( handle , ldm - p + 1 , & beta , & m [ IDX2F ( p , q , ldm )], 1 ); } int main ( void ){ cudaError_t cudaStat ; cublasStatus_t stat ; cublasHandle_t handle ; int i , j ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { a [ IDX2F ( i , j , M )] = ( float )(( i -1 ) * N + j ); } } cudaStat = cudaMalloc (( void ** ) & devPtrA , M * N * sizeof ( * a )); if ( cudaStat != cudaSuccess ) { printf ( \"device memory allocation failed\" ); free ( a ); return EXIT_FAILURE ; } stat = cublasCreate ( & handle ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"CUBLAS initialization failed \\n \" ); free ( a ); cudaFree ( devPtrA ); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } modify ( handle , devPtrA , M , N , 2 , 3 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } cudaFree ( devPtrA ); cublasDestroy ( handle ); for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2F ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } //Example 2. Application Using C and cuBLAS: 0-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include <cuda_runtime.h> #include \"cublas_v2.h\" #define M 6 #define N 5 #define IDX2C(i,j,ld) (((j)*(ld))+(i)) static __inline__ void modify ( cublasHandle_t handle , float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( handle , n - q , & alpha , & m [ IDX2C ( p , q , ldm )], ldm ); cublasSscal ( handle , ldm - p , & beta , & m [ IDX2C ( p , q , ldm )], 1 ); } int main ( void ){ cudaError_t cudaStat ; cublasStatus_t stat ; cublasHandle_t handle ; int i , j ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { a [ IDX2C ( i , j , M )] = ( float )( i * N + j + 1 ); } } cudaStat = cudaMalloc (( void ** ) & devPtrA , M * N * sizeof ( * a )); if ( cudaStat != cudaSuccess ) { printf ( \"device memory allocation failed\" ); free ( a ); return EXIT_FAILURE ; } stat = cublasCreate ( & handle ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"CUBLAS initialization failed \\n \" ); free ( a ); cudaFree ( devPtrA ); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } modify ( handle , devPtrA , M , N , 1 , 2 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != CUBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); free ( a ); cudaFree ( devPtrA ); cublasDestroy ( handle ); return EXIT_FAILURE ; } cudaFree ( devPtrA ); cublasDestroy ( handle ); for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2C ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } 2. Using the cuBLAS API  2.1. General Description  This section describes how to use the cuBLAS library API. 2.1.1. Error Status  All cuBLAS library function calls return the error status cublasStatus_t . 2.1.2. cuBLAS Context  The application must initialize a handle to the cuBLAS library context by calling the cublasCreate() function. Then, the handle is explicitly passed to every subsequent library function call. Once the application finishes using the library, it must call the function cublasDestroy() to release the resources associated with the cuBLAS library context. This approach allows the user to explicitly control the library setup when using multiple host threads and multiple GPUs. For example, the application can use cudaSetDevice() to associate different devices with different host threads and in each of those host threads it can initialize a unique handle to the cuBLAS library context, which will use the particular device associated with that host thread. Then, the cuBLAS library function calls made with different handles will automatically dispatch the computation to different devices. The device associated with a particular cuBLAS context is assumed to remain unchanged between the corresponding cublasCreate() and cublasDestroy() calls. In order for the cuBLAS library to use a different device in the same host thread, the application must set the new device to be used by calling cudaSetDevice() and then create another cuBLAS context, which will be associated with the new device, by calling cublasCreate() . A cuBLAS library context is tightly coupled with the CUDA context that is current at the time of the cublasCreate() call. An application that uses multiple CUDA contexts is required to create a cuBLAS context per CUDA context and make sure the former never outlives the latter. 2.1.3. Thread Safety  The library is thread safe and its functions can be called from multiple host threads, even with the same handle. When multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cuBLAS calls in all threads. It is even more true for the destruction of the handle. So it is not recommended that multiple thread share the same cuBLAS handle. 2.1.4. Results Reproducibility  By design, all cuBLAS API routines from a given toolkit version, generate the same bit-wise results at every run when executed on GPUs with the same architecture and the same number of SMs. However, bit-wise reproducibility is not guaranteed across toolkit versions because the implementation might differ due to some implementation changes. This guarantee holds when a single CUDA stream is active only. If multiple concurrent streams are active, the library may optimize total performance by picking different internal implementations. Note The non-deterministic behavior of multi-stream execution is due to library optimizations in selecting internal workspace for the routines running in parallel streams. To avoid this effect user can either: provide a separate workspace for each used stream using the cublasSetWorkspace() function, or have one cuBLAS handle per stream, or use cublasLtMatmul() instead of GEMM-family of functions and provide user owned workspace, or set a debug environment variable CUBLAS_WORKSPACE_CONFIG to :16:8 (may limit overall performance) or :4096:8 (will increase library footprint in GPU memory by approximately 24MiB). Any of those settings will allow for deterministic behavior even with multiple concurrent streams sharing a single cuBLAS handle. This behavior is expected to change in a future release. For some routines such as cublas<t>symv and cublas<t>hemv , an alternate significantly faster routine can be chosen using the routine cublasSetAtomicsMode() . In that case, the results are not guaranteed to be bit-wise reproducible because atomics are used for the computation. 2.1.5. Scalar Parameters  There are two categories of the functions that use scalar parameters : Functions that take alpha and/or beta parameters by reference on the host or the device as scaling factors, such as gemm . Functions that return a scalar result on the host or the device such as amax() , amin , asum() , rotg() , rotmg() , dot() and nrm2() . For the functions of the first category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , the scalar parameters alpha and/or beta can be on the stack or allocated on the heap, shouldn’t be placed in managed memory. Underneath, the CUDA kernels related to those functions will be launched with the value of alpha and/or beta . Therefore if they were allocated on the heap, they can be freed just after the return of the call even though the kernel launch is asynchronous. When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , alpha and/or beta must be accessible on the device and their values should not be modified until the kernel is done. Note that since cudaFree() does an implicit cudaDeviceSynchronize() , cudaFree() can still be called on alpha and/or beta just after the call but it would defeat the purpose of using this pointer mode in that case. For the functions of the second category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , these functions block the CPU, until the GPU has completed its computation and the results have been copied back to the Host. When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , these functions return immediately. In this case, similar to matrix and vector results, the scalar result is ready only when execution of the routine on the GPU has completed. This requires proper synchronization in order to read the result from the host. In either case, the pointer mode CUBLAS_POINTER_MODE_DEVICE allows the library functions to execute completely asynchronously from the Host even when alpha and/or beta are generated by a previous kernel. For example, this situation can arise when iterative methods for solution of linear systems and eigenvalue problems are implemented using the cuBLAS library. 2.1.6. Parallelism with Streams  If the application uses the results computed by multiple independent tasks, CUDA™ streams can be used to overlap the computation performed in these tasks. The application can conceptually associate each stream with each task. In order to achieve the overlap of computation between the tasks, the user should create CUDA™ streams using the function cudaStreamCreate() and set the stream to be used by each individual cuBLAS library routine by calling cublasSetStream() just before calling the actual cuBLAS routine. Note that cublasSetStream() resets the user-provided workspace to the default workspace pool; see cublasSetWorkspace() . Then, the computation performed in separate streams would be overlapped automatically when possible on the GPU. This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work. We recommend using the new cuBLAS API with scalar parameters and results passed by reference in the device memory to achieve maximum overlap of the computation when using streams. A particular application of streams, batching of multiple small kernels, is described in the following section. 2.1.7. Batching Kernels  In this section, we explain how to use streams to batch the execution of small kernels. For instance, suppose that we have an application where we need to make many small independent matrix-matrix multiplications with dense matrices. It is clear that even with millions of small independent matrices we will not be able to achieve the same GFLOPS rate as with a one large matrix. For example, a single \\(n \\times n\\) large matrix-matrix multiplication performs \\(n^{3}\\) operations for \\(n^{2}\\) input size, while 1024 \\(\\frac{n}{32} \\times \\frac{n}{32}\\) small matrix-matrix multiplications perform \\(1024\\left( \\frac{n}{32} \\right)^{3} = \\frac{n^{3}}{32}\\) operations for the same input size. However, it is also clear that we can achieve a significantly better performance with many small independent matrices compared with a single small matrix. The architecture family of GPUs allows us to execute multiple kernels simultaneously. Hence, in order to batch the execution of independent kernels, we can run each of them in a separate stream. In particular, in the above example we could create 1024 CUDA™ streams using the function cudaStreamCreate() , then preface each call to cublas<t>gemm() with a call to cublasSetStream() with a different stream for each of the matrix-matrix multiplications (note that cublasSetStream() resets user-provided workspace to the default workspace pool, see cublasSetWorkspace() ). This will ensure that when possible the different computations will be executed concurrently. Although the user can create many streams, in practice it is not possible to have more than 32 concurrent kernels executing at the same time. 2.1.8. Cache Configuration  On some devices, L1 cache and shared memory use the same hardware resources. The cache configuration can be set directly with the CUDA Runtime function cudaDeviceSetCacheConfig. The cache configuration can also be set specifically for some functions using the routine cudaFuncSetCacheConfig. Please refer to the CUDA Runtime API documentation for details about the cache configuration settings. Because switching from one configuration to another can affect kernels concurrency, the cuBLAS Library does not set any cache configuration preference and relies on the current setting. However, some cuBLAS routines, especially Level-3 routines, rely heavily on shared memory. Thus the cache preference setting might affect adversely their performance. 2.1.9. Static Library Support  The cuBLAS Library is also delivered in a static form as libcublas_static.a on Linux. The static cuBLAS library and all other static math libraries depend on a common thread abstraction layer library called libculibos.a . For example, on Linux, to compile a small application using cuBLAS, against the dynamic library, the following command can be used: nvcc myCublasApp . c - lcublas - o myCublasApp Whereas to compile against the static cuBLAS library, the following command must be used: nvcc myCublasApp . c - lcublas_static - lculibos - o myCublasApp It is also possible to use the native Host C++ compiler. Depending on the Host operating system, some additional libraries like pthread or dl might be needed on the linking line. The following command on Linux is suggested : g ++ myCublasApp . c - lcublas_static - lculibos - lcudart_static - lpthread - ldl - I < cuda - toolkit - path >/ include - L < cuda - toolkit - path >/ lib64 - o myCublasApp Note that in the latter case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. Starting with release 11.2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library. 2.1.10. GEMM Algorithms Numerical Behavior  Some GEMM algorithms split the computation along the dimension K to increase the GPU occupancy, especially when the dimension K is large compared to dimensions M and N. When this type of algorithm is chosen by the cuBLAS heuristics or explicitly by the user, the results of each split is summed deterministically into the resulting matrix to get the final result. For the routines cublas<t>gemmEx and cublasGemmEx() , when the compute type is greater than the output type, the sum of the split chunks can potentially lead to some intermediate overflows thus producing a final resulting matrix with some overflows. Those overflows might not have occurred if all the dot products had been accumulated in the compute type before being converted at the end in the output type. This computation side-effect can be easily exposed when the computeType is CUDA_R_32F and Atype, Btype and Ctype are in CUDA_R_16F. This behavior can be controlled using the compute precision mode CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION with cublasSetMathMode() 2.1.11. Tensor Core Usage  Tensor cores were first introduced with Volta GPUs (compute capability 7.0 and above) and significantly accelerate matrix multiplications. Starting with cuBLAS version 11.0.0, the library may automatically make use of Tensor Core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cuBLAS (see cublasSetMathMode() , cublasMath_t ). It should be noted that the library will pick a Tensor Core enabled implementation wherever it determines that it would provide the best performance. The best performance when using Tensor Cores can be achieved when the matrix dimensions and pointers meet certain memory alignment requirements. Specifically, all of the following conditions must be satisfied to get the most performance out of Tensor Cores: ((op_A == CUBLAS_OP_N ? m : k) * AtypeSize) % 16 == 0 ((op_B == CUBLAS_OP_N ? k : n) * BtypeSize) % 16 == 0 (m * CtypeSize) % 16 == 0 (lda * AtypeSize) % 16 == 0 (ldb * BtypeSize) % 16 == 0 (ldc * CtypeSize) % 16 == 0 intptr_t(A) % 16 == 0 intptr_t(B) % 16 == 0 intptr_t(C) % 16 == 0 To conduct matrix multiplication with FP8 types (see 8-bit Floating Point Data Types (FP8) Usage ), you must ensure that your matrix dimensions and pointers meet the optimal requirements listed above.  Aside from FP8, there are no longer any restrictions on matrix dimensions and memory alignments to use Tensor Cores (starting with cuBLAS version 11.0.0). 2.1.12. CUDA Graphs Support  cuBLAS routines can be captured in CUDA Graph stream capture without restrictions in most situations. The exception are routines that output results into host buffers (e.g. cublas<t>dot while pointer mode CUBLAS_POINTER_MODE_HOST is configured), as it enforces synchronization. For input coefficients (such as alpha , beta ) behavior depends on the pointer mode setting: In the case of CUBLAS(LT)_POINTER_MODE_HOST , coefficient values are captured in the graph. In the case of pointer modes with device pointers, coefficient value is accessed using the device pointer at the time of graph execution. Note When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync . However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.13. 64-bit Integer Interface  cuBLAS version 12 introduced 64-bit integer capable functions. Each 64-bit integer function is equivalent to a 32-bit integer function with the following changes: The function name has _64 suffix. The dimension (problem size) data type changed from int to int64_t . Examples of dimension: m , n , and k . The leading dimension data type changed from int to int64_t . Examples of leading dimension: lda , ldb , and ldc . The vector increment data type changed from int to int64_t . Examples of vector increment: incx and incy . For example, consider the following 32-bit integer functions: cublasStatus_t cublasSetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ); cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ); cublasStatus_t cublasSsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * A , int lda ); The equivalent 64-bit integer functions are: cublasStatus_t cublasSetMatrix_64 ( int64_t rows , int64_t cols , int64_t elemSize , const void * A , int64_t lda , void * B , int64_t ldb ); cublasStatus_t cublasIsamax_64 ( cublasHandle_t handle , int64_t n , const float * x , int64_t incx , int64_t * result ); cublasStatus_t cublasSsyr_64 ( cublasHandle_t handle , cublasFillMode_t uplo , int64_t n , const float * alpha , const float * x , int64_t incx , float * A , int64_t lda ); Not every function has a 64-bit integer equivalent. For instance, cublasSetMathMode() doesn’t have any arguments that could meaningfully be int64_t . For documentation brevity, the 64-bit integer APIs are not explicitly listed, but only mentioned that they exist for the relevant functions. 2.2. cuBLAS Datatypes Reference  2.2.1. cublasHandle_t  The cublasHandle_t type is a pointer type to an opaque structure holding the cuBLAS library context. The cuBLAS library context must be initialized using cublasCreate() and the returned handle must be passed to all subsequent library function calls. The context should be destroyed at the end using cublasDestroy() . 2.2.2. cublasStatus_t  The type is used for function status returns. All cuBLAS library functions return their status, which can have the following values. Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The cuBLAS library was not initialized. This is usually caused by the lack of a prior cublasCreate() call, an error in the CUDA Runtime API called by the cuBLAS routine, or an error in the hardware setup. To correct: call cublasCreate() before the function call; and check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. CUBLAS_STATUS_ALLOC_FAILED Resource allocation failed inside the cuBLAS library. This is usually caused by a cudaMalloc() failure. To correct: prior to the function call, deallocate previously allocated memory as much as possible. CUBLAS_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. CUBLAS_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by compute capability lower than 5.0. To correct: compile and run the application on a device with appropriate compute capability. CUBLAS_STATUS_MAPPING_ERROR An access to GPU memory space failed, which is usually caused by a failure to bind a texture. To correct: before the function call, unbind any previously bound textures. CUBLAS_STATUS_EXECUTION_FAILED The GPU program failed to execute. This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons. To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. CUBLAS_STATUS_INTERNAL_ERROR An internal cuBLAS operation failed. This error is usually caused by a cudaMemcpyAsync() failure. To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion. CUBLAS_STATUS_NOT_SUPPORTED The functionality requested is not supported. CUBLAS_STATUS_LICENSE_ERROR The functionality requested requires some license and an error was detected when trying to check the current licensing. This error can happen if the license is not present or is expired or if the environment variable NVIDIA_LICENSE_FILE is not set properly. 2.2.3. cublasOperation_t  The cublasOperation_t type indicates which operation needs to be performed with the dense matrix. Its values correspond to Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_OP_N The non-transpose operation is selected. CUBLAS_OP_T The transpose operation is selected. CUBLAS_OP_C The conjugate transpose operation is selected. 2.2.4. cublasFillMode_t  The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function. Its values correspond to Fortran characters L or l (lower) and U or u (upper) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_FILL_MODE_LOWER The lower part of the matrix is filled. CUBLAS_FILL_MODE_UPPER The upper part of the matrix is filled. CUBLAS_FILL_MODE_FULL The full matrix is filled. 2.2.5. cublasDiagType_t  The type indicates whether the main diagonal of the dense matrix is unity and consequently should not be touched or modified by the function. Its values correspond to Fortran characters ‘N’ or ‘n’ (non-unit) and ‘U’ or ‘u’ (unit) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_DIAG_NON_UNIT The matrix diagonal has non-unit elements. CUBLAS_DIAG_UNIT The matrix diagonal has unit elements. 2.2.6. cublasSideMode_t  The type indicates whether the dense matrix is on the left or right side in the matrix equation solved by a particular function. Its values correspond to Fortran characters ‘L’ or ‘l’ (left) and ‘R’ or ‘r’ (right) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_SIDE_LEFT The matrix is on the left side in the equation. CUBLAS_SIDE_RIGHT The matrix is on the right side in the equation. 2.2.7. cublasPointerMode_t  The cublasPointerMode_t type indicates whether the scalar values are passed by reference on the host or device. It is important to point out that if several scalar values are present in the function call, all of them must conform to the same single pointer mode. The pointer mode can be set and retrieved using cublasSetPointerMode() and cublasGetPointerMode() routines, respectively. Value Meaning CUBLAS_POINTER_MODE_HOST The scalars are passed by reference on the host. CUBLAS_POINTER_MODE_DEVICE The scalars are passed by reference on the device. 2.2.8. cublasAtomicsMode_t  The type indicates whether cuBLAS routines which has an alternate implementation using atomics can be used. The atomics mode can be set and queried using cublasSetAtomicsMode() and cublasGetAtomicsMode() and routines, respectively. Value Meaning CUBLAS_ATOMICS_NOT_ALLOWED The usage of atomics is not allowed. CUBLAS_ATOMICS_ALLOWED The usage of atomics is allowed. 2.2.9. cublasGemmAlgo_t  cublasGemmAlgo_t type is an enumerant to specify the algorithm for matrix-matrix multiplication on GPU architectures up to sm_75 . On sm_80 and newer GPU architectures, this enumarant has no effect. cuBLAS has the following algorithm options: Value Meaning CUBLAS_GEMM_DEFAULT Apply Heuristics to select the GEMM algorithm CUBLAS_GEMM_ALGO0 to CUBLAS_GEMM_ALGO23 Explicitly choose an Algorithm [0,23]. Note: Doesn’t have effect on NVIDIA Ampere architecture GPUs and newer. CUBLAS_GEMM_DEFAULT_TENSOR_OP [DEPRECATED] This mode is deprecated and will be removed in a future release. Apply Heuristics to select the GEMM algorithm, while allowing use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). CUBLAS_GEMM_ALGO0_TENSOR_OP to CUBLAS_GEMM_ALGO15_TENSOR_OP [DEPRECATED] Those values are deprecated and will be removed in a future release. Explicitly choose a Tensor core GEMM Algorithm [0,15]. Allows use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). Note: Doesn’t have effect on NVIDIA Ampere architecture GPUs and newer. 2.2.10. cublasMath_t  cublasMath_t enumerate type is used in cublasSetMathMode() to choose compute precision modes as defined in the following table. Since this setting does not directly control the use of Tensor Cores, the mode CUBLAS_TENSOR_OP_MATH is being deprecated, and will be removed in a future release. Value Meaning CUBLAS_DEFAULT_MATH This is the default and highest-performance mode that uses compute and intermediate storage precisions with at least the same number of mantissa and exponent bits as requested. Tensor Cores will be used whenever possible. CUBLAS_PEDANTIC_MATH This mode uses the prescribed precision and standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging. This mode might not be as performant as the other modes. CUBLAS_TF32_TENSOR_OP_MATH Enable acceleration of single-precision routines using TF32 tensor cores. CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION Forces any reductions during matrix multiplications to use the accumulator type (that is, compute type) and not the output type in case of mixed precision routines where output type precision is less than the compute type precision. This is a flag that can be set (using a bitwise or operation) alongside any of the other values. CUBLAS_TENSOR_OP_MATH [DEPRECATED] This mode is deprecated and will be removed in a future release. Allows the library to use Tensor Core operations whenever possible. For single precision GEMM routines cuBLAS will use the CUBLAS_COMPUTE_32F_FAST_16F compute type. 2.2.11. cublasComputeType_t  cublasComputeType_t enumerate type is used in cublasGemmEx() and cublasLtMatmul() (including all batched and strided batched variants) to choose compute precision modes as defined below. Value Meaning CUBLAS_COMPUTE_16F This is the default and highest-performance mode for 16-bit half precision floating point and all compute and intermediate storage precisions with at least 16-bit half precision. Tensor Cores will be used whenever possible. CUBLAS_COMPUTE_16F_PEDANTIC This mode uses 16-bit half precision floating point standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging. This mode might not be as performant as the other modes since it disables use of tensor cores. CUBLAS_COMPUTE_32F This is the default 32-bit single precision floating point and uses compute and intermediate storage precisions of at least 32-bits. CUBLAS_COMPUTE_32F_PEDANTIC Uses 32-bit single precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M). CUBLAS_COMPUTE_32F_FAST_16F Allows the library to use Tensor Cores with automatic down-conversion and 16-bit half-precision compute for 32-bit input and output matrices. CUBLAS_COMPUTE_32F_FAST_16BF Allows the library to use Tensor Cores with automatic down-convesion and bfloat16 compute for 32-bit input and output matrices. See Alternate Floating Point section for more details on bfloat16. CUBLAS_COMPUTE_32F_FAST_TF32 Allows the library to use Tensor Cores with TF32 compute for 32-bit input and output matrices. See Alternate Floating Point section for more details on TF32 compute. CUBLAS_COMPUTE_64F This is the default 64-bit double precision floating point and uses compute and intermediate storage precisions of at least 64-bits. CUBLAS_COMPUTE_64F_PEDANTIC Uses 64-bit double precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M). CUBLAS_COMPUTE_32I This is the default 32-bit integer mode and uses compute and intermediate storage precisions of at least 32-bits. CUBLAS_COMPUTE_32I_PEDANTIC Uses 32-bit integer arithmetic for all phases of calculations. Note Setting the environment variable NVIDIA_TF32_OVERRIDE = 0 will override any defaults or programmatic configuration of NVIDIA libraries, and consequently, cuBLAS will not accelerate FP32 computations with TF32 tensor cores. 2.3. CUDA Datatypes Reference  The chapter describes types shared by multiple CUDA Libraries and defined in the header file library_types.h . 2.3.1. cudaDataType_t  The cudaDataType_t type is an enumerant to specify the data precision. It is used when the data reference does not carry the type itself (e.g void *) For example, it is used in the routine cublasSgemmEx() . Value Meaning CUDA_R_16F the data type is a 16-bit real half precision floating-point CUDA_C_16F the data type is a 32-bit structure comprised of two half precision floating-points representing a complex number. CUDA_R_16BF the data type is a 16-bit real bfloat16 floating-point CUDA_C_16BF the data type is a 32-bit structure comprised of two bfloat16 floating-points representing a complex number. CUDA_R_32F the data type is a 32-bit real single precision floating-point CUDA_C_32F the data type is a 64-bit structure comprised of two single precision floating-points representing a complex number. CUDA_R_64F the data type is a 64-bit real double precision floating-point CUDA_C_64F the data type is a 128-bit structure comprised of two double precision floating-points representing a complex number. CUDA_R_8I the data type is a 8-bit real signed integer CUDA_C_8I the data type is a 16-bit structure comprised of two 8-bit signed integers representing a complex number. CUDA_R_8U the data type is a 8-bit real unsigned integer CUDA_C_8U the data type is a 16-bit structure comprised of two 8-bit unsigned integers representing a complex number. CUDA_R_32I the data type is a 32-bit real signed integer CUDA_C_32I the data type is a 64-bit structure comprised of two 32-bit signed integers representing a complex number. CUDA_R_8F_E4M3 the data type is an 8-bit real floating point in E4M3 format CUDA_R_8F_E5M2 the data type is an 8-bit real floating point in E5M2 format 2.3.2. libraryPropertyType_t  The libraryPropertyType_t is used as a parameter to specify which property is requested when using the routine cublasGetProperty() Value Meaning MAJOR_VERSION enumerant to query the major version MINOR_VERSION enumerant to query the minor version PATCH_LEVEL number to identify the patch level 2.4. cuBLAS Helper Function Reference  2.4.1. cublasCreate()  cublasStatus_t cublasCreate ( cublasHandle_t * handle ) This function initializes the cuBLAS library and creates a handle to an opaque structure holding the cuBLAS library context. It allocates hardware resources on the host and device and must be called prior to making any other cuBLAS library calls. The cuBLAS library context is tied to the current CUDA device. To use the library on multiple devices, one cuBLAS handle needs to be created for each device. Furthermore, for a given device, multiple cuBLAS handles with different configurations can be created. Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. For multi-threaded applications that use the same device from different threads, the recommended programming model is to create one cuBLAS handle per thread and use that cuBLAS handle for the entire life of the thread. Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_NOT_INITIALIZED the CUDA™ Runtime initialization failed CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_INVALID_VALUE handle == NULL 2.4.2. cublasDestroy()  cublasStatus_t cublasDestroy ( cublasHandle_t handle ) This function releases hardware resources used by the cuBLAS library. This function is usually the last call with a particular handle to the cuBLAS library. Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.3. cublasGetVersion()  cublasStatus_t cublasGetVersion ( cublasHandle_t handle , int * version ) This function returns the version number of the cuBLAS library. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the provided storage for library version number is not initialized (NULL) Note This function can be safely called with the handle set to NULL.  This allows users to get the version of the library without a handle.  Another way to do this is with cublasGetProperty() . 2.4.4. cublasGetProperty()  cublasStatus_t cublasGetProperty ( libraryPropertyType type , int * value ) This function returns the value of the requested property in memory pointed to by value. Refer to libraryPropertyType for supported types. Return Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully CUBLAS_STATUS_INVALID_VALUE Invalid type value If invalid type value or value == NULL 2.4.5. cublasGetStatusName()  const char * cublasGetStatusName ( cublasStatus_t status ) This function returns the string representation of a given status. Return Value Meaning NULL-terminated string The string representation of the status 2.4.6. cublasGetStatusString()  const char * cublasGetStatusString ( cublasStatus_t status ) This function returns the description string for a given status. Return Value Meaning NULL-terminated string The description of the status 2.4.7. cublasSetStream()  cublasStatus_t cublasSetStream ( cublasHandle_t handle , cudaStream_t streamId ) This function sets the cuBLAS library stream, which will be used to execute all subsequent calls to the cuBLAS library functions. If the cuBLAS library stream is not set, all kernels use the default NULL stream. In particular, this routine can be used to change the stream between kernel launches and then to reset the cuBLAS library stream back to NULL . Additionally this function unconditionally resets the cuBLAS library workspace back to the default workspace pool (see cublasSetWorkspace() ). Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.8. cublasSetWorkspace()  cublasStatus_t cublasSetWorkspace ( cublasHandle_t handle , void * workspace , size_t workspaceSizeInBytes ) This function sets the cuBLAS library workspace to a user-owned device buffer, which will be used to execute all subsequent calls to the cuBLAS library functions (on the currently set stream). If the cuBLAS library workspace is not set, all kernels will use the default workspace pool allocated during the cuBLAS context creation. In particular, this routine can be used to change the workspace between kernel launches. The workspace pointer has to be aligned to at least 256 bytes, otherwise CUBLAS_STATUS_INVALID_VALUE error is returned. The cublasSetStream() function unconditionally resets the cuBLAS library workspace back to the default workspace pool. Calling this function, including with workspaceSizeInBytes equal to 0, will prevent the cuBLAS library from utilizing the default workspace. Too small workspaceSizeInBytes may cause some routines to fail with CUBLAS_STATUS_ALLOC_FAILED error returned or cause large regressions in performance. Workspace size equal to or larger than 16KiB is enough to prevent CUBLAS_STATUS_ALLOC_FAILED error, while a larger workspace can provide performance benefits for some routines. Note If the stream set by cublasSetStream() is cudaStreamPerThread and there are multiple threads using the same cuBLAS library handle, then users must manually manage synchronization to avoid possible race conditions in the user provided workspace. Alternatively, users may rely on the default workspace pool which safely guards against race conditions. The table below shows the recommended size of user-provided workspace.\nThis is based on the cuBLAS default workspace pool size which is GPU architecture dependent. GPU Architecture Recommended workspace size NVIDIA Hopper Architecture 32 MiB Other 4 MiB The possible error values returned by this function and their meanings are listed below. Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the workspace pointer wasn’t aligned to at least 256 bytes 2.4.9. cublasGetStream()  cublasStatus_t cublasGetStream ( cublasHandle_t handle , cudaStream_t * streamId ) This function gets the cuBLAS library stream, which is being used to execute all calls to the cuBLAS library functions. If the cuBLAS library stream is not set, all kernels use the default NULL stream. Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was returned successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE streamId == NULL 2.4.10. cublasGetPointerMode()  cublasStatus_t cublasGetPointerMode ( cublasHandle_t handle , cublasPointerMode_t * mode ) This function obtains the pointer mode used by the cuBLAS library. Please see the section on the cublasPointerMode_t type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was obtained successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode == NULL 2.4.11. cublasSetPointerMode()  cublasStatus_t cublasSetPointerMode ( cublasHandle_t handle , cublasPointerMode_t mode ) This function sets the pointer mode used by the cuBLAS library. The default is for the values to be passed by reference on the host. Please see the section on the cublasPointerMode_t type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode is not CUBLAS_POINTER_MODE_HOST or CUBLAS_POINTER_MODE_DEVICE 2.4.12. cublasSetVector()  cublasStatus_t cublasSetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface . This function copies n elements from a vector x in host memory space to a vector y in GPU memory space. Elements in both vectors are assumed to have a size of elemSize bytes. The storage spacing between consecutive elements is given by incx for the source vector x and by incy for the destination vector y . Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that matrix. Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.13. cublasGetVector()  cublasStatus_t cublasGetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface . This function copies n elements from a vector x in GPU memory space to a vector y in host memory space. Elements in both vectors are assumed to have a size of elemSize bytes. The storage spacing between consecutive elements is given by incx for the source vector and incy for the destination vector y . Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that matrix. Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.14. cublasSetMatrix()  cublasStatus_t cublasSetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ) This function supports the 64-bit Integer Interface . This function copies a tile of rows x cols elements from a matrix A in host memory space to a matrix B in GPU memory space. It is assumed that each element requires storage of elemSize bytes and that both matrices are stored in column-major format, with the leading dimension of the source matrix A and destination matrix B given in lda and ldb , respectively. The leading dimension indicates the number of rows of the allocated matrix, even if only a submatrix of it is being used. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.15. cublasGetMatrix()  cublasStatus_t cublasGetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ) This function supports the 64-bit Integer Interface . This function copies a tile of rows x cols elements from a matrix A in GPU memory space to a matrix B in host memory space. It is assumed that each element requires storage of elemSize bytes and that both matrices are stored in column-major format, with the leading dimension of the source matrix A and destination matrix B given in lda and ldb , respectively. The leading dimension indicates the number of rows of the allocated matrix, even if only a submatrix of it is being used. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.16. cublasSetVectorAsync()  cublasStatus_t cublasSetVectorAsync ( int n , int elemSize , const void * hostPtr , int incx , void * devicePtr , int incy , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasSetVector() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.17. cublasGetVectorAsync()  cublasStatus_t cublasGetVectorAsync ( int n , int elemSize , const void * devicePtr , int incx , void * hostPtr , int incy , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasGetVector() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSize<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.18. cublasSetMatrixAsync()  cublasStatus_t cublasSetMatrixAsync ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasSetMatrix() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.19. cublasGetMatrixAsync()  cublasStatus_t cublasGetMatrixAsync ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb , cudaStream_t stream ) This function supports the 64-bit Integer Interface . This function has the same functionality as cublasGetMatrix() , with the exception that the data transfer is done asynchronously (with respect to the host) using the given CUDA™ stream parameter. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters rows, cols<0 or elemSize, lda, ldb<=0 CUBLAS_STATUS_MAPPING_ERROR there was an error accessing GPU memory 2.4.20. cublasSetAtomicsMode()  cublasStatus_t cublasSetAtomicsMode ( cublasHandlet handle , cublasAtomicsMode_t mode ) Some routines like cublas<t>symv and cublas<t>hemv have an alternate implementation that use atomics to cumulate results. This implementation is generally significantly faster but can generate results that are not strictly identical from one run to the others. Mathematically, those different results are not significant but when debugging those differences can be prejudicial. This function allows or disallows the usage of atomics in the cuBLAS library for all routines which have an alternate implementation. When not explicitly specified in the documentation of any cuBLAS routine, it means that this routine does not have an alternate implementation that use atomics. When atomics mode is disabled, each cuBLAS routine should produce the same results from one run to the other when called with identical parameters on the same Hardware. The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED . Please see the section on the type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.21. cublasGetAtomicsMode()  cublasStatus_t cublasGetAtomicsMode ( cublasHandle_t handle , cublasAtomicsMode_t * mode ) This function queries the atomic mode of a specific cuBLAS context. The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED . Please see the section on the type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was queried successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the argument mode is a NULL pointer 2.4.22. cublasSetMathMode()  cublasStatus_t cublasSetMathMode ( cublasHandle_t handle , cublasMath_t mode ) The cublasSetMathMode() function enables you to choose the compute precision modes as defined by cublasMath_t . Users are allowed to set the compute precision mode as a logical combination of them (except the deprecated CUBLAS_TENSOR_OP_MATH ). For example, cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION) . Please note that the default math mode is CUBLAS_DEFAULT_MATH . For matrix and compute precisions allowed for cublasGemmEx() and cublasLtMatmul() APIs and their strided variants please refer to: cublasGemmEx() , cublasGemmBatchedEx() , cublasGemmStridedBatchedEx() , and cublasLtMatmul() . Return Value Meaning CUBLAS_STATUS_SUCCESS the math mode was set successfully. CUBLAS_STATUS_INVALID_VALUE an invalid value for mode was specified. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.23. cublasGetMathMode()  cublasStatus_t cublasGetMathMode ( cublasHandle_t handle , cublasMath_t * mode ) This function returns the math mode used by the library routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the math type was returned successfully. CUBLAS_STATUS_INVALID_VALUE if mode is NULL. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.24. cublasSetSmCountTarget()  cublasStatus_t cublasSetSmCountTarget ( cublasHandle_t handle , int smCountTarget ) The cublasSetSmCountTarget() function allows overriding the number of multiprocessors available to the library during kernels execution. This option can be used to improve the library performance when cuBLAS routines are known to run concurrently with other work on different CUDA streams. E.g. a NVIDIA A100 GPU has 108 SM and there is a concurrent kenrel running with grid size of 8, one can use cublasSetSmCountTarget() with value 100 to override the library heuristics to optimize for running on 100 multiprocessors. When set to 0 the library returns to its default behavior. The input value should not exceed the device’s multiprocessor count, which can be obtained using cudaDeviceGetAttribute . Negative values are not accepted. The user must ensure thread safety when modifying the library handle with this routine similar to when using cublasSetStream() , etc. Return Value Meaning CUBLAS_STATUS_SUCCESS SM count target was set successfully. CUBLAS_STATUS_INVALID_VALUE the value of smCountTarget outside of the allowed range. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.25. cublasGetSmCountTarget()  cublasStatus_t cublasGetSmCountTarget ( cublasHandle_t handle , int * smCountTarget ) This function obtains the value previously programmed to the library handle. Return Value Meaning CUBLAS_STATUS_SUCCESS SM count target was set successfully. CUBLAS_STATUS_INVALID_VALUE smCountTarget is NULL. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.26. cublasLoggerConfigure()  cublasStatus_t cublasLoggerConfigure ( int logIsOn , int logToStdOut , int logToStdErr , const char * logFileName ) This function configures logging during runtime. Besides this type of configuration, it is possible to configure logging with special environment variables which will be checked by libcublas: CUBLAS_LOGINFO_DBG - Setup env. variable to “1” means turn on logging (by default logging is off). CUBLAS_LOGDEST_DBG - Setup env. variable encodes how to log. “stdout”, “stderr” means to output log messages to stdout or stderr, respectively. In the other case, its specifies “filename” of file. Parameters logIsOn Input . Turn on/off logging completely. By default is off, but is turned on by calling cublasSetLoggerCallback() to user defined callback function. logToStdOut Input . Turn on/off logging to standard output I/O stream. By default is off. logToStdErr Input . Turn on/off logging to standard error I/O stream. By default is off. logFileName Input . Turn on/off logging to file in filesystem specified by it’s name. cublasLoggerConfigure() copies the content of logFileName . You should provide null pointer if you are not interested in this type of logging. Returns CUBLAS_STATUS_SUCCESS Success. 2.4.27. cublasGetLoggerCallback()  cublasStatus_t cublasGetLoggerCallback ( cublasLogCallback * userCallback ) This function retrieves function pointer to previously installed custom user defined callback function via cublasSetLoggerCallback() or zero otherwise. Parameters userCallback Output . Pointer to user defined callback function. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE userCallback is NULL 2.4.28. cublasSetLoggerCallback()  cublasStatus_t cublasSetLoggerCallback ( cublasLogCallback userCallback ) This function installs a custom user defined callback function via cublas C public API. Parameters userCallback Input . Pointer to user defined callback function. Returns CUBLAS_STATUS_SUCCESS Success. 2.5. cuBLAS Level-1 Function Reference  In this chapter we describe the Level-1 Basic Linear Algebra Subprograms (BLAS1) functions that perform scalar and vector based operations. We will use abbreviations < type > for type and < t > for the corresponding short type to make a more concise and clear presentation of the implemented functions. Unless otherwise specified < type > and < t > have the following meanings: <type> <t> Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision When the parameters and returned values of the function differ, which sometimes happens for complex input, the <t> can also have the following meanings Sc , Cs , Dz and Zd . The abbreviation \\(\\mathbf{Re}(\\cdot)\\) and \\(\\mathbf{Im}(\\cdot)\\) will stand for the real and imaginary part of a number, respectively. Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. Also, the \\(\\bar{\\alpha}\\) will denote the complex conjugate of \\(\\alpha\\) . In general throughout the documentation, the lower case Greek symbols \\(\\alpha\\) and \\(\\beta\\) will denote scalars, lower case English letters in bold type \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) will denote vectors and capital English letters \\(A\\) , \\(B\\) and \\(C\\) will denote matrices. 2.5.1. cublasI<t>amax()  cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamax ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamax ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamax ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface . This function finds the (smallest) index of the element of the maximum magnitude. Hence, the result is the first \\(i\\) such that \\(\\left| \\mathbf{Im}\\left( {x\\lbrack j\\rbrack} \\right) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j\\rbrack} \\right) \\right|\\) is maximum for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{ incx}\\) . Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: isamax , idamax , icamax , izamax 2.5.2. cublasI<t>amin()  cublasStatus_t cublasIsamin ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamin ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamin ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamin ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface . This function finds the (smallest) index of the element of the minimum magnitude. Hence, the result is the first \\(i\\) such that \\(\\left| \\mathbf{Im}\\left( {x\\lbrack j\\rbrack} \\right) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j\\rbrack} \\right) \\right|\\) is minimum for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: isamin 2.5.3. cublas<t>asum()  cublasStatus_t cublasSasum ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDasum ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScasum ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDzasum ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface . This function computes the sum of the absolute values of the elements of vector x . Hence, the result is \\(\\left. \\sum_{i = 1}^{n} \\middle| \\mathbf{Im}\\left( {x\\lbrack j\\rbrack} \\right) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j\\rbrack} \\right) \\right|\\) where \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) . Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0.0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: sasum , dasum , scasum , dzasum 2.5.4. cublas<t>axpy()  cublasStatus_t cublasSaxpy ( cublasHandle_t handle , int n , const float * alpha , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDaxpy ( cublasHandle_t handle , int n , const double * alpha , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCaxpy ( cublasHandle_t handle , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZaxpy ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function multiplies the vector x by the scalar \\(\\alpha\\) and adds it to the vector y overwriting the latest vector with the result. Hence, the performed operation is \\(\\mathbf{y}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack k\\rbrack + \\mathbf{y}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. alpha host or device input <type> scalar used for multiplication. n input number of elements in the vector x and y . x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.5.5. cublas<t>copy()  cublasStatus_t cublasScopy ( cublasHandle_t handle , int n , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDcopy ( cublasHandle_t handle , int n , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCcopy ( cublasHandle_t handle , int n , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZcopy ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function copies the vector x into the vector y . Hence, the performed operation is \\(\\mathbf{y}\\lbrack j\\rbrack = \\mathbf{x}\\lbrack k\\rbrack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x and y . x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device output <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: scopy , dcopy , ccopy , zcopy 2.5.6. cublas<t>dot()  cublasStatus_t cublasSdot ( cublasHandle_t handle , int n , const float * x , int incx , const float * y , int incy , float * result ) cublasStatus_t cublasDdot ( cublasHandle_t handle , int n , const double * x , int incx , const double * y , int incy , double * result ) cublasStatus_t cublasCdotu ( cublasHandle_t handle , int n , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * result ) cublasStatus_t cublasCdotc ( cublasHandle_t handle , int n , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * result ) cublasStatus_t cublasZdotu ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * result ) cublasStatus_t cublasZdotc ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * result ) This function supports the 64-bit Integer Interface . This function computes the dot product of vectors x and y . Hence, the result is \\(\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack k\\rbrack \\times \\mathbf{y}\\lbrack j\\rbrack} \\right)\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . result host or device output the resulting dot product, which is 0.0 if n<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sdot , ddot , cdotu , cdotc , zdotu , zdotc 2.5.7. cublas<t>nrm2()  cublasStatus_t cublasSnrm2 ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDnrm2 ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScnrm2 ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDznrm2 ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface . This function computes the Euclidean norm of the vector x . The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \\(\\sqrt{\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack j\\rbrack \\times \\mathbf{x}\\lbrack j\\rbrack} \\right)}\\) where \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) in exact arithmetic. Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with n elements. incx input stride between consecutive elements of x . result host or device output the resulting norm, which is 0.0 if n,incx<=0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE result is NULL For references please refer to: snrm2 , dnrm2 , scnrm2 , dznrm2 2.5.8. cublas<t>rot()  cublasStatus_t cublasSrot ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * c , const float * s ) cublasStatus_t cublasDrot ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * c , const double * s ) cublasStatus_t cublasCrot ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy , const float * c , const cuComplex * s ) cublasStatus_t cublasCsrot ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy , const float * c , const float * s ) cublasStatus_t cublasZrot ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy , const double * c , const cuDoubleComplex * s ) cublasStatus_t cublasZdrot ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy , const double * c , const double * s ) This function supports the 64-bit Integer Interface . This function applies Givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \\(G = \\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\) to vectors x and y . Hence, the result is \\(\\mathbf{x}\\lbrack k\\rbrack = c \\times \\mathbf{x}\\lbrack k\\rbrack + s \\times \\mathbf{y}\\lbrack j\\rbrack\\) and \\(\\mathbf{y}\\lbrack j\\rbrack = - s \\times \\mathbf{x}\\lbrack k\\rbrack + c \\times \\mathbf{y}\\lbrack j\\rbrack\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . c host or device input cosine element of the rotation matrix. s host or device input sine element of the rotation matrix. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.5.9. cublas<t>rotg()  cublasStatus_t cublasSrotg ( cublasHandle_t handle , float * a , float * b , float * c , float * s ) cublasStatus_t cublasDrotg ( cublasHandle_t handle , double * a , double * b , double * c , double * s ) cublasStatus_t cublasCrotg ( cublasHandle_t handle , cuComplex * a , cuComplex * b , float * c , cuComplex * s ) cublasStatus_t cublasZrotg ( cublasHandle_t handle , cuDoubleComplex * a , cuDoubleComplex * b , double * c , cuDoubleComplex * s ) This function supports the 64-bit Integer Interface . This function constructs the Givens rotation matrix \\(G = \\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\) that zeros out the second entry of a \\(2 \\times 1\\) vector \\(\\left( {a,b} \\right)^{T}\\) . Then, for real numbers we can write \\(\\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\begin{pmatrix}\na \\\\\nb \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nr \\\\\n0 \\\\\n\\end{pmatrix}\\) where \\(c^{2} + s^{2} = 1\\) and \\(r = a^{2} + b^{2}\\) . The parameters \\(a\\) and \\(b\\) are overwritten with \\(r\\) and \\(z\\) , respectively. The value of \\(z\\) is such that \\(c\\) and \\(s\\) may be recovered using the following rules: \\(\\left( {c,s} \\right) = \\begin{cases}\n\\left( {\\sqrt{1 - z^{2}},z} \\right) & {\\text{ if }\\left| z \\middle| < 1 \\right.} \\\\\n\\left( {0.0,1.0} \\right) & {\\text{ if }\\left| z \\middle| = 1 \\right.} \\\\\n\\left( 1/z,\\sqrt{1 - z^{2}} \\right) & {\\text{ if }\\left| z \\middle| > 1 \\right.} \\\\\n\\end{cases}\\) For complex numbers we can write \\(\\begin{pmatrix}\nc & s \\\\\n{- \\bar{s}} & c \\\\\n\\end{pmatrix}\\begin{pmatrix}\na \\\\\nb \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nr \\\\\n0 \\\\\n\\end{pmatrix}\\) where \\(c^{2} + \\left( {\\bar{s} \\times s} \\right) = 1\\) and \\(r = \\frac{a}{|a|} \\times \\parallel \\left( {a,b} \\right)^{T} \\parallel_{2}\\) with \\(\\parallel \\left( {a,b} \\right)^{T} \\parallel_{2} = \\sqrt{\\left| a|^{2} + \\middle| b|^{2} \\right.}\\) for \\(a \\neq 0\\) and \\(r = b\\) for \\(a = 0\\) . Finally, the parameter \\(a\\) is overwritten with \\(r\\) on exit. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. a host or device in/out <type> scalar that is overwritten with \\(r\\) . b host or device in/out <type> scalar that is overwritten with \\(z\\) . c host or device output cosine element of the rotation matrix. s host or device output sine element of the rotation matrix. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotg , drotg , crotg , zrotg 2.5.10. cublas<t>rotm()  cublasStatus_t cublasSrotm ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * param ) cublasStatus_t cublasDrotm ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * param ) This function supports the 64-bit Integer Interface . This function applies the modified Givens transformation \\(H = \\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) to vectors x and y . Hence, the result is \\(\\mathbf{x}\\lbrack k\\rbrack = h_{11} \\times \\mathbf{x}\\lbrack k\\rbrack + h_{12} \\times \\mathbf{y}\\lbrack j\\rbrack\\) and \\(\\mathbf{y}\\lbrack j\\rbrack = h_{21} \\times \\mathbf{x}\\lbrack k\\rbrack + h_{22} \\times \\mathbf{y}\\lbrack j\\rbrack\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. The elements , , and of matrix \\(H\\) are stored in param[1] , param[2] , param[3] and param[4] , respectively. The flag=param[0] defines the following predefined values for the matrix \\(H\\) entries flag=-1.0 flag= 0.0 flag= 1.0 flag=-2.0 \\(\\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & h_{12} \\\\\nh_{21} & {1.0} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\nh_{11} & {1.0} \\\\\n{- 1.0} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & {0.0} \\\\\n{0.0} & {1.0} \\\\\n\\end{pmatrix}\\) Notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . param host or device input <type> vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\(H\\) . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotm , drotm 2.5.11. cublas<t>rotmg()  cublasStatus_t cublasSrotmg ( cublasHandle_t handle , float * d1 , float * d2 , float * x1 , const float * y1 , float * param ) cublasStatus_t cublasDrotmg ( cublasHandle_t handle , double * d1 , double * d2 , double * x1 , const double * y1 , double * param ) This function supports the 64-bit Integer Interface . This function constructs the modified Givens transformation \\(H = \\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) that zeros out the second entry of a \\(2 \\times 1\\) vector \\(\\left( {\\sqrt{d1}*x1,\\sqrt{d2}*y1} \\right)^{T}\\) . The flag=param[0] defines the following predefined values for the matrix \\(H\\) entries flag=-1.0 flag= 0.0 flag= 1.0 flag=-2.0 \\(\\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & h_{12} \\\\\nh_{21} & {1.0} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\nh_{11} & {1.0} \\\\\n{- 1.0} & h_{22} \\\\\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n{1.0} & {0.0} \\\\\n{0.0} & {1.0} \\\\\n\\end{pmatrix}\\) Notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. d1 host or device in/out <type> scalar that is overwritten on exit. d2 host or device in/out <type> scalar that is overwritten on exit. x1 host or device in/out <type> scalar that is overwritten on exit. y1 host or device input <type> scalar. param host or device output <type> vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\(H\\) . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotmg , drotmg 2.5.12. cublas<t>scal()  cublasStatus_t cublasSscal ( cublasHandle_t handle , int n , const float * alpha , float * x , int incx ) cublasStatus_t cublasDscal ( cublasHandle_t handle , int n , const double * alpha , double * x , int incx ) cublasStatus_t cublasCscal ( cublasHandle_t handle , int n , const cuComplex * alpha , cuComplex * x , int incx ) cublasStatus_t cublasCsscal ( cublasHandle_t handle , int n , const float * alpha , cuComplex * x , int incx ) cublasStatus_t cublasZscal ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , cuDoubleComplex * x , int incx ) cublasStatus_t cublasZdscal ( cublasHandle_t handle , int n , const double * alpha , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function scales the vector x by the scalar \\(\\alpha\\) and overwrites it with the result. Hence, the performed operation is \\(\\mathbf{x}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. alpha host or device input <type> scalar used for multiplication. n input number of elements in the vector x . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. :class: table-no-stripes  Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 2.5.13. cublas<t>swap()  cublasStatus_t cublasSswap ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy ) cublasStatus_t cublasDswap ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy ) cublasStatus_t cublasCswap ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZswap ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function interchanges the elements of vector x and y . Hence, the performed operation is \\(\\left. \\mathbf{y}\\lbrack j\\rbrack\\Leftrightarrow\\mathbf{x}\\lbrack k\\rbrack \\right.\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x and y . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sswap , dswap , cswap , zswap 2.6. cuBLAS Level-2 Function Reference  In this chapter we describe the Level-2 Basic Linear Algebra Subprograms (BLAS2) functions that perform matrix-vector operations. 2.6.1. cublas<t>gbmv()  cublasStatus_t cublasSgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the banded matrix-vector multiplication \\(\\mathbf{y} = \\alpha\\text{ op}(A)\\mathbf{x} + \\beta\\mathbf{y}\\) where \\(A\\) is a banded matrix with \\(kl\\) subdiagonals and \\(ku\\) superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Also, for matrix \\(A\\) \\(\\text{ op}(A) = \\begin{cases}\nA & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_N}$} \\\\\nA^{T} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_T}$} \\\\\nA^{H} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_C}$} \\\\\n\\end{cases}\\) The banded matrix \\(A\\) is stored column by column, with the main diagonal stored in row \\(ku + 1\\) (starting in first position), the first superdiagonal stored in row \\(ku\\) (starting in second position), the first subdiagonal stored in row \\(ku + 2\\) (starting in first position), etc. So that in general, the element \\(A\\left( {i,j} \\right)\\) is stored in the memory location A(ku+1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\left\\lbrack {\\max\\left( {1,j - ku} \\right),\\min\\left( {m,j + kl} \\right)} \\right\\rbrack\\) . Also, the elements in the array \\(A\\) that do not conceptually correspond to the elements in the banded matrix (the top left \\(ku \\times ku\\) and bottom right \\(kl \\times kl\\) triangles) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A ) that is non- or (conj.) transpose. m input number of rows of matrix A . n input number of columns of matrix A . kl input number of subdiagonals of matrix A . ku input number of superdiagonals of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda>=kl+ku+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements if transa == CUBLAS_OP_N and m elements otherwise. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta == 0 then y does not have to be a valid input. y device in/out <type> vector with m elements if transa == CUBLAS_OP_N and n elements otherwise. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m, n, kl, ku < 0 or if lda < (kl+ku+1) or if incx, incy == 0 or if trans != CUBLAS_OP_N, CUBLAS_OP_T, CUBLAS_OP_C or alpha, beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgbmv , dgbmv , cgbmv , zgbmv 2.6.2. cublas<t>gemv()  cublasStatus_t cublasSgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication \\(\\textbf{y} = \\alpha\\text{ op}(A)\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(m \\times n\\) matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Also, for matrix \\(A\\) \\(\\text{ op}(A) = \\begin{cases}\nA & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_N}$} \\\\\nA^{T} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_T}$} \\\\\nA^{H} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_C}$} \\\\\n\\end{cases}\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A ) that is non- or (conj.) transpose. m input number of rows of matrix A . n input number of columns of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda >= max(1,m) . Before entry, the leading m by n part of the array A must contain the matrix of coefficients. Unchanged on exit. lda input leading dimension of two-dimensional array used to store matrix A . lda must be at least max(1,m) . x device input <type> vector at least (1+(n-1)*abs(incx)) elements if transa==CUBLAS_OP_N and at least (1+(m-1)*abs(incx)) elements otherwise. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector at least (1+(m-1)*abs(incy)) elements if transa==CUBLAS_OP_N and at least (1+(n-1)*abs(incy)) elements otherwise. incy input stride between consecutive elements of y The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 or incx,incy=0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemv , dgemv , cgemv , zgemv 2.6.3. cublas<t>ger()  cublasStatus_t cublasSger ( cublasHandle_t handle , int m , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * A , int lda ) cublasStatus_t cublasDger ( cublasHandle_t handle , int m , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * A , int lda ) cublasStatus_t cublasCgeru ( cublasHandle_t handle , int m , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasCgerc ( cublasHandle_t handle , int m , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZgeru ( cublasHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) cublasStatus_t cublasZgerc ( cublasHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the rank-1 update \\(A = \\begin{cases}\n{\\alpha\\mathbf{xy}^{T} + A} & \\text{if ger(),geru() is called} \\\\\n{\\alpha\\mathbf{xy}^{H} + A} & \\text{if gerc() is called} \\\\\n\\end{cases}\\) where \\(A\\) is a \\(m \\times n\\) matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. m input number of rows of matrix A . n input number of columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with m elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . A device in/out <type> array of dimension lda x n with lda >= max(1,m) . lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 if incx = 0 or incy = 0 or if alpha == NULL or lda < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sger , dger , cgeru , cgerc , zgeru , zgerc 2.6.4. cublas<t>sbmv()  cublasStatus_t cublasSsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric banded matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric banded matrix with \\(k\\) subdiagonals and superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the symmetric banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1, the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the symmetric banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k),j\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda >= k+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if alpha == NULL or beta == NULL or lda < (1 + k ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssbmv , dsbmv 2.6.5. cublas<t>spmv()  cublasStatus_t cublasSspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * AP , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * AP , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric packed matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\(A\\) . alpha host or device input <type> scalar used for multiplication. AP device input <type> array with \\(A\\) stored in packed format. x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device input <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sspmv , dspmv 2.6.6. cublas<t>spr()  cublasStatus_t cublasSspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * AP ) cublasStatus_t cublasDspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * AP ) This function supports the 64-bit Integer Interface . This function performs the packed symmetric rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{T} + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\(A\\) . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . AP device in/out <type> array with \\(A\\) stored in packed format. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sspr , dspr 2.6.7. cublas<t>spr2()  cublasStatus_t cublasSspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * AP ) cublasStatus_t cublasDspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * AP ) This function supports the 64-bit Integer Interface . This function performs the packed symmetric rank-2 update \\(A = \\alpha\\left( {\\textbf{x}\\textbf{y}^{T} + \\textbf{y}\\textbf{x}^{T}} \\right) + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\(A\\) . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . AP device in/out <type> array with \\(A\\) stored in packed format. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sspr2 , dspr2 2.6.8. cublas<t>symv()  cublasStatus_t cublasSsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , /* host or device pointer */ const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric matrix-vector multiplication. \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in lower or upper mode, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. This function has an alternate faster implementation using atomics that can be enabled with cublasSetAtomicsMode() . Please see the section on the function cublasSetAtomicsMode() for more details about the usage of atomics. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or lda < n CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymv , dsymv 2.6.9. cublas<t>syr()  cublasStatus_t cublasSsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * A , int lda ) cublasStatus_t cublasDsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * A , int lda ) cublasStatus_t cublasCsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{T} + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in column-major format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . A device in/out <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr , dsyr 2.6.10. cublas<t>syr2()  cublasStatus_t cublasSsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * A , int lda cublasStatus_t cublasDsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * A , int lda cublasStatus_t cublasCsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda cublasStatus_t cublasZsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda This function supports the 64-bit Integer Interface . This function performs the symmetric rank-2 update \\(A = \\alpha\\left( {\\textbf{x}\\textbf{y}^{T} + \\textbf{y}\\textbf{x}^{T}} \\right) + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . A device in/out <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if alpha == NULL or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr2 , dsyr2 2.6.11. cublas<t>tbmv()  cublasStatus_t cublasStbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular banded matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular banded matrix, and \\(\\mathbf{x}\\) is a vector. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k,j)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix . A device input <type> array of dimension lda x n , with lda>=k+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < (1 + k ) CUBLAS_STATUS_ALLOC_FAILED the allocation of internal scratch memory failed CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stbmv , dtbmv , ctbmv , ztbmv 2.6.12. cublas<t>tbsv()  cublasStatus_t cublasStbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the triangular banded linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular banded matrix, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(\\mathbf{x}\\) overwrites the right-hand-sides \\(\\mathbf{b}\\) on exit. No test for singularity or near-singularity is included in this function. If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k,j)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . A device input <type> array of dimension lda x n , with lda >= k+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < (1 + k ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stbsv , dtbsv , ctbsv , ztbsv 2.6.13. cublas<t>tpmv()  cublasStatus_t cublasStpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * AP , float * x , int incx ) cublasStatus_t cublasDtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * AP , double * x , int incx ) cublasStatus_t cublasCtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * AP , cuComplex * x , int incx ) cublasStatus_t cublasZtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * AP , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular packed matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular matrix stored in packed format, and \\(\\mathbf{x}\\) is a vector. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(A(i,j)\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . AP device input <type> array with \\(A\\) stored in packed format. x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or if uplo != CUBLAS_FILL_MODE_UPPER, CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N, CUBLAS_OP_T, CUBLAS_OP_C or diag != CUBLAS_DIAG_UNIT, CUBLAS_DIAG_NON_UNIT CUBLAS_STATUS_ALLOC_FAILED the allocation of internal scratch memory failed CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stpmv , dtpmv , ctpmv , ztpmv 2.6.14. cublas<t>tpsv()  cublasStatus_t cublasStpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * AP , float * x , int incx ) cublasStatus_t cublasDtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * AP , double * x , int incx ) cublasStatus_t cublasCtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * AP , cuComplex * x , int incx ) cublasStatus_t cublasZtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * AP , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the packed triangular linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular matrix stored in packed format, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(\\mathbf{x}\\) overwrites the right-hand-sides \\(\\mathbf{b}\\) on exit. No test for singularity or near-singularity is included in this function. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix are unity and should not be accessed. n input number of rows and columns of matrix A . AP device input <type> array with A stored in packed format. x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stpsv , dtpsv , ctpsv , ztpsv 2.6.15. cublas<t>trmv()  cublasStatus_t cublasStrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\(\\mathbf{x}\\) is a vector. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) (that is, non- or conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . A device input <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < max(1, n ) CUBLAS_STATUS_ALLOC_FAILED the allocation of internal scratch memory failed CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strmv , dtrmv , ctrmv , ztrmv 2.6.16. cublas<t>trsv()  cublasStatus_t cublasStrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the triangular linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(\\mathbf{x}\\) overwrites the right-hand-sides \\(\\mathbf{b}\\) on exit. No test for singularity or near-singularity is included in this function. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. n input number of rows and columns of matrix A . A device input <type> array of dimension lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . x device in/out <type> vector with n elements. incx input stride between consecutive elements of x . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if diag != CUBLAS_DIAG_UNIT , CUBLAS_DIAG_NON_UNIT or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsv , dtrsv , ctrsv , ztrsv 2.6.17. cublas<t>hemv()  cublasStatus_t cublasChemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in lower or upper mode, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. This function has an alternate faster implementation using atomics that can be enabled with Please see the section on the for more details about the usage of atomics Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x n , with lda>=max(1,n) . The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or lda < n CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chemv , zhemv 2.6.18. cublas<t>hbmv()  cublasStatus_t cublasChbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian banded matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian banded matrix with \\(k\\) subdiagonals and superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the Hermitian banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack j,\\min(m,j + k)\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the Hermitian banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k),j\\rbrack\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\) triangle) are not referenced. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x n , with lda>=k+1 . The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if incx = 0 or incy = 0 or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < ( k + 1) or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chbmv , zhbmv 2.6.19. cublas<t>hpmv()  cublasStatus_t cublasChpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian packed matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. AP device input <type> array with A stored in packed format. The imaginary parts of the diagonal elements are assumed to be zero. x device input <type> vector with n elements. incx input stride between consecutive elements of x . beta host or device input <type> scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out <type> vector with n elements. incy input stride between consecutive elements of y . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or incy == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chpmv , zhpmv 2.6.20. cublas<t>her()  cublasStatus_t cublasCher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in column-major format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . A device in/out <type> array of dimensions lda x n , with lda>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if lda < max(1, n ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher , zher 2.6.21. cublas<t>her2()  cublasStatus_t cublasCher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank-2 update \\(A = \\alpha\\textbf{x}\\textbf{y}^{H} + \\overset{ˉ}{\\alpha}\\textbf{y}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . A device in/out <type> array of dimension lda x n with lda>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or incy == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if lda < max(1, n ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher2, zher2 2.6.22. cublas<t>hpr()  cublasStatus_t cublasChpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * AP ) cublasStatus_t cublasZhpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface . This function performs the packed Hermitian rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . AP device in/out <type> array with A stored in packed format. The imaginary parts of the diagonal elements are assumed and set to zero. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chpr , zhpr 2.6.23. cublas<t>hpr2()  cublasStatus_t cublasChpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * AP ) cublasStatus_t cublasZhpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface . This function performs the packed Hermitian rank-2 update \\(A = \\alpha\\textbf{x}\\textbf{y}^{H} + \\overset{ˉ}{\\alpha}\\textbf{y}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . alpha host or device input <type> scalar used for multiplication. x device input <type> vector with n elements. incx input stride between consecutive elements of x . y device input <type> vector with n elements. incy input stride between consecutive elements of y . AP device in/out <type> array with A stored in packed format. The imaginary parts of the diagonal elements are assumed and set to zero. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if incx == 0 or incy == 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chpr2, zhpr2 2.6.24. cublas<t>gemvBatched()  cublasStatus_t cublasSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * const Aarray [], int lda , const float * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) cublasStatus_t cublasDgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * const Aarray [], int lda , const double * const xarray [], int incx , const double * beta , double * const yarray [], int incy , int batchCount ) cublasStatus_t cublasCgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * const Aarray [], int lda , const cuComplex * const xarray [], int incx , const cuComplex * beta , cuComplex * const yarray [], int incy , int batchCount ) cublasStatus_t cublasZgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * const Aarray [], int lda , const cuDoubleComplex * const xarray [], int incx , const cuDoubleComplex * beta , cuDoubleComplex * const yarray [], int incy , int batchCount ) #if defined(__cplusplus) cublasStatus_t cublasHSHgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * const Aarray [], int lda , const __half * const xarray [], int incx , const float * beta , __half * const yarray [], int incy , int batchCount ) cublasStatus_t cublasHSSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * const Aarray [], int lda , const __half * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) cublasStatus_t cublasTSTgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * const Aarray [], int lda , const __nv_bfloat16 * const xarray [], int incx , const float * beta , __nv_bfloat16 * const yarray [], int incy , int batchCount ) cublasStatus_t cublasTSSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * const Aarray [], int lda , const __nv_bfloat16 * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) #endif This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication of a batch of matrices and vectors. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors. The address of the input matrix and vector, and the output vector of each instance of the batch are read from arrays of pointers passed to the function by the caller. \\(\\textbf{y}\\lbrack i\\rbrack = \\alpha\\text{op}(A\\lbrack i\\rbrack)\\textbf{x}\\lbrack i\\rbrack + \\beta\\textbf{y}\\lbrack i\\rbrack,\\text{ for i} \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) is an array of pointers to matrice \\(A\\lbrack i\\rbrack\\) stored in column-major format with dimension \\(m \\times n\\) , and \\(\\textbf{x}\\) and \\(\\textbf{y}\\) are arrays of pointers to vectors. Also, for matrix \\(A\\lbrack i\\rbrack\\) , \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A\\lbrack i\\rbrack}^{T} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A\\lbrack i\\rbrack}^{H} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note \\(\\textbf{y}\\lbrack i\\rbrack\\) vectors must not overlap, i.e. the individual gemv operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemv in different CUDA streams, rather than use this API. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A[i] ) that is non- or (conj.) transpose. m input number of rows of matrix A[i] . n input number of columns of matrix A[i] . alpha host or device input <type> scalar used for multiplication. Aarray device input array of pointers to <type> array, with each array of dim. lda x n with lda>=max(1,m) . All pointers must meet certain alignment criteria. Please see below for details. lda input leading dimension of two-dimensional array used to store each matrix A[i] . xarray device input array of pointers to <type> array, with each dimension n if trans==CUBLAS_OP_N and m otherwise. All pointers must meet certain alignment criteria. Please see below for details. incx input stride of each one-dimensional array x[i]. beta host or device input <type> scalar used for multiplication. If beta == 0 , y does not have to be a valid input. yarray device in/out array of pointers to <type> array. It has dimensions m if trans==CUBLAS_OP_N and n otherwise. Vectors y[i] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. incy input stride of each one-dimensional array y[i]. batchCount input number of pointers contained in Aarray, xarray and yarray. If math mode enables fast math modes when using cublasSgemvBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k % 4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,batchCount<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.6.25. cublas<t>gemvStridedBatched()  cublasStatus_t cublasSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * A , int lda , long long int strideA , const float * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasDgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * A , int lda , long long int strideA , const double * x , int incx , long long int stridex , const double * beta , double * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasCgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * x , int incx , long long int stridex , const cuComplex * beta , cuComplex * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasZgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , long long int strideA , const cuDoubleComplex * x , int incx , long long int stridex , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasHSHgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * A , int lda , long long int strideA , const __half * x , int incx , long long int stridex , const float * beta , __half * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasHSSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * A , int lda , long long int strideA , const __half * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasTSTgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * A , int lda , long long int strideA , const __nv_bfloat16 * x , int incx , long long int stridex , const float * beta , __nv_bfloat16 * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasTSSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * A , int lda , long long int strideA , const __nv_bfloat16 * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication of a batch of matrices and vectors. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors. Input matrix A and vector x, and output vector y for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. Pointers to A matrix, x and y vectors for the first instance are passed to the function by the user along with offsets in number of elements - strideA, stridex and stridey that determine the locations of input matrices and vectors, and output vectors in future instances. \\(\\textbf{y} + i*{stridey} = \\alpha\\text{op}(A + i*{strideA})(\\textbf{x} + i*{stridex}) + \\beta(\\textbf{y} + i*{stridey}),\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) is an array of pointers to matrix stored in column-major format with dimension \\(A\\lbrack i\\rbrack\\) \\(m \\times n\\) , and \\(\\textbf{x}\\) and \\(\\textbf{y}\\) are arrays of pointers to vectors. Also, for matrix \\(A\\lbrack i\\rbrack\\) \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A\\lbrack i\\rbrack}^{T} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A\\lbrack i\\rbrack}^{H} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note \\(\\textbf{y}\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemv operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemv in different CUDA streams, rather than use this API. Note In the table below, we use A[i], x[i], y[i] as notation for A matrix, and x and y vectors in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, stridex, stridey away from A[i-1], x[i-1], y[i-1] . The unit for the offset is number of elements and must not be zero . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A[i] ) that is non- or (conj.) transpose. m input number of rows of matrix A[i] . n input number of columns of matrix A[i] . alpha host or device input <type> scalar used for multiplication. A device input <type>* pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x n with lda>=max(1,m) . lda input leading dimension of two-dimensional array used to store each matrix A[i] . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] x device input <type>* pointer to the x vector corresponding to the first instance of the batch, with each dimension n if trans==CUBLAS_OP_N and m otherwise. incx input stride of each one-dimensional array x[i]. stridex input Value of type long long int that gives the offset in number of elements between x[i] and x[i+1] beta host or device input <type> scalar used for multiplication. If beta == 0 , y does not have to be a valid input. y device in/out <type>* pointer to the y vector corresponding to the first instance of the batch, with each dimension m if trans==CUBLAS_OP_N and n otherwise. Vectors y[i] should not overlap; otherwise, undefined behavior is expected. incy input stride of each one-dimensional array y[i]. stridey input Value of type long long int that gives the offset in number of elements between y[i] and y[i+1] batchCount input number of GEMVs to perform in the batch. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,batchCount<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.7. cuBLAS Level-3 Function Reference  In this chapter we describe the Level-3 Basic Linear Algebra Subprograms (BLAS3) functions that perform matrix-matrix operations. 2.7.1. cublas<t>gemm()  cublasStatus_t cublasSgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) cublasStatus_t cublasHgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * A , int lda , const __half * B , int ldb , const __half * beta , __half * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or if alpha , beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_ARCH_MISMATCH in the case of cublasHgemm() the device does not support math in half precision. CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemm , dgemm , cgemm , zgemm 2.7.2. cublas<t>gemm3m()  cublasStatus_t cublasCgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the complex matrix-matrix multiplication, using Gauss complexity reduction algorithm. This can lead to an increase in performance up to 25% \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Note These 2 routines are only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A ) that is non- or (conj.) transpose. transb input Operation op( B ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A ) and C . n input Number of columns of matrix op( B ) and C . k input Number of columns of op( A ) and rows of op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input Leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input Leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input Leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If m , n , k < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or if alpha , beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capabilites lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: cgemm , zgemm 2.7.3. cublas<t>gemmBatched()  cublasStatus_t cublasHgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * const Aarray [], int lda , const __half * const Barray [], int ldb , const __half * beta , __half * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasSgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * const Aarray [], int lda , const float * const Barray [], int ldb , const float * beta , float * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasDgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * const Aarray [], int lda , const double * const Barray [], int ldb , const double * beta , double * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasCgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * const Aarray [], int lda , const cuComplex * const Barray [], int ldb , const cuComplex * beta , cuComplex * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasZgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * const Aarray [], int lda , const cuDoubleComplex * const Barray [], int ldb , const cuDoubleComplex * beta , cuDoubleComplex * const Carray [], int ldc , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication of a batch of matrices. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. \\(C\\lbrack i\\rbrack = \\alpha\\text{op}(A\\lbrack i\\rbrack)\\text{op}(B\\lbrack i\\rbrack) + \\beta C\\lbrack i\\rbrack,\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A[i] ) that is non- or (conj.) transpose. transb input operation op( B[i] ) that is non- or (conj.) transpose. m input number of rows of matrix op( A[i] ) and C[i] . n input number of columns of op( B[i] ) and C[i] . k input number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input <type> scalar used for multiplication. Aarray device input array of pointers to <type> array, with each array of dim. lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. All pointers must meet certain alignment criteria. Please see below for details. lda input leading dimension of two-dimensional array used to store each matrix A[i] . Barray device input array of pointers to <type> array, with each array of dim. ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise. All pointers must meet certain alignment criteria. Please see below for details. ldb input leading dimension of two-dimensional array used to store each matrix B[i] . beta host or device input <type> scalar used for multiplication. If beta == 0 , C does not have to be a valid input. Carray device in/out array of pointers to <type> array. It has dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. ldc input leading dimension of two-dimensional array used to store each matrix C[i] . batchCount input number of pointers contained in Aarray, Barray and Carray. If math mode enables fast math modes when using cublasSgemmBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k , batchCount < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_ARCH_MISMATCH cublasHgemmBatched() is only supported for GPU with architecture capabilities equal or greater than 5.3 2.7.4. cublas<t>gemmStridedBatched()  cublasStatus_t cublasHgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * A , int lda , long long int strideA , const __half * B , int ldb , long long int strideB , const __half * beta , __half * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasSgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * A , int lda , long long int strideA , const float * B , int ldb , long long int strideB , const float * beta , float * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasDgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , long long int strideA , const double * B , int ldb , long long int strideB , const double * beta , double * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasCgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * B , int ldb , long long int strideB , const cuComplex * beta , cuComplex * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasCgemm3mStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * B , int ldb , long long int strideB , const cuComplex * beta , cuComplex * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasZgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , long long int strideA , const cuDoubleComplex * B , int ldb , long long int strideB , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc , long long int strideC , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication of a batch of matrices. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. Pointers to A, B and C matrices for the first instance are passed to the function by the user along with offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances. \\(C + i*{strideC} = \\alpha\\text{op}(A + i*{strideA})\\text{op}(B + i*{strideB}) + \\beta(C + i*{strideC}),\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1] . The unit for the offset is number of elements and must not be zero . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A[i] ) that is non- or (conj.) transpose. transb input operation op( B[i] ) that is non- or (conj.) transpose. m input number of rows of matrix op( A[i] ) and C[i] . n input number of columns of op( B[i] ) and C[i] . k input number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input <type> scalar used for multiplication. A device input <type>* pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store each matrix A[i] . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] B device input <type>* pointer to the B matrix corresponding to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise. ldb input leading dimension of two-dimensional array used to store each matrix B[i] . strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] beta host or device input <type> scalar used for multiplication. If beta == 0 , C does not have to be a valid input. C device in/out <type>* pointer to the C matrix corresponding to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix C[i] . strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] batchCount input number of GEMMs to perform in the batch. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k , batchCount < 0 or if transa , transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_ARCH_MISMATCH cublasHgemmStridedBatched() is only supported for GPU with architecture capabilities equal or greater than 5.3 2.7.5. cublas<t>gemmGroupedBatched()  cublasStatus_t cublasSgemmGroupedBatched ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const float alpha_array [], const float * const Aarray [], const int lda_array [], const float * const Barray [], const int ldb_array [], const float beta_array [], float * const Carray [], const int ldc_array [], int group_count , const int group_size []) cublasStatus_t cublasDgemmGroupedBatched ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const double alpha_array [], const double * const Aarray [], const int lda_array [], const double * const Barray [], const int ldb_array [], const double beta_array [], double * const Carray [], const int ldc_array [], int group_count , const int group_size []) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication on groups of matrices. A given group is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.  This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemm ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], lda_array [ i ], Barray [ idx ], ldb_array [ i ], beta_array [ i ], Carray [ idx ], ldc_array [ i ]); idx += 1 ; end end where \\(\\text{$\\mathrm{alpha\\_array}$}\\) and \\(\\text{$\\mathrm{beta\\_array}$}\\) are arrays of scaling factors, and \\(\\text{Aarray}\\) , \\(\\text{Barray}\\) and \\(\\text{Carray}\\) are arrays of pointers to matrices stored in column-major format.  For a given index, \\(\\text{idx}\\) , that is part of group \\(i\\) , the dimensions are: \\(\\text{op}(\\text{Aarray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{op}(\\text{Barray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{Carray}\\lbrack\\text{idx}\\rbrack\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) Note This API takes arrays of two different lengths.  The arrays of dimensions, leading dimensions, transpositions, and scaling factors are of length group_count and the arrays of matrices are of length problem_count where \\(\\text{$\\mathrm{problem\\_count}$} = \\sum_{i = 0}^{\\text{$\\mathrm{group\\_count}$} - 1} \\text{$\\mathrm{group\\_size}$}\\lbrack i\\rbrack\\) For matrix \\(A[\\text{idx}]\\) in group \\(i\\) \\(\\text{op}(A[\\text{idx}]) = \\left\\{ \\begin{matrix}\nA[\\text{idx}] & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA[\\text{idx}]^{T} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA[\\text{idx}]^{H} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B[\\text{idx}])\\) is defined similarly for matrix \\(B[\\text{idx}]\\) in group \\(i\\) . Note \\(C\\lbrack\\text{idx}\\rbrack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemmBatched in different CUDA streams, rather than use this API. Param. Memory In/out Meaning Array Length handle input handle to the cuBLAS library context. transa_array host input array containing the operations, op( A[idx] ), that is non- or (conj.) transpose for each group. group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj.) transpose for each group. group_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group. group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group. group_count k_array host input array containing the number of columns of op( A[idx] ) and rows of op( B[idx] ) for each group. group_count alpha_array host input array containing the <type> scalar used for multiplication for each group. group_count Aarray device input array of pointers to <type> array, with each array of dim. lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group. group_count Barray device input array of pointers to <type> array, with each array of dim. ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group. group_count beta_array host input array containing the <type> scalar used for multiplication for each group. group_count Carray device in/out array of pointers to <type> array. It has dimensions ldc[i] x n[i] with ldc[i]>=max(1,m[i]) . Matrices C[idx] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. problem_count ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group. group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group. group_count If math mode enables fast math modes when using cublasSgemmGroupedBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is required that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count < 0 or if m_array[i] , n_array[i] , k_array[i] , group_size[i] < 0 or if transa_array[i] , transb_array[i] != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda_array[i] < max(1, m_array[i] ) if transa_array[i] == CUBLAS_OP_N and lda_array[i] < max(1, k_array[i] ) otherwise or if ldb_array[i] < max(1, k_array[i] ) if transb_array[i] == CUBLAS_OP_N and ldb_array[i] < max(1, n_array[i] ) otherwise or if ldc_array[i] < max(1, m_array[i] ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_NOT_SUPPORTED the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE 2.7.6. cublas<t>symm()  cublasStatus_t cublasSsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a symmetric matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n < 0 or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or if ldc < max(1, m ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymm , dsymm , csymm , zsymm 2.7.7. cublas<t>syrk()  cublasStatus_t cublasSsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A. beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 2.7.8. cublas<t>syr2k()  cublasStatus_t cublasSsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank- \\(2k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\text{op}(B)\\text{op}(A)^{T}) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr2k , dsyr2k , csyr2k , zsyr2k 2.7.9. cublas<t>syrkx()  cublasStatus_t cublasSsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs a variation of the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrices \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric. A usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublas<t>dgmm . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk and ssyr2k , dsyr2k , csyr2k , zsyr2k 2.7.10. cublas<t>trmm()  cublasStatus_t cublasStrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * A , int lda , const float * B , int ldb , float * C , int ldc ) cublasStatus_t cublasDtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * A , int lda , const double * B , int ldb , double * C , int ldc ) cublasStatus_t cublasCtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , cuComplex * C , int ldc ) cublasStatus_t cublasZtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the triangular matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha\\text{op}(A)B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha B\\text{op}(A)} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrix, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Notice that in order to achieve better parallelism cuBLAS differs from the BLAS API only for this routine. The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLAS API assumes an out-of-place implementation (with results written into C). The application can obtain the in-place functionality of BLAS in the cuBLAS API by passing the address of the matrix B in place of the matrix C. No other overlapping in the input parameters is supported. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A sized accordingly. alpha host or device input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . C device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or C == NULL if C needs to be scaled CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strmm , dtrmm , ctrmm , ztrmm 2.7.11. cublas<t>trsm()  cublasStatus_t cublasStrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * A , int lda , float * B , int ldb ) cublasStatus_t cublasDtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * A , int lda , double * B , int ldb ) cublasStatus_t cublasCtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , cuComplex * B , int ldb ) cublasStatus_t cublasZtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb ) This function supports the 64-bit Integer Interface . This function solves the triangular linear system with multiple right-hand-sides \\(\\left\\{ \\begin{matrix}\n{\\text{op}(A)X = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{X\\text{op}(A) = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\) and \\(B\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(X\\) overwrites the right-hand-sides \\(B\\) on exit. No test for singularity or near-singularity is included in this function. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of X . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A is sized accordingly. alpha host or device input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device in/out <type> array. It has dimensions ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if diag != CUBLAS_DIAG_NON_UNIT , CUBLAS_DIAG_UNIT or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or alpha == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsm , dtrsm , ctrsm , ztrsm 2.7.12. cublas<t>trsmBatched()  cublasStatus_t cublasStrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * const A [], int lda , float * const B [], int ldb , int batchCount ); cublasStatus_t cublasDtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * const A [], int lda , double * const B [], int ldb , int batchCount ); cublasStatus_t cublasCtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * const A [], int lda , cuComplex * const B [], int ldb , int batchCount ); cublasStatus_t cublasZtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * const A [], int lda , cuDoubleComplex * const B [], int ldb , int batchCount ); This function supports the 64-bit Integer Interface . This function solves an array of triangular linear systems with multiple right-hand-sides \\(\\left\\{ \\begin{matrix}\n{\\text{op}(A\\lbrack i\\rbrack)X\\lbrack i\\rbrack = \\alpha B\\lbrack i\\rbrack} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{X\\lbrack i\\rbrack\\text{op}(A\\lbrack i\\rbrack) = \\alpha B\\lbrack i\\rbrack} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\lbrack i\\rbrack\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\lbrack i\\rbrack\\) and \\(B\\lbrack i\\rbrack\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\lbrack i\\rbrack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A^{H}\\lbrack i\\rbrack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(X\\lbrack i\\rbrack\\) overwrites the right-hand-sides \\(B\\lbrack i\\rbrack\\) on exit. No test for singularity or near-singularity is included in this function. This function works for any sizes but is intended to be used for matrices of small sizes where the launch overhead is a significant factor. For bigger sizes, it might be advantageous to call batchCount times the regular cublas<t>trsm within a set of CUDA streams. The current implementation is limited to devices with compute capability above or equal 2.0. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A[i] is on the left or right of X[i] . uplo input indicates if matrix A[i] lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A[i] ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A[i] are unity and should not be accessed. m input number of rows of matrix B[i] , with matrix A[i] sized accordingly. n input number of columns of matrix B[i] , with matrix A[i] is sized accordingly. alpha host or device input <type> scalar used for multiplication, if alpha==0 then A[i] is not referenced and B[i] does not have to be a valid input. A device input array of pointers to <type> array, with each array of dim. lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A[i] . B device in/out array of pointers to <type> array, with each array of dim. ldb x n with ldb>=max(1,m) . Matrices B[i] should not overlap; otherwise, undefined behavior is expected. ldb input leading dimension of two-dimensional array used to store matrix B[i] . batchCount input number of pointers contained in A and B. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or batchCount < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if diag != CUBLAS_DIAG_NON_UNIT , CUBLAS_DIAG_UNIT or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or ldb < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsm , dtrsm , ctrsm , ztrsm 2.7.13. cublas<t>hemm()  cublasStatus_t cublasChemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZhemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a Hermitian matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if side != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, m ) if side == CUBLAS_SIDE_LEFT and lda < max(1, n ) otherwise or if ldb < max(1, m ) or if ldc < max(1, m ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chemm , zhemm 2.7.14. cublas<t>herk()  cublasStatus_t cublasCherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk 2.7.15. cublas<t>her2k()  cublasStatus_t cublasCher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank- \\(2k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\overset{ˉ}{\\alpha}\\text{op}(B)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher2k , zher2k 2.7.16. cublas<t>herkx()  cublasStatus_t cublasCherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs a variation of the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian. An usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublas<t>dgmm . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input real scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if uplo != CUBLAS_FILL_MODE_LOWER , CUBLAS_FILL_MODE_UPPER or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or if alpha == NULL or beta == NULL or C == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk and cher2k , zher2k 2.8. BLAS-like Extension  This section describes the BLAS-extension functions that perform matrix-matrix operations. 2.8.1. cublas<t>geam()  cublasStatus_t cublasSgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const float * alpha , const float * A , int lda , const float * beta , const float * B , int ldb , float * C , int ldc ) cublasStatus_t cublasDgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const double * alpha , const double * A , int lda , const double * beta , const double * B , int ldb , double * C , int ldc ) cublasStatus_t cublasCgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , const cuComplex * B , int ldb , cuComplex * C , int ldc ) cublasStatus_t cublasZgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , const cuDoubleComplex * B , int ldb , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix addition/transposition \\(C = \\alpha\\text{op}(A) + \\beta\\text{op}(B)\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times n\\) , \\(\\text{op}(B)\\) \\(m \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . The operation is out-of-place if C does not overlap A or B. The in-place mode supports the following two operations, \\(C = \\alpha\\text{*}C + \\beta\\text{op}(B)\\) \\(C = \\alpha\\text{op}(A) + \\beta\\text{*}C\\) For in-place mode, if C = A , ldc = lda and transa = CUBLAS_OP_N . If C = B , ldc = ldb and transb = CUBLAS_OP_N . If the user does not meet above requirements, CUBLAS_STATUS_INVALID_VALUE is returned. The operation includes the following special cases: the user can reset matrix C to zero by setting *alpha=*beta=0 . the user can transpose matrix A by setting *alpha=1 and *beta=0 . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . alpha host or device input <type> scalar used for multiplication. If *alpha == 0 , A does not have to be a valid input. A device input <type> array of dimensions lda x n with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,m) if transb == CUBLAS_OP_N and ldb x m with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If *beta == 0 , B does not have to be a valid input. C device output <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if transa != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, n ) otherwise or if ldb < max(1, m ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or if A == C , (( CUBLAS_OP_N != transa ) || ( lda != ldc )) or if B == C , (( CUBLAS_OP_N != transb ) || ( ldb != ldc )) or alpha == NULL or beta == NULL CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.2. cublas<t>dgmm()  cublasStatus_t cublasSdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const float * A , int lda , const float * x , int incx , float * C , int ldc ) cublasStatus_t cublasDdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const double * A , int lda , const double * x , int incx , double * C , int ldc ) cublasStatus_t cublasCdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const cuComplex * A , int lda , const cuComplex * x , int incx , cuComplex * C , int ldc ) cublasStatus_t cublasZdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{A \\times diag(X)} & {\\text{if }\\textsf{mode == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n{diag(X) \\times A} & {\\text{if }\\textsf{mode == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(m \\times n\\) . \\(X\\) is a vector of size \\(n\\) if mode == CUBLAS_SIDE_RIGHT and of size \\(m\\) if mode == CUBLAS_SIDE_LEFT . \\(X\\) is gathered from one-dimensional array x with stride incx . The absolute value of incx is the stride and the sign of incx is direction of the stride. If incx is positive, then we forward x from the first element. Otherwise, we backward x from the last element. The formula of X is \\(X\\lbrack j\\rbrack = \\left\\{ \\begin{matrix}\n{x\\lbrack j \\times incx\\rbrack} & {\\text{if }incx \\geq 0} \\\\\n{x\\lbrack(\\chi - 1) \\times |incx| - j \\times |incx|\\rbrack} & {\\text{if }incx < 0} \\\\\n\\end{matrix} \\right.\\) where \\(\\chi = m\\) if mode == CUBLAS_SIDE_LEFT and \\(\\chi = n\\) if mode == CUBLAS_SIDE_RIGHT . Example 1: if the user wants to perform \\(diag(diag(B)) \\times A\\) , then \\(incx = ldb + 1\\) where \\(ldb\\) is leading dimension of matrix B , either row-major or column-major. Example 2: if the user wants to perform \\(\\alpha \\times A\\) , then there are two choices, either cublas<t>geam() with *beta=0 and transa == CUBLAS_OP_N or cublas<t>dgmm() with incx=0 and x[0]=alpha . The operation is out-of-place. The in-place only works if lda = ldc . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. mode input left multiply if mode == CUBLAS_SIDE_LEFT or right multiply if mode == CUBLAS_SIDE_RIGHT m input number of rows of matrix A and C . n input number of columns of matrix A and C . A device input <type> array of dimensions lda x n with lda>=max(1,m) lda input leading dimension of two-dimensional array used to store the matrix A . x device input one-dimensional <type> array of size \\(|inc| \\times m\\) if mode == CUBLAS_SIDE_LEFT and \\(|inc| \\times n\\) if mode == CUBLAS_SIDE_RIGHT incx input stride of one-dimensional array x . C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or if mode != CUBLAS_SIDE_LEFT , CUBLAS_SIDE_RIGHT or if lda < max(1, m ) or ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.3. cublas<t>getrfBatched()  cublasStatus_t cublasSgetrfBatched ( cublasHandle_t handle , int n , float * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasDgetrfBatched ( cublasHandle_t handle , int n , double * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasCgetrfBatched ( cublasHandle_t handle , int n , cuComplex * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasZgetrfBatched ( cublasHandle_t handle , int n , cuDoubleComplex * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format with dimensions nxn and leading dimension lda . This function performs the LU factorization of each Aarray[i] for i = 0, …, batchSize-1 by the following equation \\(\\text{P}\\text{*}{Aarray}\\lbrack i\\rbrack = L\\text{*}U\\) where P is a permutation matrix which represents partial pivoting with row interchanges. L is a lower triangular matrix with unit diagonal and U is an upper triangular matrix. Formally P is written by a product of permutation matrices Pj , for j = 1,2,...,n , say P = P1 * P2 * P3 * .... * Pn . Pj is a permutation matrix which interchanges two rows of vector x when performing Pj*x . Pj can be constructed by j element of PivotArray[i] by the following Matlab code // In Matlab PivotArray[i] is an array of base-1. // In C, PivotArray[i] is base-0. Pj = eye ( n ); swap Pj ( j , : ) and Pj ( PivotArray [ i ][ j ] , : ) L and U are written back to original matrix A , and diagonal elements of L are discarded. The L and U can be constructed by the following Matlab code // A is a matrix of nxn after getrf. L = eye ( n ); for j = 1 : n L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : n U ( i , i : n ) = A ( i , i : n ) end If matrix A(=Aarray[i]) is singular, getrf still works and the value of info(=infoArray[i]) reports first row index that LU factorization cannot proceed. If info is k , U(k,k) is zero. The equation P*A=L*U still holds, however L and U reconstruction needs different Matlab code as follows: // A is a matrix of nxn after getrf. // info is k, which means U(k,k) is zero. L = eye ( n ); for j = 1 : k -1 L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : k -1 U ( i , i : n ) = A ( i , i : n ) end for i = k : n U ( i , k : n ) = A ( i , k : n ) end This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>getrfBatched supports non-pivot LU factorization if PivotArray is NULL. cublas<t>getrfBatched supports arbitrary dimension. cublas<t>getrfBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of rows and columns of Aarray[i] . Aarray device input/output array of pointers to <type> array, with each array of dim. n x n with lda>=max(1,n) . Matrices Aarray[i] should not overlap; otherwise, undefined behavior is expected. lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . PivotArray device output array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If PivotArray is NULL, pivoting is disabled. infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of factorization of Aarray[i] . If info=0, the execution is successful. If info = -j, the j-th parameter had an illegal value. If info = k, U(k,k) is 0. The factorization has been completed, but U is exactly singular. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,batchSize,lda <0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgeqrf , dgeqrf , cgeqrf , zgeqrf 2.8.4. cublas<t>getrsBatched()  cublasStatus_t cublasSgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const float * const Aarray [], int lda , const int * devIpiv , float * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasDgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const double * const Aarray [], int lda , const int * devIpiv , double * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasCgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const cuComplex * const Aarray [], int lda , const int * devIpiv , cuComplex * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasZgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const cuDoubleComplex * const Aarray [], int lda , const int * devIpiv , cuDoubleComplex * const Barray [], int ldb , int * info , int batchSize ); This function solves an array of systems of linear equations of the form: \\(\\text{op}(A\\lbrack i \\rbrack) X\\lbrack i\\rbrack = B\\lbrack i\\rbrack\\) where \\(A\\lbrack i\\rbrack\\) is a matrix which has been LU factorized with pivoting, \\(X\\lbrack i\\rbrack\\) and \\(B\\lbrack i\\rbrack\\) are \\(n \\times {nrhs}\\) matrices. Also, for matrix \\(A\\) \\(\\text{op}(A\\lbrack i\\rbrack) = \\left\\{ \\begin{matrix}\n{A\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n{A^{H}\\lbrack i\\rbrack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>getrsBatched supports non-pivot LU factorization if devIpiv is NULL. cublas<t>getrsBatched supports arbitrary dimension. cublas<t>getrsBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows and columns of Aarray[i] . nrhs input number of columns of Barray[i] . Aarray device input array of pointers to <type> array, with each array of dim. n x n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . devIpiv device input array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If devIpiv is NULL, pivoting for all Aarray[i] is ignored. Barray device input/output array of pointers to <type> array, with each array of dim. n x nrhs with ldb>=max(1,n) . Matrices Barray[i] should not overlap; otherwise, undefined behavior is expected. ldb input leading dimension of two-dimensional array used to store each solution matrix Barray[i] . info host output If info=0, the execution is successful. If info = -j, the j-th parameter had an illegal value. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or nrhs < 0 or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) or ldb < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgeqrs , dgeqrs , cgeqrs , zgeqrs 2.8.5. cublas<t>getriBatched()  cublasStatus_t cublasSgetriBatched ( cublasHandle_t handle , int n , const float * const Aarray [], int lda , int * PivotArray , float * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasDgetriBatched ( cublasHandle_t handle , int n , const double * const Aarray [], int lda , int * PivotArray , double * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasCgetriBatched ( cublasHandle_t handle , int n , const cuComplex * const Aarray [], int lda , int * PivotArray , cuComplex * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasZgetriBatched ( cublasHandle_t handle , int n , const cuDoubleComplex * const Aarray [], int lda , int * PivotArray , cuDoubleComplex * const Carray [], int ldc , int * infoArray , int batchSize ); Aarray and Carray are arrays of pointers to matrices stored in column-major format with dimensions n*n and leading dimension lda and ldc respectively. This function performs the inversion of matrices A[i] for i = 0, …, batchSize-1 . Prior to calling cublas<t>getriBatched, the matrix A[i] must be factorized first using the routine cublas<t>getrfBatched. After the call of cublas<t>getrfBatched, the matrix pointing by Aarray[i] will contain the LU factors of the matrix A[i] and the vector pointing by (PivotArray+i) will contain the pivoting sequence. Following the LU factorization, cublas<t>getriBatched uses forward and backward triangular solvers to complete inversion of matrices A[i] for i = 0, …, batchSize-1 . The inversion is out-of-place, so memory space of Carray[i] cannot overlap memory space of Array[i]. Typically all parameters in cublas<t>getrfBatched would be passed into cublas<t>getriBatched. For example, // step 1: perform in-place LU decomposition, P*A = L*U. //      Aarray[i] is n*n matrix A[i] cublasDgetrfBatched ( handle , n , Aarray , lda , PivotArray , infoArray , batchSize ); //      check infoArray[i] to see if factorization of A[i] is successful or not. //      Array[i] contains LU factorization of A[i] // step 2: perform out-of-place inversion, Carray[i] = inv(A[i]) cublasDgetriBatched ( handle , n , Aarray , lda , PivotArray , Carray , ldc , infoArray , batchSize ); //      check infoArray[i] to see if inversion of A[i] is successful or not. The user can check singularity from either cublas<t>getrfBatched or cublas<t>getriBatched. This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. If cublas<t>getrfBatched is performed by non-pivoting, PivotArray of cublas<t>getriBatched should be NULL. cublas<t>getriBatched supports arbitrary dimension. cublas<t>getriBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of rows and columns of Aarray[i] . Aarray device input array of pointers to <type> array, with each array of dimension n*n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . PivotArray device output array of size n*batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If PivotArray is NULL, pivoting is disabled. Carray device output array of pointers to <type> array, with each array of dimension n*n with ldc>=max(1,n) . Matrices Carray[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix Carray[i] . infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of inversion of A[i] . If info=0, the execution is successful. If info = k, U(k,k) is 0. The U is exactly singular and the inversion failed. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or lda < 0 or ldc < 0 or batchSize < 0 or lda < n or ldc < n CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.6. cublas<t>matinvBatched()  cublasStatus_t cublasSmatinvBatched ( cublasHandle_t handle , int n , const float * const A [], int lda , float * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasDmatinvBatched ( cublasHandle_t handle , int n , const double * const A [], int lda , double * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasCmatinvBatched ( cublasHandle_t handle , int n , const cuComplex * const A [], int lda , cuComplex * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasZmatinvBatched ( cublasHandle_t handle , int n , const cuDoubleComplex * const A [], int lda , cuDoubleComplex * const Ainv [], int lda_inv , int * info , int batchSize ); A and Ainv are arrays of pointers to matrices stored in column-major format with dimensions n*n and leading dimension lda and lda_inv respectively. This function performs the inversion of matrices A[i] for i = 0, …, batchSize-1 . This function is a short cut of cublas<t>getrfBatched plus cublas<t>getriBatched . However it doesn’t work if n is greater than 32. If not, the user has to go through cublas<t>getrfBatched and cublas<t>getriBatched . If the matrix A[i] is singular, then info[i] reports singularity, the same as cublas<t>getrfBatched . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of rows and columns of A[i] . A device input array of pointers to <type> array, with each array of dimension n*n with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store each matrix A[i] . Ainv device output array of pointers to <type> array, with each array of dimension n*n with lda_inv>=max(1,n) . Matrices Ainv[i] should not overlap; otherwise, undefined behavior is expected. lda_inv input leading dimension of two-dimensional array used to store each matrix Ainv[i] . info device output array of size batchSize that info[i] contains the information of inversion of A[i] . If info[i]=0, the execution is successful. If info[i]=k, U(k,k) is 0. The U is exactly singular and the inversion failed. batchSize input number of pointers contained in A. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or lda < 0 or lda_inv < 0 or batchSize < 0 or if lda < n or lda_inv < n or n > 32 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.7. cublas<t>geqrfBatched()  cublasStatus_t cublasSgeqrfBatched ( cublasHandle_t handle , int m , int n , float * const Aarray [], int lda , float * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasDgeqrfBatched ( cublasHandle_t handle , int m , int n , double * const Aarray [], int lda , double * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasCgeqrfBatched ( cublasHandle_t handle , int m , int n , cuComplex * const Aarray [], int lda , cuComplex * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasZgeqrfBatched ( cublasHandle_t handle , int m , int n , cuDoubleComplex * const Aarray [], int lda , cuDoubleComplex * const TauArray [], int * info , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format with dimensions m x n and leading dimension lda . TauArray is an array of pointers to vectors of dimension of at least max (1, min(m, n) . This function performs the QR factorization of each Aarray[i] for i = 0, ...,batchSize-1 using Householder reflections. Each matrix Q[i] is represented as a product of elementary reflectors and is stored in the lower part of each Aarray[i] as follows : Q[j] = H[j][1] H[j][2] . . . H[j](k), where k = min(m,n). Each H[j][i] has the form H[j][i] = I - tau[j] * v * v' where tau[j] is a real scalar, and v is a real vector with v(1:i-1) = 0 and v(i) = 1 ; v(i+1:m) is stored on exit in Aarray[j][i+1:m,i] , and tau in TauArray[j][i] . This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>geqrfBatched supports arbitrary dimension. cublas<t>geqrfBatched only supports compute capability 2.0 or above. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. m input number of rows Aarray[i] . n input number of columns of Aarray[i] . Aarray device input array of pointers to <type> array, with each array of dim. m x n with lda>=max(1,m) . lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . TauArray device output array of pointers to <type> vector, with each vector of dim. max(1,min(m,n)) . info host output If info=0, the parameters passed to the function are valid If info<0, the parameter in postion -info is invalid batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or batchSize < 0 or lda < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgeqrf , dgeqrf , cgeqrf , zgeqrf 2.8.8. cublas<t>gelsBatched()  cublasStatus_t cublasSgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , float * const Aarray [], int lda , float * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasDgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , double * const Aarray [], int lda , double * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasCgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , cuComplex * const Aarray [], int lda , cuComplex * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasZgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , cuDoubleComplex * const Aarray [], int lda , cuDoubleComplex * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format. Carray is an array of pointers to matrices stored in column-major format. This function find the least squares solution of a batch of overdetermined systems: it solves the least squares problem described as follows : minimize || Carray [ i ] - Aarray [ i ] * Xarray [ i ] || , with i = 0 , ..., batchSize -1 On exit, each Aarray[i] is overwritten with their QR factorization and each Carray[i] is overwritten with the least square solution cublas<t>gelsBatched supports only the non-transpose operation and only solves over-determined systems (m >= n). cublas<t>gelsBatched only supports compute capability 2.0 or above. This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. trans input operation op( Aarray[i] ) that is non- or (conj.) transpose. Only non-transpose operation is currently supported. m input number of rows of each Aarray[i] and Carray[i] if trans == CUBLAS_OP_N , numbers of columns of each Aarray[i] otherwise (not supported currently). n input number of columns of each Aarray[i] if trans == CUBLAS_OP_N , and number of rows of each Aarray[i] and Carray[i] otherwise (not supported currently). nrhs input number of columns of each Carray[i] . Aarray device input/output array of pointers to <type> array, with each array of dim. m x n with lda>=max(1,m) if trans == CUBLAS_OP_N , and n x m with lda>=max(1,n) otherwise (not supported currently). Matrices Aarray[i] should not overlap; otherwise, undefined behavior is expected. lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . Carray device input/output array of pointers to <type> array, with each array of dim. m x nrhs with ldc>=max(1,m) if trans == CUBLAS_OP_N , and n x nrhs with lda>=max(1,n) otherwise (not supported currently). Matrices Carray[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix Carray[i] . info host output If info=0, the parameters passed to the function are valid If info<0, the parameter in position -info is invalid devInfoArray device output optional array of integers of dimension batchsize. If non-null, every element devInfoArray[i] contain a value V with the following meaning: V = 0 : the i-th problem was sucessfully solved V > 0 : the V-th diagonal element of the Aarray[i] is zero. Aarray[i] does not have full rank. batchSize input number of pointers contained in Aarray and Carray The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or nrhs < 0 or batchSize < 0 or lda < max(1, m ) or ldc < max(1, m ) CUBLAS_STATUS_NOT_SUPPORTED the parameters m <n or trans is different from non-transpose. CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgels , dgels , cgels , zgels 2.8.9. cublas<t>tpttr()  cublasStatus_t cublasStpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * AP , float * A , int lda ); cublasStatus_t cublasDtpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * AP , double * A , int lda ); cublasStatus_t cublasCtpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * AP , cuComplex * A , int lda ); cublasStatus_t cublasZtpttr ( cublasHandle_t handle , cublasFillMode_t uplo int n , const cuDoubleComplex * AP , cuDoubleComplex * A , int lda ); This function performs the conversion from the triangular packed format to the triangular format If uplo == CUBLAS_FILL_MODE_LOWER then the elements of AP are copied into the lower triangular part of the triangular matrix A and the upper part of A is left untouched. If uplo == CUBLAS_FILL_MODE_UPPER then the elements of AP are copied into the upper triangular part of the triangular matrix A and the lower part of A is left untouched. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates if matrix AP contains lower or upper part of matrix A . n input number of rows and columns of matrix A . AP device input <type> array with \\(A\\) stored in packed format. A device output <type> array of dimensions lda x n , with lda>=max(1,n) . The opposite side of A is left untouched. lda input leading dimension of two-dimensional array used to store matrix A . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: stpttr , dtpttr , ctpttr , ztpttr 2.8.10. cublas<t>trttp()  cublasStatus_t cublasStrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * A , int lda , float * AP ); cublasStatus_t cublasDtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * A , int lda , double * AP ); cublasStatus_t cublasCtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * A , int lda , cuComplex * AP ); cublasStatus_t cublasZtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * AP ); This function performs the conversion from the triangular format to the triangular packed format If uplo == CUBLAS_FILL_MODE_LOWER then the lower triangular part of the triangular matrix A is copied into the array AP . If uplo == CUBLAS_FILL_MODE_UPPER then then the upper triangular part of the triangular matrix A is copied into the array AP . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. uplo input indicates which matrix A lower or upper part is referenced. n input number of rows and columns of matrix A . A device input <type> array of dimensions lda x n , with lda>=max(1,n) . lda input leading dimension of two-dimensional array used to store matrix A . AP device output <type> array with \\(A\\) stored in packed format. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or lda < max(1, n ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strttp , dtrttp , ctrttp , ztrttp 2.8.11. cublas<t>gemmEx()  cublasStatus_t cublasSgemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const float * beta , void * C , cudaDataType_t Ctype , int ldc ) cublasStatus_t cublasCgemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const cuComplex * beta , void * C , cudaDataType_t Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemm . In this function the input matrices and output matrices can have a lower precision but the computation is still done in the type <t> . For example, in the type float for cublasSgemmEx() and in the type cuComplex for cublasCgemmEx() . \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input handle to the cuBLAS library context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input enumerant specifying the datatype of matrix A . lda input leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input enumerant specifying the datatype of matrix B . ldb input leading dimension of two-dimensional array used to store matrix B . beta host or device input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . Ctype input enumerant specifying the datatype of matrix C . ldc input leading dimension of a two-dimensional array used to store the matrix C . The matrix types combinations supported for cublasSgemmEx() are listed below: C A/B CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_8I CUDA_R_16BF CUDA_R_16F CUDA_R_32F The matrix types combinations supported for cublasCgemmEx() are listed below : C A/B CUDA_C_32F CUDA_C_8I CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ARCH_MISMATCH cublasCgemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0 CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters Atype , Btype and Ctype is not supported CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or ldc < max(1, m ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemm For more information about the numerical behavior of some GEMM algorithms, refer to the GEMM Algorithms Numerical Behavior section. 2.8.12. cublasGemmEx()  cublasStatus_t cublasGemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const void * beta , void * C , cudaDataType_t Ctype , int ldc , cublasComputeType_t computeType , cublasGemmAlgo_t algo ) #if defined(__cplusplus) cublasStatus_t cublasGemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType Atype , int lda , const void * B , cudaDataType Btype , int ldb , const void * beta , void * C , cudaDataType Ctype , int ldc , cudaDataType computeType , cublasGemmAlgo_t algo ) #endif This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemm that allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Supported combinations of arguments are listed further down in this section. Note The second variant of cublasGemmEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t . C applications would still compile with the updated function signature. This function is only supported on devices with compute capability 5.0 or later. \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A ) that is non- or (conj.) transpose. transb input Operation op( B ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A ) and C . n input Number of columns of matrix op( B ) and C . k input Number of columns of op( A ) and rows of op( B ). alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. A device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store the matrix A . B device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input Enumerant specifying the datatype of matrix B . ldb input Leading dimension of two-dimensional array used to store matrix B . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. If beta==0 , C does not have to be a valid input. C device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of a two-dimensional array used to store the matrix C . computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F Note CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC compute types are only supported with A, B being 4-byte aligned and lda, ldb being multiples of 4. For better performance, it is also recommended that IMMA kernels requirements for a regular data ordering listed here are met. The possible error values returned by this function and their meanings are listed in the following table. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or Atype or Btype or Ctype or algo is not supported CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. Starting with release 11.2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library. Also refer to: sgemm. For more information about the numerical behavior of some GEMM algorithms, refer to the GEMM Algorithms Numerical Behavior section. 2.8.13. cublasGemmBatchedEx()  cublasStatus_t cublasGemmBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * const Aarray [], cudaDataType_t Atype , int lda , const void * const Barray [], cudaDataType_t Btype , int ldb , const void * beta , void * const Carray [], cudaDataType_t Ctype , int ldc , int batchCount , cublasComputeType_t computeType , cublasGemmAlgo_t algo ) #if defined(__cplusplus) cublasStatus_t cublasGemmBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * const Aarray [], cudaDataType Atype , int lda , const void * const Barray [], cudaDataType Btype , int ldb , const void * beta , void * const Carray [], cudaDataType Ctype , int ldc , int batchCount , cudaDataType computeType , cublasGemmAlgo_t algo ) #endif This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemmBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrix arrays, the precision of computation and the GEMM algorithm to be run. Like cublas<t>gemmBatched , the batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. Supported combinations of arguments are listed further down in this section. Note The second variant of cublasGemmBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t . C applications would still compile with the updated function signature. \\(C\\lbrack i\\rbrack = \\alpha\\text{op}(A\\lbrack i\\rbrack)\\text{op}(B\\lbrack i\\rbrack) + \\beta C\\lbrack i\\rbrack,\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A[i] ) that is non- or (conj.) transpose. transb input Operation op( B[i] ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A[i] ) and C[i] . n input Number of columns of matrix op( B[i] ) and C[i] . k input Number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. Aarray device input Array of pointers to <Atype> array, with each array of dim. lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. All pointers must meet certain alignment criteria. Please see below for details. Atype input Enumerant specifying the datatype of Aarray . lda input Leading dimension of two-dimensional array used to store the matrix A[i] . Barray device input Array of pointers to <Btype> array, with each array of dim. ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. All pointers must meet certain alignment criteria. Please see below for details. Btype input Enumerant specifying the datatype of Barray . ldb input Leading dimension of two-dimensional array used to store matrix B[i] . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. If beta==0 , C[i] does not have to be a valid input. Carray device in/out Array of pointers to <Ctype> array. It has dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. Ctype input Enumerant specifying the datatype of Carray . ldc input Leading dimension of a two-dimensional array used to store each matrix C[i] . batchCount input Number of pointers contained in Aarray, Barray and Carray. computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F If Atype is CUDA_R_16F or CUDA_R_16BF , or computeType is any of the FAST options, or when math mode or algo enable fast math modes, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k%8==0 then ensure intptr_t(ptr) % 16 == 0 , if k%2==0 then ensure intptr_t(ptr) % 4 == 0 . Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4. For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal to or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or Atype or Btype or Ctype or algo or computeType is not supported CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. Also refer to: sgemm. 2.8.14. cublasGemmStridedBatchedEx()  cublasStatus_t cublasGemmStridedBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType_t Atype , int lda , long long int strideA , const void * B , cudaDataType_t Btype , int ldb , long long int strideB , const void * beta , void * C , cudaDataType_t Ctype , int ldc , long long int strideC , int batchCount , cublasComputeType_t computeType , cublasGemmAlgo_t algo ) #if defined(__cplusplus) cublasStatus_t cublasGemmStridedBatchedEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const void * alpha , const void * A , cudaDataType Atype , int lda , long long int strideA , const void * B , cudaDataType Btype , int ldb , long long int strideB , const void * beta , void * C , cudaDataType Ctype , int ldc , long long int strideC , int batchCount , cudaDataType computeType , cublasGemmAlgo_t algo ) #endif This function supports the 64-bit Integer Interface . This function is an extension of cublas<t>gemmStridedBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Like cublas<t>gemmStridedBatched , the batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. Pointers to A, B and C matrices for the first instance are passed to the function by the user along with the offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances. Note The second variant of cublasGemmStridedBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType_t instead of cublasComputeType_t . C applications would still compile with the updated function signature. \\(C + i*{strideC} = \\alpha\\text{op}(A + i*{strideA})\\text{op}(B + i*{strideB}) + \\beta(C + i*{strideC}),\\text{ for i } \\in \\lbrack 0,batchCount - 1\\rbrack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i\\rbrack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i\\rbrack)\\) \\(k \\times n\\) and \\(C\\lbrack i\\rbrack\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B\\lbrack i\\rbrack)\\) is defined similarly for matrix \\(B\\lbrack i\\rbrack\\) . Note \\(C\\lbrack i\\rbrack\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublas<t>gemm in different CUDA streams, rather than use this API. Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1] . The unit for the offset is number of elements and must not be zero . Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A[i] ) that is non- or (conj.) transpose. transb input Operation op( B[i] ) that is non- or (conj.) transpose. m input Number of rows of matrix op( A[i] ) and C[i] . n input Number of columns of matrix op( B[i] ) and C[i] . k input Number of columns of op( A[i] ) and rows of op( B[i] ). alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. A device input Pointer to <Atype> matrix, A, corresponds to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of A . lda input Leading dimension of two-dimensional array used to store the matrix A[i] . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] . B device input Pointer to <Btype> matrix, B, corresponds to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input Enumerant specifying the datatype of B . ldb input Leading dimension of two-dimensional array used to store matrix B[i] . strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. If beta==0 , C[i] does not have to be a valid input. C device in/out Pointer to <Ctype> matrix, C, corresponds to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. Ctype input Enumerant specifying the datatype of C . ldc input Leading dimension of a two-dimensional array used to store each matrix C[i] . strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] . batchCount input Number of GEMMs to perform in the batch. computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmStridedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4. For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m < 0 or n < 0 or k < 0 or if transa or transb != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, m ) if transa == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldb < max(1, k ) if transb == CUBLAS_OP_N and ldb < max(1, n ) otherwise or if ldc < max(1, m ) or Atype or Btype or Ctype or algo or computeType is not supported CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU Also refer to: sgemm. 2.8.15. cublasGemmGroupedBatchedEx()  cublasStatus_t cublasGemmGroupedBatchedEx ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const void * alpha_array , const void * const Aarray [], cudaDataType_t Atype , const int lda_array [], const void * const Barray [], cudaDataType_t Btype , const int ldb_array [], const void * beta_array , void * const Carray [], cudaDataType_t Ctype , const int ldc_array [], int group_count , const int group_size [], cublasComputeType_t computeType ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication on groups of matrices. A given group is considered to be “uniform”, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.  This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemmEx ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], Atype , lda_array [ i ], Barray [ idx ], Btype , ldb_array [ i ], beta_array [ i ], Carray [ idx ], Ctype , ldc_array [ i ], computeType , CUBLAS_GEMM_DEFAULT ); idx += 1 ; end end where \\(\\text{$\\mathrm{alpha\\_array}$}\\) and \\(\\text{$\\mathrm{beta\\_array}$}\\) are arrays of scaling factors, and \\(\\text{Aarray}\\) , \\(\\text{Barray}\\) and \\(\\text{Carray}\\) are arrays of pointers to matrices stored in column-major format.  For a given index, \\(\\text{idx}\\) , that is part of group \\(i\\) , the dimensions are: \\(\\text{op}(\\text{Aarray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{op}(\\text{Barray}\\lbrack\\text{idx}\\rbrack)\\) : \\(\\text{$\\mathrm{k\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) \\(\\text{Carray}\\lbrack\\text{idx}\\rbrack\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i\\rbrack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i\\rbrack\\) Note This API takes arrays of two different lengths.  The arrays of dimensions, leading dimensions, transpositions, and scaling factors are of length group_count and the arrays of matrices are of length problem_count where \\(\\text{$\\mathrm{problem\\_count}$} = \\sum_{i = 0}^{\\text{$\\mathrm{group\\_count}$} - 1} \\text{$\\mathrm{group\\_size}$}\\lbrack i\\rbrack\\) For matrix \\(A[\\text{idx}]\\) in group \\(i\\) \\(\\text{op}(A[\\text{idx}]) = \\left\\{ \\begin{matrix}\nA[\\text{idx}] & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA[\\text{idx}]^{T} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA[\\text{idx}]^{H} & {\\text{if }\\textsf{$\\mathrm{transa\\_array}\\lbrack i\\rbrack$ == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B[\\text{idx}])\\) is defined similarly for matrix \\(B[\\text{idx}]\\) in group \\(i\\) . Note \\(C\\lbrack\\text{idx}\\rbrack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublasGemmBatchedEx() in different CUDA streams, rather than use this API. Param. Memory In/out Meaning Array Length handle input handle to the cuBLAS library context. transa_array host input array containing the operations, op( A[idx] ), that is non- or (conj.) transpose for each group. group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj.) transpose for each group. group_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group. group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group. group_count k_array host input array containing the number of columns of op( A[idx] ) and rows of op( B[idx] ) for each group. group_count alpha_array host input array containing the <type> scalar used for multiplication for each group. group_count Aarray device input array of pointers to <type> array, with each array of dim. lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count Atype input Enumerant specifying the datatype of A . lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group. group_count Barray device input array of pointers to <type> array, with each array of dim. ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise. All pointers must meet certain alignment criteria. Please see below for details. problem_count Btype input Enumerant specifying the datatype of B . ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group. group_count beta_array host input array containing the <type> scalar used for multiplication for each group. group_count Carray device in/out array of pointers to <type> array. It has dimensions ldc[i] x n[i] with ldc[i]>=max(1,m[i]) . Matrices C[idx] should not overlap; otherwise, undefined behavior is expected. All pointers must meet certain alignment criteria. Please see below for details. problem_count Ctype input Enumerant specifying the datatype of C . ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group. group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group. group_count computeType input Enumerant specifying the computation type. cublasGemmGroupedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F If Atype is CUDA_R_16F or CUDA_R_16BF or if the computeType is any of the FAST options, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is required that they meet the following rule: if (k * AtypeSize) % 16 == 0 then ensure intptr_t(ptr) % 16 == 0 , if (k * AtypeSize) % 4 == 0 then ensure intptr_t(ptr) % 4 == 0 . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count < 0 or if m_array[i] , n_array[i] , k_array[i] , group_size[i] < 0 or if transa_array[i] , transb_array[i] != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda_array[i] < max(1, m_array[i] ) if transa_array[i] == CUBLAS_OP_N and lda_array[i] < max(1, k_array[i] ) otherwise or if ldb_array[i] < max(1, k_array[i] ) if transb_array[i] == CUBLAS_OP_N and ldb_array[i] < max(1, n_array[i] ) otherwise or if ldc_array[i] < max(1, m_array[i] ) CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_NOT_SUPPORTED the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE Atype or Btype or Ctype or computeType is not supported 2.8.16. cublasCsyrkEx()  cublasStatus_t cublasCsyrkEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCsyrk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input Operation op( A ) that is non- or transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A. beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCsyrkEx() are listed below: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 2.8.17. cublasCsyrk3mEx()  cublasStatus_t cublasCsyrk3mEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCsyrk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex . This routine is implemented using the Gauss complexity reduction algorithm which can lead to an increase in performance up to 25% This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input Operation op( A ) that is non- or transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A. beta host or device input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCsyrk3mEx() are listed below : A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 2.8.18. cublasCherkEx()  cublasStatus_t cublasCherkEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCherk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input Operation op( A ) that is non- or (conj.) transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCherkEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: cherk 2.8.19. cublasCherk3mEx()  cublasStatus_t cublasCherk3mEx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const void * A , cudaDataType Atype , int lda , const float * beta , cuComplex * C , cudaDataType Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasCherk() where the input matrix and output matrix can have a lower precision but the computation is still done in the type cuComplex . This routine is implemented using the Gauss complexity reduction algorithm which can lead to an increase in performance up to 25% This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Note This routine is only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. uplo input Indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input Operation op( A ) that is non- or (conj.) transpose. n input Number of rows of matrix op( A ) and C . k input Number of columns of matrix op( A ). alpha host or device input <type> scalar used for multiplication. A device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of matrix A . lda input Leading dimension of two-dimensional array used to store matrix A . beta input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. Ctype input Enumerant specifying the datatype of matrix C . ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCherk3mEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If n < 0 or k < 0 or if uplo != CUBLAS_FILL_MODE_UPPER , CUBLAS_FILL_MODE_LOWER or if trans != CUBLAS_OP_N , CUBLAS_OP_C , CUBLAS_OP_T or if lda < max(1, n ) if trans == CUBLAS_OP_N and lda < max(1, k ) otherwise or if ldc < max(1, n ) or Atype or Ctype is not supported CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype and Ctype is not supported. CUBLAS_STATUS_ARCH_MISMATCH The device has a compute capability lower than 5.0. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. For references please refer to: cherk 2.8.20. cublasNrm2Ex()  cublasStatus_t cublasNrm2Ex ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , void * result , cudaDataType resultType , cudaDataType executionType ) This function supports the 64-bit Integer Interface . This function is an API generalization of the routine cublas<t>nrm2 where input data, output data and compute type can be specified independently. This function computes the Euclidean norm of the vector x . The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \\(\\sqrt{\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack j\\rbrack \\times \\mathbf{x}\\lbrack j\\rbrack} \\right)}\\) where \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) in exact arithmetic. Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . x device input <type> vector with n elements. xType input enumerant specifying the datatype of vector x . incx input stride between consecutive elements of x . result host or device output the resulting norm, which is 0.0 if n,incx<=0 . resultType input enumerant specifying the datatype of the result . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasNrm2Ex() are listed below : x result execution CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_R_64F CUDA_R_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the reduction buffer could not be allocated CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType , resultType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE If xType or resultType or executionType is not supported or result == NULL For references please refer to: snrm2 , dnrm2 , scnrm2 , dznrm2 2.8.21. cublasAxpyEx()  cublasStatus_t cublasAxpyEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , const void * x , cudaDataType xType , int incx , void * y , cudaDataType yType , int incy , cudaDataType executiontype ); This function supports the 64-bit Integer Interface . This function is an API generalization of the routine cublas<t>axpy where input data, output data and compute type can be specified independently. This function multiplies the vector x by the scalar \\(\\alpha\\) and adds it to the vector y overwriting the latest vector with the result. Hence, the performed operation is \\(\\mathbf{y}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack k\\rbrack + \\mathbf{y}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. n input Number of elements in the vector x and y . alpha host or device input <type> scalar used for multiplication. alphaType input Enumerant specifying the datatype of scalar alpha . x device input <type> vector with n elements. xType input Enumerant specifying the datatype of vector x . incx input Stride between consecutive elements of x . y device in/out <type> vector with n elements. yType input Enumerant specifying the datatype of vector y . incy input Stride between consecutive elements of y . executionType input Enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasAxpyEx() are listed in the following table: alpha x y execution CUDA_R_32F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , and executionType is not supported. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. CUBLAS_STATUS_INVALID_VALUE alphaType or xType or yType or executionType is not supported. For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.8.22. cublasDotEx()  cublasStatus_t cublasDotEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); cublasStatus_t cublasDotcEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); These functions support the 64-bit Integer Interface . These functions are an API generalization of the routines cublas<t>dot and cublas<t>dotc where input data, output data and compute type can be specified independently. Note: cublas<t>dotc is dot product conjugated, cublas<t>dotu is dot product unconjugated. This function computes the dot product of vectors x and y . Hence, the result is \\(\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack k\\rbrack \\times \\mathbf{y}\\lbrack j\\rbrack} \\right)\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. n input Number of elements in the vectors x and y . x device input <type> vector with n elements. xType input Enumerant specifying the datatype of vector x . incx input Stride between consecutive elements of x . y device input <type> vector with n elements. yType input Enumerant specifying the datatype of vector y . incy input Stride between consecutive elements of y . result host or device output The resulting dot product, which is 0.0 if n<=0 . resultType input Enumerant specifying the datatype of the result . executionType input Enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasDotEx() and cublasDotcEx() are listed below: x y result execution CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_ALLOC_FAILED The reduction buffer could not be allocated. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , resultType and executionType is not supported. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. CUBLAS_STATUS_INVALID_VALUE xType or yType or resultType or executionType is not supported. For references please refer to: sdot , ddot , cdotu , cdotc , zdotu , zdotc 2.8.23. cublasRotEx()  cublasStatus_t cublasRotEx ( cublasHandle_t handle , int n , void * x , cudaDataType xType , int incx , void * y , cudaDataType yType , int incy , const void * c , /* host or device pointer */ const void * s , cudaDataType csType , cudaDataType executiontype ); This function supports the 64-bit Integer Interface . This function is an extension to the routine cublas<t>rot where input data, output data, cosine/sine type, and compute type can be specified independently. This function applies Givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \\(G = \\begin{pmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{pmatrix}\\) to vectors x and y . Hence, the result is \\(\\mathbf{x}\\lbrack k\\rbrack = c \\times \\mathbf{x}\\lbrack k\\rbrack + s \\times \\mathbf{y}\\lbrack j\\rbrack\\) and \\(\\mathbf{y}\\lbrack j\\rbrack = - s \\times \\mathbf{x}\\lbrack k\\rbrack + c \\times \\mathbf{y}\\lbrack j\\rbrack\\) where \\(k = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incy}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vectors x and y . x device in/out <type> vector with n elements. xType input enumerant specifying the datatype of vector x . incx input stride between consecutive elements of x . y device in/out <type> vector with n elements. yType input enumerant specifying the datatype of vector y . incy input stride between consecutive elements of y . c host or device input cosine element of the rotation matrix. s host or device input sine element of the rotation matrix. csType input enumerant specifying the datatype of c and s . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasRotEx() are listed below : executionType xType / yType csType CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_R_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.8.24. cublasScalEx()  cublasStatus_t cublasScalEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , void * x , cudaDataType xType , int incx , cudaDataType executionType ); This function supports the 64-bit Integer Interface . This function scales the vector x by the scalar \\(\\alpha\\) and overwrites it with the result. Hence, the performed operation is \\(\\mathbf{x}\\lbrack j\\rbrack = \\alpha \\times \\mathbf{x}\\lbrack j\\rbrack\\) for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1} \\right)*\\text{incx}\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x . alpha host or device input <type> scalar used for multiplication. alphaType input enumerant specifying the datatype of scalar alpha . x device in/out <type> vector with n elements. xType input enumerant specifying the datatype of vector x . incx input stride between consecutive elements of x . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasScalEx() are listed below : alpha x execution CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE alphaType or xType or executionType is not supported For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 3. Using the cuBLASLt API  3.1. General Description  The cuBLASLt library is a new lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API. This new library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. Once a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data. Note The cuBLASLt library does not guarantee the support of all possible sizes and configurations, however, since CUDA 12.2 update 2, the problem size limitations on m, n, and batch size have been largely resolved. The main focus of the library is to provide the most performant kernels, which might have some implied limitations. Some non-standard configurations may require a user to handle them manually, typically by decomposing the problem into smaller parts (see Problem Size Limitations ). 3.1.1. Problem Size Limitations  There are inherent problem size limitations that are a result of limitations in CUDA grid dimensions.  For example, many kernels do not support batch sizes greater than 65535 due to a limitation on the z dimension of a grid.  There are similar restriction on the m and n values for a given problem. In cases where a problem cannot be run by a single kernel, cuBLASLt will attempt to decompose the problem into multiple sub-problems and solve it by running the kernel on each sub-problem. There are some restrictions on cuBLASLt internal problem decomposition which are summarized below: Amax computations are not supported.  This means that CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER must be left unset (see cublasLtMatmulDescAttributes_t ) All matrix layouts must have CUBLASLT_MATRIX_LAYOUT_ORDER set to CUBLASLT_ORDER_COL (see cublasLtOrder_t ) cuBLASLt will not partition along the n dimension when CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_DRELU_BGRAD or CUBLASLT_EPILOGUE_DGELU_BGRAD (see cublasLtEpilogue_t ) To overcome these limitations, a user may want to partition the problem themself, launch kernels for each sub-problem, and compute any necessary reductions to combine the results. 3.1.2. Heuristics Cache  cuBLASLt uses heuristics to pick the most suitable matmul kernel for execution based on the problem sizes, GPU configuration, and other parameters. This requires performing some computations on the host CPU, which could take tens of microseconds. To overcome this overhead, it is recommended to query the heuristics once using cublasLtMatmulAlgoGetHeuristic() and then reuse the result for subsequent computations using cublasLtMatmul() . For the cases where querying heuristics once and then reusing them is not feasible, cuBLASLt implements a heuristics cache that maps matmul problems to kernels previously selected by heuristics. The heuristics cache uses an LRU-like eviction policy and is thread-safe. The user can control the heuristics cache capacity with the CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable or with the cublasLtHeuristicsCacheSetCapacity() function which has higher precedence. The capacity is measured in number of entries and might be rounded up to the nearest multiple of some factor for performance reasons. Each entry takes about 360 bytes but is subject to change. The default capacity is 8192 entries. Note Setting capacity to zero disables the cache completely. This can be useful for workloads that do not have a steady state and for which cache operations may have higher overhead than regular heuristics computations. Note The cache is not ideal for performance reasons, so it is sometimes necessary to increase its capacity 1.5x-2.x over the anticipated number of unique matmul problems to achieve a nearly perfect hit rate. See also: cublasLtHeuristicsCacheGetCapacity() , cublasLtHeuristicsCacheSetCapacity() . 3.1.3. cuBLASLt Logging  cuBLASLt logging mechanism can be enabled by setting the following environment variables before launching the target application: CUBLASLT_LOG_LEVEL=<level> , where <level> is one of the following levels: “0” - Off - logging is disabled (default) “1” - Error - only errors will be logged “2” - Trace - API calls that launch CUDA kernels will log their parameters and important information “3” - Hints - hints that can potentially improve the application’s performance “4” - Info - provides general information about the library execution, may contain details about heuristic status “5” - API Trace - API calls will log their parameter and important information CUBLASLT_LOG_MASK=<mask> , where <mask> is a combination of the following flags: “0” - Off “1” - Error “2” - Trace “4” - Hints “8” - Info “16” - API Trace For example, use CUBLASLT_LOG_MASK=5 to enable Error and Hints messages. CUBLASLT_LOG_FILE=<file_name> , where <file_name> is a path to a logging file.  File name may contain %i , that will be replaced with the process ID. For example <file_name>_%i.log . If CUBLASLT_LOG_FILE is not defined, the log messages are printed to stdout. Another option is to use the experimental cuBLASLt logging API. See: cublasLtLoggerSetCallback() , cublasLtLoggerSetFile() , cublasLtLoggerOpenFile() , cublasLtLoggerSetLevel() , cublasLtLoggerSetMask() , cublasLtLoggerForceDisable() 3.1.4. 8-bit Floating Point Data Types (FP8) Usage  FP8 was first introduced with Ada and Hopper GPUs (compute capability 8.9 and above) and is designed to further accelerate matrix multiplications. There are two types of FP8 available: CUDA_R_8F_E4M3 is designed to be accurate at a smaller dynamic range than half precision. The E4 and M3 represent a 4-bit exponent and a 3-bit mantissa respectively. For more details, see __nv__fp8__e4m3 . CUDA_R_8F_E5M2 is designed to be accurate at a similar dynamic range as half precision. The E5 and M2 represent a 5-bit exponent and a 2-bit mantissa respectively. For more information see __nv__fp8__e5m2 . Note Unless otherwise stated, FP8 refers to both CUDA_R_8F_E4M3 and CUDA_R_8F_E5M2 . In order to maintain accurate FP8 matrix multiplications, we define native compute FP8 matrix multiplication as follows: \\[D = scale_D \\cdot (\\alpha \\cdot scale_A \\cdot scale_B \\cdot \\text{op}(A) \\text{op}(B) + \\beta \\cdot scale_C \\cdot C)\\] where A, B, and C are input matrices, and scaleA, scaleB, scaleC, scaleD, alpha, and beta are input scalars. This differs from the other matrix multiplication routines because of this addition of scaling factors for each matrix. The scaleA, scaleB, and scaleC are used for de-quantization, and scaleD is used for quantization. Note that all the scaling factors are applied multiplicatively. This means that sometimes it is necessary to use a scaling factor or its reciprocal depending on the context in which it is applied. For more information on FP8, see cublasLtMatmul() and cublasLtMatmulDescAttributes_t . For FP8 matrix multiplications, epilogues and amaxD may be computed as follows: \\[\\begin{split}D_{temp}, Aux_{temp} & = \\mathop{Epilogue}(\\alpha \\cdot scale_A \\cdot scale_B \\cdot \\text{op}(A) \\text{op}(B) + \\beta \\cdot scale_C \\cdot C) \\\\\namax_{D} & = \\mathop{absmax}(D_{temp}) \\\\\namax_{Aux} & = \\mathop{absmax}(Aux_{temp}) \\\\\nD & = scale_D * D_{temp} \\\\\nAux & = scale_{Aux} * Aux_{temp} \\\\\\end{split}\\] Here Aux is an auxiliary output of an epilogue function like GELU, scaleAux is an optional scaling factor that can be applied to Aux, and amaxAux is the maximum absolute value in Aux before scaling. For more information, see attributes CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER in cublasLtMatmulDescAttributes_t . 3.1.5. Disabling CPU Instructions  As mentioned in the Heuristics Cache section, cuBLASLt heuristics perform some compute-intensive operations on the host CPU.\nTo speed-up the operations, the implementation detects CPU capabilities and may use special instructions, such as Advanced Vector Extensions (AVX) on x86-64 CPUs.\nHowever, in some rare cases this might be not desirable. For instance, using advanced instructions may result in CPU running at a lower frequency, which would affect performance of the other host code. The user can optionally instruct the cuBLASLt library to not use some CPU instructions with the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable or with the cublasLtDisableCpuInstructionsSetMask() function which has higher precedence.\nThe default mask is 0, meaning that there are no restrictions. Please check cublasLtDisableCpuInstructionsSetMask() for more information. 3.1.6. Atomics Synchronization  Atomics synchronization allows optimizing matmul workloads by enabling cublasLtMatmul() to have a producer or consumer relationship with another concurrently running kernel. This allows overlapping computation and communication with a finer granularity. Conceptually, matmul is provided with an array containing 32-bit integer counters, and then: In the consumer mode, either matrix A is partitioned into chunks by rows, or matrix B is partitioned into chunks by columns 1 . A chunk can be read from memory and used in computations only when the corresponding atomic counter reaches value of 0. The producer should execute a memory fence to ensure that the written value is visible to the concurrently running matmul kernel 2 . In the producer mode, the output matrix C (or D in the out-of-place mode), is partitioned by rows or columns, and after a chunk is computed, the corresponding atomic counter is set to 0. Each counter must be initialized to 1 before the matmul kernel runs. 1 The current implementation allows partitioning either the rows or the columns of the matrixes, but not both. Batched cases are not supported. 2 One possible implementation of a memory fence is cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope::thread_scope_device) (see cuda::atomic_thread_fence() for more details). The array of counters are passed to matmuls via the CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER compute descriptor attributes for the consumer and producer modes respectively 3 . The arrays must have a sufficient number of elements for all the chunks. 3 The current implementation allows to only enable either the producer or the consumer mode, but not both. Matmul will return an error if both input and output counter pointers to a non-NULL value. The number of chunks is controlled by CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS compute descriptor attributes. Both of these attributes must be set to a value greater than zero for the feature to be enabled. For the column-major layout, the number of chunks must satisfy: \\[\\begin{split}0 \\leq \\text{$\\mathrm{NUM\\_CHUNKS\\_ROWS}$} \\leq & \\mathop{\\text{floor}}\\left( \\frac{\\text{M}}{\\text{$\\mathrm{TILE\\_SIZE\\_M}$} * \\text{$\\mathrm{CLUSTER\\_SHAPE\\_M}$}} \\right) \\\\\n0 \\leq \\text{$\\mathrm{NUM\\_CHUNKS\\_COLS}$} \\leq & \\mathop{\\text{floor}}\\left( \\frac{\\text{N}}{\\text{$\\mathrm{TILE\\_SIZE\\_N}$} * \\text{$\\mathrm{CLUSTER\\_SHAPE\\_N}$}} \\right)\\end{split}\\] For row-major layout, M and N in tile size and cluster shape must be swapped. These restrictions mean that it is required to first query heuristic via cublasLtMatmulAlgoGetHeuristic() and inspect the result for tile and cluster shapes, and only then set the number of chunks. The pseudocode below shows the principles of operation: // The code below shows operation when partitioning over // rows assuming column-major layout and TN case. // // The case when partitioning is done over columns or // row-major case are handled in a similar fashion, // with the main difference being the offsets // computations. // // Note that the actual implementation does not // guarantee in which order the chunks are computed, // and may employ various optimizations to improve // overall performance. // // Here: //   - A, B, C -- input matrices in the column-major layout //   - lda -- leading dimension of matrix A //   - M, N, K -- the original problem dimensions //   - counters_in[] and counters_out[] -- the arrays of //     input and output atomic counters // for ( int i = 0 ; i < NUM_CHUNKS_ROWS ; i ++ ) { // Consumer: wait for the input counter to become 0 if ( consumer ) { while ( counters_in [ i ] != 0 ); // spin } // compute chunk dimensions chunk_m_begin = floor (( double ) M / NUM_CHUNKS_ROWS * i ); chunk_m_end = floor (( double ) M / NUM_CHUNKS_ROWS * ( i + 1 )); chunk_m = chunk_m_end - chunk_m_begin ; // Compute the current chunk matmul ( chunk_m , N , K , A [ chunk_m_begin * lda ], // A is col-major transposed B , // B is not partitioned C [ chunk_m_begin ] // C is col-major non-transposed ); // Producer: set the counter to 0 when done if ( producer ) { counters_out [ i ] = 0 ; // make the written value visible to the consumer kernel memory_fence (); } } It should be noted that, in general, CUDA programming model provides few kernel co-scheduling guarantees. Thus, use of this feature requires careful orchestration of producer and consumer kernels launch order and resource availability, as it easy to create a deadlock situation. A deadlock may occur in the following cases (this is not an exhaustive list): If a producer kernel cannot start because consumer kernel was launched first and is occupying some of SMs that are needed by the producer kernel to launch. It is strongly recommended to set CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET to carve out some SMs for non-matmul (typically communication) kernels to execute on. If cudaDeviceSynchronize() is called after consumer kernel starts but before the producer kernel does. When lazy module loading is enabled, and producer kernel cannot be loaded while the consumer kernel is running due to locking in the CUDA runtime library. Both kernels also must be loaded before they are run together to avoid this situation. Using CUDA Graphs is another way to avoid deadlocks due to lazy loading. Note This feature is aimed at advanced users and is only available on Hopper architecture for FP8 non-batched cases with fast accumulation mode enabled, and is considered to have beta quality due to the large number of restrictions on its use. 3.2. cuBLASLt Code Examples  Please visit https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLASLt for updated code examples. 3.3. cuBLASLt Datatypes Reference  3.3.1. cublasLtClusterShape_t  cublasLtClusterShape_t is an enumerated type used to configure thread block cluster dimensions. Thread block clusters add an optional hierarchical level and are made up of thread blocks. Similar to thread blocks, these can be one, two, or three-dimensional. See also Thread Block Clusters . Value Description CUBLASLT_CLUSTER_SHAPE_AUTO Cluster shape is automatically selected. CUBLASLT_CLUSTER_SHAPE_1x1x1 Cluster shape is 1 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_1x2x1 Cluster shape is 1 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_1x4x1 Cluster shape is 1 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_2x1x1 Cluster shape is 2 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_2x2x1 Cluster shape is 2 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_2x4x1 Cluster shape is 2 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_4x1x1 Cluster shape is 4 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_4x2x1 Cluster shape is 4 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_4x4x1 Cluster shape is 4 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_1x8x1 Cluster shape is 1 x 8 x 1. CUBLASLT_CLUSTER_SHAPE_8x1x1 Cluster shape is 8 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_2x8x1 Cluster shape is 2 x 8 x 1. CUBLASLT_CLUSTER_SHAPE_8x2x1 Cluster shape is 8 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_1x16x1 Cluster shape is 1 x 16 x 1. CUBLASLT_CLUSTER_SHAPE_16x1x1 Cluster shape is 16 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_1x3x1 Cluster shape is 1 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_1x5x1 Cluster shape is 1 x 5 x 1. CUBLASLT_CLUSTER_SHAPE_1x6x1 Cluster shape is 1 x 6 x 1. CUBLASLT_CLUSTER_SHAPE_1x7x1 Cluster shape is 1 x 7 x 1. CUBLASLT_CLUSTER_SHAPE_1x9x1 Cluster shape is 1 x 9 x 1. CUBLASLT_CLUSTER_SHAPE_1x10x1 Cluster shape is 1 x 10 x 1. CUBLASLT_CLUSTER_SHAPE_1x11x1 Cluster shape is 1 x 11 x 1. CUBLASLT_CLUSTER_SHAPE_1x12x1 Cluster shape is 1 x 12 x 1. CUBLASLT_CLUSTER_SHAPE_1x13x1 Cluster shape is 1 x 13 x 1. CUBLASLT_CLUSTER_SHAPE_1x14x1 Cluster shape is 1 x 14 x 1. CUBLASLT_CLUSTER_SHAPE_1x15x1 Cluster shape is 1 x 15 x 1. CUBLASLT_CLUSTER_SHAPE_2x3x1 Cluster shape is 2 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_2x5x1 Cluster shape is 2 x 5 x 1. CUBLASLT_CLUSTER_SHAPE_2x6x1 Cluster shape is 2 x 6 x 1. CUBLASLT_CLUSTER_SHAPE_2x7x1 Cluster shape is 2 x 7 x 1. CUBLASLT_CLUSTER_SHAPE_3x1x1 Cluster shape is 3 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_3x2x1 Cluster shape is 3 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_3x3x1 Cluster shape is 3 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_3x4x1 Cluster shape is 3 x 4 x 1. CUBLASLT_CLUSTER_SHAPE_3x5x1 Cluster shape is 3 x 5 x 1. CUBLASLT_CLUSTER_SHAPE_4x3x1 Cluster shape is 4 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_5x1x1 Cluster shape is 5 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_5x2x1 Cluster shape is 5 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_5x3x1 Cluster shape is 5 x 3 x 1. CUBLASLT_CLUSTER_SHAPE_6x1x1 Cluster shape is 6 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_6x2x1 Cluster shape is 6 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_7x1x1 Cluster shape is 7 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_7x2x1 Cluster shape is 7 x 2 x 1. CUBLASLT_CLUSTER_SHAPE_9x1x1 Cluster shape is 9 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_10x1x1 Cluster shape is 10 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_11x1x1 Cluster shape is 11 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_12x1x1 Cluster shape is 12 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_13x1x1 Cluster shape is 13 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_14x1x1 Cluster shape is 14 x 1 x 1. CUBLASLT_CLUSTER_SHAPE_15x1x1 Cluster shape is 15 x 1 x 1. 3.3.2. cublasLtEpilogue_t  The cublasLtEpilogue_t is an enum type to set the postprocessing options for the epilogue. Value Description CUBLASLT_EPILOGUE_DEFAULT = 1 No special postprocessing, just scale and quantize the results if necessary. CUBLASLT_EPILOGUE_RELU = 2 Apply ReLU point-wise transform to the results ( x := max(x, 0) ). CUBLASLT_EPILOGUE_RELU_AUX = CUBLASLT_EPILOGUE_RELU | 128 Apply ReLU point-wise transform to the results ( x := max(x, 0) ). This epilogue mode produces an extra output, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_BIAS = 4 Apply (broadcast) bias from the bias vector. Bias vector length must match matrix D rows, and it must be packed (such as stride between vector elements is 1). Bias vector is broadcast to all columns and added before applying the final postprocessing. CUBLASLT_EPILOGUE_RELU_BIAS = CUBLASLT_EPILOGUE_RELU | CUBLASLT_EPILOGUE_BIAS Apply bias and then ReLU transform. CUBLASLT_EPILOGUE_RELU_AUX_BIAS = CUBLASLT_EPILOGUE_RELU_AUX | CUBLASLT_EPILOGUE_BIAS Apply bias and then ReLU transform. This epilogue mode produces an extra output, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DRELU = 8 | 128 Apply ReLu gradient to matmul output. Store ReLu gradient in the output matrix. This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DRELU_BGRAD = CUBLASLT_EPILOGUE_DRELU | 16 Apply independently ReLu and Bias gradient to matmul output. Store ReLu gradient in the output matrix, and Bias gradient in the bias buffer (see CUBLASLT_MATMUL_DESC_BIAS_POINTER). This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_GELU = 32 Apply GELU point-wise transform to the results ( x := GELU(x) ). CUBLASLT_EPILOGUE_GELU_AUX = CUBLASLT_EPILOGUE_GELU | 128 Apply GELU point-wise transform to the results ( x := GELU(x) ). This epilogue mode outputs GELU input as a separate matrix (useful for training). See CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_GELU_BIAS = CUBLASLT_EPILOGUE_GELU | CUBLASLT_EPILOGUE_BIAS Apply Bias and then GELU transform 4 . CUBLASLT_EPILOGUE_GELU_AUX_BIAS = CUBLASLT_EPILOGUE_GELU_AUX | CUBLASLT_EPILOGUE_BIAS Apply Bias and then GELU transform 4 . This epilogue mode outputs GELU input as a separate matrix (useful for training). See CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DGELU = 64 | 128 Apply GELU gradient to matmul output. Store GELU gradient in the output matrix. This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_DGELU_BGRAD = CUBLASLT_EPILOGUE_DGELU | 16 Apply independently GELU and Bias gradient to matmul output. Store GELU gradient in the output matrix, and Bias gradient in the bias buffer (see CUBLASLT_MATMUL_DESC_BIAS_POINTER). This epilogue mode requires an extra input, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_BGRADA = 256 Apply Bias gradient to the input matrix A. The bias size corresponds to the number of rows of the matrix D. The reduction happens over the GEMM’s “k” dimension. Store Bias gradient in the bias buffer, see CUBLASLT_MATMUL_DESC_BIAS_POINTER of cublasLtMatmulDescAttributes_t . CUBLASLT_EPILOGUE_BGRADB = 512 Apply Bias gradient to the input matrix B. The bias size corresponds to the number of columns of the matrix D. The reduction happens over the GEMM’s “k” dimension. Store Bias gradient in the bias buffer, see CUBLASLT_MATMUL_DESC_BIAS_POINTER of cublasLtMatmulDescAttributes_t . NOTES: 4 ( 1 , 2 ) GELU (Gaussian Error Linear Unit) is approximated by: \\({0.5}x\\left( 1 + \\text{tanh}\\left( \\sqrt{2/\\pi}\\left( x + {0.044715}x^{3} \\right) \\right) \\right)\\) 3.3.3. cublasLtHandle_t  The cublasLtHandle_t type is a pointer type to an opaque structure holding the cuBLASLt  library context. Use cublasLtCreate() to initialize the cuBLASLt library context and return a handle to an opaque structure holding the cuBLASLt library context, and use cublasLtDestroy() to destroy a previously created cuBLASLt library context descriptor and release the resources. Note cuBLAS handle ( cublasHandle_t ) encapsulates a cuBLASLt handle. Any valid cublasHandle_t can be used in place of cublasLtHandle_t with a simple cast. However, unlike a cuBLAS handle, a cuBLASLt handle is not tied to any particular CUDA context. 3.3.4. cublasLtLoggerCallback_t  cublasLtLoggerCallback_t is a callback function pointer type. A callback function can be set using cublasLtLoggerSetCallback() . Parameters : Parameter Memory Input / Output Description logLevel Output See cuBLASLt Logging . functionName Output The name of the API that logged this message. message Output The log message. 3.3.5. cublasLtMatmulAlgo_t  cublasLtMatmulAlgo_t is an opaque structure holding the description of the matrix multiplication algorithm. This structure can be trivially serialized and later restored for use with the same version of cuBLAS library to save on selecting the right configuration again. 3.3.6. cublasLtMatmulAlgoCapAttributes_t  cublasLtMatmulAlgoCapAttributes_t enumerates matrix multiplication algorithm capability attributes that can be retrieved from an initialized cublasLtMatmulAlgo_t descriptor using cublasLtMatmulAlgoCapGetAttribute() . Value Description Data Type CUBLASLT_ALGO_CAP_SPLITK_SUPPORT Support for split-K. Boolean (0 or 1) to express if split-K implementation is supported. 0 means no support, and supported otherwise. See CUBLASLT_ALGO_CONFIG_SPLITK_NUM of cublasLtMatmulAlgoConfigAttributes_t . int32_t CUBLASLT_ALGO_CAP_REDUCTION_SCHEME_MASK Mask to express the types of reduction schemes supported, see cublasLtReductionScheme_t . If the reduction scheme is not masked out then it is supported. For example: int isReductionSchemeComputeTypeSupported ? (reductionSchemeMask & CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE) == CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE ? 1 : 0; uint32_t CUBLASLT_ALGO_CAP_CTA_SWIZZLING_SUPPORT Support for CTA-swizzling. Boolean (0 or 1) to express if CTA-swizzling implementation is supported. 0 means no support, and 1 means supported value of 1; other values are reserved. See also CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING of cublasLtMatmulAlgoConfigAttributes_t . uint32_t CUBLASLT_ALGO_CAP_STRIDED_BATCH_SUPPORT Support strided batch. 0 means no support, supported otherwise. int32_t CUBLASLT_ALGO_CAP_OUT_OF_PLACE_RESULT_SUPPORT Support results out of place (D != C in D = alpha.A.B + beta.C). 0 means no support, supported otherwise. int32_t CUBLASLT_ALGO_CAP_UPLO_SUPPORT Syrk (symmetric rank k update)/herk (Hermitian rank k update) support (on top of regular gemm). 0 means no support, supported otherwise. int32_t CUBLASLT_ALGO_CAP_TILE_IDS The tile ids possible to use. See cublasLtMatmulTile_t . If no tile ids are supported then use CUBLASLT_MATMUL_TILE_UNDEFINED. Use cublasLtMatmulAlgoCapGetAttribute() with sizeInBytes = 0 to query the actual count. Array of uint32_t CUBLASLT_ALGO_CAP_STAGES_IDS The stages ids possible to use. See cublasLtMatmulStages_t . If no stages ids are supported then use CUBLASLT_MATMUL_STAGES_UNDEFINED. Use cublasLtMatmulAlgoCapGetAttribute() with sizeInBytes = 0 to query the actual count. Array of uint32_t CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX Custom option range is from 0 to CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX (inclusive). See CUBLASLT_ALGO_CONFIG_CUSTOM_OPTION of cublasLtMatmulAlgoConfigAttributes_t . int32_t CUBLASLT_ALGO_CAP_MATHMODE_IMPL Indicates whether the algorithm is using regular compute or tensor operations. 0 means regular compute, 1 means tensor operations.\nDEPRECATED int32_t CUBLASLT_ALGO_CAP_GAUSSIAN_IMPL Indicate whether the algorithm implements the Gaussian optimization of complex matrix multiplication. 0 means regular compute; 1 means Gaussian. See cublasMath_t .\nDEPRECATED int32_t CUBLASLT_ALGO_CAP_CUSTOM_MEMORY_ORDER Indicates whether the algorithm supports custom (not COL or ROW memory order). 0 means only COL and ROW memory order is allowed, non-zero means that algo might have different requirements. See cublasLtOrder_t . int32_t CUBLASLT_ALGO_CAP_POINTER_MODE_MASK Bitmask enumerating the pointer modes the algorithm supports. See cublasLtPointerModeMask_t . uint32_t CUBLASLT_ALGO_CAP_EPILOGUE_MASK Bitmask enumerating the kinds of postprocessing algorithm supported in the epilogue. See cublasLtEpilogue_t . uint32_t CUBLASLT_ALGO_CAP_LD_NEGATIVE Support for negative ld for all of the matrices. 0 means no support, supported otherwise. uint32_t CUBLASLT_ALGO_CAP_NUMERICAL_IMPL_FLAGS Details about algorithm’s implementation that affect it’s numerical behavior. See cublasLtNumericalImplFlags_t . uint64_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_A_BYTES Minimum alignment required for A matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_B_BYTES Minimum alignment required for B matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_C_BYTES Minimum alignment required for C matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_MIN_ALIGNMENT_D_BYTES Minimum alignment required for D matrix in bytes. uint32_t CUBLASLT_ALGO_CAP_ATOMIC_SYNC Support for synchronization via atomic counters. See Atomics Synchronization . int32_t 3.3.7. cublasLtMatmulAlgoConfigAttributes_t  cublasLtMatmulAlgoConfigAttributes_t is an enumerated type that contains the configuration attributes for cuBLASLt matrix multiply algorithms. The configuration attributes are algorithm-specific, and can be set. The attributes configuration of a given algorithm should agree with its capability attributes. Use cublasLtMatmulAlgoConfigGetAttribute() and cublasLtMatmulAlgoConfigSetAttribute() to get and set the attribute value of a matmul algorithm descriptor. Value Description Data Type CUBLASLT_ALGO_CONFIG_ID Read-only attribute. Algorithm index. See cublasLtMatmulAlgoGetIds() . Set by cublasLtMatmulAlgoInit() . int32_t CUBLASLT_ALGO_CONFIG_TILE_ID Tile id. See cublasLtMatmulTile_t . Default: CUBLASLT_MATMUL_TILE_UNDEFINED . uint32_t CUBLASLT_ALGO_CONFIG_STAGES_ID stages id, see cublasLtMatmulStages_t . Default: CUBLASLT_MATMUL_STAGES_UNDEFINED . uint32_t CUBLASLT_ALGO_CONFIG_SPLITK_NUM Number of K splits. If the number of K splits is greater than one, SPLITK_NUM parts of matrix multiplication will be computed in parallel. The results will be accumulated according to CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME . uint32_t CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME Reduction scheme to use when splitK value > 1. Default: CUBLASLT_REDUCTION_SCHEME_NONE . See cublasLtReductionScheme_t . uint32_t CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING Enable/Disable CTA swizzling. Change mapping from CUDA grid coordinates to parts of the matrices. Possible values: 0 and 1; other values reserved. uint32_t CUBLASLT_ALGO_CONFIG_CUSTOM_OPTION Custom option value. Each algorithm can support some custom options that don’t fit the description of the other configuration attributes. See the CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX of cublasLtMatmulAlgoCapAttributes_t for the accepted range for a specific case. uint32_t CUBLASLT_ALGO_CONFIG_INNER_SHAPE_ID Inner shape ID. Refer to cublasLtMatmulInnerShape_t. Default: CUBLASLT_MATMUL_INNER_SHAPE_UNDEFINED . uint16_t CUBLASLT_ALGO_CONFIG_CLUSTER_SHAPE_ID Cluster shape ID. Refer to cublasLtClusterShape_t. Default: CUBLASLT_CLUSTER_SHAPE_AUTO . uint16_t 3.3.8. cublasLtMatmulDesc_t  The cublasLtMatmulDesc_t is a pointer to an opaque structure holding the description of the matrix multiplication operation cublasLtMatmul() . A descriptor can be created by calling cublasLtMatmulDescCreate() and destroyed by calling cublasLtMatmulDescDestroy() . 3.3.9. cublasLtMatmulDescAttributes_t  cublasLtMatmulDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix multiply operation. Use cublasLtMatmulDescGetAttribute() and cublasLtMatmulDescSetAttribute() to get and set the attribute value of a matmul descriptor. Attribute Name Description Data Type CUBLASLT_MATMUL_DESC_COMPUTE_TYPE Compute type. Defines the data type used for multiply and accumulate operations, and the accumulator during the matrix multiplication. See cublasComputeType_t . int32_t CUBLASLT_MATMUL_DESC_SCALE_TYPE Scale type. Defines the data type of the scaling factors alpha and beta . The accumulator value and the value from matrix C are typically converted to scale type before final scaling. The value is then converted from scale type to the type of matrix D before storing in memory. Default value is aligned with CUBLASLT_MATMUL_DESC_COMPUTE_TYPE. See cudaDataType_t . int32_t CUBLASLT_MATMUL_DESC_POINTER_MODE Specifies alpha and beta are passed by reference, whether they are scalars on the host or on the device, or device vectors. Default value is: CUBLASLT_POINTER_MODE_HOST (i.e., on the host). See cublasLtPointerMode_t . int32_t CUBLASLT_MATMUL_DESC_TRANSA Specifies the type of transformation operation that should be performed on matrix A. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_TRANSB Specifies the type of transformation operation that should be performed on matrix B. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_TRANSC Specifies the type of transformation operation that should be performed on matrix C. Currently only CUBLAS_OP_N is supported. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_FILL_MODE Indicates whether the lower or upper part of the dense matrix was filled, and consequently should be used by the function. Default value is: CUBLAS_FILL_MODE_FULL .See cublasFillMode_t . int32_t CUBLASLT_MATMUL_DESC_EPILOGUE Epilogue function. See cublasLtEpilogue_t . Default value is: CUBLASLT_EPILOGUE_DEFAULT . uint32_t CUBLASLT_MATMUL_DESC_BIAS_POINTER Bias or Bias gradient vector pointer in the device memory. Input vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BIAS , CUBLASLT_EPILOGUE_RELU_BIAS , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_GELU_BIAS , CUBLASLT_EPILOGUE_GELU_AUX_BIAS . Output vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_DRELU_BGRAD , CUBLASLT_EPILOGUE_DGELU_BGRAD , CUBLASLT_EPILOGUE_BGRADA . Output vector with length that matches the number of columns of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BGRADB . Bias vector elements are the same type as alpha and beta (see CUBLASLT_MATMUL_DESC_SCALE_TYPE in this table) when matrix D datatype is CUDA_R_8I and same as matrix D datatype otherwise. See the datatypes table under cublasLtMatmul() for detailed mapping. Default value is: NULL. void * / const void * CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE Stride (in elements) to the next bias or bias gradient vector for strided batch operations.  The default value is 0. int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER Pointer for epilogue auxiliary buffer. Output vector for ReLu bit-mask in forward pass when CUBLASLT_EPILOGUE_RELU_AUX or CUBLASLT_EPILOGUE_RELU_AUX_BIAS epilogue is used. Input vector for ReLu bit-mask in backward pass when CUBLASLT_EPILOGUE_DRELU or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Output of GELU input matrix in forward pass when CUBLASLT_EPILOGUE_GELU_AUX_BIAS epilogue is used. Input of GELU input matrix for backward pass when CUBLASLT_EPILOGUE_DGELU or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue is used. For aux data type, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE . Routines that don’t dereference this pointer, like cublasLtMatmulAlgoGetHeuristic() depend on its value to determine expected pointer alignment. Requires setting the CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD attribute. void * / const void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD Leading dimension for epilogue auxiliary buffer. ReLu bit-mask matrix leading dimension in elements (i.e. bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU_BGRAD , or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Must be divisible by 128 and be no less than the number of rows in the output matrix. GELU input matrix leading dimension in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DGELU ,  or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used. Must be divisible by 8 and be no less than the number of rows in the output matrix. int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_BATCH_STRIDE Batch stride for epilogue auxiliary buffer. ReLu bit-mask matrix batch stride in elements (i.e. bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Must be divisible by 128. GELU input matrix batch stride in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used. Must be divisible by 8. Default value: 0. int64_t CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE Batch stride for alpha vector. Used together with CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST when matrix D’s CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT is greater than 1. If CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO is set then CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE must be set to 0 as this mode doesn’t support batched alpha vector. Default value: 0. int64_t CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET Number of SMs to target for parallel execution. Optimizes heuristics for execution on a different number of SMs when user expects a concurrent stream to be using some of the device resources. Default value: 0. int32_t CUBLASLT_MATMUL_DESC_A_SCALE_POINTER Device pointer to the scale factor value that converts data in matrix A to the compute data type range. The scaling factor must have the same type as the compute type. If not specified, or set to NULL, the scaling factor is assumed to be 1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL const void* CUBLASLT_MATMUL_DESC_B_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix B. Default value: NULL const void* CUBLASLT_MATMUL_DESC_C_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix C. Default value: NULL const void* CUBLASLT_MATMUL_DESC_D_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix D. Default value: NULL const void* CUBLASLT_MATMUL_DESC_AMAX_D_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the output matrix. The computed value has the same type as the compute type. If not specified, or set to NULL, the maximum absolute value is not computed. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE The type of the data that will be stored in CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . If unset (or set to the default value of -1), the data type is set to be the output matrix element data type (DType) with some exceptions: ReLu uses a bit-mask. For FP8 kernels with an output type (DType) of CUDA_R_8F_E4M3 , the data type can be set to a non-default value if: AType and BType are CUDA_R_8F_E4M3 . Bias Type is CUDA_R_16F . CType is CUDA_R_16BF or CUDA_R_16F CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_GELU_AUX When CType is CUDA_R_16BF , the data type may be set to CUDA_R_16BF or CUDA_R_8F_E4M3 . When CType is CUDA_R_16F , the data type may be set to CUDA_R_16F . Otherwise, the data type should be left unset or set to the default value of -1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER Device pointer to the scaling factor value to convert results from compute type data range to storage data range in the auxiliary matrix that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . The scaling factor value must have the same type as the compute type. If not specified, or set to NULL, the scaling factor is assumed to be 1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the buffer that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . The computed value has the same type as the compute type. If not specified, or set to NULL, the maximum absolute value is not computed. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE. Default value: NULL void * CUBLASLT_MATMUL_DESC_FAST_ACCUM Flag for managing FP8 fast accumulation mode. When enabled, problem execution might be faster but at the cost of lower accuracy because intermediate results will not periodically be promoted to a higher precision. Default value: 0 - fast accumulation mode is disabled int8_t CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE Type of the bias or bias gradient vector in the device memory. Bias case: see CUBLASLT_EPILOGUE_BIAS . If unset (or set to the default value of -1), the bias vector elements are the same type as the elements of the output matrix (Dtype) with the following exceptions: IMMA kernels with computeType= CUDA_R_32I and Ctype=CUDA_R_8I where the bias vector elements are the same type as alpha, beta ( CUBLASLT_MATMUL_DESC_SCALE_TYPE=CUDA_R_32F ) For FP8 kernels with an output type of CUDA_R_32F , CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 . See cublasLtMatmul() for more details. Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER Pointer to a device array of input atomic counters consumed by a matmul. When a counter reaches zero, computation of the corresponding chunk of the output tensor is allowed to start. Default: NULL. See Atomics Synchronization . int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER Pointer to a device array of output atomic counters produced by a matmul. A matmul kernel sets a counter to zero when the computations of the corresponding chunk of the output tensor have completed. All the counters must be initialized to 1 before a matmul kernel is run. Default: NULL. See Atomics Synchronization . int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS Number of atomic synchronization chunks in the row dimension of the output matrix D. Each chunk corresponds to a single atomic counter. Default: 0 (atomics synchronization disabled).  See Atomics Synchronization . int32_t CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS Number of atomic synchronization chunks in the column dimension of the output matrix D. Each chunk corresponds to a single atomic counter. Default: 0 (atomics synchronization disabled).  See Atomics Synchronization . int32_t 3.3.10. cublasLtMatmulHeuristicResult_t  cublasLtMatmulHeuristicResult_t is a descriptor that holds the configured matrix multiplication algorithm descriptor and its runtime properties. Member Description cublasLtMatmulAlgo_t algo Must be initialized with cublasLtMatmulAlgoInit() if the preference CUBLASLT_MATMUL_PERF_SEARCH_MODE is set to CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID. See cublasLtMatmulSearch_t . size_t workspaceSize; Actual size of workspace memory required. cublasStatus_t state; Result status. Other fields are valid only if, after call to cublasLtMatmulAlgoGetHeuristic() , this member is set to CUBLAS_STATUS_SUCCESS. float wavesCount; Waves count is a device utilization metric. A wavesCount value of 1.0f suggests that when the kernel is launched it will fully occupy the GPU. int reserved[4]; Reserved. 3.3.11. cublasLtMatmulInnerShape_t  cublasLtMatmulInnerShape_t is an enumerated type used to configure various aspects of the internal kernel design. This does not impact the CUDA grid size. Value Description CUBLASLT_MATMUL_INNER_SHAPE_UNDEFINED Inner shape is undefined. CUBLASLT_MATMUL_INNER_SHAPE_MMA884 Inner shape is MMA884. CUBLASLT_MATMUL_INNER_SHAPE_MMA1684 Inner shape is MMA1684. CUBLASLT_MATMUL_INNER_SHAPE_MMA1688 Inner shape is MMA1688. CUBLASLT_MATMUL_INNER_SHAPE_MMA16816 Inner shape is MMA16816. 3.3.12. cublasLtMatmulPreference_t  The cublasLtMatmulPreference_t is a pointer to an opaque structure holding the description of the preferences for cublasLtMatmulAlgoGetHeuristic() configuration. Use cublasLtMatmulPreferenceCreate() to create one instance of the descriptor and cublasLtMatmulPreferenceDestroy() to destroy a previously created descriptor and release the resources. 3.3.13. cublasLtMatmulPreferenceAttributes_t  cublasLtMatmulPreferenceAttributes_t is an enumerated type used to apply algorithm search preferences while fine-tuning the heuristic function. Use cublasLtMatmulPreferenceGetAttribute() and cublasLtMatmulPreferenceSetAttribute() to get and set the attribute value of a matmul preference descriptor. Value Description Data Type CUBLASLT_MATMUL_PREF_SEARCH_MODE Search mode. See cublasLtMatmulSearch_t . Default is CUBLASLT_SEARCH_BEST_FIT. uint32_t CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES Maximum allowed workspace memory. Default is 0 (no workspace memory allowed). uint64_t CUBLASLT_MATMUL_PREF_REDUCTION_SCHEME_MASK Reduction scheme mask. See cublasLtReductionScheme_t . Only algorithm configurations specifying CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME that is not masked out by this attribute are allowed. For example, a mask value of 0x03 will allow only INPLACE and COMPUTE_TYPE reduction schemes. Default is CUBLASLT_REDUCTION_SCHEME_MASK (i.e., allows all reduction schemes). uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_A_BYTES Minimum buffer alignment for matrix A (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix A, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_B_BYTES Minimum buffer alignment for matrix B (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix B, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_C_BYTES Minimum buffer alignment for matrix C (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix C, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_D_BYTES Minimum buffer alignment for matrix D (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix D, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MAX_WAVES_COUNT Maximum wave count. See cublasLtMatmulHeuristicResult_t ::wavesCount. Selecting a non-zero value will exclude algorithms that report device utilization higher than specified. Default is 0.0f. float CUBLASLT_MATMUL_PREF_IMPL_MASK Numerical implementation details mask. See cublasLtNumericalImplFlags_t . Filters heuristic result to only include algorithms that use the allowed implementations. default: uint64_t(-1) (allow everything) uint64_t 3.3.14. cublasLtMatmulSearch_t  cublasLtMatmulSearch_t is an enumerated type that contains the attributes for heuristics search type. Value Description Data Type CUBLASLT_SEARCH_BEST_FIT Request heuristics for the best algorithm for the given use case. CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID Request heuristics only for the pre-configured algo id. 3.3.15. cublasLtMatmulTile_t  cublasLtMatmulTile_t is an enumerated type used to set the tile size in rows x columns. See also CUTLASS: Fast Linear Algebra in CUDA C++ . Value Description CUBLASLT_MATMUL_TILE_UNDEFINED Tile size is undefined. CUBLASLT_MATMUL_TILE_8x8 Tile size is 8 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x16 Tile size is 8 rows x 16 columns. CUBLASLT_MATMUL_TILE_16x8 Tile size is 16 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x32 Tile size is 8 rows x 32 columns. CUBLASLT_MATMUL_TILE_16x16 Tile size is 16 rows x 16 columns. CUBLASLT_MATMUL_TILE_32x8 Tile size is 32 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x64 Tile size is 8 rows x 64 columns. CUBLASLT_MATMUL_TILE_16x32 Tile size is 16 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x16 Tile size is 32 rows x 16 columns. CUBLASLT_MATMUL_TILE_64x8 Tile size is 64 rows x 8 columns. CUBLASLT_MATMUL_TILE_32x32 Tile size is 32 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x64 Tile size is 32 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x32 Tile size is 64 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x128 Tile size is 32 rows x 128 columns. CUBLASLT_MATMUL_TILE_64x64 Tile size is 64 rows x 64 columns. CUBLASLT_MATMUL_TILE_128x32 Tile size is 128 rows x 32 columns. CUBLASLT_MATMUL_TILE_64x128 Tile size is 64 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x64 Tile size is 128 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x256 Tile size is 64 rows x 256 columns. CUBLASLT_MATMUL_TILE_128x128 Tile size is 128 rows x 128 columns. CUBLASLT_MATMUL_TILE_256x64 Tile size is 256 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x512 Tile size is 64 rows x 512 columns. CUBLASLT_MATMUL_TILE_128x256 Tile size is 128 rows x 256 columns. CUBLASLT_MATMUL_TILE_256x128 Tile size is 256 rows x 128 columns. CUBLASLT_MATMUL_TILE_512x64 Tile size is 512 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x96 Tile size is 64 rows x 96 columns. CUBLASLT_MATMUL_TILE_96x64 Tile size is 96 rows x 64 columns. CUBLASLT_MATMUL_TILE_96x128 Tile size is 96 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x160 Tile size is 128 rows x 160 columns. CUBLASLT_MATMUL_TILE_160x128 Tile size is 160 rows x 128 columns. CUBLASLT_MATMUL_TILE_192x128 Tile size is 192 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x192 Tile size is 128 rows x 192 columns. CUBLASLT_MATMUL_TILE_128x96 Tile size is 128 rows x 96 columns. 3.3.16. cublasLtMatmulStages_t  cublasLtMatmulStages_t is an enumerated type used to configure the size and number of shared memory buffers where input elements are staged. Number of staging buffers defines kernel’s pipeline depth. Value Description CUBLASLT_MATMUL_STAGES_UNDEFINED Stage size is undefined. CUBLASLT_MATMUL_STAGES_16x1 Stage size is 16, number of stages is 1. CUBLASLT_MATMUL_STAGES_16x2 Stage size is 16, number of stages is 2. CUBLASLT_MATMUL_STAGES_16x3 Stage size is 16, number of stages is 3. CUBLASLT_MATMUL_STAGES_16x4 Stage size is 16, number of stages is 4. CUBLASLT_MATMUL_STAGES_16x5 Stage size is 16, number of stages is 5. CUBLASLT_MATMUL_STAGES_16x6 Stage size is 16, number of stages is 6. CUBLASLT_MATMUL_STAGES_32x1 Stage size is 32, number of stages is 1. CUBLASLT_MATMUL_STAGES_32x2 Stage size is 32, number of stages is 2. CUBLASLT_MATMUL_STAGES_32x3 Stage size is 32, number of stages is 3. CUBLASLT_MATMUL_STAGES_32x4 Stage size is 32, number of stages is 4. CUBLASLT_MATMUL_STAGES_32x5 Stage size is 32, number of stages is 5. CUBLASLT_MATMUL_STAGES_32x6 Stage size is 32, number of stages is 6. CUBLASLT_MATMUL_STAGES_64x1 Stage size is 64, number of stages is 1. CUBLASLT_MATMUL_STAGES_64x2 Stage size is 64, number of stages is 2. CUBLASLT_MATMUL_STAGES_64x3 Stage size is 64, number of stages is 3. CUBLASLT_MATMUL_STAGES_64x4 Stage size is 64, number of stages is 4. CUBLASLT_MATMUL_STAGES_64x5 Stage size is 64, number of stages is 5. CUBLASLT_MATMUL_STAGES_64x6 Stage size is 64, number of stages is 6. CUBLASLT_MATMUL_STAGES_128x1 Stage size is 128, number of stages is 1. CUBLASLT_MATMUL_STAGES_128x2 Stage size is 128, number of stages is 2. CUBLASLT_MATMUL_STAGES_128x3 Stage size is 128, number of stages is 3. CUBLASLT_MATMUL_STAGES_128x4 Stage size is 128, number of stages is 4. CUBLASLT_MATMUL_STAGES_128x5 Stage size is 128, number of stages is 5. CUBLASLT_MATMUL_STAGES_128x6 Stage size is 128, number of stages is 6. CUBLASLT_MATMUL_STAGES_32x10 Stage size is 32, number of stages is 10. CUBLASLT_MATMUL_STAGES_8x4 Stage size is 8, number of stages is 4. CUBLASLT_MATMUL_STAGES_16x10 Stage size is 16, number of stages is 10. CUBLASLT_MATMUL_STAGES_8x5 Stage size is 8, number of stages is 5. CUBLASLT_MATMUL_STAGES_8x3 Stage size is 8, number of stages is 3. CUBLASLT_MATMUL_STAGES_8xAUTO Stage size is 8, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_16xAUTO Stage size is 16, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_32xAUTO Stage size is 32, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_64xAUTO Stage size is 64, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_128xAUTO Stage size is 128, number of stages is selected automatically. 3.3.17. cublasLtNumericalImplFlags_t  cublasLtNumericalImplFlags_t : a set of bit-flags that can be specified to select implementation details that may affect numerical behavior of algorithms. Flags below can be combined using the bit OR operator “|”. Value Description CUBLASLT_NUMERICAL_IMPL_FLAGS_FMA Specify that the implementation is based on [H,F,D]FMA (fused multiply-add) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_HMMA Specify that the implementation is based on HMMA (tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_IMMA Specify that the implementation is based on IMMA (integer tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_DMMA Specify that the implementation is based on DMMA (double precision tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_TENSOR_OP_MASK Mask to filter implementations using any of the above kinds of tensor operations. CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_TYPE_MASK Mask to filter implementation details about multiply-accumulate instructions used. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_16F Specify that the implementation’s inner dot product is using half precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32F Specify that the implementation’s inner dot product is using single precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_64F Specify that the implementation’s inner dot product is using double precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32I Specify that the implementation’s inner dot product is using 32 bit signed integer precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_TYPE_MASK Mask to filter implementation details about accumulator used. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16F Specify that the implementation’s inner dot product multiply-accumulate instruction is using half-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16BF Specify that the implementation’s inner dot product multiply-accumulate instruction is using bfloat16 inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_TF32 Specify that the implementation’s inner dot product multiply-accumulate instruction is using TF32 inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_32F Specify that the implementation’s inner dot product multiply-accumulate instruction is using single-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_64F Specify that the implementation’s inner dot product multiply-accumulate instruction is using double-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_8I Specify that the implementation’s inner dot product multiply-accumulate instruction is using 8-bit integer inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_INPUT_TYPE_MASK Mask to filter implementation details about accumulator input used. CUBLASLT_NUMERICAL_IMPL_FLAGS_GAUSSIAN Specify that the implementation applies Gauss complexity reduction algorithm to reduce arithmetic complexity of the complex matrix multiplication problem 3.3.18. cublasLtMatrixLayout_t  The cublasLtMatrixLayout_t is a pointer to an opaque structure holding the description of a matrix layout. Use cublasLtMatrixLayoutCreate() to create one instance of the descriptor and cublasLtMatrixLayoutDestroy() to destroy a previously created descriptor and release the resources. 3.3.19. cublasLtMatrixLayoutAttribute_t  cublasLtMatrixLayoutAttribute_t is a descriptor structure containing the attributes that define the details of the matrix operation. Use cublasLtMatrixLayoutGetAttribute() and cublasLtMatrixLayoutSetAttribute() to get and set the attribute value of a matrix layout descriptor. Attribute Name Description Data Type CUBLASLT_MATRIX_LAYOUT_TYPE Specifies the data precision type. See cudaDataType_t . uint32_t CUBLASLT_MATRIX_LAYOUT_ORDER Specifies the memory order of the data of the matrix. Default value is CUBLASLT_ORDER_COL. See cublasLtOrder_t . int32_t CUBLASLT_MATRIX_LAYOUT_ROWS Describes the number of rows in the matrix. Normally only values that can be expressed as int32_t are supported. uint64_t CUBLASLT_MATRIX_LAYOUT_COLS Describes the number of columns in the matrix. Normally only values that can be expressed as int32_t are supported. uint64_t CUBLASLT_MATRIX_LAYOUT_LD The leading dimension of the matrix. For CUBLASLT_ORDER_COL this is the stride (in elements) of matrix column. See also cublasLtOrder_t . Currently only non-negative values are supported. Must be large enough so that matrix memory locations are not overlapping (e.g., greater or equal to CUBLASLT_MATRIX_LAYOUT_ROWS in case of CUBLASLT_ORDER_COL). int64_t CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT Number of matmul operations to perform in the batch. Default value is 1. See also CUBLASLT_ALGO_CAP_STRIDED_BATCH_SUPPORT in cublasLtMatmulAlgoCapAttributes_t . int32_t CUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET Stride (in elements) to the next matrix for the strided batch operation. Default value is 0. When matrix type is planar-complex (CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0), batch stride is interpreted by cublasLtMatmul() in number of real valued sub-elements. E.g. for data of type CUDA_C_16F, offset of 1024B is encoded as a stride of value 512 (since each element of the real and imaginary matrices is a 2B (16bit) floating point type). NOTE: A bug in cublasLtMatrixTransform() causes it to interpret the batch stride for a planar-complex matrix as if it was specified in number of complex elements. Therefore an offset of 1024B must be encoded as stride value 256 when calling cublasLtMatrixTransform() (each complex element is 4B with real and imaginary values 2B each). This behavior is expected to be corrected in the next major cuBLAS version. int64_t CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET Stride (in bytes) to the imaginary plane for planar-complex layout. Default value is 0, indicating that the layout is regular (real and imaginary parts of complex numbers are interleaved in memory for each element). int64_t 3.3.20. cublasLtMatrixTransformDesc_t  The cublasLtMatrixTransformDesc_t is a pointer to an opaque structure holding the description of a matrix transformation operation. Use cublasLtMatrixTransformDescCreate() to create one instance of the descriptor and cublasLtMatrixTransformDescDestroy() to destroy a previously created descriptor and release the resources. 3.3.21. cublasLtMatrixTransformDescAttributes_t  cublasLtMatrixTransformDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix transform operation. Use cublasLtMatrixTransformDescGetAttribute() and cublasLtMatrixTransformDescSetAttribute() to set the attribute value of a matrix transform descriptor. Transform Attribute Name Description Data Type CUBLASLT_MATRIX_TRANSFORM_DESC_SCALE_TYPE Scale type. Inputs are converted to the scale type for scaling and summation, and results are then converted to the output type to store in the memory. For the supported data types see cudaDataType_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_POINTER_MODE Specifies the scalars alpha and beta are passed by reference whether on the host or on the device. Default value is: CUBLASLT_POINTER_MODE_HOST (i.e., on the host). See cublasLtPointerMode_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSA Specifies the type of operation that should be performed on the matrix A. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSB Specifies the type of operation that should be performed on the matrix B. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t 3.3.22. cublasLtOrder_t  cublasLtOrder_t is an enumerated type used to indicate the data ordering of the matrix. Value Data Order Description CUBLASLT_ORDER_COL Data is ordered in column-major format. The leading dimension is the stride (in elements) to the beginning of next column in memory. CUBLASLT_ORDER_ROW Data is ordered in row-major format. The leading dimension is the stride (in elements) to the beginning of next row in memory. CUBLASLT_ORDER_COL32 Data is ordered in column-major ordered tiles of 32 columns. The leading dimension is the stride (in elements) to the beginning of next group of 32-columns. For example, if the matrix has 33 columns and 2 rows, then the leading dimension must be at least (32) * 2 = 64. CUBLASLT_ORDER_COL4_4R2_8C Data is ordered in column-major ordered tiles of composite tiles with total 32 columns and 8 rows. A tile is composed of interleaved inner tiles of 4 columns within 4 even or odd rows in an alternating pattern. The leading dimension is the stride (in elements) to the beginning of the first 32 column x 8 row tile for the next 32-wide group of columns. For example, if the matrix has 33 columns and 1 row, the leading dimension must be at least (32 * 8) * 1 = 256. CUBLASLT_ORDER_COL32_2R_4R4 Data is ordered in column-major ordered tiles of composite tiles with total 32 columns ands 32 rows. Element offset within the tile is calculated as (((row%8)/2*4+row/8)*2+row%2)*32+col. Leading dimension is the stride (in elements) to the beginning of the first 32 column x 32 row tile for the next 32-wide group of columns. E.g. if matrix has 33 columns and 1 row, ld must be at least (32*32)*1 = 1024. 3.3.23. cublasLtPointerMode_t  cublasLtPointerMode_t is an enumerated type used to set the pointer mode for the scaling factors alpha and beta . Value Description CUBLASLT_POINTER_MODE_HOST = CUBLAS_POINTER_MODE_HOST Matches CUBLAS_POINTER_MODE_HOST, and the pointer targets a single value host memory. CUBLASLT_POINTER_MODE_DEVICE = CUBLAS_POINTER_MODE_DEVICE Matches CUBLAS_POINTER_MODE_DEVICE, and the pointer targets a single value device memory. CUBLASLT_POINTER_MODE_DEVICE_VECTOR = 2 Pointers target device memory vectors of length equal to the number of rows of matrix D. CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO = 3 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is zero. CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST = 4 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is a single value in host memory. 3.3.24. cublasLtPointerModeMask_t  cublasLtPointerModeMask_t is an enumerated type used to define and query the pointer mode capability. Value Description CUBLASLT_POINTER_MODE_MASK_HOST = 1 See CUBLASLT_POINTER_MODE_HOST in cublasLtPointerMode_t . CUBLASLT_POINTER_MODE_MASK_DEVICE = 2 See CUBLASLT_POINTER_MODE_DEVICE in cublasLtPointerMode_t . CUBLASLT_POINTER_MODE_MASK_DEVICE_VECTOR = 4 See CUBLASLT_POINTER_MODE_DEVICE_VECTOR in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_ZERO = 8 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_HOST = 16 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST in cublasLtPointerMode_t 3.3.25. cublasLtReductionScheme_t  cublasLtReductionScheme_t is an enumerated type used to specify a reduction scheme for the portions of the dot-product calculated in parallel (i.e., “split - K”). Value Description CUBLASLT_REDUCTION_SCHEME_NONE Do not apply reduction. The dot-product will be performed in one sequence. CUBLASLT_REDUCTION_SCHEME_INPLACE Reduction is performed “in place” using the output buffer, parts are added up in the output data type. Workspace is only used for counters that guarantee sequentiality. CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE Reduction done out of place in a user-provided workspace. The intermediate results are stored in the compute type in the workspace and reduced in a separate step. CUBLASLT_REDUCTION_SCHEME_OUTPUT_TYPE Reduction done out of place in a user-provided workspace. The intermediate results are stored in the output type in the workspace and reduced in a separate step. CUBLASLT_REDUCTION_SCHEME_MASK Allows all reduction schemes. 3.4. cuBLASLt API Reference  3.4.1. cublasLtCreate()  cublasStatus_t cublasLtCreate ( cublasLtHandle_t * lighthandle ) This function initializes the cuBLASLt library and creates a handle to an opaque structure holding the cuBLASLt library context. It allocates light hardware resources on the host and device, and must be called prior to making any other cuBLASLt library calls. The cuBLASLt library context is tied to the current CUDA device. To use the library on multiple devices, one cuBLASLt handle should be created for each device. Parameters: Parameter Memory Input / Output Description lightHandle Output Pointer to the allocated cuBLASLt handle for the created cuBLASLt context. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The allocation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The cuBLASLt library was not initialized. This usually happens: when cublasLtCreate() is not called first an error in the CUDA Runtime API called by the cuBLASLt routine, or an error in the hardware setup. CUBLAS_STATUS_ALLOC_FAILED Resource allocation failed inside the cuBLASLt library. This is usually caused by a cudaMalloc() failure. To correct: prior to the function call, deallocate the previously allocated memory as much as possible. CUBLAS_STATUS_INVALID_VALUE lighthandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.2. cublasLtDestroy()  cublasStatus_t cublasLtDestroy ( cublasLtHandle_t lightHandle ) This function releases hardware resources used by the cuBLASLt library. This function is usually the last call with a particular handle to the cuBLASLt library. Because cublasLtCreate() allocates some internal resources and the release of those resources by calling cublasLtDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the cuBLASLt handle to be destroyed. Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The cuBLASLt context was successfully destroyed. CUBLAS_STATUS_NOT_INITIALIZED The cuBLASLt library was not initialized. CUBLAS_STATUS_INVALID_VALUE lightHandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.3. cublasLtDisableCpuInstructionsSetMask()  unsigned cublasLtDisableCpuInstructionsSetMask ( unsigned mask ); Instructs cuBLASLt library to not use CPU instructions specified by the flags in the mask .\nThe function takes precedence over the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable. Parameters: mask – the flags combined with bitwise OR(|) operator that specify which CPU instructions should not be used. Supported flags: Value Description 0x1 x86-64 AVX512 ISA. Returns: the previous value of the mask . 3.4.4. cublasLtGetCudartVersion()  size_t cublasLtGetCudartVersion ( void ); This function returns the version number of the CUDA Runtime library. Parameters: None. Returns: size_t - The version number of the CUDA Runtime library. 3.4.5. cublasLtGetProperty()  cublasStatus_t cublasLtGetProperty ( libraryPropertyType type , int * value ); This function returns the value of the requested property by writing it to the memory location pointed to by the value parameter. Parameters : Parameter Memory Input / Output Description type Input Of the type libraryPropertyType , whose value is requested from the property. See libraryPropertyType_t . value Output Pointer to the host memory location where the requested information should be written. Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The requested libraryPropertyType information is successfully written at the provided address. CUBLAS_STATUS_INVALID_VALUE If invalid value of the type input argument or value == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.6. cublasLtGetStatusName()  const char * cublasLtGetStatusName ( cublasStatus_t status ); Returns the string representation of a given status. Parameters: cublasStatus_t - the status. Returns: const char* - the NULL-terminated string. 3.4.7. cublasLtGetStatusString()  const char * cublasLtGetStatusString ( cublasStatus_t status ); Returns the description string for a given status. Parameters: cublasStatus_t - the status. Returns: const char* - the NULL-terminated string. 3.4.8. cublasLtHeuristicsCacheGetCapacity()  cublasStatus_t cublasLtHeuristicsCacheGetCapacity ( size_t * capacity ); Returns the Heuristics Cache capacity. Parameters: Parameter Description capacity The pointer to the returned capacity value. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully written. CUBLAS_STATUS_INVALID_VALUE The capacity was successfully set. 3.4.9. cublasLtHeuristicsCacheSetCapacity()  cublasStatus_t cublasLtHeuristicsCacheSetCapacity ( size_t capacity ); Sets the Heuristics Cache capacity. Set the capacity to 0 to disable the heuristics cache. This function takes precedence over CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable. Parameters: Parameter Description capacity The desirable heuristics cache capacity. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully set. 3.4.10. cublasLtGetVersion()  size_t cublasLtGetVersion ( void ); This function returns the version number of cuBLASLt library. Parameters: None. Returns: size_t - The version number of cuBLASLt library. 3.4.11. cublasLtLoggerSetCallback()  cublasStatus_t cublasLtLoggerSetCallback ( cublasLtLoggerCallback_t callback ); Experimental: This function sets the logging callback function. Parameters : Parameter Memory Input / Output Description callback Input Pointer to a callback function. See cublasLtLoggerCallback_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the callback function was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.12. cublasLtLoggerSetFile()  cublasStatus_t cublasLtLoggerSetFile ( FILE * file ); Experimental: This function sets the logging output file. Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle. Parameters : Parameter Memory Input / Output Description file Input Pointer to an open file. File should have write permission. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging file was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.13. cublasLtLoggerOpenFile()  cublasStatus_t cublasLtLoggerOpenFile ( const char * logFile ); Experimental: This function opens a logging output file in the given path. Parameters : Parameter Memory Input / Output Description logFile Input Path of the logging output file. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging file was successfully opened. See cublasStatus_t for a complete list of valid return codes. 3.4.14. cublasLtLoggerSetLevel()  cublasStatus_t cublasLtLoggerSetLevel ( int level ); Experimental: This function sets the value of the logging level. Parameters : Parameter Memory Input / Output Description level Input Value of the logging level. See cuBLASLt Logging . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If the value was not a valid logging level. See cuBLASLt Logging . CUBLAS_STATUS_SUCCESS If the logging level was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.15. cublasLtLoggerSetMask()  cublasStatus_t cublasLtLoggerSetMask ( int mask ); Experimental: This function sets the value of the logging mask. Parameters : Parameter Memory Input / Output Description mask Input Value of the logging mask. See cuBLASLt Logging . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging mask was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.16. cublasLtLoggerForceDisable()  cublasStatus_t cublasLtLoggerForceDisable (); Experimental: This function disables logging for the entire run. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging was successfully disabled. See cublasStatus_t for a complete list of valid return codes. 3.4.17. cublasLtMatmul()  cublasStatus_t cublasLtMatmul ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t computeDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * B , cublasLtMatrixLayout_t Bdesc , const void * beta , const void * C , cublasLtMatrixLayout_t Cdesc , void * D , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , void * workspace , size_t workspaceSizeInBytes , cudaStream_t stream ); This function computes the matrix multiplication of matrices A and B to produce the output matrix D, according to the following operation: D = alpha*(A*B) + beta*(C), where A , B , and C are input matrices, and alpha and beta are input scalars. Note This function supports both in-place matrix multiplication ( C == D and Cdesc == Ddesc ) and out-of-place matrix multiplication ( C != D , both matrices must have the same data type, number of rows, number of columns, batch size, and memory order). In the out-of-place case, the leading dimension of C can be different from the leading dimension of D. Specifically the leading dimension of C can be 0 to achieve row or column broadcast. If Cdesc is omitted, this function assumes it to be equal to Ddesc . The workspace pointer must be aligned to at least a multiple of 256 bytes.\nThe recommendations on workspaceSizeInBytes are the same as mentioned in the cublasSetWorkspace() section. Datatypes Supported: cublasLtMatmul() supports the following computeType, scaleType, Atype/Btype, and Ctype. Footnotes can be found at the end of this section. Table 1. When A, B, C, and D are Regular Column- or Row-major Matrices  computeType scaleType Atype/Btype Ctype Bias Type 5 CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported. CUDA_R_32F CUDA_R_8I CUDA_R_8I Non-default epilogue not supported. CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUDA_R_8I CUDA_R_32F Non-default epilogue not supported. CUDA_R_16BF CUDA_R_32F CUDA_R_32F 5 CUDA_R_16F CUDA_R_32F CUDA_R_32F 5 CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_8I 6 CUDA_C_32F 6 Non-default epilogue not supported. CUDA_C_32F 6 CUDA_C_32F 6 CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_32F 6 CUDA_C_32F 6 Non-default epilogue not supported. CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F 5 CUDA_C_64F 6 CUDA_C_64F 6 CUDA_C_64F 6 Non-default epilogue not supported. To use IMMA kernels, one of the following sets of requirements, with the first being the preferred one, must be met: Using a regular data ordering: All matrix pointers must be 4-byte aligned. For even better performance, this condition should hold with 16 instead of 4. Leading dimensions of matrices A, B, C must be multiples of 4. Only the “TN” format is supported - A must be transposed and B non-transposed. Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST . With the latter mode, the kernels support the CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE attribute. Dimensions m and k must be multiples of 4. Using the IMMA-specific data ordering on Ampere or Turing (but not Hopper) architecture - CUBLASLT_ORDER_COL32` for matrices A, C, D, and CUBLASLT_ORDER_COL4_4R2_8C (on Turing or Ampere architecture) or CUBLASLT_ORDER_COL32_2R_4R4 (on Ampere architecture) for matrix B: Leading dimensions of matrices A, B, C must fulfill conditions specific to the memory ordering (see cublasLtOrder_t ). Matmul descriptor must specify CUBLAS_OP_T on matrix B and CUBLAS_OP_N (default) on matrix A and C. If scaleType CUDA_R_32I is used, the only supported values for alpha and beta are 0 or 1 . Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE , CUBLASLT_POINTER_MODE_DEVICE_VECTOR or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO . These kernels do not support CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE . Only the “NT” format is supported - A must be transposed and B non-transposed. Table 2. When A, B, C, and D Use Layouts for IMMA  computeType scaleType Atype/Btype Ctype Bias Type CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported. CUDA_R_32F CUDA_R_8I CUDA_R_8I CUDA_R_32F To use FP8 kernels, the following set of requirements must be satisfied: All matrix dimensions must meet the optimal requirements listed in Tensor Core Usage (i.e. pointers and matrix dimension must support 16-byte alignment). A must be transposed and B non-transposed (The “TN” format). The compute type must be CUBLAS_COMPUTE_32F . The scale type must be CUDA_R_32F . See the table below when using FP8 kernels: Table 3. When A, B, C, and D Use Layouts for FP8  AType BType CType DType Bias Type CUDA_R_8F_E4M3 CUDA_R_8F_E4M3 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_8F_E5M2 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_8F_E4M3 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_8F_E5M2 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 And finally, see below table when A,B,C,D are planar-complex matrices ( CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0 , see cublasLtMatrixLayoutAttribute_t ) to make use of mixed precision tensor core acceleration. Table 4. When A, B, C, and D are Planar-Complex Matrices  computeType scaleType Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_C_32F CUDA_C_16F 6 CUDA_C_16F 6 CUDA_C_32F 6 CUDA_C_16BF 6 CUDA_C_16BF 6 CUDA_C_32F 6 NOTES: 5 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 ) ReLU, dReLu, GELU, dGELU and Bias epilogue modes (see CUBLASLT_MATMUL_DESC_EPILOGUE in cublasLtMatmulDescAttributes_t ) are not supported when D matrix memory order is defined as CUBLASLT_ORDER_ROW . For best performance when using the bias vector, specify zero beta and set pointer mode to CUBLASLT_POINTER_MODE_HOST . 6 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ) Use of CUBLAS_ORDER_ROW together with CUBLAS_OP_C (Hermitian operator) is not supported unless all of A, B, C, and D matrices use the CUBLAS_ORDER_ROW ordering. Parameters: Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . alpha, beta Device or host Input Pointers to the scalars used in the multiplication. A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc, Bdesc and Cdesc. Adesc, Bdesc and Cdesc. Input Handles to the previous created descriptors of the type cublasLtMatrixLayout_t . D Device Output Pointer to the GPU memory associated with the descriptor Ddesc. Ddesc Input Handle to the previous created descriptor of the type cublasLtMatrixLayout_t . algo Input Handle for matrix multiplication algorithm to be used. See cublasLtMatmulAlgo_t . When NULL, an implicit heuritics query with default search preferences will be performed to determine actual algorithm to use. workspace Device Pointer to the workspace buffer allocated in the GPU memory. Must be 256B aligned (i.e. lowest 8 bits of address must be 0). workspaceSizeInBytes Input Size of the workspace. stream Host Input The CUDA stream where all the GPU work will be submitted. Returns: Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized. CUBLAS_STATUS_INVALID_VALUE If the parameters are unexpectedly NULL, in conflict or in an impossible configuration. For example, when workspaceSizeInBytes is less than workspace required by the configured algo. CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device doesn’t support the configured operation. CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device. CUBLAS_STATUS_EXECUTION_FAILED If CUDA reported an execution error from the device. CUBLAS_STATUS_SUCCESS If the operation completed successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.18. cublasLtMatmulAlgoCapGetAttribute()  cublasStatus_t cublasLtMatmulAlgoCapGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoCapAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried capability attribute for an initialized cublasLtMatmulAlgo_t descriptor structure. The capability attribute value is retrieved from the enumerated type cublasLtMatmulAlgoCapAttributes_t . For example, to get list of supported Tile IDs: cublasLtMatmulTile_t tiles [ CUBLASLT_MATMUL_TILE_END ]; size_t num_tiles , size_written ; if ( cublasLtMatmulAlgoCapGetAttribute ( algo , CUBLASLT_ALGO_CAP_TILE_IDS , tiles , sizeof ( tiles ), & size_written ) == CUBLAS_STATUS_SUCCESS ) { num_tiles = size_written / sizeof ( tiles [ 0 ]);} Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. See cublasLtMatmulAlgo_t . attr Input The capability attribute whose value will be retrieved by this function. See cublasLtMatmulAlgoCapAttributes_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.19. cublasLtMatmulAlgoCheck()  cublasStatus_t cublasLtMatmulAlgoCheck ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , cublasLtMatmulHeuristicResult_t * result ); This function performs the correctness check on the matrix multiply algorithm descriptor for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D. It checks whether the descriptor is supported on the current device, and returns the result containing the required workspace and the calculated wave count. Note CUBLAS_STATUS_SUCCESS doesn’t fully guarantee that the algo will run. The algo will fail if, for example, the buffers are not correctly aligned. However, if cublasLtMatmulAlgoCheck() fails, the algo will not run. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t . algo Input Descriptor which specifies which matrix multiplication algorithm should be used. See cublasLtMatmulAlgo_t . May point to result->algo . result Output Pointer to the structure holding the results returned by this function. The results comprise of the required workspace and the calculated wave count. The algo field is never updated. See cublasLtMatmulHeuristicResult_t . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If matrix layout descriptors or the operation descriptor do not match the algo descriptor. CUBLAS_STATUS_NOT_SUPPORTED If the algo configuration or data type combination is not currently supported on the given device. CUBLAS_STATUS_ARCH_MISMATCH If the algo configuration cannot be run using the selected device. CUBLAS_STATUS_SUCCESS If the check was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.20. cublasLtMatmulAlgoConfigGetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor. The configuration attribute value is retrieved from the enumerated type cublasLtMatmulAlgoConfigAttributes_t . Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. See cublasLtMatmulAlgo_t . attr Input The configuration attribute whose value will be retrieved by this function. See cublasLtMatmulAlgoConfigAttributes_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.21. cublasLtMatmulAlgoConfigSetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigSetAttribute ( cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor. The configuration attribute is an enumerant of the type cublasLtMatmulAlgoConfigAttributes_t . Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. See cublasLtMatmulAlgo_t . attr Input The configuration attribute whose value will be set by this function. See cublasLtMatmulAlgoConfigAttributes_t . buf Input The value to which the configuration attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.22. cublasLtMatmulAlgoGetHeuristic()  cublasStatus_t cublasLtMatmulAlgoGetHeuristic ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , cublasLtMatmulPreference_t preference , int requestedAlgoCount , cublasLtMatmulHeuristicResult_t heuristicResultsArray [] int * returnAlgoCount ); This function retrieves the possible algorithms for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D. The output is placed in heuristicResultsArray[] in the order of increasing estimated compute time. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t . preference Input Pointer to the structure holding the heuristic search preferences descriptor. See cublasLtMatmulPreference_t . requestedAlgoCount Input Size of the heuristicResultsArray (in elements). This is the requested maximum number of algorithms to return. heuristicResultsArray[] Output Array containing the algorithm heuristics and associated runtime characteristics, returned by this function, in the order of increasing estimated compute time. returnAlgoCount Output Number of algorithms returned by this function. This is the number of heuristicResultsArray elements written. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero. CUBLAS_STATUS_NOT_SUPPORTED If no heuristic function available for current configuration. CUBLAS_STATUS_SUCCESS If query was successful. Inspect heuristicResultsArray[0 to (returnAlgoCount -1)].state for the status of the results. See cublasStatus_t for a complete list of valid return codes. Note This function may load some kernels using CUDA Driver API which may fail when there is no available GPU memory. Do not allocate the entire VRAM before running cublasLtMatmulAlgoGetHeuristic() . 3.4.23. cublasLtMatmulAlgoGetIds()  cublasStatus_t cublasLtMatmulAlgoGetIds ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int requestedAlgoCount , int algoIdsArray [], int * returnAlgoCount ); This function retrieves the IDs of all the matrix multiply algorithms that are valid, and can potentially be run by the cublasLtMatmul() function, for given types of the input matrices A, B and C, and of the output matrix D. Note The IDs are returned in no particular order. To make sure the best possible algo is contained in the list, make requestedAlgoCount large enough to receive the full list. The list is guaranteed to be full if returnAlgoCount < requestedAlgoCount . Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeType, scaleType, Atype, Btype, Ctype, and Dtype Inputs Data types of the computation type, scaling factors and of the operand matrices. See cudaDataType_t . requestedAlgoCount Input Number of algorithms requested. Must be > 0. algoIdsArray[] Output Array containing the algorithm IDs returned by this function. returnAlgoCount Output Number of algorithms actually returned by this function. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero. CUBLAS_STATUS_SUCCESS If query was successful. Inspect returnAlgoCount to get actual number of IDs available. See cublasStatus_t for a complete list of valid return codes. 3.4.24. cublasLtMatmulAlgoInit()  cublasStatus_t cublasLtMatmulAlgoInit ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int algoId , cublasLtMatmulAlgo_t * algo ); This function initializes the matrix multiply algorithm structure for the cublasLtMatmul() , for a specified matrix multiply algorithm and input matrices A, B and C, and the output matrix D. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeType Input Compute type. See CUBLASLT_MATMUL_DESC_COMPUTE_TYPE of cublasLtMatmulDescAttributes_t . scaleType Input Scale type. See CUBLASLT_MATMUL_DESC_SCALE_TYPE of cublasLtMatmulDescAttributes_t . Usually same as computeType. Atype, Btype, Ctype, and Dtype Input Datatype precision for the input and output matrices. See cudaDataType_t . algoId Input Specifies the algorithm being initialized. Should be a valid algoId returned by the cublasLtMatmulAlgoGetIds() function. algo Input Pointer to the opaque structure to be initialized. See cublasLtMatmulAlgo_t . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If algo is NULL or algoId is outside the recognized range. CUBLAS_STATUS_NOT_SUPPORTED If algoId is not supported for given combination of data types. CUBLAS_STATUS_SUCCESS If the structure was successfully initialized. See cublasStatus_t for a complete list of valid return codes. 3.4.25. cublasLtMatmulDescCreate()  cublasStatus_t cublasLtMatmulDescCreate ( cublasLtMatmulDesc_t * matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function creates a matrix multiply descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor created by this function. See cublasLtMatmulDesc_t . computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function creates. See cublasComputeType_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.26. cublasLtMatmulDescInit()  cublasStatus_t cublasLtMatmulDescInit ( cublasLtMatmulDesc_t matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function initializes a matrix multiply descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor initialized by this function. See cublasLtMatmulDesc_t . computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function initializes. See cublasComputeType_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.27. cublasLtMatmulDescDestroy()  cublasStatus_t cublasLtMatmulDescDestroy ( cublasLtMatmulDesc_t matmulDesc ); This function destroys a previously created matrix multiply descriptor object. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the structure holding the matrix multiply descriptor that should be destroyed by this function. See cublasLtMatmulDesc_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.28. cublasLtMatmulDescGetAttribute()  cublasStatus_t cublasLtMatmulDescGetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply descriptor. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function. See cublasLtMatmulDesc_t . attr Input The attribute that will be retrieved by this function. See cublasLtMatmulDescAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.29. cublasLtMatmulDescSetAttribute()  cublasStatus_t cublasLtMatmulDescSetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply descriptor. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function. See cublasLtMatmulDesc_t . attr Input The attribute that will be set by this function. See cublasLtMatmulDescAttributes_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.30. cublasLtMatmulPreferenceCreate()  cublasStatus_t cublasLtMatmulPreferenceCreate ( cublasLtMatmulPreference_t * pref ); This function creates a matrix multiply heuristic search preferences descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences descriptor created by this function. See cublasLtMatrixLayout_t . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.31. cublasLtMatmulPreferenceInit()  cublasStatus_t cublasLtMatmulPreferenceInit ( cublasLtMatmulPreference_t pref ); This function initializes a matrix multiply heuristic search preferences descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences descriptor created by this function. See cublasLtMatrixLayout_t . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.32. cublasLtMatmulPreferenceDestroy()  cublasStatus_t cublasLtMatmulPreferenceDestroy ( cublasLtMatmulPreference_t pref ); This function destroys a previously created matrix multiply preferences descriptor object. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the structure holding the matrix multiply preferences descriptor that should be destroyed by this function. See cublasLtMatmulPreference_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.33. cublasLtMatmulPreferenceGetAttribute()  cublasStatus_t cublasLtMatmulPreferenceGetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply heuristic search preferences descriptor. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply heuristic search preferences descriptor queried by this function. See cublasLtMatmulPreference_t . attr Input The attribute that will be queried by this function. See cublasLtMatmulPreferenceAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.34. cublasLtMatmulPreferenceSetAttribute()  cublasStatus_t cublasLtMatmulPreferenceSetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply preferences descriptor. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply preferences descriptor queried by this function. See cublasLtMatmulPreference_t . attr Input The attribute that will be set by this function. See cublasLtMatmulPreferenceAttributes_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.35. cublasLtMatrixLayoutCreate()  cublasStatus_t cublasLtMatrixLayoutCreate ( cublasLtMatrixLayout_t * matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function creates a matrix layout descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor created by this function. See cublasLtMatrixLayout_t . type Input Enumerant that specifies the data precision for the matrix layout descriptor this function creates. See cudaDataType . rows, cols Input Number of rows and columns of the matrix. ld Input The leading dimension of the matrix. In column major layout, this is the number of elements to jump to reach the next column. Thus ld >= m (number of rows). Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If the memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.36. cublasLtMatrixLayoutInit()  cublasStatus_t cublasLtMatrixLayoutInit ( cublasLtMatrixLayout_t matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function initializes a matrix layout descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor initialized by this function. See cublasLtMatrixLayout_t . type Input Enumerant that specifies the data precision for the matrix layout descriptor this function initializes. See cudaDataType . rows, cols Input Number of rows and columns of the matrix. ld Input The leading dimension of the matrix. In column major layout, this is the number of elements to jump to reach the next column. Thus ld >= m (number of rows). Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If the memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.37. cublasLtMatrixLayoutDestroy()  cublasStatus_t cublasLtMatrixLayoutDestroy ( cublasLtMatrixLayout_t matLayout ); This function destroys a previously created matrix layout descriptor object. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the structure holding the matrix layout descriptor that should be destroyed by this function. See cublasLtMatrixLayout_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.38. cublasLtMatrixLayoutGetAttribute()  cublasStatus_t cublasLtMatrixLayoutGetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to the specified matrix layout descriptor. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function. See cublasLtMatrixLayout_t . attr Input The attribute being queried for. See cublasLtMatrixLayoutAttribute_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.39. cublasLtMatrixLayoutSetAttribute()  cublasStatus_t cublasLtMatrixLayoutSetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix layout descriptor. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function. See cublasLtMatrixLayout_t . attr Input The attribute that will be set by this function. See cublasLtMatrixLayoutAttribute_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf , the attribute buffer. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match size of internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.40. cublasLtMatrixTransform()  cublasStatus_t cublasLtMatrixTransform ( cublasLtHandle_t lightHandle , cublasLtMatrixTransformDesc_t transformDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * beta , const void * B , cublasLtMatrixLayout_t Bdesc , void * C , cublasLtMatrixLayout_t Cdesc , cudaStream_t stream ); This function computes the matrix transformation operation on the input matrices A and B, to produce the output matrix C, according to the below operation: C = alpha*transformation(A) + beta*transformation(B), where A , B are input matrices, and alpha and beta are input scalars. The transformation operation is defined by the transformDesc pointer. This function can be used to change the memory order of data or to scale and shift the values. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . transformDesc Input Pointer to the opaque descriptor holding the matrix transformation operation. See cublasLtMatrixTransformDesc_t . alpha, beta Device or host Input Pointers to the scalars used in the multiplication. A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc , Bdesc and Cdesc . Adesc, Bdesc and Cdesc. Input Handles to the previous created descriptors of the type cublasLtMatrixLayout_t . Adesc or Bdesc can be NULL if corresponding pointer is NULL and corresponding scalar is zero. stream Host Input The CUDA stream where all the GPU work will be submitted. Returns : Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized. CUBLAS_STATUS_INVALID_VALUE If the parameters are in conflict or in an impossible configuration. For example, when A is not NULL, but Adesc is NULL. CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device does not support the configured operation. CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device. CUBLAS_STATUS_EXECUTION_FAILED If CUDA reported an execution error from the device. CUBLAS_STATUS_SUCCESS If the operation completed successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.41. cublasLtMatrixTransformDescCreate()  cublasStatus_t cublasLtMatrixTransformDescCreate ( cublasLtMatrixTransformDesc_t * transformDesc , cudaDataType scaleType ); This function creates a matrix transform descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor created by this function. See cublasLtMatrixTransformDesc_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.42. cublasLtMatrixTransformDescInit()  cublasStatus_t cublasLtMatrixTransformDescInit ( cublasLtMatrixTransformDesc_t transformDesc , cudaDataType scaleType ); This function initializes a matrix transform descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor initialized by this function. See cublasLtMatrixTransformDesc_t . scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. See cublasStatus_t for a complete list of valid return codes. 3.4.43. cublasLtMatrixTransformDescDestroy()  cublasStatus_t cublasLtMatrixTransformDescDestroy ( cublasLtMatrixTransformDesc_t transformDesc ); This function destroys a previously created matrix transform descriptor object. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the structure holding the matrix transform descriptor that should be destroyed by this function. See cublasLtMatrixTransformDesc_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. See cublasStatus_t for a complete list of valid return codes. 3.4.44. cublasLtMatrixTransformDescGetAttribute()  cublasStatus_t cublasLtMatrixTransformDescGetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix transform descriptor. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function. See cublasLtMatrixTransformDesc_t . attr Input The attribute that will be retrieved by this function. See cublasLtMatrixTransformDescAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. See cublasStatus_t for a complete list of valid return codes. 3.4.45. cublasLtMatrixTransformDescSetAttribute()  cublasStatus_t cublasLtMatrixTransformDescSetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix transform descriptor. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function. See cublasLtMatrixTransformDesc_t . attr Input The attribute that will be set by this function. See cublasLtMatrixTransformDescAttributes_t . buf Input The value to which the specified attribute should be set. sizeInBytes Input Size of buf buffer (in bytes) for verification. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes does not match size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully. See cublasStatus_t for a complete list of valid return codes. 4. Using the cuBLASXt API  4.1. General description  The cuBLASXt API of cuBLAS exposes a multi-GPU capable host interface: when using this API the application only needs to allocate the required matrices on the host memory space. Additionally, the current implementation supports managed memory on Linux with GPU devices that have compute capability 6.x or greater but treats it as host memory. Managed memory is not supported on Windows. There are no restriction on the sizes of the matrices as long as they can fit into the host memory. The cuBLASXt API takes care of allocating the memory across the designated GPUs and dispatched the workload between them and finally retrieves the results back to the host. The cuBLASXt API supports only the compute-intensive BLAS3 routines (e.g matrix-matrix operations) where the PCI transfers back and forth from the GPU can be amortized. The cuBLASXt API has its own header file cublasXt.h . Starting with release 8.0, cuBLASXt API allows any of the matrices to be located on a GPU device. Note : The cuBLASXt API is only supported on 64-bit platforms. 4.1.1. Tiling design approach  To be able to share the workload between multiples GPUs, the cuBLASXt API uses a tiling strategy : every matrix is divided in square tiles of user-controllable dimension BlockDim x BlockDim. The resulting matrix tiling defines the static scheduling policy : each resulting tile is affected to a GPU in a round robin fashion One CPU thread is created per GPU and is responsible to do the proper memory transfers and cuBLAS operations to compute all the tiles that it is responsible for. From a performance point of view, due to this static scheduling strategy, it is better that compute capabilites and PCI bandwidth are the same for every GPU. The figure below illustrates the tiles distribution between 3 GPUs. To compute the first tile G0 from C, the CPU thread 0 responsible of GPU0, have to load 3 tiles from the first row of A and tiles from the first columun of B in a pipeline fashion in order to overlap memory transfer and computations and sum the results into the first tile G0 of C before to move on to the next tile G0. Example of cublasXt<t>gemm tiling for 3 Gpus  When the tile dimension is not an exact multiple of the dimensions of C, some tiles are partially filled on the right border or/and the bottom border. The current implementation does not pad the incomplete tiles but simply keep track of those incomplete tiles by doing the right reduced cuBLAS opearations : this way, no extra computation is done. However it still can lead to some load unbalance when all GPUS do not have the same number of incomplete tiles to work on. When one or more matrices are located on some GPU devices, the same tiling approach and workload sharing is applied. The memory transfers are in this case done between devices. However, when the computation of a tile and some data are located on the same GPU device, the memory transfer to/from the local data into tiles is bypassed and the GPU operates directly on the local data. This can lead to a significant performance increase, especially when only one GPU is used for the computation. The matrices can be located on any GPU device, and do not have to be located on the same GPU device. Furthermore, the matrices can even be located on a GPU device that do not participate to the computation. On the contrary of the cuBLAS API, even if all matrices are located on the same device, the cuBLASXt API is still a blocking API from the host point of view : the data results wherever located will be valid on the call return and no device synchronization is required. 4.1.2. Hybrid CPU-GPU computation  In the case of very large problems, the cuBLASXt API offers the possibility to offload some of the computation to the host CPU. This feature can be setup with the routines cublasXtSetCpuRoutine() and cublasXtSetCpuRatio() The workload affected to the CPU is put aside : it is simply a percentage of the resulting matrix taken from the bottom and the right side whichever dimension is bigger. The GPU tiling is done after that on the reduced resulting matrix. If any of the matrices is located on a GPU device, the feature is ignored and all computation will be done only on the GPUs This feature should be used with caution because it could interfere with the CPU threads responsible of feeding the GPUs. Currently, only the routine cublasXt<t>gemm supports this feature. 4.1.3. Results reproducibility  Currently all cuBLASXt API routines from a given toolkit version, generate the same bit-wise results when the following conditions are respected : all GPUs particating to the computation have the same compute capabilities and the same number of SMs. the tiles size is kept the same between run. either the CPU hybrid computation is not used or the CPU Blas provided is also guaranteed to produce reproducible results. 4.2. cuBLASXt API Datatypes Reference  4.2.1. cublasXtHandle_t  The cublasXtHandle_t type is a pointer type to an opaque structure holding the cuBLASXt API context. The cuBLASXt API context must be initialized using cublasXtCreate() and the returned handle must be passed to all subsequent cuBLASXt API function calls. The context should be destroyed at the end using cublasXtDestroy() . 4.2.2. cublasXtOpType_t  The cublasOpType_t enumerates the four possible types supported by BLAS routines. This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration. Value Meaning CUBLASXT_FLOAT float or single precision type CUBLASXT_DOUBLE double precision type CUBLASXT_COMPLEX single precision complex CUBLASXT_DOUBLECOMPLEX double precision complex 4.2.3. cublasXtBlasOp_t  The cublasXtBlasOp_t type enumerates the BLAS3 or BLAS-like routine supported by cuBLASXt API. This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration. Value Meaning CUBLASXT_GEMM GEMM routine CUBLASXT_SYRK SYRK routine CUBLASXT_HERK HERK routine CUBLASXT_SYMM SYMM routine CUBLASXT_HEMM HEMM routine CUBLASXT_TRSM TRSM routine CUBLASXT_SYR2K SYR2K routine CUBLASXT_HER2K HER2K routine CUBLASXT_SPMM SPMM routine CUBLASXT_SYRKX SYRKX routine CUBLASXT_HERKX HERKX routine 4.2.4. cublasXtPinningMemMode_t  The type is used to enable or disable the Pinning Memory mode through the routine cubasMgSetPinningMemMode Value Meaning CUBLASXT_PINNING_DISABLED the Pinning Memory mode is disabled CUBLASXT_PINNING_ENABLED the Pinning Memory mode is enabled 4.3. cuBLASXt API Helper Function Reference  4.3.1. cublasXtCreate()  cublasStatus_t cublasXtCreate ( cublasXtHandle_t * handle ) This function initializes the cuBLASXt API and creates a handle to an opaque structure holding the cuBLASXt API context. It allocates hardware resources on the host and device and must be called prior to making any other cuBLASXt API calls. Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_NOT_SUPPORTED cuBLASXt API is only supported on 64-bit platform 4.3.2. cublasXtDestroy()  cublasStatus_t cublasXtDestroy ( cublasXtHandle_t handle ) This function releases hardware resources used by the cuBLASXt API context. The release of GPU resources may be deferred until the application exits. This function is usually the last call with a particular handle to the cuBLASXt API. Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 4.3.3. cublasXtDeviceSelect()  cublasXtDeviceSelect ( cublasXtHandle_t handle , int nbDevices , int deviceId []) This function allows the user to provide the number of GPU devices and their respective Ids that will participate to the subsequent cuBLASXt API Math function calls. This function will create a cuBLAS context for every GPU provided in that list. Currently the device configuration is static and cannot be changed between Math function calls. In that regard, this function should be called only once after cublasXtCreate . To be able to run multiple configurations, multiple cuBLASXt API contexts should be created. Return Value Meaning CUBLAS_STATUS_SUCCESS User call was sucessful CUBLAS_STATUS_INVALID_VALUE Access to at least one of the device could not be done or a cuBLAS context could not be created on at least one of the device CUBLAS_STATUS_ALLOC_FAILED Some resources could not be allocated. 4.3.4. cublasXtSetBlockDim()  cublasXtSetBlockDim ( cublasXtHandle_t handle , int blockDim ) This function allows the user to set the block dimension used for the tiling of the matrices for the subsequent Math function calls. Matrices are split in square tiles of blockDim x blockDim dimension. This function can be called anytime and will take effect for the following Math function calls. The block dimension should be chosen in a way to optimize the math operation and to make sure that the PCI transfers are well overlapped with the computation. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blockDim <= 0 4.3.5. cublasXtGetBlockDim()  cublasXtGetBlockDim ( cublasXtHandle_t handle , int * blockDim ) This function allows the user to query the block dimension used for the tiling of the matrices. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful 4.3.6. cublasXtSetCpuRoutine()  cublasXtSetCpuRoutine ( cublasXtHandle_t handle , cublasXtBlasOp_t blasOp , cublasXtOpType_t type , void * blasFunctor ) This function allows the user to provide a CPU implementation of the corresponding BLAS routine. This function can be used with the function cublasXtSetCpuRatio() to define an hybrid computation between the CPU and the GPUs. Currently the hybrid feature is only supported for the xGEMM routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blasOp or type define an invalid combination CUBLAS_STATUS_NOT_SUPPORTED CPU-GPU Hybridization for that routine is not supported 4.3.7. cublasXtSetCpuRatio()  cublasXtSetCpuRatio ( cublasXtHandle_t handle , cublasXtBlasOp_t blasOp , cublasXtOpType_t type , float ratio ) This function allows the user to define the percentage of workload that should be done on a CPU in the context of an hybrid computation. This function can be used with the function cublasXtSetCpuRoutine() to define an hybrid computation between the CPU and the GPUs. Currently the hybrid feature is only supported for the xGEMM routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blasOp or type define an invalid combination CUBLAS_STATUS_NOT_SUPPORTED CPU-GPU Hybridization for that routine is not supported 4.3.8. cublasXtSetPinningMemMode()  cublasXtSetPinningMemMode ( cublasXtHandle_t handle , cublasXtPinningMemMode_t mode ) This function allows the user to enable or disable the Pinning Memory mode. When enabled, the matrices passed in subsequent cuBLASXt API calls will be pinned/unpinned using the CUDART routine cudaHostRegister() and cudaHostUnregister() respectively if the matrices are not already pinned. If a matrix happened to be pinned partially, it will also not be pinned. Pinning the memory improve PCI transfer performace and allows to overlap PCI memory transfer with computation. However pinning/unpinning the memory take some time which might not be amortized. It is advised that the user pins the memory on its own using cudaMallocHost() or cudaHostRegister() and unpin it when the computation sequence is completed. By default, the Pinning Memory mode is disabled. Note The Pinning Memory mode should not enabled when matrices used for different calls to cuBLASXt API overlap. cuBLASXt determines that a matrix is pinned or not if the first address of that matrix is pinned using cudaHostGetFlags() , thus cannot know if the matrix is already partially pinned or not. This is especially true in multi-threaded application where memory could be partially or totally pinned or unpinned while another thread is accessing that memory. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE the mode value is different from CUBLASXT_PINNING_DISABLED and CUBLASXT_PINNING_ENABLED 4.3.9. cublasXtGetPinningMemMode()  cublasXtGetPinningMemMode ( cublasXtHandle_t handle , cublasXtPinningMemMode_t * mode ) This function allows the user to query the Pinning Memory mode. By default, the Pinning Memory mode is disabled. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful 4.4. cuBLASXt API Math Functions Reference  In this chapter we describe the actual Linear Agebra routines that cuBLASXt API supports. We will use abbreviations < type > for type and < t > for the corresponding short type to make a more concise and clear presentation of the implemented functions. Unless otherwise specified < type > and < t > have the following meanings: <type> <t> Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision The abbreviation \\(\\mathbf{Re}(\\cdot)\\) and \\(\\mathbf{Im}(\\cdot)\\) will stand for the real and imaginary part of a number, respectively. Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. Also, the \\(\\bar{\\alpha}\\) will denote the complex conjugate of \\(\\alpha\\) . In general throughout the documentation, the lower case Greek symbols \\(\\alpha\\) and \\(\\beta\\) will denote scalars, lower case English letters in bold type \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) will denote vectors and capital English letters \\(A\\) , \\(B\\) and \\(C\\) will denote matrices. 4.4.1. cublasXt<t>gemm()  cublasStatus_t cublasXtSgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , size_t m , size_t n , size_t k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasXtDgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasXtCgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs the matrix-matrix multiplication \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) . Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. transa input operation op( A ) that is non- or (conj.) transpose. transb input operation op( B ) that is non- or (conj.) transpose. m input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication. If beta==0 , C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sgemm , dgemm , cgemm , zgemm 4.4.2. cublasXt<t>hemm()  cublasStatus_t cublasXtChemm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZhemm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the Hermitian matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a Hermitian matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. The imaginary parts of the diagonal elements are assumed to be zero. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: chemm , zhemm 4.4.3. cublasXt<t>symm()  cublasStatus_t cublasXtSsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the symmetric matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a symmetric matrix stored in lower or upper mode, \\(A\\) and \\(A\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. m input number of rows of matrix A and B , with matrix A sized accordingly. n input number of columns of matrix C and A , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymm , dsymm , csymm , zsymm 4.4.4. cublasXt<t>syrk()  cublasStatus_t cublasXtSsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * beta , float * C , int ldc ) cublasStatus_t cublasXtDsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * beta , double * C , int ldc ) cublasStatus_t cublasXtCsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A. beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk 4.4.5. cublasXt<t>syr2k()  cublasStatus_t cublasXtSsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the symmetric rank- \\(2k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\text{op}(B)\\text{op}(A)^{T}) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyr2k , dsyr2k , csyr2k , zsyr2k 4.4.6. cublasXt<t>syrkx()  cublasStatus_t cublasXtSsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs a variation of the symmetric rank- \\(k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric. An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C host or device in/out <type> array of dimensions ldc x n with ldc>=max(1,n) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssyrk , dsyrk , csyrk , zsyrk and ssyr2k , dsyr2k , csyr2k , zsyr2k 4.4.7. cublasXt<t>herk()  cublasStatus_t cublasXtCherk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZherk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) . Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk 4.4.8. cublasXt<t>her2k()  cublasStatus_t cublasXtCher2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const float * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZher2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const double * beta , cuDoubleComplex * C , size_t ldc ) This function performs the Hermitian rank- \\(2k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\overset{ˉ}{\\alpha}\\text{op}(B)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cher2k , zher2k 4.4.9. cublasXt<t>herkx()  cublasStatus_t cublasXtCherkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const float * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZherkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const double * beta , cuDoubleComplex * C , size_t ldc ) This function performs a variation of the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively. Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix}\n{A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\n{A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian. An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasXt<t>dgmm . Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. trans input operation op( A ) that is non- or (conj.) transpose. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). alpha host input <type> scalar used for multiplication. A host or device input <type> array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . beta host input real scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n , with ldc>=max(1,n) . The imaginary parts of the diagonal elements are assumed and set to zero. ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,k<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: cherk , zherk and cher2k , zher2k 4.4.10. cublasXt<t>trsm()  cublasStatus_t cublasXtStrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const float * alpha , const float * A , size_t lda , float * B , size_t ldb ) cublasStatus_t cublasXtDtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const double * alpha , const double * A , size_t lda , double * B , size_t ldb ) cublasStatus_t cublasXtCtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , cuComplex * B , size_t ldb ) cublasStatus_t cublasXtZtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , cuDoubleComplex * B , size_t ldb ) This function solves the triangular linear system with multiple right-hand-sides \\(\\left\\{ \\begin{matrix}\n{\\text{op}(A)X = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{X\\text{op}(A) = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\) and \\(B\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) The solution \\(X\\) overwrites the right-hand-sides \\(B\\) on exit. No test for singularity or near-singularity is included in this function. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of X . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A is sized accordingly. alpha host input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device in/out <type> array. It has dimensions ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strsm , dtrsm , ctrsm , ztrsm 4.4.11. cublasXt<t>trmm()  cublasStatus_t cublasXtStrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , float * C , size_t ldc ) cublasStatus_t cublasXtDtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , double * C , size_t ldc ) cublasStatus_t cublasXtCtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , cuDoubleComplex * C , size_t ldc ) This function performs the triangular matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha\\text{op}(A)B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha B\\text{op}(A)} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrix, and \\(\\alpha\\) is a scalar. Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix}\nA & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\\nA^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\\nA^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\\n\\end{matrix} \\right.\\) Notice that in order to achieve better parallelism, similarly to the cublas API, cuBLASXt API differs from the BLAS API for this routine. The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLASXt API assumes an out-of-place implementation (with results written into C). The application can still obtain the in-place functionality of BLAS in the cuBLASXt API by passing the address of the matrix B in place of the matrix C. No other overlapping in the input parameters is supported. Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or (conj.) transpose. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. A host or device input <type> array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A . B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . C host or device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: strmm , dtrmm , ctrmm , ztrmm 4.4.12. cublasXt<t>spmm()  cublasStatus_t cublasXtSspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const float * alpha , const float * AP , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ); cublasStatus_t cublasXtDspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const double * alpha , const double * AP , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ); cublasStatus_t cublasXtCspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ); cublasStatus_t cublasXtZspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ); This function performs the symmetric packed matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix}\n{\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\\n{\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\\n\\end{matrix} \\right.\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) and \\(\\beta\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) . Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage. Note The packed matrix AP must be located on the host or managed memory whereas the other matrices can be located on the host or any GPU device Param. Memory In/out Meaning handle input handle to the cuBLASXt API context. side input indicates if matrix A is on the left or right of B . uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. m input number of rows of matrix A and B , with matrix A sized accordingly. n input number of columns of matrix C and A , with matrix A sized accordingly. alpha host input <type> scalar used for multiplication. AP host input <type> array with \\(A\\) stored in packed format. B host or device input <type> array of dimension ldb x n with ldb>=max(1,m) . ldb input leading dimension of two-dimensional array used to store matrix B . beta host input <type> scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C host or device in/out <type> array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n<0 CUBLAS_STATUS_NOT_SUPPORTED the matrix AP is located on a GPU device CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: ssymm , dsymm , csymm , zsymm 5. Using the cuBLASDx API  The cuBLASDx library (preview) is a device side API extension for performing BLAS calculations inside CUDA kernels.\nBy fusing numerical operations you can decrease latency and further improve performance of your applications. You can access cuBLASDx documentation here . cuBLASDx is not a part of the CUDA Toolkit. You can download cuBLASDx separately from here . 6. Using the cuBLAS Legacy API  This section does not provide a full reference of each Legacy API datatype and entry point. Instead, it describes how to use the API, especially where this is different from the regular cuBLAS API. Note that in this section, all references to the “cuBLAS Library” refer to the Legacy cuBLAS API only. Warning The legacy cuBLAS API is deprecated and will be removed in future release. 6.1. Error Status  The cublasStatus type is used for function status returns. The cuBLAS Library helper functions return status directly, while the status of core functions can be retrieved using cublasGetError() . Notice that reading the error status via cublasGetError() , resets the internal error state to CUBLAS_STATUS_SUCCESS . Currently, the following values for are defined: Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ALLOC_FAILED the resource allocation failed CUBLAS_STATUS_INVALID_VALUE an invalid numerical value was used as an argument CUBLAS_STATUS_ARCH_MISMATCH an absent device architectural feature is required CUBLAS_STATUS_MAPPING_ERROR an access to GPU memory space failed CUBLAS_STATUS_EXECUTION_FAILED the GPU program failed to execute CUBLAS_STATUS_INTERNAL_ERROR an internal operation failed CUBLAS_STATUS_NOT_SUPPORTED the feature required is not supported This legacy type corresponds to type cublasStatus_t in the cuBLAS library API. 6.2. Initialization and Shutdown  The functions cublasInit() and cublasShutdown() are used to initialize and shutdown the cuBLAS library. It is recommended for cublasInit() to be called before any other function is invoked. It allocates hardware resources on the GPU device that is currently bound to the host thread from which it was invoked. The legacy initialization and shutdown functions are similar to the cuBLAS library API routines cublasCreate() and cublasDestroy() . 6.3. Thread Safety  The legacy API is not thread safe when used with multiple host threads and devices. It is recommended to be used only when utmost compatibility with Fortran is required and when a single host thread is used to setup the library and make all the functions calls. 6.4. Memory Management  The memory used by the legacy cuBLAS library API is allocated and released using functions cublasAlloc() and cublasFree() , respectively. These functions create and destroy an object in the GPU memory space capable of holding an array of n elements, where each element requires elemSize bytes of storage. Please see the legacy cuBLAS API header file “cublas.h” for the prototypes of these functions. The function cublasAlloc() is a wrapper around the function cudaMalloc() , therefore device pointers returned by cublasAlloc() can be passed to any CUDA™ device kernel functions. However, these device pointers can not be dereferenced in the host code. The function cublasFree() is a wrapper around the function cudaFree() . 6.5. Scalar Parameters  In the legacy cuBLAS API, scalar parameters are passed by value from the host. Also, the few functions that do return a scalar result, such as dot() and nrm2(), return the resulting value on the host, and hence these routines will wait for kernel execution on the device to complete before returning, which makes parallelism with streams impractical. However, the majority of functions do not return any value, in order to be more compatible with Fortran and the existing BLAS libraries. 6.6. Helper Functions  In this section we list the helper functions provided by the legacy cuBLAS API and their functionality. For the exact prototypes of these functions please refer to the legacy cuBLAS API header file “cublas.h”. Helper function Meaning cublasInit() initialize the library cublasShutdown() shuts down the library cublasGetError() retrieves the error status of the library cublasSetKernelStream() sets the stream to be used by the library cublasAlloc() allocates the device memory for the library cublasFree() releases the device memory allocated for the library cublasSetVector() copies a vector x on the host to a vector on the GPU cublasGetVector() copies a vector x on the GPU to a vector on the host cublasSetMatrix() copies a \\(m \\times n\\) tile from a matrix on the host to the GPU cublasGetMatrix() copies a \\(m \\times n\\) tile from a matrix on the GPU to the host cublasSetVectorAsync() similar to cublasSetVector() , but the copy is asynchronous cublasGetVectorAsync() similar to cublasGetVector() , but the copy is asynchronous cublasSetMatrixAsync() similar to cublasSetMatrix() , but the copy is asynchronous cublasGetMatrixAsync() similar to cublasGetMatrix() , but the copy is asynchronous 6.7. Level-1,2,3 Functions  The Level-1,2,3 cuBLAS functions (also called core functions) have the same name and behavior as the ones listed in the chapters 3, 4 and 5 in this document. Please refer to the legacy cuBLAS API header file “cublas.h” for their exact prototype. Also, the next section talks a bit more about the differences between the legacy and the cuBLAS API prototypes, more specifically how to convert the function calls from one API to another. 6.8. Converting Legacy to the cuBLAS API  There are a few general rules that can be used to convert from legacy to the cuBLAS API: Exchange the header file “cublas.h” for “cublas_v2.h”. Exchange the type cublasStatus for cublasStatus_t . Exchange the function cublasSetKernelStream() for cublasSetStream() . Exchange the function cublasAlloc() and cublasFree() for cudaMalloc() and cudaFree() , respectively. Notice that cudaMalloc() expects the size of the allocated memory to be provided in bytes (usually simply provide n x elemSize to allocate n elements, each of size elemSize bytes). Declare the cublasHandle_t cuBLAS library handle. Initialize the handle using cublasCreate() . Also, release the handle once finished using cublasDestroy() . Add the handle as the first parameter to all the cuBLAS library function calls. Change the scalar parameters to be passed by reference, instead of by value (usually simply adding “&” symbol in C/C++ is enough, because the parameters are passed by reference on the host by default ). However, note that if the routine is running asynchronously, then the variable holding the scalar parameter cannot be changed until the kernels that the routine dispatches are completed. See the CUDA C++ Programming Guide for a detailed discussion of how to use streams. Change the parameter characters N or n (non-transpose operation), T or t (transpose operation) and C or c (conjugate transpose operation) to CUBLAS_OP_N , CUBLAS_OP_T and CUBLAS_OP_C , respectively. Change the parameter characters L or l (lower part filled) and U or u (upper part filled) to CUBLAS_FILL_MODE_LOWER and CUBLAS_FILL_MODE_UPPER , respectively. Change the parameter characters N or n (non-unit diagonal) and U or u (unit diagonal) to CUBLAS_DIAG_NON_UNIT and CUBLAS_DIAG_UNIT , respectively. Change the parameter characters L or l (left side) and R or r (right side) to CUBLAS_SIDE_LEFT and CUBLAS_SIDE_RIGHT , respectively. If the legacy API function returns a scalar value, add an extra scalar parameter of the same type passed by reference, as the last parameter to the same function. Instead of using cublasGetError() , use the return value of the function itself to check for errors. Finally, please use the function prototypes in the header files cublas.h and cublas_v2.h to check the code for correctness. 6.9. Examples  For sample code references that use the legacy cuBLAS API please see the two examples below. They show an application written in C using the legacy cuBLAS library API with two indexing styles (Example A.1. “Application Using C and cuBLAS: 1-based indexing” and Example A.2. “Application Using C and cuBLAS: 0-based Indexing”). This application is analogous to the one using the cuBLAS library API that is shown in the Introduction chapter. Example A.1. Application Using C and cuBLAS: 1-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include \"cublas.h\" #define M 6 #define N 5 #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) static __inline__ void modify ( float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( n - q + 1 , alpha , & m [ IDX2F ( p , q , ldm )], ldm ); cublasSscal ( ldm - p + 1 , beta , & m [ IDX2F ( p , q , ldm )], 1 ); } int main ( void ){ int i , j ; cublasStatus stat ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { a [ IDX2F ( i , j , M )] = ( float )(( i -1 ) * M + j ); } } cublasInit (); stat = cublasAlloc ( M * N , sizeof ( * a ), ( void ** ) & devPtrA ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"device memory allocation failed\" ); cublasShutdown (); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } modify ( devPtrA , M , N , 2 , 3 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } cublasFree ( devPtrA ); cublasShutdown (); for ( j = 1 ; j <= N ; j ++ ) { for ( i = 1 ; i <= M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2F ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } Example A.2. Application Using C and cuBLAS: 0-based indexing //----------------------------------------------------------- #include <stdio.h> #include <stdlib.h> #include <math.h> #include \"cublas.h\" #define M 6 #define N 5 #define IDX2C(i,j,ld) (((j)*(ld))+(i)) static __inline__ void modify ( float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( n - q , alpha , & m [ IDX2C ( p , q , ldm )], ldm ); cublasSscal ( ldm - p , beta , & m [ IDX2C ( p , q , ldm )], 1 ); } int main ( void ){ int i , j ; cublasStatus stat ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { a [ IDX2C ( i , j , M )] = ( float )( i * M + j + 1 ); } } cublasInit (); stat = cublasAlloc ( M * N , sizeof ( * a ), ( void ** ) & devPtrA ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"device memory allocation failed\" ); cublasShutdown (); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } modify ( devPtrA , M , N , 1 , 2 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } cublasFree ( devPtrA ); cublasShutdown (); for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2C ( i , j , M )]); } printf ( \" \\n \" ); } free ( a ); return EXIT_SUCCESS ; } 7. cuBLAS Fortran Bindings  The cuBLAS library is implemented using the C-based CUDA toolchain. Thus, it provides a C-style API. This makes interfacing to applications written in C and C++ trivial, but the library can also be used by applications written in Fortran. In particular, the cuBLAS library uses 1-based indexing and Fortran-style column-major storage for multidimensional data to simplify interfacing to Fortran applications. Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform and toolchain. In particular, differences may exist in the following areas: symbol names (capitalization, name decoration) argument passing (by value or reference) passing of string arguments (length information) passing of pointer arguments (size of the pointer) returning floating-point or compound data types (for example single-precision or complex data types) To provide maximum flexibility in addressing those differences, the cuBLAS Fortran interface is provided in the form of wrapper functions and is part of the Toolkit delivery. The C source code of those wrapper functions is located in the src directory and provided in two different forms: the thunking wrapper interface located in the file fortran_thunking.c the direct wrapper interface located in the file fortran.c The code of one of those two files needs to be compiled into an application for it to call the cuBLAS API functions. Providing source code allows users to make any changes necessary for a particular platform and toolchain. The code in those two C files has been used to demonstrate interoperability with the compilers g77 3.2.3 and g95 0.91 on 32-bit Linux, g77 3.4.5 and g95 0.91 on 64-bit Linux, Intel Fortran 9.0 and Intel Fortran 10.0 on 32-bit and 64-bit Microsoft Windows XP, and g77 3.4.0 and g95 0.92 on Mac OS X. Note that for g77, use of the compiler flag -fno-second-underscore is required to use these wrappers as provided. Also, the use of the default calling conventions with regard to argument and return value passing is expected. Using the flag -fno-f2c changes the default calling convention with respect to these two items. The thunking wrappers allow interfacing to existing Fortran applications without any changes to the application. During each call, the wrappers allocate GPU memory, copy source data from CPU memory space to GPU memory space, call cuBLAS, and finally copy back the results to CPU memory space and deallocate the GPU memory. As this process causes very significant call overhead, these wrappers are intended for light testing, not for production code. To use the thunking wrappers, the application needs to be compiled with the file fortran_thunking.c . The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all BLAS functions. To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using cuBLAS_ALLOC and cuBLAS_FREE ) and to copy data between GPU and CPU memory spaces (using cuBLAS_SET_VECTOR , cuBLAS_GET_VECTOR , cuBLAS_SET_MATRIX , and cuBLAS_GET_MATRIX ). The sample wrappers provided in fortran.c map device pointers to the OS-dependent type size_t , which is 32-bit wide on 32-bit platforms and 64-bit wide on a 64-bit platforms. One approach to deal with index arithmetic on device pointers in Fortran code is to use C-style macros, and use the C preprocessor to expand these, as shown in the example below. On Linux and Mac OS X, one way of pre-processing is to use the option -E -x f77-cpp-input when using g77 compiler, or simply the option -cpp when using g95 or gfortran. On Windows platforms with Microsoft Visual C/C++, using ’cl -EP’ achieves similar results. ! Example B.1. Fortran 77 Application Executing on the Host ! ---------------------------------------------------------- subroutine modify ( m , ldm , n , p , q , alpha , beta ) implicit none integer ldm , n , p , q real * 4 m ( ldm , * ) , alpha , beta external cublas_sscal call cublas_sscal ( n - p + 1 , alpha , m ( p , q ), ldm ) call cublas_sscal ( ldm - p + 1 , beta , m ( p , q ), 1 ) return end program matrixmod implicit none integer M , N parameter ( M = 6 , N = 5 ) real * 4 a ( M , N ) integer i , j external cublas_init external cublas_shutdown do j = 1 , N do i = 1 , M a ( i , j ) = ( i - 1 ) * M + j enddo enddo call cublas_init call modify ( a , M , N , 2 , 3 , 1 6.0 , 1 2.0 ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7.0$)\" ) a ( i , j ) enddo write ( * , * ) \"\" enddo stop end When traditional fixed-form Fortran 77 code is ported to use the cuBLAS library, line length often increases when the BLAS calls are exchanged for cuBLAS calls. Longer function names and possible macro expansion are contributing factors. Inadvertently exceeding the maximum line length can lead to run-time errors that are difficult to find, so care should be taken not to exceed the 72-column limit if fixed form is retained. The examples in this chapter show a small application implemented in Fortran 77 on the host and the same application with the non-thunking wrappers after it has been ported to use the cuBLAS library. The second example should be compiled with ARCH_64 defined as 1 on 64-bit OS system and as 0 on 32-bit OS system. For example for g95 or gfortran, this can be done directly on the command line by using the option -cpp -DARCH_64=1 . ! Example B.2. Same Application Using Non-thunking cuBLAS Calls !------------------------------------------------------------- #define IDX2F (i,j,ld) ((((j)-1)*(ld))+((i)-1)) subroutine modify ( devPtrM , ldm , n , p , q , alpha , beta ) implicit none integer sizeof_real parameter ( sizeof_real = 4 ) integer ldm , n , p , q #if ARCH_64 integer * 8 devPtrM #else integer * 4 devPtrM #endif real * 4 alpha , beta call cublas_sscal ( n - p + 1 , alpha , 1 devPtrM + IDX2F ( p , q , ldm ) * sizeof_real , 2 ldm ) call cublas_sscal ( ldm - p + 1 , beta , 1 devPtrM + IDX2F ( p , q , ldm ) * sizeof_real , 2 1 ) return end program matrixmod implicit none integer M , N , sizeof_real #if ARCH_64 integer * 8 devPtrA #else integer * 4 devPtrA #endif parameter ( M = 6 , N = 5 , sizeof_real = 4 ) real * 4 a ( M , N ) integer i , j , stat external cublas_init , cublas_set_matrix , cublas_get_matrix external cublas_shutdown , cublas_alloc integer cublas_alloc , cublas_set_matrix , cublas_get_matrix do j = 1 , N do i = 1 , M a ( i , j ) = ( i - 1 ) * M + j enddo enddo call cublas_init stat = cublas_alloc ( M * N , sizeof_real , devPtrA ) if ( stat . NE . 0 ) then write ( * , * ) \"device memory allocation failed\" call cublas_shutdown stop endif stat = cublas_set_matrix ( M , N , sizeof_real , a , M , devPtrA , M ) if ( stat . NE . 0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data download failed\" call cublas_shutdown stop endif — — Code block continues below. Space added for formatting purposes. — — call modify ( devPtrA , M , N , 2 , 3 , 16.0 , 12.0 ) stat = cublas_get_matrix ( M , N , sizeof_real , devPtrA , M , a , M ) if ( stat . NE .0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data upload failed\" call cublas_shutdown stop endif call cublas_free ( devPtrA ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7.0$)\" ) a ( i , j ) enddo write ( * , * ) \"\" enddo stop end 8. Interaction with Other Libraries and Tools  This section describes important requirements and recommendations that ensure correct use of cuBLAS with other libraries and utilities. 8.1. nvprune  nvprune enables pruning relocatable host objects and static libraries to only contain device code for the specific target architectures. In case of cuBLAS, particular care must be taken if using nvprune with compute capabilities, whose minor revision number is different than 0. To reduce binary size, cuBLAS may only store major revision equivalents of CUDA binary files for kernels reused between different minor revision versions. Therefore, to ensure that a pruned library does not fail for arbitrary problems, the user must keep binaries for a selected architecture and all prior minor architectures in its major architecture. For example, the following call prunes libcublas_static.a to contain only sm_75 (Turing) and sm_70 (Volta) cubins: nvprune -- generate - code code = sm_70 -- generate - code code = sm_75 libcublasLt_static . a - o libcublasLt_static_sm70_sm75 . a which should be used instead of: nvprune - arch = sm_75 libcublasLt_static . a - o libcublasLt_static_sm75 . a 9. Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: Portions of the SGEMM, DGEMM, CGEMM and ZGEMM library routines were written by Vasily Volkov of the University of California. Portions of the SGEMM, DGEMM and ZGEMM library routines were written by Davide Barbieri of the University of Rome Tor Vergata. Portions of the DGEMM and SGEMM library routines optimized for Fermi architecture were developed by the University of Tennessee. Subsequently, several other routines that are optimized for the Fermi architecture have been derived from these initial DGEMM and SGEMM implementations. The substantial optimizations of the STRSV, DTRSV, CTRSV and ZTRSV library routines were developed by Jonathan Hogg of The Science and Technology Facilities Council (STFC). Subsequently, some optimizations of the STRSM, DTRSM, CTRSM and ZTRSM have been derived from these TRSV implementations. Substantial optimizations of the SYMV and HEMV library routines were developed by Ahmad Abdelfattah, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST). Substantial optimizations of the TRMM and TRSM library routines were developed by Ali Charara, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST). This product includes {fmt} - A modern formatting library https://fmt.dev Copyright (c) 2012 - present, Victor Zverovich. This product includes spdlog - Fast C++ logging library. https://github.com/gabime/spdlog The MIT License (MIT). This product includes SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT https://sleef.org Boost Software License - Version 1.0 - August 17th, 2003. This product includes Frozen - a header-only, constexpr alternative to gperf for C++14 users. https://github.com/serge-sans-paille/frozen Apache License - Version 2.0, January 2004. This product includes Boost C++ Libraries - free peer-reviewed portable C++ source libraries https://www.boost.org/ Boost Software License - Version 1.0 - August 17th, 2003. This product includes Zstandard - a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios. https://github.com/facebook/zstd The BSD License. 10. Notices  10.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cusparse/index.html", "parent_url": "https://docs.nvidia.com/cuda/cusparse/index.html", "content_type": "text/html", "text": "cuSPARSE 1. Introduction 1.1. Library Organization and Features 1.2. Static Library Support 1.3. Library Dependencies 2. Using the cuSPARSE API 2.1. APIs Usage Notes 2.2. Deprecated APIs 2.3. Thread Safety 2.4. Result Reproducibility 2.5. NaN and Inf Propagation 2.6. Parallelism with Streams 2.7. Compatibility and Versioning 2.8. Optimization Notes 3. cuSPARSE Storage Formats 3.1. Index Base 3.2. Vector Formats 3.2.1. Dense Vector Format 3.2.2. Sparse Vector Format 3.3. Matrix Formats 3.3.1. Dense Matrix Format 3.3.2. Coordinate (COO) 3.3.3. Compressed Sparse Row (CSR) 3.3.4. Compressed Sparse Column (CSC) 3.3.5. Sliced Ellpack (SELL) 3.3.6. Block Sparse Row (BSR) 3.3.7. Blocked Ellpack (BLOCKED-ELL) 3.3.8. Extended BSR Format (BSRX) [DEPRECATED] 4. cuSPARSE Basic APIs 4.1. cuSPARSE Types Reference 4.1.1. cudaDataType_t 4.1.2. cusparseStatus_t 4.1.3. cusparseHandle_t 4.1.4. cusparsePointerMode_t 4.1.5. cusparseOperation_t 4.1.6. cusparseDiagType_t 4.1.7. cusparseFillMode_t 4.1.8. cusparseIndexBase_t 4.1.9. cusparseDirection_t 4.2. cuSPARSE Management API 4.2.1. cusparseCreate() 4.2.2. cusparseDestroy() 4.2.3. cusparseGetErrorName() 4.2.4. cusparseGetErrorString() 4.2.5. cusparseGetProperty() 4.2.6. cusparseGetVersion() 4.2.7. cusparseGetPointerMode() 4.2.8. cusparseSetPointerMode() 4.2.9. cusparseGetStream() 4.2.10. cusparseSetStream() 4.3. cuSPARSE Logging API 4.3.1. cusparseLoggerSetCallback() 4.3.2. cusparseLoggerSetFile() 4.3.3. cusparseLoggerOpenFile() 4.3.4. cusparseLoggerSetLevel() 4.3.5. cusparseLoggerSetMask() 5. cuSPARSE Legacy APIs 5.1. Naming Conventions 5.2. cuSPARSE Legacy Types Reference 5.2.1. cusparseAction_t 5.2.2. cusparseMatDescr_t 5.2.3. cusparseMatrixType_t 5.2.4. cusparseColorInfo_t [DEPRECATED] 5.2.5. cusparseSolvePolicy_t [DEPRECATED] 5.2.6. bsric02Info_t [DEPRECATED] 5.2.7. bsrilu02Info_t [DEPRECATED] 5.2.8. bsrsm2Info_t [DEPRECATED] 5.2.9. bsrsv2Info_t [DEPRECATED] 5.2.10. csric02Info_t [DEPRECATED] 5.2.11. csrilu02Info_t [DEPRECATED] 5.3. cuSPARSE Helper Function Reference 5.3.1. cusparseCreateColorInfo() [DEPRECATED] 5.3.2. cusparseCreateMatDescr() 5.3.3. cusparseDestroyColorInfo() [DEPRECATED] 5.3.4. cusparseDestroyMatDescr() 5.3.5. cusparseGetMatDiagType() 5.3.6. cusparseGetMatFillMode() 5.3.7. cusparseGetMatIndexBase() 5.3.8. cusparseGetMatType() 5.3.9. cusparseSetMatDiagType() 5.3.10. cusparseSetMatFillMode() 5.3.11. cusparseSetMatIndexBase() 5.3.12. cusparseSetMatType() 5.3.13. cusparseCreateCsric02Info() [DEPRECATED] 5.3.14. cusparseDestroyCsric02Info() [DEPRECATED] 5.3.15. cusparseCreateCsrilu02Info() [DEPRECATED] 5.3.16. cusparseDestroyCsrilu02Info() [DEPRECATED] 5.3.17. cusparseCreateBsrsv2Info() [DEPRECATED] 5.3.18. cusparseDestroyBsrsv2Info() [DEPRECATED] 5.3.19. cusparseCreateBsrsm2Info() [DEPRECATED] 5.3.20. cusparseDestroyBsrsm2Info() [DEPRECATED] 5.3.21. cusparseCreateBsric02Info() [DEPRECATED] 5.3.22. cusparseDestroyBsric02Info() [DEPRECATED] 5.3.23. cusparseCreateBsrilu02Info() [DEPRECATED] 5.3.24. cusparseDestroyBsrilu02Info() [DEPRECATED] 5.3.25. cusparseCreatePruneInfo() [DEPRECATED] 5.3.26. cusparseDestroyPruneInfo() [DEPRECATED] 5.4. cuSPARSE Level 2 Function Reference 5.4.1. cusparse<t>bsrmv() 5.4.2. cusparse<t>bsrxmv() [DEPRECATED] 5.4.3. cusparse<t>bsrsv2_bufferSize() [DEPRECATED] 5.4.4. cusparse<t>bsrsv2_analysis() [DEPRECATED] 5.4.5. cusparse<t>bsrsv2_solve() [DEPRECATED] 5.4.6. cusparseXbsrsv2_zeroPivot() [DEPRECATED] 5.4.7. cusparse<t>gemvi() 5.5. cuSPARSE Level 3 Function Reference 5.5.1. cusparse<t>bsrmm() 5.5.2. cusparse<t>bsrsm2_bufferSize() [DEPRECATED] 5.5.3. cusparse<t>bsrsm2_analysis() [DEPRECATED] 5.5.4. cusparse<t>bsrsm2_solve() [DEPRECATED] 5.5.5. cusparseXbsrsm2_zeroPivot() [DEPRECATED] 5.6. cuSPARSE Extra Function Reference 5.6.1. cusparse<t>csrgeam2() 5.7. cuSPARSE Preconditioners Reference 5.7.1. Incomplete Cholesky Factorization: level 0 [DEPRECATED] 5.7.1.1. cusparse<t>csric02_bufferSize() [DEPRECATED] 5.7.1.2. cusparse<t>csric02_analysis() [DEPRECATED] 5.7.1.3. cusparse<t>csric02() [DEPRECATED] 5.7.1.4. cusparseXcsric02_zeroPivot()  [DEPRECATED] 5.7.1.5. cusparse<t>bsric02_bufferSize()  [DEPRECATED] 5.7.1.6. cusparse<t>bsric02_analysis()  [DEPRECATED] 5.7.1.7. cusparse<t>bsric02()  [DEPRECATED] 5.7.1.8. cusparseXbsric02_zeroPivot()  [DEPRECATED] 5.7.2. Incomplete LU Factorization: level 0 [DEPRECATED] 5.7.2.1. cusparse<t>csrilu02_numericBoost() [DEPRECATED] 5.7.2.2. cusparse<t>csrilu02_bufferSize() [DEPRECATED] 5.7.2.3. cusparse<t>csrilu02_analysis() [DEPRECATED] 5.7.2.4. cusparse<t>csrilu02() [DEPRECATED] 5.7.2.5. cusparseXcsrilu02_zeroPivot() [DEPRECATED] 5.7.2.6. cusparse<t>bsrilu02_numericBoost() [DEPRECATED] 5.7.2.7. cusparse<t>bsrilu02_bufferSize() [DEPRECATED] 5.7.2.8. cusparse<t>bsrilu02_analysis() [DEPRECATED] 5.7.2.9. cusparse<t>bsrilu02() [DEPRECATED] 5.7.2.10. cusparseXbsrilu02_zeroPivot() [DEPRECATED] 5.7.3. Tridiagonal Solve 5.7.3.1. cusparse<t>gtsv2_buffSizeExt() 5.7.3.2. cusparse<t>gtsv2() 5.7.3.3. cusparse<t>gtsv2_nopivot_bufferSizeExt() 5.7.3.4. cusparse<t>gtsv2_nopivot() 5.7.4. Batched Tridiagonal Solve 5.7.4.1. cusparse<t>gtsv2StridedBatch_bufferSizeExt() 5.7.4.2. cusparse<t>gtsv2StridedBatch() 5.7.4.3. cusparse<t>gtsvInterleavedBatch() 5.7.5. Batched Pentadiagonal Solve 5.7.5.1. cusparse<t>gpsvInterleavedBatch() 5.8. cuSPARSE Reorderings Reference 5.8.1. cusparse<t>csrcolor() [DEPRECATED] 5.9. cuSPARSE Format Conversion Reference 5.9.1. cusparse<t>bsr2csr() 5.9.2. cusparse<t>gebsr2gebsc() 5.9.3. cusparse<t>gebsr2gebsr() 5.9.4. cusparse<t>gebsr2csr() 5.9.5. cusparse<t>csr2gebsr() 5.9.6. cusparse<t>coo2csr() 5.9.7. cusparse<t>csr2coo() 5.9.8. cusparseCsr2cscEx2() 5.9.9. cusparse<t>nnz() 5.9.10. cusparseCreateIdentityPermutation() [DEPRECATED] 5.9.11. cusparseXcoosort() 5.9.12. cusparseXcsrsort() 5.9.13. cusparseXcscsort() 5.9.14. cusparseXcsru2csr() [DEPRECATED] 5.9.15. cusparseXpruneDense2csr() [DEPRECATED] 5.9.16. cusparseXpruneCsr2csr()  [DEPRECATED] 5.9.17. cusparseXpruneDense2csrPercentage()  [DEPRECATED] 5.9.18. cusparseXpruneCsr2csrByPercentage() [DEPRECATED] 5.9.19. cusparse<t>nnz_compress()  [DEPRECATED] 6. cuSPARSE Generic APIs 6.1. Generic Types Reference 6.1.1. cusparseFormat_t 6.1.2. cusparseOrder_t 6.1.3. cusparseIndexType_t 6.2. Dense Vector APIs 6.2.1. cusparseCreateDnVec() 6.2.2. cusparseDestroyDnVec() 6.2.3. cusparseDnVecGet() 6.2.4. cusparseDnVecGetValues() 6.2.5. cusparseDnVecSetValues() 6.3. Sparse Vector APIs 6.3.1. cusparseCreateSpVec() 6.3.2. cusparseDestroySpVec() 6.3.3. cusparseSpVecGet() 6.3.4. cusparseSpVecGetIndexBase() 6.3.5. cusparseSpVecGetValues() 6.3.6. cusparseSpVecSetValues() 6.4. Dense Matrix APIs 6.4.1. cusparseCreateDnMat() 6.4.2. cusparseDestroyDnMat() 6.4.3. cusparseDnMatGet() 6.4.4. cusparseDnMatGetValues() 6.4.5. cusparseDnMatSetValues() 6.4.6. cusparseDnMatGetStridedBatch() 6.4.7. cusparseDnMatSetStridedBatch() 6.5. Sparse Matrix APIs 6.5.1. Coordinate (COO) 6.5.1.1. cusparseCreateCoo() 6.5.1.2. cusparseCooGet() 6.5.1.3. cusparseCooSetPointers() 6.5.1.4. cusparseCooSetStridedBatch() 6.5.2. Compressed Sparse Row (CSR) 6.5.2.1. cusparseCreateCsr() 6.5.2.2. cusparseCsrGet() 6.5.2.3. cusparseCsrSetPointers() 6.5.2.4. cusparseCsrSetStridedBatch() 6.5.3. Compressed Sparse Column (CSC) 6.5.3.1. cusparseCreateCsc() 6.5.3.2. cusparseCscGet() 6.5.3.3. cusparseCscSetPointers() 6.5.4. Blocked-Ellpack (Blocked-ELL) 6.5.4.1. cusparseCreateBlockedEll() 6.5.4.2. cusparseBlockedEllGet() 6.5.5. Sliced-Ellpack (SELL) 6.5.5.1. cusparseCreateSlicedEll() 6.5.6. Block Sparse Row (BSR) 6.5.6.1. cusparseCreateBsr() 6.5.6.2. cusparseBsrSetStridedBatch() 6.5.7. All Sparse Formats 6.5.7.1. cusparseDestroySpMat() 6.5.7.2. cusparseSpMatGetSize() 6.5.7.3. cusparseSpMatGetFormat() 6.5.7.4. cusparseSpMatGetIndexBase() 6.5.7.5. cusparseSpMatGetValues() 6.5.7.6. cusparseSpMatSetValues() 6.5.7.7. cusparseSpMatGetStridedBatch() 6.5.7.8. cusparseSpMatGetAttribute() 6.5.7.9. cusparseSpMatSetAttribute() 6.6. Generic API Functions 6.6.1. cusparseAxpby() 6.6.2. cusparseGather() 6.6.3. cusparseScatter() 6.6.4. cusparseRot() [DEPRECATED] 6.6.5. cusparseSpVV() 6.6.6. cusparseSpMV() 6.6.7. cusparseSpSV() 6.6.8. cusparseSpMM() 6.6.9. cusparseSpMMOp() 6.6.10. cusparseSpSM() 6.6.11. cusparseSDDMM() 6.6.12. cusparseSpGEMM() 6.6.13. cusparseSpGEMMreuse() 6.6.14. cusparseSparseToDense() 6.6.15. cusparseDenseToSparse() 7. cuSPARSE Fortran Bindings 7.1. Fortran Application 8. Acknowledgements 9. Bibliography 10. Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks cuSPARSE » 1. Introduction v12.5 | PDF | Archive cuSPARSE The API reference guide for cuSPARSE, the CUDA sparse matrix library. 1. Introduction  The cuSPARSE library contains a set of GPU-accelerated basic linear algebra subroutines used for handling sparse matrices that perform significantly faster than CPU-only alternatives. Depending on the specific operation, the library targets matrices with sparsity ratios in the range between 70%-99.9%. It is implemented on top of the NVIDIA® CUDA™ runtime (which is part of the CUDA Toolkit) and is designed to be called from C and C++. see also cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication cuSPARSE Release Notes : cuda-toolkit-release-notes cuSPARSE GitHub Samples : CUDALibrarySamples Nvidia Developer Forum : GPU-Accelerated Libraries Provide Feedback : Math-Libs-Feedback @ nvidia . com Recent cuSPARSE/cuSPARSELt Blog Posts and GTC presentations : Exploiting NVIDIA Ampere Structured Sparsity with cuSPARSELt Accelerating Matrix Multiplication with Block Sparse Format and NVIDIA Tensor Cores Just-In-Time Link-Time Optimization Adoption in cuSPARSE/cuFFT: Use Case Overview Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines Making the Most of Structured Sparsity in the NVIDIA Ampere Architecture The library routines provide the following functionalities: Operations between a sparse vector and a dense vector : sum, dot product, scatter, gather Operations between a dense matrix and a sparse vector : multiplication Operations between a sparse matrix and a dense vector : multiplication, triangular solver, tridiagonal solver, pentadiagonal solver Operations between a sparse matrix and a dense matrix : multiplication, triangular solver, tridiagonal solver, pentadiagonal solver Operations between a sparse matrix and a sparse matrix : sum, multiplication Operations between dense matrices with output a sparse matrix : multiplication Sparse matrix preconditioners : Incomplete Cholesky Factorization (level 0),  Incomplete LU Factorization (level 0) Reordering and Conversion operations between different sparse matrix storage formats 1.1. Library Organization and Features  The cuSPARSE library is organized in two set of APIs: The Legacy APIs , inspired by the Sparse BLAS standard, provide a limited set of functionalities and will not be improved in future releases , even if standard maintenance is still ensured. Some routines in this category could be deprecated and removed in the short-term. A replacement will be provided for the most important of them during the deprecation process. The Generic APIs provide the standard interface layer of cuSPARSE . They allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (SpMV) and sparse matrix-matrix multiplication (SpMM), in a flexible way. The new APIs have the following capabilities and features: Set matrix data layouts , number of batches , and storage formats (for example, CSR, COO, and so on). Set input/output/compute data types. This also allows mixed data-type computation . Set types of sparse vector/matrix indices (e.g. 32-bit, 64-bit). Choose the algorithm for the computation. Guarantee external device memory for internal operations. Provide extensive consistency checks across input matrices and vectors. This includes the validation of sizes, data types, layout, allowed operations, etc. Provide constant descriptors for vector and matrix inputs to support const-safe interface and guarantee that the APIs do not modify their inputs. 1.2. Static Library Support  Starting with CUDA 6.5, the cuSPARSE library is also delivered in a static form as libcusparse_static.a on Linux. For example, to compile a small application using cuSPARSE against the dynamic library , the following command can be used: nvcc my_cusparse_app . cu - lcusparse - o my_cusparse_app Whereas to compile against the static library , the following command has to be used: nvcc my_cusparse_app . cu - lcusparse_static - o my_cusparse_app It is also possible to use the native Host C++ compiler. Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line. The following command on Linux is suggested: gcc my_cusparse_app . c - lcusparse_static - lcudart_static - lpthread - ldl - I < cuda - toolkit - path >/ include - L < cuda - toolkit - path >/ lib64 - o my_cusparse_app Note that in the latter case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. 1.3. Library Dependencies  Starting with CUDA 12.0, cuSPARSE will depend on nvJitLink library for JIT (Just-In-Time) LTO (Link-Time-Optimization) capabilities; refer to the cusparseSpMMOp APIs for more information. If the user links to the dynamic library , the environment variables for loading the libraries at run-time (such as LD_LIBRARY_PATH on Linux and PATH on Windows) must include the path where libnvjitlink.so is located. If it is in the same directory as cuSPARSE, the user doesn’t need to take any action. If linking to the static library , the user needs to link with -lnvjitlink and set the environment variables for loading the libraries at compile-time LIBRARY_PATH/PATH accordingly. 2. Using the cuSPARSE API  This chapter describes how to use the cuSPARSE library API. It is not a reference for the cuSPARSE API data types and functions; that is provided in subsequent chapters. 2.1. APIs Usage Notes  The cuSPARSE library allows developers to access the computational resources of the NVIDIA graphics processing unit (GPU). The cuSPARSE APIs assume that input and output data (vectors and matrices) reside in GPU (device) memory . The input and output scalars (e.g. \\(\\alpha\\) and \\(\\beta\\) ) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host. This allows library functions to execute asynchronously using streams even when they are generated by a previous kernel resulting in maximum parallelism. The handle to the cuSPARSE library context is initialized using the function and is explicitly passed to every subsequent library function call. This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs. The error status cusparseStatus_t is returned by all cuSPARSE library function calls. It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc() , cudaFree() , cudaMemcpy() , and cudaMemcpyAsync() . The cuSPARSE library functions are executed asynchronously with respect to the host and may return control to the application on the host before the result is ready. Developers can use the cudaDeviceSynchronize() function to ensure that the execution of a particular cuSPARSE library routine has completed. A developer can also use the cudaMemcpy() routine to copy data from the device to the host and vice versa, using the cudaMemcpyDeviceToHost and cudaMemcpyHostToDevice parameters, respectively. In this case there is no need to add a call to cudaDeviceSynchronize() because the call to cudaMemcpy() with the above parameters is blocking and completes only when the results are ready on the host. 2.2. Deprecated APIs  The cuSPARSE library documentation explicitly indicates the set of APIs/enumerators/data structures that are deprecated. The library policy for deprecated APIs is the following: An API is marked [[DEPRECATED]] on a release X.Y (e.g. 11.2) The documentation indices a replacement if available Otherwise, the functionality will not be maintained in the future The API will be removed in the release X+1.0 (e.g. 12.0) Correctness bugs are still addressed even for deprecated APIs, while performance issues are not always ensured. In addition to the documentation, deprecated APIs generate a compile-time warning for most platforms when used. Deprecation warnings can be disabled by defining the macro DISABLE_CUSPARSE_DEPRECATED before including cusparse.h or by passing the flag -DDISABLE_CUSPARSE_DEPRECATED to the compiler. 2.3. Thread Safety  The library is thread safe and its functions can be called from multiple host threads, even with the same handle. When multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cuSPARSE calls in all threads. It is even more true for the destruction of the handle. So it is not recommended that multiple thread share the same cuSPARSE handle. 2.4. Result Reproducibility  The design of cuSPARSE prioritizes performance over bit-wise reproducibility. Operations using transpose or conjugate-transpose cusparseOperation_t have no reproducibility guarantees. For the remaining operations,\nperforming the same API call twice with the exact same arguments,\non the same machine, with the same executable will produce bit-wise identical results.\nThis bit-wise reproducibility can be disrupted by changes to:\nhardware, CUDA drivers, cuSPARSE version, memory alignment of the data, or algorithm selection. 2.5. NaN and Inf Propagation  Floating-point numbers have special values for NaN (not-a-number) and Inf (infinity).\nFunctions in cuSPARSE make no guarantees about the propagation of NaN and Inf. The cuSPARSE algorithms evaluate assuming all finite floating-point values.\nNaN and Inf appear in the output only if the algorithms happen to generate or propagate them.\nBecause the algorithms are subject to change based on toolkit version and runtime considerations,\nso too are the propagation behaviours of NaN and Inf. NaN propagation is different in cuSPARSE than in\ntypical dense numerical linear algebra, such as cuBLAS.\nThe dot product between vectors [0, 1, 0] and [1, 1, NaN] is NaN when using typical dense numerical algorithms,\nbut will be 1.0 with typical sparse numerical algorithms. 2.6. Parallelism with Streams  If the application performs several small independent computations, or if it makes data transfers in parallel with the computation, CUDA streams can be used to overlap these tasks. The application can conceptually associate a stream with each task. To achieve the overlap of computation between the tasks, the developer should create CUDA streams using the function cudaStreamCreate() and set the stream to be used by each individual cuSPARSE library routine by calling cusparseSetStream() just before calling the actual cuSPARSE routine. Then, computations performed in separate streams would be overlapped automatically on the GPU, when possible. This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work, or when there is a data transfer that can be performed in parallel with the computation. When streams are used, we recommend using the new cuSPARSE API with scalar parameters and results passed by reference in the device memory to achieve maximum computational overlap. Although a developer can create many streams, in practice it is not possible to have more than 16 concurrent kernels executing at the same time. 2.7. Compatibility and Versioning  The cuSPARSE APIs are intended to be backward compatible at the source level with future releases (unless stated otherwise in the release notes of a specific future release). In other words, if a program uses cuSPARSE, it should continue to compile and work correctly with newer versions of cuSPARSE without source code changes. cuSPARSE is not guaranteed to be backward compatible at the binary level. Using different versions of the cusparse.h header file and shared library is not supported. Using different versions of cuSPARSE and the CUDA runtime is not supported. The library uses the standard version semantic convention for identify different releases. The version takes the form of four fields joined by periods: MAJOR.MINOR.PATCH.BUILD These version fields are incremented based on the following rules: MAJOR : API breaking changes or new CUDA major version (breaking changes at lower level, e.g. drivers, compilers, libraries) MINOR : new APIs and functionalities PATCH : Bug fixes or performance improvements (or * new CUDA release) BUILD : Internal build number * Different CUDA toolkit releases ensure distinct library versions even if there are no changes at library level. 2.8. Optimization Notes  Most of the cuSPARSE routines can be optimized by exploiting CUDA Graphs capture and Hardware Memory Compression features. More in details, a single cuSPARSE call or a sequence of calls can be captured by a CUDA Graph and executed in a second moment. This minimizes kernels launch overhead and allows the CUDA runtime to optimize the whole workflow. A full example of CUDA graphs capture applied to a cuSPARSE routine can be found in cuSPARSE Library Samples - CUDA Graph . Secondly, the data types and functionalities involved in cuSPARSE are suitable for Hardware Memory Compression available in Ampere GPU devices (compute capability 8.0) or above. The feature allows memory compression for data with enough zero bytes without no loss of information. The device memory must be allocation with the CUDA driver APIs . A full example of Hardware Memory Compression applied to a cuSPARSE routine can be found in cuSPARSE Library Samples - Memory Compression . 3. cuSPARSE Storage Formats  The cuSPARSE library supports dense and sparse vector, and dense and sparse matrix formats. 3.1. Index Base  The library supports zero- and one-based indexing to ensure the compatibility with C/C++ and Fortran languages respectively. The index base is selected through the cusparseIndexBase_t type. 3.2. Vector Formats  This section describes dense and sparse vector formats. 3.2.1. Dense Vector Format  Dense vectors are represented with a single data array that is stored linearly in memory, such as the following \\(7 \\times 1\\) dense vector. Dense vector representation  3.2.2. Sparse Vector Format  Sparse vectors are represented with two arrays. The values array stores the nonzero values from the equivalent array in dense format. The indices array represent the positions of the corresponding nonzero values in the equivalent array in dense format. For example, the dense vector in section 3.2.1 can be stored as a sparse vector with zero-based or one-based indexing. Sparse vector representation  Note The cuSPARSE routines assume that the indices are provided in increasing order and that each index appears only once. In the opposite case, the correctness of the computation is not always ensured. 3.3. Matrix Formats  Dense and several sparse formats for matrices are discussed in this section. 3.3.1. Dense Matrix Format  A dense matrix can be stored in both row-major and column-major memory layout (ordering) and it is represented by the following parameters. The number of rows in the matrix. The number of columns in the matrix. The leading dimension , which must be Greater than or equal to the number of columns in the row-major layout Greater than or equal to the number of rows in the column-major layout The pointers to the values array of length \\(rows \\times leading\\; dimension\\) in the row-major layout \\(columns \\times leading\\; dimension\\) in the column-major layout The following figure represents a \\(5 \\times 2\\) dense matrix with both memory layouts Dense matrix representations  The indices within the matrix represents the contiguous locations in memory. The leading dimension is useful to represent a sub-matrix within the original one Sub-matrix representations  3.3.2. Coordinate (COO)  A sparse matrix stored in COO format is represented by the following parameters. The number of rows in the matrix. The number of columns in the matrix. The number of non-zero elements ( nnz ) in the matrix. The pointers to the row indices array of length nnz that contains the row indices of the corresponding elements in the values array . The pointers to the column indices array of length nnz that contains the column indices of the corresponding elements in the values array . The pointers to the values array of length nnz that holds all nonzero values of the matrix in row-major ordering. Each entry of the COO representation consists of a <row, column> pair. The COO format is assumed to be sorted by row . The following example shows a \\(5 \\times 4\\) matrix represented in COO format. Note cuSPARSE supports both sorted and unsorted column indices within a given row. Note If the column indices within a given row are not unique, the correctness of the computation is not always ensured. Given an entry in the COO format (zero-base), the corresponding position in the dense matrix is computed as: // row-major rows_indices [ i ] * leading_dimension + column_indices [ i ] // column-major column_indices [ i ] * leading_dimension + rows_indices [ i ] 3.3.3. Compressed Sparse Row (CSR)  The CSR format is similar to COO, where the row indices are compressed and replaced by an array of offsets . A sparse matrix stored in CSR format is represented by the following parameters. The number of rows in the matrix. The number of columns in the matrix. The number of non-zero elements ( nnz ) in the matrix. The pointers to the row offsets array of length number of rows + 1 that represents the starting position of each row in the columns and values arrays . The pointers to the column indices array of length nnz that contains the column indices of the corresponding elements in the values array . The pointers to the values array of length nnz that holds all nonzero values of the matrix in row-major ordering. The following example shows a \\(5 \\times 4\\) matrix represented in CSR format. Note cuSPARSE supports both sorted and unsorted column indices within a given row. Note If the column indices within a given row are not unique, the correctness of the computation is not always ensured. Given an entry in the CSR format (zero-base), the corresponding position in the dense matrix is computed as: // row-major row * leading_dimension + column_indices [ row_offsets [ row ] + k ] // column-major column_indices [ row_offsets [ row ] + k ] * leading_dimension + row 3.3.4. Compressed Sparse Column (CSC)  The CSC format is similar to COO, where the column indices are compressed and replaced by an array of offsets . A sparse matrix stored in CSC format is represented by the following parameters. The number of rows in the matrix. The number of columns in the matrix. The number of non-zero elements ( nnz ) in the matrix. The pointers to the column offsets array of length number of column + 1 that represents the starting position of each column in the columns and values arrays . The pointers to the row indices array of length nnz that contains row indices of the corresponding elements in the values array . The pointers to the values array of length nnz that holds all nonzero values of the matrix in column-major ordering. The following example shows a \\(5 \\times 4\\) matrix represented in CSC format. Note The CSR format has exactly the same memory layout as its transpose in CSC format (and vice versa). Note cuSPARSE supports both sorted and unsorted row indices within a given column. Note If the row indices within a given column are not unique, the correctness of the computation is not always ensured. Given an entry in the CSC format (zero-base), the corresponding position in the dense matrix is computed as: // row-major column * leading_dimension + row_indices [ column_offsets [ column ] + k ] // column-major row_indices [ column_offsets [ column ] + k ] * leading_dimension + column 3.3.5. Sliced Ellpack (SELL)  The Sliced Ellpack format is standardized and well-known as the state of the art.\nThis format allows to significantly improve the performance of all problems that involve low variability in the number of nonzero elements per row. A matrix in the Sliced Ellpack format is divided into slices of an exact number of rows ( \\(sliceSize\\) ), defined by the user.\nThe maximum row length (i.e.,  the maximum non-zeros per row) is found for each slice, and every row in the slice is padded to the maximum row length.\nThe value -1 is used for padding. A \\(m \\times n\\) sparse matrix \\(A\\) is equivalent to a sliced sparse matrix \\(A_{s}\\) with \\(nslices = \\left \\lceil{\\frac{m}{sliceSize}}\\right \\rceil\\) slice rows and \\(n\\) columns.\nTo improve memory coalescing and memory utilization, each slice is stored in column-major order. A sparse matrix stored in SELL format is represented by the following parameters. The number of slices . The number of rows in the matrix. The number of columns in the matrix. The number of non-zero elements ( nnz ) in the matrix. The total number elements ( sellValuesSize ), including non-zero values and padded elements. The pointer to the slice offsets of length \\(nslices + 1\\) that holds offsets of the slides corresponding to the columns and values arrays. The pointer to the column indices array of length sellValuesSize that contains column indices of the corresponding elements in the values array. The column indices are stored in column-major layout. Value -1 refers to padding. The pointer to the values array of length sellValuesSize that holds all non-zero values and padding in column-major layout. The following example shows a \\(5 \\times 4\\) matrix represented in SELL format. 3.3.6. Block Sparse Row (BSR)  The BSR format is similar to CSR, where the column indices represent two-dimensional blocks instead of a single matrix entry. A matrix in the Block Sparse Row format is organized into blocks of size \\(blockSize\\) , defined by the user. A \\(m \\times n\\) sparse matrix \\(A\\) is equivalent to a block sparse matrix \\(A_{B}\\) : \\(mb \\times nb\\) with \\(mb = \\frac{m}{blockSize}\\) block rows and \\(nb = \\frac{n}{blockSize}\\) block columns .\nIf \\(m\\) or \\(n\\) is not multiple of \\(blockSize\\) , the user needs to pad the matrix with zeros. Note cuSPARSE currently supports only square blocks. The BSR format stores the blocks in row-major ordering. However, the internal storage format of blocks can be column-major ( cusparseDirection_t=CUSPARSE_DIRECTION_COLUMN ) or row-major ( cusparseDirection_t=CUSPARSE_DIRECTION_ROW ), independently of the base index. A sparse matrix stored in BSR format is represented by the following parameters. The block size . The number of row blocks in the matrix. The number of column blocks in the matrix. The number of non-zero blocks ( nnzb ) in the matrix. The pointers to the row block offsets array of length number of row blocks + 1 that represents the starting position of each row block in the columns and values arrays . The pointers to the column block indices array of length nnzb that contains the location of the corresponding elements in the values array. The pointers to the values array of length nnzb that holds all nonzero values of the matrix. The following example shows a \\(4 \\times 7\\) matrix represented in BSR format. 3.3.7. Blocked Ellpack (BLOCKED-ELL)  The Blocked Ellpack format is similar to the standard Ellpack, where the column indices represent two-dimensional blocks instead of a single matrix entry. A matrix in the Blocked Ellpack format is organized into blocks of size \\(blockSize\\) , defined by the user. The number of columns per row \\(nEllCols\\) is also defined by the user ( \\(nEllCols \\le n\\) ). A \\(m \\times n\\) sparse matrix \\(A\\) is equivalent to a Blocked-ELL matrix \\(A_{B}\\) : \\(mb \\times nb\\) with \\(mb = \\left \\lceil{\\frac{m}{blockSize}}\\right \\rceil\\) block rows , and \\(nb = \\left \\lceil{\\frac{nEllCols}{blockSize}}\\right \\rceil\\) block columns.\nIf \\(m\\) or \\(n\\) is not multiple of \\(blockSize\\) , then the remaining elements are zero. A sparse matrix stored in Blocked-ELL format is represented by the following parameters. The block size . The number of rows in the matrix. The number of columns in the matrix. The number of columns per row ( nEllCols ) in the matrix. The pointers to the column block indices array of length \\(mb \\times nb\\) that contains the location of the corresponding elements in the values array. Empty blocks can be represented with -1 index. The pointers to the values array of length \\(m \\times nEllCols\\) that holds all nonzero values of the matrix in row-major ordering. The following example shows a \\(9 \\times 9\\) matrix represented in Blocked-ELL format. 3.3.8. Extended BSR Format (BSRX) [DEPRECATED]  BSRX is the same as the BSR format, but the array bsrRowPtrA is separated into two parts. The first nonzero block of each row is still specified by the array bsrRowPtrA , which is the same as in BSR, but the position next to the last nonzero block of each row is specified by the array bsrEndPtrA . Briefly, BSRX format is simply like a 4-vector variant of BSR format. Matrix A is represented in BSRX format by the following parameters. blockDim (integer) Block dimension of matrix A . mb (integer) The number of block rows of A . nb (integer) The number of block columns of A . nnzb (integer) number of nonzero blocks in the matrix A . bsrValA (pointer) Points to the data array of length \\(nnzb \\ast blockDim^{2}\\) that holds all the elements of the nonzero blocks of A . The block elements are stored in either column-major order or row-major order. bsrRowPtrA (pointer) Points to the integer array of length mb that holds indices into the arrays bsrColIndA and bsrValA ; bsrRowPtrA(i) is the position of the first nonzero block of the i th block row in bsrColIndA and bsrValA . bsrEndPtrA (pointer) Points to the integer array of length mb that holds indices into the arrays bsrColIndA and bsrValA ; bsrRowPtrA(i) is the position next to the last nonzero block of the i th block row in bsrColIndA and bsrValA . bsrColIndA (pointer) Points to the integer array of length nnzb that contains the column indices of the corresponding blocks in array bsrValA . A simple conversion between BSR and BSRX can be done as follows. Suppose the developer has a 2×3 block sparse matrix \\(A_{b}\\) represented as shown. \\(A_{b} = \\begin{bmatrix}\nA_{00} & A_{01} & A_{02} \\\\\nA_{10} & A_{11} & A_{12} \\\\\n\\end{bmatrix}\\) Assume it has this BSR format. \\(\\begin{matrix}\n\\text{bsrValA of BSR} & = & \\begin{bmatrix}\nA_{00} & A_{01} & A_{10} & A_{11} & A_{12} \\\\\n\\end{bmatrix} \\\\\n\\text{bsrRowPtrA of BSR} & = & \\begin{bmatrix}\n{0\\phantom{.0}} & {2\\phantom{.0}} & 5 \\\\\n\\end{bmatrix} \\\\\n\\text{bsrColIndA of BSR} & = & \\begin{bmatrix}\n{0\\phantom{.0}} & {1\\phantom{.0}} & {0\\phantom{.0}} & {1\\phantom{.0}} & 2 \\\\\n\\end{bmatrix} \\\\\n\\end{matrix}\\) The bsrRowPtrA of the BSRX format is simply the first two elements of the bsrRowPtrA BSR format. The bsrEndPtrA of BSRX format is the last two elements of the bsrRowPtrA of BSR format. \\(\\begin{matrix}\n\\text{bsrRowPtrA of BSRX} & = & \\begin{bmatrix}\n{0\\phantom{.0}} & 2 \\\\\n\\end{bmatrix} \\\\\n\\text{bsrEndPtrA of BSRX} & = & \\begin{bmatrix}\n{2\\phantom{.0}} & 5 \\\\\n\\end{bmatrix} \\\\\n\\end{matrix}\\) The advantage of the BSRX format is that the developer can specify a submatrix in the original BSR format by modifying bsrRowPtrA and bsrEndPtrA while keeping bsrColIndA and bsrValA unchanged. For example, to create another block matrix \\(\\widetilde{A} = \\begin{bmatrix}\nO & O & O \\\\\nO & A_{11} & O \\\\\n\\end{bmatrix}\\) that is slightly different from \\(A\\) , the developer can keep bsrColIndA and bsrValA , but reconstruct \\(\\widetilde{A}\\) by properly setting of bsrRowPtrA and bsrEndPtrA . The following 4-vector characterizes \\(\\widetilde{A}\\) . \\(\\begin{matrix}\n{\\text{bsrValA of }\\widetilde{A}} & = & \\begin{bmatrix}\nA_{00} & A_{01} & A_{10} & A_{11} & A_{12} \\\\\n\\end{bmatrix} \\\\\n{\\text{bsrColIndA of }\\widetilde{A}} & = & \\begin{bmatrix}\n{0\\phantom{.0}} & {1\\phantom{.0}} & {0\\phantom{.0}} & {1\\phantom{.0}} & 2 \\\\\n\\end{bmatrix} \\\\\n{\\text{bsrRowPtrA of }\\widetilde{A}} & = & \\begin{bmatrix}\n{0\\phantom{.0}} & 3 \\\\\n\\end{bmatrix} \\\\\n{\\text{bsrEndPtrA of }\\widetilde{A}} & = & \\begin{bmatrix}\n{0\\phantom{.0}} & 4 \\\\\n\\end{bmatrix} \\\\\n\\end{matrix}\\) 4. cuSPARSE Basic APIs  4.1. cuSPARSE Types Reference  4.1.1. cudaDataType_t  The section describes the types shared by multiple CUDA Libraries and defined in the header file library_types.h . The cudaDataType type is an enumerator to specify the data precision. It is used when the data reference does not carry the type itself (e.g. void* ). For example, it is used in the routine cusparseSpMM() . Value Meaning Data Type Header CUDA_R_16F The data type is 16-bit IEEE-754 floating-point __half cuda_fp16.h CUDA_C_16F The data type is 16-bit complex IEEE-754 floating-point __half2 cuda_fp16.h [DEPRECATED] CUDA_R_16BF The data type is 16-bit bfloat floating-point __nv_bfloat16 cuda_bf16.h CUDA_C_16BF The data type is 16-bit complex bfloat floating-point __nv_bfloat162 cuda_bf16.h [DEPRECATED] CUDA_R_32F The data type is 32-bit IEEE-754 floating-point float CUDA_C_32F The data type is 32-bit complex IEEE-754 floating-point cuComplex cuComplex.h CUDA_R_64F The data type is 64-bit IEEE-754 floating-point double CUDA_C_64F The data type is 64-bit complex IEEE-754 floating-point cuDoubleComplex cuComplex.h CUDA_R_8I The data type is 8-bit integer int8_t stdint.h CUDA_R_32I The data type is 32-bit integer int32_t stdint.h IMPORTANT: The Generic API routines allow all data types reported in the respective section of the documentation only on GPU architectures with native support for them. If a specific GPU model does not provide native support for a given data type, the routine returns CUSPARSE_STATUS_ARCH_MISMATCH error. Unsupported data types and Compute Capability (CC): __half on GPUs with CC < 53 (e.g. Kepler) __nv_bfloat16 on GPUs with CC < 80 (e.g. Kepler, Maxwell, Pascal, Volta, Turing) see https://developer.nvidia.com/cuda-gpus 4.1.2. cusparseStatus_t  This data type represents the status returned by the library functions and it can have the following values Value Description CUSPARSE_STATUS_SUCCESS The operation completed successfully CUSPARSE_STATUS_NOT_INITIALIZED The cuSPARSE library was not initialized. This is usually caused by the lack of a prior call, an error in the CUDA Runtime API called by the cuSPARSE routine, or an error in the hardware setup To correct: call cusparseCreate() prior to the function call; and check that the hardware, an appropriate version of the driver, and the cuSPARSE library are correctly installed The error also applies to generic APIs ( Generic APIs reference ) for indicating a matrix/vector descriptor not initialized CUSPARSE_STATUS_ALLOC_FAILED Resource allocation failed inside the cuSPARSE library. This is usually caused by a device memory allocation ( cudaMalloc() ) or by a host memory allocation failure To correct: prior to the function call, deallocate previously allocated memory as much as possible CUSPARSE_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example) To correct: ensure that all the parameters being passed have valid values CUSPARSE_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture To correct: compile and run the application on a device with appropriate compute capability CUSPARSE_STATUS_EXECUTION_FAILED The GPU program failed to execute. This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons To correct: check that the hardware, an appropriate version of the driver, and the cuSPARSE library are correctly installed CUSPARSE_STATUS_INTERNAL_ERROR An internal cuSPARSE operation failed To correct: check that the hardware, an appropriate version of the driver, and the cuSPARSE library are correctly installed. Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine completion CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED The matrix type is not supported by this function. This is usually caused by passing an invalid matrix descriptor to the function To correct: check that the fields in cusparseMatDescr_t descrA were set correctly CUSPARSE_STATUS_NOT_SUPPORTED The operation or data type combination is currently not supported by the function CUSPARSE_STATUS_INSUFFICIENT_RESOURCES The resources for the computation, such as GPU global or shared memory, are not sufficient to complete the operation. The error can also indicate that the current computation mode (e.g. bit size of sparse matrix indices) does not allow to handle the given input 4.1.3. cusparseHandle_t  This is a pointer type to an opaque cuSPARSE context, which the user must initialize by calling prior to calling cusparseCreate() any other library function. The handle created and returned by cusparseCreate() must be passed to every cuSPARSE function. 4.1.4. cusparsePointerMode_t  This type indicates whether the scalar values are passed by reference on the host or device. It is important to point out that if several scalar values are passed by reference in the function call, all of them will conform to the same single pointer mode. The pointer mode can be set and retrieved using cusparseSetPointerMode() and cusparseGetPointerMode() routines, respectively. Value Meaning CUSPARSE_POINTER_MODE_HOST The scalars are passed by reference on the host. CUSPARSE_POINTER_MODE_DEVICE The scalars are passed by reference on the device. 4.1.5. cusparseOperation_t  This type indicates which operations is applied to the related input (e.g. sparse matrix, or vector). Value Meaning CUSPARSE_OPERATION_NON_TRANSPOSE The non-transpose operation is selected. CUSPARSE_OPERATION_TRANSPOSE The transpose operation is selected. CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE The conjugate transpose operation is selected. 4.1.6. cusparseDiagType_t  This type indicates if the matrix diagonal entries are unity. The diagonal elements are always assumed to be present, but if CUSPARSE_DIAG_TYPE_UNIT is passed to an API routine, then the routine assumes that all diagonal entries are unity and will not read or modify those entries. Note that in this case the routine assumes the diagonal entries are equal to one, regardless of what those entries are actually set to in memory. Value Meaning CUSPARSE_DIAG_TYPE_NON_UNIT The matrix diagonal has non-unit elements. CUSPARSE_DIAG_TYPE_UNIT The matrix diagonal has unit elements. 4.1.7. cusparseFillMode_t  This type indicates if the lower or upper part of a matrix is stored in sparse storage. Value Meaning CUSPARSE_FILL_MODE_LOWER The lower triangular part is stored. CUSPARSE_FILL_MODE_UPPER The upper triangular part is stored. 4.1.8. cusparseIndexBase_t  This type indicates if the base of the matrix indices is zero or one. Value Meaning CUSPARSE_INDEX_BASE_ZERO The base index is zero (C compatibility). CUSPARSE_INDEX_BASE_ONE The base index is one  (Fortran compatibility). 4.1.9. cusparseDirection_t  This type indicates whether the elements of a dense matrix should be parsed by rows or by columns (assuming column-major storage in memory of the dense matrix) in function cusparse[S|D|C|Z]nnz. Besides storage format of blocks in BSR format is also controlled by this type. Value Meaning CUSPARSE_DIRECTION_ROW The matrix should be parsed by rows. CUSPARSE_DIRECTION_COLUMN The matrix should be parsed by columns. 4.2. cuSPARSE Management API  The cuSPARSE functions for managing the library are described in this section. 4.2.1. cusparseCreate()  cusparseStatus_t cusparseCreate ( cusparseHandle_t * handle ) This function initializes the cuSPARSE library and creates a handle on the cuSPARSE context. It must be called before any other cuSPARSE API function is invoked. It allocates hardware resources necessary for accessing the GPU. Param. In/out Meaning handle IN The pointer to the handle to the cuSPARSE context See cusparseStatus_t for the description of the return status 4.2.2. cusparseDestroy()  cusparseStatus_t cusparseDestroy ( cusparseHandle_t handle ) This function releases CPU-side resources used by the cuSPARSE library. The release of GPU-side resources may be deferred until the application shuts down. Param. In/out Meaning handle IN The handle to the cuSPARSE context See cusparseStatus_t for the description of the return status 4.2.3. cusparseGetErrorName()  const char * cusparseGetErrorString ( cusparseStatus_t status ) The function returns the string representation of an error code enum name. If the error code is not recognized, “unrecognized error code” is returned. Param. In/out Meaning status IN Error code to convert to string const char* OUT Pointer to a NULL-terminated string 4.2.4. cusparseGetErrorString()  const char * cusparseGetErrorString ( cusparseStatus_t status ) Returns the description string for an error code. If the error code is not recognized, “unrecognized error code” is returned. Param. In/out Meaning status IN Error code to convert to string const char* OUT Pointer to a NULL-terminated string 4.2.5. cusparseGetProperty()  cusparseStatus_t cusparseGetProperty ( libraryPropertyType type , int * value ) The function returns the value of the requested property. Refer to libraryPropertyType for supported types. Param. In/out Meaning type IN Requested property value OUT Value of the requested property libraryPropertyType (defined in library_types.h ): Value Meaning MAJOR_VERSION Enumerator to query the major version MINOR_VERSION Enumerator to query the minor version PATCH_LEVEL Number to identify the patch level See cusparseStatus_t for the description of the return status 4.2.6. cusparseGetVersion()  cusparseStatus_t cusparseGetVersion ( cusparseHandle_t handle , int * version ) This function returns the version number of the cuSPARSE library. Param. In/out Meaning handle IN cuSPARSE handle version OUT The version number of the library See cusparseStatus_t for the description of the return status 4.2.7. cusparseGetPointerMode()  cusparseStatus_t cusparseGetPointerMode ( cusparseHandlet handle , cusparsePointerMode_t * mode ) This function obtains the pointer mode used by the cuSPARSE library. Please see the section on the cusparsePointerMode_t type for more details. Param. In/out Meaning handle IN The handle to the cuSPARSE context mode OUT One of the enumerated pointer mode types See cusparseStatus_t for the description of the return status 4.2.8. cusparseSetPointerMode()  cusparseStatus_t cusparseSetPointerMode ( cusparseHandle_t handle , cusparsePointerMode_t mode ) This function sets the pointer mode used by the cuSPARSE library. The default is for the values to be passed by reference on the host. Please see the section on the cublasPointerMode_t type for more details. Param. In/out Meaning handle IN The handle to the cuSPARSE context mode IN One of the enumerated pointer mode types See cusparseStatus_t for the description of the return status 4.2.9. cusparseGetStream()  cusparseStatus_t cusparseGetStream ( cusparseHandle_t handle , cudaStream_t * streamId ) This function gets the cuSPARSE library stream, which is being used to to execute all calls to the cuSPARSE library functions. If the cuSPARSE library stream is not set, all kernels use the default NULL stream. Param. In/out Meaning handle IN The handle to the cuSPARSE context streamId OUT The stream used by the library See cusparseStatus_t for the description of the return status 4.2.10. cusparseSetStream()  cusparseStatus_t cusparseSetStream ( cusparseHandle_t handle , cudaStream_t streamId ) This function sets the stream to be used by the cuSPARSE library to execute its routines. Param. In/out Meaning handle IN The handle to the cuSPARSE context streamId IN The stream to be used by the library See cusparseStatus_t for the description of the return status 4.3. cuSPARSE Logging API  cuSPARSE logging mechanism can be enabled by setting the following environment variables before launching the target application: CUSPARSE_LOG_LEVEL=<level> - while level is one of the following levels: 0 - Off - logging is disabled (default) 1 - Error - only errors will be logged 2 - Trace - API calls that launch CUDA kernels will log their parameters and important information 3 - Hints - hints that can potentially improve the application’s performance 4 - Info - provides general information about the library execution, may contain details about heuristic status 5 - API Trace - API calls will log their parameter and important information CUSPARSE_LOG_MASK=<mask> - while mask is a combination of the following masks: 0 - Off 1 - Error 2 - Trace 4 - Hints 8 - Info 16 - API Trace CUSPARSE_LOG_FILE=<file_name> - while file name is a path to a logging file. File name may contain %i , that will be replaced with the process id. E.g <file_name>_%i.log . If CUSPARSE_LOG_FILE is not defined, the log messages are printed to stdout . Starting from CUDA 12.3, it is also possible to dump sparse matrices (CSR, CSC, COO, SELL, BSR) in binary files during the creation by setting the environment variable CUSPARSE_STORE_INPUT_MATRIX . Later on, the binary files can be send to Math-Libs-Feedback @ nvidia . com for debugging and reproducibility purposes of a specific correctness/performance issue. Another option is to use the experimental cuSPARSE logging API. See: cusparseLoggerSetCallback() cusparseLoggerSetFile() cusparseLoggerOpenFile() cusparseLoggerSetLevel() cusparseLoggerSetMask() cusparseLoggerForceDisable() Note The logging mechanism is not available for the legacy APIs. 4.3.1. cusparseLoggerSetCallback()  cusparseStatus_t cusparseLoggerSetCallback ( cusparseLoggerCallback_t callback ) Experimental : The function sets the logging callback function. Param. In/out Meaning callback IN Pointer to a callback function where cusparseLoggerCallback_t has the following signature: void ( * cusparseLoggerCallback_t )( int logLevel , const char * functionName , const char * message ) Param. In/out Meaning logLevel IN Selected log level functionName IN The name of the API that logged this message message IN The log message See cusparseStatus_t for the description of the return status 4.3.2. cusparseLoggerSetFile()  cusparseStatus_t cusparseLoggerSetFile ( FILE * file ) Experimental : The function sets the logging output file. Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle. Param. In/out Meaning file IN Pointer to an open file. File should have write permission See cusparseStatus_t for the description of the return status 4.3.3. cusparseLoggerOpenFile()  cusparseStatus_t cusparseLoggerOpenFile ( const char * logFile ) Experimental : The function opens a logging output file in the given path. Param. In/out Meaning logFile IN Path of the logging output file See cusparseStatus_t for the description of the return status 4.3.4. cusparseLoggerSetLevel()  cusparseStatus_t cusparseLoggerSetLevel ( int level ) Experimental : The function sets the value of the logging level. path. Param. In/out Meaning level IN Value of the logging level See cusparseStatus_t for the description of the return status 4.3.5. cusparseLoggerSetMask()  cusparseStatus_t cusparseLoggerSetMask ( int mask ) Experimental : The function sets the value of the logging mask. Param. In/out Meaning mask IN Value of the logging mask See cusparseStatus_t for the description of the return status 5. cuSPARSE Legacy APIs  5.1. Naming Conventions  The cuSPARSE legacy functions are available for data types float , double , cuComplex , and cuDoubleComplex . The sparse Level 2, and Level 3 functions follow this naming convention: cusparse < t >[< matrix data format >]< operation >[< output matrix data format >] where < t > can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively. The < matrix data format > can be dense , coo , csr , or csc , corresponding to the dense, coordinate, compressed sparse row, and compressed sparse column formats, respectively. 5.2. cuSPARSE Legacy Types Reference  5.2.1. cusparseAction_t  This type indicates whether the operation is performed only on indices or on data and indices. Value Meaning CUSPARSE_ACTION_SYMBOLIC the operation is performed only on indices. CUSPARSE_ACTION_NUMERIC the operation is performed on data and indices. 5.2.2. cusparseMatDescr_t  This structure is used to describe the shape and properties of a matrix. typedef struct { cusparseMatrixType_t MatrixType ; cusparseFillMode_t FillMode ; cusparseDiagType_t DiagType ; cusparseIndexBase_t IndexBase ; } cusparseMatDescr_t ; 5.2.3. cusparseMatrixType_t  This type indicates the type of matrix stored in sparse storage. Notice that for symmetric, Hermitian and triangular matrices only their lower or upper part is assumed to be stored. The whole idea of matrix type and fill mode is to keep minimum storage for symmetric/Hermitian matrix, and also to take advantage of symmetric property on SpMV (Sparse Matrix Vector multiplication). To compute y=A*x when A is symmetric and only lower triangular part is stored, two steps are needed. First step is to compute y=(L+D)*x and second step is to compute y=L^T*x + y . Given the fact that the transpose operation y=L^T*x is 10x slower than non-transpose version y=L*x , the symmetric property does not show up any performance gain. It is better for the user to extend the symmetric matrix to a general matrix and apply y=A*x with matrix type CUSPARSE_MATRIX_TYPE_GENERAL . In general, SpMV, preconditioners (incomplete Cholesky or incomplete LU) and triangular solver are combined together in iterative solvers, for example PCG and GMRES. If the user always uses general matrix (instead of symmetric matrix), there is no need to support other than general matrix in preconditioners. Therefore the new routines, [bsr|csr]sv2 (triangular solver), [bsr|csr]ilu02 (incomplete LU) and [bsr|csr]ic02 (incomplete Cholesky), only support matrix type CUSPARSE_MATRIX_TYPE_GENERAL . Value Meaning CUSPARSE_MATRIX_TYPE_GENERAL the matrix is general. CUSPARSE_MATRIX_TYPE_SYMMETRIC the matrix is symmetric. CUSPARSE_MATRIX_TYPE_HERMITIAN the matrix is Hermitian. CUSPARSE_MATRIX_TYPE_TRIANGULAR the matrix is triangular. 5.2.4. cusparseColorInfo_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csrcolor() . 5.2.5. cusparseSolvePolicy_t [DEPRECATED]  This type indicates whether level information is generated and used in csrsv2, csric02, csrilu02, bsrsv2, bsric02 and bsrilu02 . Value Meaning CUSPARSE_SOLVE_POLICY_NO_LEVEL no level information is generated and used. CUSPARSE_SOLVE_POLICY_USE_LEVEL generate and use level information. 5.2.6. bsric02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsric02_bufferSize() , bsric02_analysis() , and bsric02() . 5.2.7. bsrilu02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrilu02_bufferSize() , bsrilu02_analysis() , and bsrilu02() . 5.2.8. bsrsm2Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrsm2_bufferSize() , bsrsm2_analysis() , and bsrsm2_solve() . 5.2.9. bsrsv2Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrsv2_bufferSize() , bsrsv2_analysis() , and bsrsv2_solve() . 5.2.10. csric02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csric02_bufferSize() , csric02_analysis() , and csric02() . 5.2.11. csrilu02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csrilu02_bufferSize() , csrilu02_analysis() , and csrilu02() . 5.3. cuSPARSE Helper Function Reference  The cuSPARSE helper functions are described in this section. 5.3.1. cusparseCreateColorInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateColorInfo ( cusparseColorInfo_t * info ) This function creates and initializes the cusparseColorInfo_t structure to default values. Input info the pointer to the cusparseColorInfo_t structure See cusparseStatus_t for the description of the return status 5.3.2. cusparseCreateMatDescr()  cusparseStatus_t cusparseCreateMatDescr ( cusparseMatDescr_t * descrA ) This function initializes the matrix descriptor. It sets the fields MatrixType and IndexBase to the default values CUSPARSE_MATRIX_TYPE_GENERAL and CUSPARSE_INDEX_BASE_ZERO , respectively, while leaving other fields uninitialized. Input descrA the pointer to the matrix descriptor. See cusparseStatus_t for the description of the return status 5.3.3. cusparseDestroyColorInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyColorInfo ( cusparseColorInfo_t info ) This function destroys and releases any memory required by the structure. Input info the pointer to the structure of csrcolor() See cusparseStatus_t for the description of the return status 5.3.4. cusparseDestroyMatDescr()  cusparseStatus_t cusparseDestroyMatDescr ( cusparseMatDescr_t descrA ) This function releases the memory allocated for the matrix descriptor. Input descrA the matrix descriptor. See cusparseStatus_t for the description of the return status 5.3.5. cusparseGetMatDiagType()  cusparseDiagType_t cusparseGetMatDiagType ( const cusparseMatDescr_t descrA ) This function returns the DiagType field of the matrix descriptor descrA . Input descrA the matrix descriptor. Returned One of the enumerated diagType types. 5.3.6. cusparseGetMatFillMode()  cusparseFillMode_t cusparseGetMatFillMode ( const cusparseMatDescr_t descrA ) This function returns the FillMode field of the matrix descriptor descrA . Input descrA the matrix descriptor. Returned One of the enumerated fillMode types. 5.3.7. cusparseGetMatIndexBase()  cusparseIndexBase_t cusparseGetMatIndexBase ( const cusparseMatDescr_t descrA ) This function returns the IndexBase field of the matrix descriptor descrA . Input descrA the matrix descriptor. Returned One of the enumerated indexBase types. 5.3.8. cusparseGetMatType()  cusparseMatrixType_t cusparseGetMatType ( const cusparseMatDescr_t descrA ) This function returns the MatrixType field of the matrix descriptor descrA . Input descrA the matrix descriptor. Returned One of the enumerated matrix types. 5.3.9. cusparseSetMatDiagType()  cusparseStatus_t cusparseSetMatDiagType ( cusparseMatDescr_t descrA , cusparseDiagType_t diagType ) This function sets the DiagType field of the matrix descriptor descrA . Input diagType One of the enumerated diagType types. Output descrA the matrix descriptor. See cusparseStatus_t for the description of the return status 5.3.10. cusparseSetMatFillMode()  cusparseStatus_t cusparseSetMatFillMode ( cusparseMatDescr_t descrA , cusparseFillMode_t fillMode ) This function sets the FillMode field of the matrix descriptor descrA . Input fillMode One of the enumerated fillMode types. Output descrA the matrix descriptor. See cusparseStatus_t for the description of the return status 5.3.11. cusparseSetMatIndexBase()  cusparseStatus_t cusparseSetMatIndexBase ( cusparseMatDescr_t descrA , cusparseIndexBase_t base ) This function sets the IndexBase field of the matrix descriptor descrA . Input base One of the enumerated indexBase types. Output descrA the matrix descriptor. See cusparseStatus_t for the description of the return status 5.3.12. cusparseSetMatType()  cusparseStatus_t cusparseSetMatType ( cusparseMatDescr_t descrA , cusparseMatrixType_t type ) This function sets the MatrixType field of the matrix descriptor descrA . Input type One of the enumerated matrix types. Output descrA the matrix descriptor. See cusparseStatus_t for the description of the return status 5.3.13. cusparseCreateCsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsric02Info ( csric02Info_t * info ); This function creates and initializes the solve and analysis structure of incomplete Cholesky to default values. Input info the pointer to the solve and analysis structure of incomplete Cholesky. See cusparseStatus_t for the description of the return status 5.3.14. cusparseDestroyCsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyCsric02Info ( csric02Info_t info ); This function destroys and releases any memory required by the structure. Input info the solve (csric02_solve) and analysis (csric02_analysis) structure. See cusparseStatus_t for the description of the return status 5.3.15. cusparseCreateCsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsrilu02Info ( csrilu02Info_t * info ); This function creates and initializes the solve and analysis structure of incomplete LU to default values. Input info the pointer to the solve and analysis structure of incomplete LU. See cusparseStatus_t for the description of the return status 5.3.16. cusparseDestroyCsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyCsrilu02Info ( csrilu02Info_t info ); This function destroys and releases any memory required by the structure. Input info the solve (csrilu02_solve) and analysis (csrilu02_analysis) structure. See cusparseStatus_t for the description of the return status 5.3.17. cusparseCreateBsrsv2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrsv2Info ( bsrsv2Info_t * info ); This function creates and initializes the solve and analysis structure of bsrsv2 to default values. Input info the pointer to the solve and analysis structure of bsrsv2. See cusparseStatus_t for the description of the return status 5.3.18. cusparseDestroyBsrsv2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrsv2Info ( bsrsv2Info_t info ); This function destroys and releases any memory required by the structure. Input info the solve (bsrsv2_solve) and analysis (bsrsv2_analysis) structure. See cusparseStatus_t for the description of the return status 5.3.19. cusparseCreateBsrsm2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrsm2Info ( bsrsm2Info_t * info ); This function creates and initializes the solve and analysis structure of bsrsm2 to default values. Input info the pointer to the solve and analysis structure of bsrsm2. See cusparseStatus_t for the description of the return status 5.3.20. cusparseDestroyBsrsm2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrsm2Info ( bsrsm2Info_t info ); This function destroys and releases any memory required by the structure. Input info the solve (bsrsm2_solve) and analysis (bsrsm2_analysis) structure. See cusparseStatus_t for the description of the return status 5.3.21. cusparseCreateBsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsric02Info ( bsric02Info_t * info ); This function creates and initializes the solve and analysis structure of block incomplete Cholesky to default values. Input info the pointer to the solve and analysis structure of block incomplete Cholesky. See cusparseStatus_t for the description of the return status 5.3.22. cusparseDestroyBsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsric02Info ( bsric02Info_t info ); This function destroys and releases any memory required by the structure. Input info the solve (bsric02_solve) and analysis (bsric02_analysis) structure. See cusparseStatus_t for the description of the return status 5.3.23. cusparseCreateBsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrilu02Info ( bsrilu02Info_t * info ); This function creates and initializes the solve and analysis structure of block incomplete LU to default values. Input info the pointer to the solve and analysis structure of block incomplete LU. See cusparseStatus_t for the description of the return status 5.3.24. cusparseDestroyBsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrilu02Info ( bsrilu02Info_t info ); This function destroys and releases any memory required by the structure. Input info the solve (bsrilu02_solve) and analysis (bsrilu02_analysis) structure. See cusparseStatus_t for the description of the return status 5.3.25. cusparseCreatePruneInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreatePruneInfo ( pruneInfo_t * info ); This function creates and initializes structure of prune to default values. Input info the pointer to the structure of prune . See cusparseStatus_t for the description of the return status 5.3.26. cusparseDestroyPruneInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyPruneInfo ( pruneInfo_t info ); This function destroys and releases any memory required by the structure. Input info the structure of prune . See cusparseStatus_t for the description of the return status 5.4. cuSPARSE Level 2 Function Reference  This chapter describes the sparse linear algebra functions that perform operations between sparse matrices and dense vectors. 5.4.1. cusparse<t>bsrmv()  cusparseStatus_t cusparseSbsrmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int mb , int nb , int nnzb , const float * alpha , const cusparseMatDescr_t descr , const float * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int blockDim , const float * x , const float * beta , float * y ) cusparseStatus_t cusparseDbsrmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int mb , int nb , int nnzb , const double * alpha , const cusparseMatDescr_t descr , const double * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int blockDim , const double * x , const double * beta , double * y ) cusparseStatus_t cusparseCbsrmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int mb , int nb , int nnzb , const cuComplex * alpha , const cusparseMatDescr_t descr , const cuComplex * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int blockDim , const cuComplex * x , const cuComplex * beta , cuComplex * y ) cusparseStatus_t cusparseZbsrmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int mb , int nb , int nnzb , const cuDoubleComplex * alpha , const cusparseMatDescr_t descr , const cuDoubleComplex * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int blockDim , const cuDoubleComplex * x , const cuDoubleComplex * beta , cuDoubleComplex * y ) This function performs the matrix-vector operation \\(\\text{y} = \\alpha \\ast \\text{op}(A) \\ast \\text{x} + \\beta \\ast \\text{y}\\) where \\(A\\text{ is an }(mb \\ast blockDim) \\times (nb \\ast blockDim)\\) sparse matrix that is defined in BSR storage format by the three arrays bsrVal , bsrRowPtr , and bsrColInd ); x and y are vectors; \\(\\alpha\\text{ and }\\beta\\) are scalars; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) bsrmv() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Several comments on bsrmv() : Only blockDim > 1 is supported Only CUSPARSE_OPERATION_NON_TRANSPOSE is supported, that is \\(\\text{y} = \\alpha \\ast A \\ast \\text{x} + \\beta{} \\ast \\text{y}\\) Only CUSPARSE_MATRIX_TYPE_GENERAL is supported. The size of vector x should be \\((nb \\ast blockDim)\\) at least, and the size of vector y should be \\((mb \\ast blockDim)\\) at least; otherwise, the kernel may return CUSPARSE_STATUS_EXECUTION_FAILED because of an out-of-bounds array. For example, suppose the user has a CSR format and wants to try bsrmv() , the following code demonstrates how to use csr2bsr() conversion and bsrmv() multiplication in single precision. // Suppose that A is m x n sparse matrix represented by CSR format, // hx is a host vector of size n, and hy is also a host vector of size m. // m and n are not multiple of blockDim. // step 1: transform CSR to BSR with column-major order int base , nnz ; int nnzb ; cusparseDirection_t dirA = CUSPARSE_DIRECTION_COLUMN ; int mb = ( m + blockDim -1 ) / blockDim ; int nb = ( n + blockDim -1 ) / blockDim ; cudaMalloc (( void ** ) & bsrRowPtrC , sizeof ( int ) * ( mb + 1 )); cusparseXcsr2bsrNnz ( handle , dirA , m , n , descrA , csrRowPtrA , csrColIndA , blockDim , descrC , bsrRowPtrC , & nnzb ); cudaMalloc (( void ** ) & bsrColIndC , sizeof ( int ) * nnzb ); cudaMalloc (( void ** ) & bsrValC , sizeof ( float ) * ( blockDim * blockDim ) * nnzb ); cusparseScsr2bsr ( handle , dirA , m , n , descrA , csrValA , csrRowPtrA , csrColIndA , blockDim , descrC , bsrValC , bsrRowPtrC , bsrColIndC ); // step 2: allocate vector x and vector y large enough for bsrmv cudaMalloc (( void ** ) & x , sizeof ( float ) * ( nb * blockDim )); cudaMalloc (( void ** ) & y , sizeof ( float ) * ( mb * blockDim )); cudaMemcpy ( x , hx , sizeof ( float ) * n , cudaMemcpyHostToDevice ); cudaMemcpy ( y , hy , sizeof ( float ) * m , cudaMemcpyHostToDevice ); // step 3: perform bsrmv cusparseSbsrmv ( handle , dirA , transA , mb , nb , nnzb , & alpha , descrC , bsrValC , bsrRowPtrC , bsrColIndC , blockDim , x , & beta , y ); Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . trans the operation \\(\\text{op}(A)\\) . Only CUSPARSE_OPERATION_NON_TRANSPOSE is supported. mb number of block rows of matrix \\(A\\) . nb number of block columns of matrix \\(A\\) . nnzb number of nonzero blocks of matrix \\(A\\) . alpha <type> scalar used for multiplication. descr the descriptor of matrix \\(A\\) . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrVal <type> array of nnz \\(( =\\) csrRowPtrA(mb) \\(-\\) csrRowPtrA(0) \\()\\) nonzero blocks of matrix \\(A\\) . bsrRowPtr integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColInd integer array of nnz \\(( =\\) csrRowPtrA(mb) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix \\(A\\) . blockDim block dimension of sparse matrix \\(A\\) , larger than zero. x <type> vector of \\(nb \\ast blockDim\\) elements. beta <type> scalar used for multiplication. If beta is zero, y does not have to be a valid input. y <type> vector of \\(mb \\ast blockDim\\) elements. Output y <type> updated vector. See cusparseStatus_t for the description of the return status. 5.4.2. cusparse<t>bsrxmv() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrxmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int sizeOfMask , int mb , int nb , int nnzb , const float * alpha , const cusparseMatDescr_t descr , const float * bsrVal , const int * bsrMaskPtr , const int * bsrRowPtr , const int * bsrEndPtr , const int * bsrColInd , int blockDim , const float * x , const float * beta , float * y ) cusparseStatus_t cusparseDbsrxmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int sizeOfMask , int mb , int nb , int nnzb , const double * alpha , const cusparseMatDescr_t descr , const double * bsrVal , const int * bsrMaskPtr , const int * bsrRowPtr , const int * bsrEndPtr , const int * bsrColInd , int blockDim , const double * x , const double * beta , double * y ) cusparseStatus_t cusparseCbsrxmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int sizeOfMask , int mb , int nb , int nnzb , const cuComplex * alpha , const cusparseMatDescr_t descr , const cuComplex * bsrVal , const int * bsrMaskPtr , const int * bsrRowPtr , const int * bsrEndPtr , const int * bsrColInd , int blockDim , const cuComplex * x , const cuComplex * beta , cuComplex * y ) cusparseStatus_t cusparseZbsrxmv ( cusparseHandle_t handle , cusparseDirection_t dir , cusparseOperation_t trans , int sizeOfMask , int mb , int nb , int nnzb , const cuDoubleComplex * alpha , const cusparseMatDescr_t descr , const cuDoubleComplex * bsrVal , const int * bsrMaskPtr , const int * bsrRowPtr , const int * bsrEndPtr , const int * bsrColInd , int blockDim , const cuDoubleComplex * x , const cuDoubleComplex * beta , cuDoubleComplex * y ) This function performs a bsrmv and a mask operation \\(\\text{y(mask)} = (\\alpha \\ast \\text{op}(A) \\ast \\text{x} + \\beta \\ast \\text{y})\\text{(mask)}\\) where \\(A\\text{ is an }(mb \\ast blockDim) \\times (nb \\ast blockDim)\\) sparse matrix that is defined in BSRX storage format by the four arrays bsrVal , bsrRowPtr , bsrEndPtr , and bsrColInd ); x and y are vectors; \\(\\alpha\\text{~and~}\\beta\\) are scalars; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) The mask operation is defined by array bsrMaskPtr which contains updated block row indices of \\(y\\) . If row \\(i\\) is not specified in bsrMaskPtr , then bsrxmv() does not touch row block \\(i\\) of \\(A\\) and \\(y\\) . For example, consider the \\(2 \\times 3\\) block matrix \\(A\\) : \\(\\begin{matrix}\n{A = \\begin{bmatrix}\nA_{11} & A_{12} & O \\\\\nA_{21} & A_{22} & A_{23} \\\\\n\\end{bmatrix}} \\\\\n\\end{matrix}\\) and its one-based BSR format (three vector form) is \\(\\begin{matrix}\n\\text{bsrVal} & = & \\begin{bmatrix}\nA_{11} & A_{12} & A_{21} & A_{22} & A_{23} \\\\\n\\end{bmatrix} \\\\\n\\text{bsrRowPtr} & = & \\begin{bmatrix}\n{1\\phantom{.0}} & {3\\phantom{.0}} & 6 \\\\\n\\end{bmatrix} \\\\\n\\text{bsrColInd} & = & \\begin{bmatrix}\n{1\\phantom{.0}} & {2\\phantom{.0}} & {1\\phantom{.0}} & {2\\phantom{.0}} & 3 \\\\\n\\end{bmatrix} \\\\\n\\end{matrix}\\) Suppose we want to do the following bsrmv operation on a matrix \\(\\overset{¯}{A}\\) which is slightly different from \\(A\\) . \\(\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\end{bmatrix}:=alpha \\ast (\\widetilde{A} = \\begin{bmatrix}\nO & O & O \\\\\nO & A_{22} & O \\\\\n\\end{bmatrix}) \\ast \\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\n\\end{bmatrix} + \\begin{bmatrix}\ny_{1} \\\\\n{beta \\ast y_{2}} \\\\\n\\end{bmatrix}\\) We don’t need to create another BSR format for the new matrix \\(\\overset{¯}{A}\\) , all that we should do is to keep bsrVal and bsrColInd unchanged, but modify bsrRowPtr and add an additional array bsrEndPtr which points to the last nonzero elements per row of \\(\\overset{¯}{A}\\) plus 1. For example, the following bsrRowPtr and bsrEndPtr can represent matrix \\(\\overset{¯}{A}\\) : \\(\\begin{matrix}\n\\text{bsrRowPtr} & = & \\begin{bmatrix}\n{1\\phantom{.0}} & 4 \\\\\n\\end{bmatrix} \\\\\n\\text{bsrEndPtr} & = & \\begin{bmatrix}\n{1\\phantom{.0}} & 5 \\\\\n\\end{bmatrix} \\\\\n\\end{matrix}\\) Further we can use a mask operator (specified by array bsrMaskPtr ) to update particular block row indices of \\(y\\) only because \\(y_{1}\\) is never changed. In this case, bsrMaskPtr \\(=\\) [2] and sizeOfMask =1. The mask operator is equivalent to the following operation: \\(\\begin{bmatrix}\n? \\\\\ny_{2} \\\\\n\\end{bmatrix}:=alpha \\ast \\begin{bmatrix}\n? & ? & ? \\\\\nO & A_{22} & O \\\\\n\\end{bmatrix} \\ast \\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\n\\end{bmatrix} + beta \\ast \\begin{bmatrix}\n? \\\\\ny_{2} \\\\\n\\end{bmatrix}\\) If a block row is not present in the bsrMaskPtr , then no calculation is performed on that row, and the corresponding value in y is unmodified. The question mark “?” is used to inidcate row blocks not in bsrMaskPtr . In this case, first row block is not present in bsrMaskPtr , so bsrRowPtr[0] and bsrEndPtr[0] are not touched also. \\(\\begin{matrix}\n\\text{bsrRowPtr} & = & \\begin{bmatrix}\n{?\\phantom{.0}} & 4 \\\\\n\\end{bmatrix} \\\\\n\\text{bsrEndPtr} & = & \\begin{bmatrix}\n{?\\phantom{.0}} & 5 \\\\\n\\end{bmatrix} \\\\\n\\end{matrix}\\) bsrxmv() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture A couple of comments on bsrxmv() : Only blockDim > 1 is supported Only CUSPARSE_OPERATION_NON_TRANSPOSE and CUSPARSE_MATRIX_TYPE_GENERAL are supported. Parameters bsrMaskPtr , bsrRowPtr , bsrEndPtr and bsrColInd are consistent with base index, either one-based or zero-based. The above example is one-based. Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . trans the operation \\(\\text{op}(A)\\) . Only CUSPARSE_OPERATION_NON_TRANSPOSE is supported. sizeOfMask number of updated block rows of \\(y\\) . mb number of block rows of matrix \\(A\\) . nb number of block columns of matrix \\(A\\) . nnzb number of nonzero blocks of matrix \\(A\\) . alpha <type> scalar used for multiplication. descr the descriptor of matrix \\(A\\) . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrVal <type> array of nnz nonzero blocks of matrix \\(A\\) . bsrMaskPtr integer array of sizeOfMask elements that contains the indices corresponding to updated block rows. bsrRowPtr integer array of mb elements that contains the start of every block row. bsrEndPtr integer array of mb elements that contains the end of the every block row plus one. bsrColInd integer array of nnzb column indices of the nonzero blocks of matrix \\(A\\) . blockDim block dimension of sparse matrix \\(A\\) , larger than zero. x <type> vector of \\(nb \\ast blockDim\\) elements. beta <type> scalar used for multiplication. If beta is zero, y does not have to be a valid input. y <type> vector of \\(mb \\ast blockDim\\) elements. See cusparseStatus_t for the description of the return status. 5.4.3. cusparse<t>bsrsv2_bufferSize() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrsv2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseDbsrsv2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseCbsrsv2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseZbsrsv2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , int * pBufferSizeInBytes ) This function returns size of the buffer used in bsrsv2 , a new sparse triangular linear system op(A)*y = \\(\\alpha\\) x . A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand-side and the solution vectors; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) Although there are six combinations in terms of parameter trans and the upper (lower) triangular part of A , bsrsv2_bufferSize() returns the maximum size buffer among these combinations. The buffer size depends on the dimensions mb , blockDim , and the number of nonzero blocks of the matrix nnzb . If the user changes the matrix, it is necessary to call bsrsv2_bufferSize() again to have the correct buffer size; otherwise a segmentation fault may occur. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation \\(\\text{op}(A)\\) . mb number of block rows of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A; must be larger than zero. Output info record of internal states based on different algorithms. pBufferSizeInBytes number of bytes of the buffer used in the bsrsv2_analysis() and bsrsv2_solve() . See cusparseStatus_t for the description of the return status. 5.4.4. cusparse<t>bsrsv2_analysis() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrsv2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrsv2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrsv2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsrsv2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the analysis phase of bsrsv2 , a new sparse triangular linear system op(A)*y = \\(\\alpha\\) x . A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand side and the solution vectors; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) The block of BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. It is expected that this function will be executed only once for a given matrix and a particular operation type. This function requires a buffer size returned by bsrsv2_bufferSize() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsrsv2_analysis() reports a structural zero and computes level information, which stored in the opaque structure info . The level information can extract more parallelism for a triangular solver. However bsrsv2_solve() can be done without level information. To disable level information, the user needs to specify the policy of the triangular solver as CUSPARSE_SOLVE_POLICY_NO_LEVEL . Function bsrsv2_analysis() always reports the first structural zero, even when parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL . No structural zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if block A(j,j) is missing for some j . The user needs to call cusparseXbsrsv2_zeroPivot() to know where the structural zero is. It is the user’s choice whether to call bsrsv2_solve() if bsrsv2_analysis() reports a structural zero. In this case, the user can still call bsrsv2_solve() , which will return a numerical zero at the same position as a structural zero. However the result x is meaningless. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation \\(\\text{op}(A)\\) . mb number of block rows of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A, larger than zero. info structure initialized using cusparseCreateBsrsv2Info() . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user, the size is return by bsrsv2_bufferSize() . Output info structure filled with information collected during the analysis phase (that should be passed to the solve phase unchanged). See cusparseStatus_t for the description of the return status. 5.4.5. cusparse<t>bsrsv2_solve() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrsv2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const float * alpha , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , const float * x , float * y , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrsv2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const double * alpha , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , const double * x , double * y , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsrsv2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cuComplex * alpha , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , const cuComplex * x , cuComplex * y , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsrsv2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , int mb , int nnzb , const cuDoubleComplex * alpha , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrsv2Info_t info , const cuDoubleComplex * x , cuDoubleComplex * y , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the solve phase of bsrsv2 , a new sparse triangular linear system op(A)*y = \\(\\alpha\\) x . A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand-side and the solution vectors; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. Function bsrsv02_solve() can support an arbitrary blockDim . This function may be executed multiple times for a given matrix and a particular operation type. This function requires a buffer size returned by bsrsv2_bufferSize() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Although bsrsv2_solve() can be done without level information, the user still needs to be aware of consistency. If bsrsv2_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrsv2_solve() can be run with or without levels. On the other hand, if bsrsv2_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrsv2_solve() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. The level information may not improve the performance, but may spend extra time doing analysis. For example, a tridiagonal matrix has no parallelism. In this case, CUSPARSE_SOLVE_POLICY_NO_LEVEL performs better than CUSPARSE_SOLVE_POLICY_USE_LEVEL . If the user has an iterative solver, the best approach is to do bsrsv2_analysis() with CUSPARSE_SOLVE_POLICY_USE_LEVEL once. Then do bsrsv2_solve() with CUSPARSE_SOLVE_POLICY_NO_LEVEL in the first run, and with CUSPARSE_SOLVE_POLICY_USE_LEVEL in the second run, and pick the fastest one to perform the remaining iterations. Function bsrsv02_solve() has the same behavior as csrsv02_solve() . That is, bsr2csr(bsrsv02(A)) = csrsv02(bsr2csr(A)) . The numerical zero of csrsv02_solve() means there exists some zero A(j,j) . The numerical zero of bsrsv02_solve() means there exists some block A(j,j) that is not invertible. Function bsrsv2_solve() reports the first numerical zero, including a structural zero. No numerical zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if A(j,j) is not invertible for some j . The user needs to call cusparseXbsrsv2_zeroPivot() to know where the numerical zero is. The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture For example, suppose L is a lower triangular matrix with unit diagonal, then the following code solves L*y=x by level information. // Suppose that L is m x m sparse matrix represented by BSR format, // The number of block rows/columns is mb, and // the number of nonzero blocks is nnzb. // L is lower triangular with unit diagonal. // Assumption: // - dimension of matrix L is m(=mb*blockDim), // - matrix L has nnz(=nnzb*blockDim*blockDim) nonzero elements, // - handle is already created by cusparseCreate(), // - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of L on device memory, // - d_x is right hand side vector on device memory. // - d_y is solution vector on device memory. // - d_x and d_y are of size m. cusparseMatDescr_t descr = 0 ; bsrsv2Info_t info = 0 ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1. ; const cusparseSolvePolicy_t policy = CUSPARSE_SOLVE_POLICY_USE_LEVEL ; const cusparseOperation_t trans = CUSPARSE_OPERATION_NON_TRANSPOSE ; const cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; // step 1: create a descriptor which contains // - matrix L is base-1 // - matrix L is lower triangular // - matrix L has unit diagonal, specified by parameter CUSPARSE_DIAG_TYPE_UNIT //   (L may not have all diagonal elements.) cusparseCreateMatDescr ( & descr ); cusparseSetMatIndexBase ( descr , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatFillMode ( descr , CUSPARSE_FILL_MODE_LOWER ); cusparseSetMatDiagType ( descr , CUSPARSE_DIAG_TYPE_UNIT ); // step 2: create a empty info structure cusparseCreateBsrsv2Info ( & info ); // step 3: query how much memory used in bsrsv2, and allocate the buffer cusparseDbsrsv2_bufferSize ( handle , dir , trans , mb , nnzb , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , & pBufferSize ); // pBuffer returned by cudaMalloc is automatically aligned to 128 bytes. cudaMalloc (( void ** ) & pBuffer , pBufferSize ); // step 4: perform analysis cusparseDbsrsv2_analysis ( handle , dir , trans , mb , nnzb , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info , policy , pBuffer ); // L has unit diagonal, so no structural zero is reported. status = cusparseXbsrsv2_zeroPivot ( handle , info , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"L(%d,%d) is missing \\n \" , structural_zero , structural_zero ); } // step 5: solve L*y = x cusparseDbsrsv2_solve ( handle , dir , trans , mb , nnzb , & alpha , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info , d_x , d_y , policy , pBuffer ); // L has unit diagonal, so no numerical zero is reported. status = cusparseXbsrsv2_zeroPivot ( handle , info , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"L(%d,%d) is zero \\n \" , numerical_zero , numerical_zero ); } // step 6: free resources cudaFree ( pBuffer ); cusparseDestroyBsrsv2Info ( info ); cusparseDestroyMatDescr ( descr ); cusparseDestroy ( handle ); Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation \\(\\text{op}(A)\\) . mb number of block rows and block columns of matrix A . alpha <type> scalar used for multiplication. descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A , larger than zero. info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged). x <type> right-hand-side vector of size m . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user, the size is returned by bsrsv2_bufferSize() . Output y <type> solution vector of size m . See cusparseStatus_t for the description of the return status. 5.4.6. cusparseXbsrsv2_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrsv2_zeroPivot ( cusparseHandle_t handle , bsrsv2Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) is either structural zero or numerical zero (singular block). Otherwise position=-1 . The position can be 0-based or 1-based, the same as the matrix. Function cusparseXbsrsv2_zeroPivot() is a blocking call. It calls cudaDeviceSynchronize() to make sure all previous kernels are done. The position can be in the host memory or device memory. The user can set the proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. info info contains a structural zero or numerical zero if the user already called bsrsv2_analysis() or bsrsv2_solve() . Output position if no structural or numerical zero, position is -1; otherwise if A(j,j) is missing or U(j,j) is zero, position=j . See cusparseStatus_t for the description of the return status 5.4.7. cusparse<t>gemvi()  cusparseStatus_t cusparseSgemvi_bufferSize ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , int nnz , int * pBufferSize ) cusparseStatus_t cusparseDgemvi_bufferSize ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , int nnz , int * pBufferSize ) cusparseStatus_t cusparseCgemvi_bufferSize ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , int nnz , int * pBufferSize ) cusparseStatus_t cusparseZgemvi_bufferSize ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , int nnz , int * pBufferSize ) cusparseStatus_t cusparseSgemvi ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , const float * alpha , const float * A , int lda , int nnz , const float * x , const int * xInd , const float * beta , float * y , cusparseIndexBase_t idxBase , void * pBuffer ) cusparseStatus_t cusparseDgemvi ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , const double * alpha , const double * A , int lda , int nnz , const double * x , const int * xInd , const double * beta , double * y , cusparseIndexBase_t idxBase , void * pBuffer ) cusparseStatus_t cusparseCgemvi ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , int nnz , const cuComplex * x , const int * xInd , const cuComplex * beta , cuComplex * y , cusparseIndexBase_t idxBase , void * pBuffer ) cusparseStatus_t cusparseZgemvi ( cusparseHandle_t handle , cusparseOperation_t transA , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , int nnz , const cuDoubleComplex * x , const int * xInd , const cuDoubleComplex * beta , cuDoubleComplex * y , cusparseIndexBase_t idxBase , void * pBuffer ) This function performs the matrix-vector operation \\(\\text{y} = \\alpha \\ast \\text{op}(A) \\ast \\text{x} + \\beta \\ast \\text{y}\\) A is an m×n dense matrix and a sparse vector x that is defined in a sparse storage format by the two arrays xVal, xInd of length nnz , and y is a dense vector; \\(\\alpha\\) and \\(\\beta\\) are scalars; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\\n\\end{cases}\\) To simplify the implementation, we have not (yet) optimized the transpose multiple case. We recommend the following for users interested in this case. Convert the matrix from CSR to CSC format using one of the csr2csc() functions. Notice that by interchanging the rows and columns of the result you are implicitly transposing the matrix. Call the gemvi() function with the cusparseOperation_t parameter set to CUSPARSE_OPERATION_NON_TRANSPOSE and with the interchanged rows and columns of the matrix stored in CSC format. This (implicitly) multiplies the vector by the transpose of the matrix in the original CSR format. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture The function cusparse<t>gemvi_bufferSize() returns the size of buffer used in cusparse<t>gemvi() . Input handle Handle to the cuSPARSE library context. trans The operation \\(\\text{op}(A)\\) . m Number of rows of matrix A . n Number of columns of matrix A . alpha <type> scalar used for multiplication. A The pointer to dense matrix A . lda Size of the leading dimension of A . nnz Number of nonzero elements of vector x . x <type> sparse vector of nnz elements of size n if \\(\\text{op}(A)=A\\) , and size m if \\(\\text{op}(A)=A^{T}\\) . xInd Indices of non-zero values in x . beta <type> scalar used for multiplication. If beta is zero, y does not have to be a valid input. y <type> dense vector of m elements if \\(\\text{op}(A)=A\\) , and n elements if \\(\\text{op}(A)=A^{T}\\) . idxBase 0 or 1, for 0 based or 1 based indexing, respectively. pBufferSize Number of elements needed the buffer used in cusparse<t>gemvi() . pBuffer Working space buffer. Output y <type> updated dense vector. See cusparseStatus_t for the description of the return status. 5.5. cuSPARSE Level 3 Function Reference  This chapter describes sparse linear algebra functions that perform operations between sparse and (usually tall) dense matrices. 5.5.1. cusparse<t>bsrmm()  cusparseStatus_t cusparseSbsrmm ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transB , int mb , int n , int kb , int nnzb , const float * alpha , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const float * B , int ldb , const float * beta , float * C , int ldc ) cusparseStatus_t cusparseDbsrmm ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transB , int mb , int n , int kb , int nnzb , const double * alpha , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const double * B , int ldb , const double * beta , double * C , int ldc ) cusparseStatus_t cusparseCbsrmm ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transB , int mb , int n , int kb , int nnzb , const cuComplex * alpha , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cusparseStatus_t cusparseZbsrmm ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transB , int mb , int n , int kb , int nnzb , const cuDoubleComplex * alpha , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs one of the following matrix-matrix operations: \\(C = \\alpha \\ast \\text{op}(A) \\ast \\text{op}(B) + \\beta \\ast C\\) A is an mb×kb sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ; B and C are dense matrices; \\(\\alpha\\text{~and~}\\beta\\) are scalars; and \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if transA == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if transA == CUSPARSE_OPERATION_TRANSPOSE (not\\ supported)} \\\\\nA^{H} & \\text{if transA == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE (not supported)} \\\\\n\\end{cases}\\) and \\(\\text{op}(B) = \\begin{cases}\nB & \\text{if transB == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nB^{T} & \\text{if transB == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nB^{H} & \\text{if transB == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE (not supported)} \\\\\n\\end{cases}\\) The function has the following limitations: Only CUSPARSE_MATRIX_TYPE_GENERAL matrix type is supported Only blockDim > 1 is supported if blockDim ≤ 4, then max(mb)/max(n) = 524,272 if 4 < blockDim ≤ 8, then max(mb) = 524,272, max(n) = 262,136 if blockDim > 8, then m < 65,535 and max(n) = 262,136 The motivation of transpose(B) is to improve memory access of matrix B . The computational pattern of A*transpose(B) with matrix B in column-major order is equivalent to A*B with matrix B in row-major order. In practice, no operation in an iterative solver or eigenvalue solver uses A*transpose(B) . However, we can perform A*transpose(transpose(B)) which is the same as A*B . For example, suppose A is mb*kb , B is k*n and C is m*n , the following code shows usage of cusparseDbsrmm() . // A is mb*kb, B is k*n and C is m*n const int m = mb * blockSize ; const int k = kb * blockSize ; const int ldb_B = k ; // leading dimension of B const int ldc = m ; // leading dimension of C // perform C:=alpha*A*B + beta*C cusparseSetMatType ( descrA , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseDbsrmm ( cusparse_handle , CUSPARSE_DIRECTION_COLUMN , CUSPARSE_OPERATION_NON_TRANSPOSE , CUSPARSE_OPERATION_NON_TRANSPOSE , mb , n , kb , nnzb , alpha , descrA , bsrValA , bsrRowPtrA , bsrColIndA , blockSize , B , ldb_B , beta , C , ldc ); Instead of using A*B , our proposal is to transpose B to Bt by first calling cublas<t>geam() , and then to perform A*transpose(Bt) . // step 1: Bt := transpose(B) const int m = mb * blockSize ; const int k = kb * blockSize ; double * Bt ; const int ldb_Bt = n ; // leading dimension of Bt cudaMalloc (( void ** ) & Bt , sizeof ( double ) * ldb_Bt * k ); double one = 1.0 ; double zero = 0.0 ; cublasSetPointerMode ( cublas_handle , CUBLAS_POINTER_MODE_HOST ); cublasDgeam ( cublas_handle , CUBLAS_OP_T , CUBLAS_OP_T , n , k , & one , B , int ldb_B , & zero , B , int ldb_B , Bt , ldb_Bt ); // step 2: perform C:=alpha*A*transpose(Bt) + beta*C cusparseDbsrmm ( cusparse_handle , CUSPARSE_DIRECTION_COLUMN , CUSPARSE_OPERATION_NON_TRANSPOSE , CUSPARSE_OPERATION_TRANSPOSE , mb , n , kb , nnzb , alpha , descrA , bsrValA , bsrRowPtrA , bsrColIndA , blockSize , Bt , ldb_Bt , beta , C , ldc ); bsrmm() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation op(A) . transB the operation op(B) . mb number of block rows of sparse matrix A . n number of columns of dense matrix op(B) and A . kb number of block columns of sparse matrix A . nnzb number of non-zero blocks of sparse matrix A . alpha <type> scalar used for multiplication. descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A , larger than zero. B array of dimensions (ldb, n) if op(B)=B and (ldb, k) otherwise. ldb leading dimension of B . If op(B)=B , it must be at least \\(\\max\\text{(1,\\ k)}\\) If op(B) != B , it must be at least max(1, n) . beta <type> scalar used for multiplication. If beta is zero, C does not have to be a valid input. C array of dimensions (ldc, n) . ldc leading dimension of C . It must be at least \\(\\max\\text{(1,\\ m)}\\) if op(A)=A and at least \\(\\max\\text{(1,\\ k)}\\) otherwise. Output C <type> updated array of dimensions (ldc, n) . See cusparseStatus_t for the description of the return status 5.5.2. cusparse<t>bsrsm2_bufferSize() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrsm2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , float * bsrSortedValA , const int * bsrSortedRowPtrA , const int * bsrSortedColIndA , int blockDim , bsrsm2Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseDbsrsm2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , double * bsrSortedValA , const int * bsrSortedRowPtrA , const int * bsrSortedColIndA , int blockDim , bsrsm2Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseCbsrsm2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , cuComplex * bsrSortedValA , const int * bsrSortedRowPtrA , const int * bsrSortedColIndA , int blockDim , bsrsm2Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseZbsrsm2_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , cuDoubleComplex * bsrSortedValA , const int * bsrSortedRowPtrA , const int * bsrSortedColIndA , int blockDim , bsrsm2Info_t info , int * pBufferSizeInBytes ) This function returns size of buffer used in bsrsm2() , a new sparse triangular linear system op(A)*op(X)= \\(\\alpha\\) op(B) . A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X are the right-hand-side and the solution matrices; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) == \\text{CUSPARSE_OPERATION_NON_TRANSPOSE}\\) Although there are six combinations in terms of parameter trans and the upper (and lower) triangular part of A , bsrsm2_bufferSize() returns the maximum size of the buffer among these combinations. The buffer size depends on dimension mb,blockDim and the number of nonzeros of the matrix, nnzb . If the user changes the matrix, it is necessary to call bsrsm2_bufferSize() again to get the correct buffer size, otherwise a segmentation fault may occur. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation op(A) . transX the operation op(X) . mb number of block rows of matrix A . n number of columns of matrix op(B) and op(X) . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A ; larger than zero. Output info record internal states based on different algorithms. pBufferSizeInBytes number of bytes of the buffer used in bsrsm2_analysis() and bsrsm2_solve() . See cusparseStatus_t for the description of the return status 5.5.3. cusparse<t>bsrsm2_analysis() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrsm2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , const float * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrsm2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , const double * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsrsm2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , const cuComplex * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsrsm2_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the analysis phase of bsrsm2() , a new sparse triangular linear system op(A)*op(X) = \\(\\alpha\\) op(B) . A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X are the right-hand-side and the solution matrices; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) == \\text{CUSPARSE_OPERATION_NON_TRANSPOSE}\\) and \\(\\text{op}(X) = \\begin{cases}\nX & \\text{if transX == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nX^{T} & \\text{if transX == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nX^{H} & \\text{if transX == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE (not supported)} \\\\\n\\end{cases}\\) and op(B) and op(X) are equal. The block of BSR format is of size blockDim*blockDim , stored in column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. It is expected that this function will be executed only once for a given matrix and a particular operation type. This function requires the buffer size returned by bsrsm2_bufferSize() . The address of pBuffer must be multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsrsm2_analysis() reports a structural zero and computes the level information stored in opaque structure info . The level information can extract more parallelism during a triangular solver. However bsrsm2_solve() can be done without level information. To disable level information, the user needs to specify the policy of the triangular solver as CUSPARSE_SOLVE_POLICY_NO_LEVEL . Function bsrsm2_analysis() always reports the first structural zero, even if the parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL . Besides, no structural zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if block A(j,j) is missing for some j . The user must call cusparseXbsrsm2_query_zero_pivot() to know where the structural zero is. If bsrsm2_analysis() reports a structural zero, the solve will return a numerical zero in the same position as the structural zero but this result X is meaningless. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation op(A) . transX the operation op(B) and op(X) . mb number of block rows of matrix A . n number of columns of matrix op(B) and op(X) . nnzb number of non-zero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A ; larger than zero. info structure initialized using cusparseCreateBsrsm2Info . policy The supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is return by bsrsm2_bufferSize() . Output info structure filled with information collected during the analysis phase (that should be passed to the solve phase unchanged). See cusparseStatus_t for the description of the return status 5.5.4. cusparse<t>bsrsm2_solve() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrsm2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const float * alpha , const cusparseMatDescr_t descrA , const float * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , const float * B , int ldb , float * X , int ldx , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrsm2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const double * alpha , const cusparseMatDescr_t descrA , const double * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , const double * B , int ldb , double * X , int ldx , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsrsm2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cuComplex * alpha , const cusparseMatDescr_t descrA , const cuComplex * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , const cuComplex * B , int ldb , cuComplex * X , int ldx , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsrsm2_solve ( cusparseHandle_t handle , cusparseDirection_t dirA , cusparseOperation_t transA , cusparseOperation_t transX , int mb , int n , int nnzb , const cuDoubleComplex * alpha , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrSortedVal , const int * bsrSortedRowPtr , const int * bsrSortedColInd , int blockDim , bsrsm2Info_t info , const cuDoubleComplex * B , int ldb , cuDoubleComplex * X , int ldx , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the solve phase of the solution of a sparse triangular linear system: \\(\\text{op}(A) \\ast \\text{op(X)} = \\alpha \\ast \\text{op(B)}\\) A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X are the right-hand-side and the solution matrices; \\(\\alpha\\) is a scalar, and \\(\\text{op}(A) == \\text{CUSPARSE_OPERATION_NON_TRANSPOSE}\\) and \\(\\text{op}(X) = \\begin{cases}\nX & \\text{if transX == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nX^{T} & \\text{if transX == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nX^{H} & \\text{not supported} \\\\\n\\end{cases}\\) Only op(A)=A is supported. op(B) and op(X) must be performed in the same way. In other words, if op(B)=B , op(X)=X . The block of BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. Function bsrsm02_solve() can support an arbitrary blockDim . This function may be executed multiple times for a given matrix and a particular operation type. This function requires the buffer size returned by bsrsm2_bufferSize() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Although bsrsm2_solve() can be done without level information, the user still needs to be aware of consistency. If bsrsm2_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrsm2_solve() can be run with or without levels. On the other hand, if bsrsm2_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrsm2_solve() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsrsm02_solve() has the same behavior as bsrsv02_solve() , reporting the first numerical zero, including a structural zero. The user must call cusparseXbsrsm2_query_zero_pivot() to know where the numerical zero is. The motivation of transpose(X) is to improve the memory access of matrix X . The computational pattern of transpose(X) with matrix X in column-major order is equivalent to X with matrix X in row-major order. In-place is supported and requires that B and X point to the same memory block, and ldb=ldx . The function supports the following properties if pBuffer != NULL : The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . transA the operation op(A) . transX the operation op(B) and op(X) . mb number of block rows of matrix A . n number of columns of matrix op(B) and op(X) . nnzb number of non-zero blocks of matrix A . alpha <type> scalar used for multiplication. descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) non-zero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A ; larger than zero. info structure initialized using cusparseCreateBsrsm2Info() . B <type> right-hand-side array. ldb leading dimension of B . If op(B)=B , ldb >= (mb*blockDim) ; otherwise, ldb >= n . ldx leading dimension of X . If op(X)=X , then ldx >= (mb*blockDim) . otherwise ldx >= n . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is returned by bsrsm2_bufferSize() . Output X <type> solution array with leading dimensions ldx . See cusparseStatus_t for the description of the return status. 5.5.5. cusparseXbsrsm2_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrsm2_zeroPivot ( cusparseHandle_t handle , bsrsm2Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) is either a structural zero or a numerical zero (singular block). Otherwise position=-1 . The position can be 0-base or 1-base, the same as the matrix. Function cusparseXbsrsm2_zeroPivot() is a blocking call. It calls cudaDeviceSynchronize() to make sure all previous kernels are done. The position can be in the host memory or device memory. The user can set the proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. info info contains a structural zero or a numerical zero if the user already called bsrsm2_analysis() or bsrsm2_solve() . Output position if no structural or numerical zero, position is -1; otherwise, if A(j,j) is missing or U(j,j) is zero, position=j . See cusparseStatus_t for the description of the return status. 5.6. cuSPARSE Extra Function Reference  This chapter describes the extra routines used to manipulate sparse matrices. 5.6.1. cusparse<t>csrgeam2()  cusparseStatus_t cusparseScsrgeam2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const float * alpha , const cusparseMatDescr_t descrA , int nnzA , const float * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const float * beta , const cusparseMatDescr_t descrB , int nnzB , const float * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , const float * csrSortedValC , const int * csrSortedRowPtrC , const int * csrSortedColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDcsrgeam2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const double * alpha , const cusparseMatDescr_t descrA , int nnzA , const double * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const double * beta , const cusparseMatDescr_t descrB , int nnzB , const double * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , const double * csrSortedValC , const int * csrSortedRowPtrC , const int * csrSortedColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseCcsrgeam2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const cuComplex * alpha , const cusparseMatDescr_t descrA , int nnzA , const cuComplex * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const cuComplex * beta , const cusparseMatDescr_t descrB , int nnzB , const cuComplex * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , const cuComplex * csrSortedValC , const int * csrSortedRowPtrC , const int * csrSortedColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseZcsrgeam2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cusparseMatDescr_t descrA , int nnzA , const cuDoubleComplex * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const cuDoubleComplex * beta , const cusparseMatDescr_t descrB , int nnzB , const cuDoubleComplex * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , const cuDoubleComplex * csrSortedValC , const int * csrSortedRowPtrC , const int * csrSortedColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcsrgeam2Nnz ( cusparseHandle_t handle , int m , int n , const cusparseMatDescr_t descrA , int nnzA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const cusparseMatDescr_t descrB , int nnzB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , int * csrSortedRowPtrC , int * nnzTotalDevHostPtr , void * workspace ) cusparseStatus_t cusparseScsrgeam2 ( cusparseHandle_t handle , int m , int n , const float * alpha , const cusparseMatDescr_t descrA , int nnzA , const float * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const float * beta , const cusparseMatDescr_t descrB , int nnzB , const float * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , float * csrSortedValC , int * csrSortedRowPtrC , int * csrSortedColIndC , void * pBuffer ) cusparseStatus_t cusparseDcsrgeam2 ( cusparseHandle_t handle , int m , int n , const double * alpha , const cusparseMatDescr_t descrA , int nnzA , const double * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const double * beta , const cusparseMatDescr_t descrB , int nnzB , const double * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , double * csrSortedValC , int * csrSortedRowPtrC , int * csrSortedColIndC , void * pBuffer ) cusparseStatus_t cusparseCcsrgeam2 ( cusparseHandle_t handle , int m , int n , const cuComplex * alpha , const cusparseMatDescr_t descrA , int nnzA , const cuComplex * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const cuComplex * beta , const cusparseMatDescr_t descrB , int nnzB , const cuComplex * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , cuComplex * csrSortedValC , int * csrSortedRowPtrC , int * csrSortedColIndC , void * pBuffer ) cusparseStatus_t cusparseZcsrgeam2 ( cusparseHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cusparseMatDescr_t descrA , int nnzA , const cuDoubleComplex * csrSortedValA , const int * csrSortedRowPtrA , const int * csrSortedColIndA , const cuDoubleComplex * beta , const cusparseMatDescr_t descrB , int nnzB , const cuDoubleComplex * csrSortedValB , const int * csrSortedRowPtrB , const int * csrSortedColIndB , const cusparseMatDescr_t descrC , cuDoubleComplex * csrSortedValC , int * csrSortedRowPtrC , int * csrSortedColIndC , void * pBuffer ) This function performs following matrix-matrix operation \\(C = \\alpha \\ast A + \\beta \\ast B\\) where A , B , and C are m×n sparse matrices (defined in CSR storage format by the three arrays csrValA|csrValB|csrValC , csrRowPtrA|csrRowPtrB|csrRowPtrC , and csrColIndA|csrColIndB|csrcolIndC respectively), and \\(\\alpha\\text{~and~}\\beta\\) are scalars. Since A and B have different sparsity patterns, cuSPARSE adopts a two-step approach to complete sparse matrix C . In the first step, the user allocates csrRowPtrC of m+1 elements and uses function cusparseXcsrgeam2Nnz() to determine csrRowPtrC and the total number of nonzero elements. In the second step, the user gathers nnzC (number of nonzero elements of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC(m)-csrRowPtrC(0)) and allocates csrValC, csrColIndC of nnzC elements respectively, then finally calls function cusparse[S|D|C|Z]csrgeam2() to complete matrix C . The general procedure is as follows: int baseC , nnzC ; /* alpha, nnzTotalDevHostPtr points to host memory */ size_t BufferSizeInBytes ; char * buffer = NULL ; int * nnzTotalDevHostPtr = & nnzC ; cusparseSetPointerMode ( handle , CUSPARSE_POINTER_MODE_HOST ); cudaMalloc (( void ** ) & csrRowPtrC , sizeof ( int ) * ( m + 1 )); /* prepare buffer */ cusparseScsrgeam2_bufferSizeExt ( handle , m , n , alpha , descrA , nnzA , csrValA , csrRowPtrA , csrColIndA , beta , descrB , nnzB , csrValB , csrRowPtrB , csrColIndB , descrC , csrValC , csrRowPtrC , csrColIndC & bufferSizeInBytes ); cudaMalloc (( void ** ) & buffer , sizeof ( char ) * bufferSizeInBytes ); cusparseXcsrgeam2Nnz ( handle , m , n , descrA , nnzA , csrRowPtrA , csrColIndA , descrB , nnzB , csrRowPtrB , csrColIndB , descrC , csrRowPtrC , nnzTotalDevHostPtr , buffer ); if ( NULL != nnzTotalDevHostPtr ){ nnzC = * nnzTotalDevHostPtr ; } else { cudaMemcpy ( & nnzC , csrRowPtrC + m , sizeof ( int ), cudaMemcpyDeviceToHost ); cudaMemcpy ( & baseC , csrRowPtrC , sizeof ( int ), cudaMemcpyDeviceToHost ); nnzC -= baseC ; } cudaMalloc (( void ** ) & csrColIndC , sizeof ( int ) * nnzC ); cudaMalloc (( void ** ) & csrValC , sizeof ( float ) * nnzC ); cusparseScsrgeam2 ( handle , m , n , alpha , descrA , nnzA , csrValA , csrRowPtrA , csrColIndA , beta , descrB , nnzB , csrValB , csrRowPtrB , csrColIndB , descrC , csrValC , csrRowPtrC , csrColIndC buffer ); Several comments on csrgeam2() : The other three combinations, NT, TN, and TT, are not supported by cuSPARSE. In order to do any one of the three, the user should use the routine csr2csc() to convert \\(A\\) | \\(B\\) to \\(A^{T}\\) | \\(B^{T}\\) . Only CUSPARSE_MATRIX_TYPE_GENERAL is supported. If either A or B is symmetric or Hermitian, then the user must extend the matrix to a full one and reconfigure the MatrixType field of the descriptor to CUSPARSE_MATRIX_TYPE_GENERAL . If the sparsity pattern of matrix C is known, the user can skip the call to function cusparseXcsrgeam2Nnz() . For example, suppose that the user has an iterative algorithm which would update A and B iteratively but keep the sparsity patterns. The user can call function cusparseXcsrgeam2Nnz() once to set up the sparsity pattern of C , then call function cusparse[S|D|C|Z]geam() only for each iteration. The pointers alpha and beta must be valid. When alpha or beta is zero, it is not considered a special case by cuSPARSE. The sparsity pattern of C is independent of the value of alpha and beta . If the user wants \\(C = 0 \\times A + 1 \\times B^{T}\\) , then csr2csc() is better than csrgeam2() . csrgeam2() is the same as csrgeam() except csrgeam2() needs explicit buffer where csrgeam() allocates the buffer internally. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. m number of rows of sparse matrix A,B,C . n number of columns of sparse matrix A,B,C . alpha <type> scalar used for multiplication. descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL only. nnzA number of nonzero elements of sparse matrix A . csrValA <type> array of nnzA \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnzA \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . beta <type> scalar used for multiplication. If beta is zero, y does not have to be a valid input. descrB the descriptor of matrix B . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL only. nnzB number of nonzero elements of sparse matrix B . csrValB <type> array of nnzB \\(( =\\) csrRowPtrB(m) \\(-\\) csrRowPtrB(0) \\()\\) nonzero elements of matrix B . csrRowPtrB integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndB integer array of nnzB \\(( =\\) csrRowPtrB(m) \\(-\\) csrRowPtrB(0) \\()\\) column indices of the nonzero elements of matrix B . descrC the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL only. Output csrValC <type> array of nnzC \\(( =\\) csrRowPtrC(m) \\(-\\) csrRowPtrC(0) \\()\\) nonzero elements of matrix C . csrRowPtrC integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndC integer array of nnzC \\(( =\\) csrRowPtrC(m) \\(-\\) csrRowPtrC(0) \\()\\) column indices of the nonzero elements of matrix C . nnzTotalDevHostPtr total number of nonzero elements in device or host memory. It is equal to (csrRowPtrC(m)-csrRowPtrC(0)) . See cusparseStatus_t for the description of the return status 5.7. cuSPARSE Preconditioners Reference  This chapter describes the routines that implement different preconditioners. 5.7.1. Incomplete Cholesky Factorization: level 0 [DEPRECATED]  Different algorithms for ic0 are discussed in this section. 5.7.1.1. cusparse<t>csric02_bufferSize() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsric02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , float * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseDcsric02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , double * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseCcsric02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseZcsric02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , int * pBufferSizeInBytes ) This function returns size of buffer used in computing the incomplete-Cholesky factorization with \\(0\\) fill-in and no pivoting: \\(A \\approx LL^{H}\\) A is an m×m sparse matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA . The buffer size depends on dimension m and nnz , the number of nonzeros of the matrix. If the user changes the matrix, it is necessary to call csric02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m number of rows and columns of matrix A . nnz number of nonzeros of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . Output info record internal states based on different algorithms pBufferSizeInBytes number of bytes of the buffer used in csric02_analysis() and csric02() See cusparseStatus_t for the description of the return status. 5.7.1.2. cusparse<t>csric02_analysis() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsric02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDcsric02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCcsric02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZcsric02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the analysis phase of the incomplete-Cholesky factorization with \\(0\\) fill-in and no pivoting: \\(A \\approx LL^{H}\\) A is an m×m sparse matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA . This function requires a buffer size returned by csric02_bufferSize() . The address of pBuffer must be multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. Function csric02_analysis() reports a structural zero and computes level information stored in the opaque structure info . The level information can extract more parallelism during incomplete Cholesky factorization. However csric02() can be done without level information. To disable level information, the user must specify the policy of csric02_analysis() and csric02() as CUSPARSE_SOLVE_POLICY_NO_LEVEL . Function csric02_analysis() always reports the first structural zero, even if the policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL . The user needs to call cusparseXcsric02_zeroPivot() to know where the structural zero is. It is the user’s choice whether to call csric02() if csric02_analysis() reports a structural zero. In this case, the user can still call csric02() , which will return a numerical zero at the same position as the structural zero. However the result is meaningless. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. m number of rows and columns of matrix A . nnz number of nonzeros of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . info structure initialized using cusparseCreateCsric02Info() . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is returned by csric02_bufferSize() . Output info number of bytes of the buffer used in csric02_analysis() and csric02() See cusparseStatus_t for the description of the return status. 5.7.1.3. cusparse<t>csric02() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsric02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , float * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDcsric02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , double * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCcsric02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuComplex * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZcsric02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuDoubleComplex * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the solve phase of the computing the incomplete-Cholesky factorization with \\(0\\) fill-in and no pivoting: \\(A \\approx LL^{H}\\) This function requires a buffer size returned by csric02_bufferSize() . The address of pBuffer must be a multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. Although csric02() can be done without level information, the user still needs to be aware of consistency. If csric02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , csric02() can be run with or without levels. On the other hand, if csric02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , csric02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Function csric02() reports the first numerical zero, including a structural zero. The user must call cusparseXcsric02_zeroPivot() to know where the numerical zero is. Function csric02() only takes the lower triangular part of matrix A to perform factorization. The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , the fill mode and diagonal type are ignored, and the strictly upper triangular part is ignored and never touched. It does not matter if A is Hermitian or not. In other words, from the point of view of csric02() A is Hermitian and only the lower triangular part is provided. Note In practice, a positive definite matrix may not have incomplete cholesky factorization. To the best of our knowledge, only matrix M can guarantee the existence of incomplete cholesky factorization. If csric02() failed cholesky factorization and reported a numerical zero, it is possible that incomplete cholesky factorization does not exist. For example, suppose A is a real m × m matrix, the following code solves the precondition system M*y = x where M is the product of Cholesky factorization L and its transpose. \\(M = LL^{H}\\) // Suppose that A is m x m sparse matrix represented by CSR format, // Assumption: // - handle is already created by cusparseCreate(), // - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory, // - d_x is right hand side vector on device memory, // - d_y is solution vector on device memory. // - d_z is intermediate result on device memory. cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; csric02Info_t info_M = 0 ; csrsv2Info_t info_L = 0 ; csrsv2Info_t info_Lt = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_Lt ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1. ; const cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_Lt = CUSPARSE_SOLVE_POLICY_USE_LEVEL ; const cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE ; const cusparseOperation_t trans_Lt = CUSPARSE_OPERATION_TRANSPOSE ; // step 1: create a descriptor which contains // - matrix M is base-1 // - matrix L is base-1 // - matrix L is lower triangular // - matrix L has non-unit diagonal cusparseCreateMatDescr ( & descr_M ); cusparseSetMatIndexBase ( descr_M , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_M , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseCreateMatDescr ( & descr_L ); cusparseSetMatIndexBase ( descr_L , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_L , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseSetMatFillMode ( descr_L , CUSPARSE_FILL_MODE_LOWER ); cusparseSetMatDiagType ( descr_L , CUSPARSE_DIAG_TYPE_NON_UNIT ); // step 2: create a empty info structure // we need one info for csric02 and two info's for csrsv2 cusparseCreateCsric02Info ( & info_M ); cusparseCreateCsrsv2Info ( & info_L ); cusparseCreateCsrsv2Info ( & info_Lt ); // step 3: query how much memory used in csric02 and csrsv2, and allocate the buffer cusparseDcsric02_bufferSize ( handle , m , nnz , descr_M , d_csrVal , d_csrRowPtr , d_csrColInd , info_M , & bufferSize_M ); cusparseDcsrsv2_bufferSize ( handle , trans_L , m , nnz , descr_L , d_csrVal , d_csrRowPtr , d_csrColInd , info_L , & pBufferSize_L ); cusparseDcsrsv2_bufferSize ( handle , trans_Lt , m , nnz , descr_L , d_csrVal , d_csrRowPtr , d_csrColInd , info_Lt , & pBufferSize_Lt ); pBufferSize = max ( bufferSize_M , max ( pBufferSize_L , pBufferSize_Lt )); // pBuffer returned by cudaMalloc is automatically aligned to 128 bytes. cudaMalloc (( void ** ) & pBuffer , pBufferSize ); // step 4: perform analysis of incomplete Cholesky on M //         perform analysis of triangular solve on L //         perform analysis of triangular solve on L' // The lower triangular part of M has the same sparsity pattern as L, so // we can do analysis of csric02 and csrsv2 simultaneously. cusparseDcsric02_analysis ( handle , m , nnz , descr_M , d_csrVal , d_csrRowPtr , d_csrColInd , info_M , policy_M , pBuffer ); status = cusparseXcsric02_zeroPivot ( handle , info_M , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"A(%d,%d) is missing \\n \" , structural_zero , structural_zero ); } cusparseDcsrsv2_analysis ( handle , trans_L , m , nnz , descr_L , d_csrVal , d_csrRowPtr , d_csrColInd , info_L , policy_L , pBuffer ); cusparseDcsrsv2_analysis ( handle , trans_Lt , m , nnz , descr_L , d_csrVal , d_csrRowPtr , d_csrColInd , info_Lt , policy_Lt , pBuffer ); // step 5: M = L * L' cusparseDcsric02 ( handle , m , nnz , descr_M , d_csrVal , d_csrRowPtr , d_csrColInd , info_M , policy_M , pBuffer ); status = cusparseXcsric02_zeroPivot ( handle , info_M , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"L(%d,%d) is zero \\n \" , numerical_zero , numerical_zero ); } // step 6: solve L*z = x cusparseDcsrsv2_solve ( handle , trans_L , m , nnz , & alpha , descr_L , // replace with cusparseSpSV d_csrVal , d_csrRowPtr , d_csrColInd , info_L , d_x , d_z , policy_L , pBuffer ); // step 7: solve L'*y = z cusparseDcsrsv2_solve ( handle , trans_Lt , m , nnz , & alpha , descr_L , // replace with cusparseSpSV d_csrVal , d_csrRowPtr , d_csrColInd , info_Lt , d_z , d_y , policy_Lt , pBuffer ); // step 6: free resources cudaFree ( pBuffer ); cusparseDestroyMatDescr ( descr_M ); cusparseDestroyMatDescr ( descr_L ); cusparseDestroyCsric02Info ( info_M ); cusparseDestroyCsrsv2Info ( info_L ); cusparseDestroyCsrsv2Info ( info_Lt ); cusparseDestroy ( handle ); The function supports the following properties if pBuffer != NULL This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. m number of rows and columns of matrix A . nnz number of nonzeros of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA_valM <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged). policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is returned by csric02_bufferSize() . Output csrValA_valM <type> matrix containing the incomplete-Cholesky lower triangular factor. See cusparseStatus_t for the description of the return status. 5.7.1.4. cusparseXcsric02_zeroPivot()  [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXcsric02_zeroPivot ( cusparseHandle_t handle , csric02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero; otherwise, position=-1 . The position can be 0-based or 1-based, the same as the matrix. Function cusparseXcsric02_zeroPivot() is a blocking call. It calls cudaDeviceSynchronize() to make sure all previous kernels are done. The position can be in the host memory or device memory. The user can set proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. info info contains structural zero or numerical zero if the user already called csric02_analysis() or csric02() . Output position if no structural or numerical zero, position is -1; otherwise, if A(j,j) is missing or L(j,j) is zero, position=j . See cusparseStatus_t for the description of the return status. 5.7.1.5. cusparse<t>bsric02_bufferSize() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsric02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseDbsric02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseCbsric02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseZbsric02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , int * pBufferSizeInBytes ) This function returns the size of a buffer used in computing the incomplete-Cholesky factorization with 0 fill-in and no pivoting \\(A \\approx LL^{H}\\) A is an (mb*blockDim)*(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA . The buffer size depends on the dimensions of mb , blockDim , and the number of nonzero blocks of the matrix nnzb . If the user changes the matrix, it is necessary to call bsric02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows and block columns of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A, larger than zero. Output info record internal states based on different algorithms. pBufferSizeInBytes number of bytes of the buffer used in bsric02_analysis() and bsric02() . See cusparseStatus_t for the description of the return status. 5.7.1.6. cusparse<t>bsric02_analysis() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsric02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsric02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsric02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsric02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the analysis phase of the incomplete-Cholesky factorization with 0 fill-in and no pivoting \\(A \\approx LL^{H}\\) A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA . The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. This function requires a buffer size returned by bsric02_bufferSize90 . The address of pBuffer must be a multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsric02_analysis() reports structural zero and computes level information stored in the opaque structure info . The level information can extract more parallelism during incomplete Cholesky factorization. However bsric02() can be done without level information. To disable level information, the user needs to specify the parameter policy of bsric02[_analysis| ] as CUSPARSE_SOLVE_POLICY_NO_LEVEL . Function bsric02_analysis always reports the first structural zero, even when parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL . The user must call cusparseXbsric02_zeroPivot() to know where the structural zero is. It is the user’s choice whether to call bsric02() if bsric02_analysis() reports a structural zero. In this case, the user can still call bsric02() , which returns a numerical zero in the same position as the structural zero. However the result is meaningless. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows and block columns of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A; must be larger than zero. info structure initialized using cusparseCreateBsric02Info() . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is returned by bsric02_bufferSize() . Output info Structure filled with information collected during the analysis phase (that should be passed to the solve phase unchanged). See cusparseStatus_t for the description of the return status. 5.7.1.7. cusparse<t>bsric02() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsric02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsric02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsric02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsric02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsric02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the solve phase of the incomplete-Cholesky factorization with 0 fill-in and no pivoting \\(A \\approx LL^{H}\\) A is an (mb*blockDim)×(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA . The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. This function requires a buffer size returned by bsric02_bufferSize() . The address of pBuffer must be a multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Although bsric02() can be done without level information, the user must be aware of consistency. If bsric02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsric02() can be run with or without levels. On the other hand, if bsric02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsric02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsric02() has the same behavior as csric02() . That is, bsr2csr(bsric02(A)) = csric02(bsr2csr(A)) . The numerical zero of csric02() means there exists some zero L(j,j) . The numerical zero of bsric02() means there exists some block Lj,j) that is not invertible. Function bsric02 reports the first numerical zero, including a structural zero. The user must call cusparseXbsric02_zeroPivot() to know where the numerical zero is. The bsric02() function only takes the lower triangular part of matrix A to perform factorization. The strictly upper triangular part is ignored and never touched. It does not matter if A is Hermitian or not. In other words, from the point of view of bsric02() , A is Hermitian and only the lower triangular part is provided. Moreover, the imaginary part of diagonal elements of diagonal blocks is ignored. For example, suppose A is a real m-by-m matrix, where m=mb*blockDim . The following code solves precondition system M*y = x , where M is the product of Cholesky factorization L and its transpose. \\(M = LL^{H}\\) // Suppose that A is m x m sparse matrix represented by BSR format, // The number of block rows/columns is mb, and // the number of nonzero blocks is nnzb. // Assumption: // - handle is already created by cusparseCreate(), // - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory, // - d_x is right hand side vector on device memory, // - d_y is solution vector on device memory. // - d_z is intermediate result on device memory. // - d_x, d_y and d_z are of size m. cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; bsric02Info_t info_M = 0 ; bsrsv2Info_t info_L = 0 ; bsrsv2Info_t info_Lt = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_Lt ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1. ; const cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_Lt = CUSPARSE_SOLVE_POLICY_USE_LEVEL ; const cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE ; const cusparseOperation_t trans_Lt = CUSPARSE_OPERATION_TRANSPOSE ; const cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; // step 1: create a descriptor which contains // - matrix M is base-1 // - matrix L is base-1 // - matrix L is lower triangular // - matrix L has non-unit diagonal cusparseCreateMatDescr ( & descr_M ); cusparseSetMatIndexBase ( descr_M , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_M , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseCreateMatDescr ( & descr_L ); cusparseSetMatIndexBase ( descr_L , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_L , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseSetMatFillMode ( descr_L , CUSPARSE_FILL_MODE_LOWER ); cusparseSetMatDiagType ( descr_L , CUSPARSE_DIAG_TYPE_NON_UNIT ); // step 2: create a empty info structure // we need one info for bsric02 and two info's for bsrsv2 cusparseCreateBsric02Info ( & info_M ); cusparseCreateBsrsv2Info ( & info_L ); cusparseCreateBsrsv2Info ( & info_Lt ); // step 3: query how much memory used in bsric02 and bsrsv2, and allocate the buffer cusparseDbsric02_bufferSize ( handle , dir , mb , nnzb , descr_M , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_M , & bufferSize_M ); cusparseDbsrsv2_bufferSize ( handle , dir , trans_L , mb , nnzb , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_L , & pBufferSize_L ); cusparseDbsrsv2_bufferSize ( handle , dir , trans_Lt , mb , nnzb , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_Lt , & pBufferSize_Lt ); pBufferSize = max ( bufferSize_M , max ( pBufferSize_L , pBufferSize_Lt )); // pBuffer returned by cudaMalloc is automatically aligned to 128 bytes. cudaMalloc (( void ** ) & pBuffer , pBufferSize ); // step 4: perform analysis of incomplete Cholesky on M //         perform analysis of triangular solve on L //         perform analysis of triangular solve on L' // The lower triangular part of M has the same sparsity pattern as L, so // we can do analysis of bsric02 and bsrsv2 simultaneously. cusparseDbsric02_analysis ( handle , dir , mb , nnzb , descr_M , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_M , policy_M , pBuffer ); status = cusparseXbsric02_zeroPivot ( handle , info_M , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"A(%d,%d) is missing \\n \" , structural_zero , structural_zero ); } cusparseDbsrsv2_analysis ( handle , dir , trans_L , mb , nnzb , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_L , policy_L , pBuffer ); cusparseDbsrsv2_analysis ( handle , dir , trans_Lt , mb , nnzb , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_Lt , policy_Lt , pBuffer ); // step 5: M = L * L' cusparseDbsric02_solve ( handle , dir , mb , nnzb , descr_M , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_M , policy_M , pBuffer ); status = cusparseXbsric02_zeroPivot ( handle , info_M , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"L(%d,%d) is not positive definite \\n \" , numerical_zero , numerical_zero ); } // step 6: solve L*z = x cusparseDbsrsv2_solve ( handle , dir , trans_L , mb , nnzb , & alpha , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_L , d_x , d_z , policy_L , pBuffer ); // step 7: solve L'*y = z cusparseDbsrsv2_solve ( handle , dir , trans_Lt , mb , nnzb , & alpha , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_Lt , d_z , d_y , policy_Lt , pBuffer ); // step 6: free resources cudaFree ( pBuffer ); cusparseDestroyMatDescr ( descr_M ); cusparseDestroyMatDescr ( descr_L ); cusparseDestroyBsric02Info ( info_M ); cusparseDestroyBsrsv2Info ( info_L ); cusparseDestroyBsrsv2Info ( info_Lt ); cusparseDestroy ( handle ); The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows and block columns of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A, larger than zero. info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged). policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user, the size is returned by bsric02_bufferSize() . Output bsrValA <type> matrix containing the incomplete-Cholesky lower triangular factor. See cusparseStatus_t for the description of the return status. 5.7.1.8. cusparseXbsric02_zeroPivot()  [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsric02_zeroPivot ( cusparseHandle_t handle , bsric02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero (the block is not positive definite). Otherwise position=-1 . The position can be 0-based or 1-based, the same as the matrix. Function cusparseXbsric02_zeroPivot() is a blocking call. It calls cudaDeviceSynchronize() to make sure all previous kernels are done. The position can be in the host memory or device memory. The user can set the proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. info info contains a structural zero or a numerical zero if the user already called bsric02_analysis() or bsric02() . Output position If no structural or numerical zero, position is -1, otherwise if A(j,j) is missing or L(j,j) is not positive definite, position=j . See cusparseStatus_t for the description of the return status. 5.7.2. Incomplete LU Factorization: level 0 [DEPRECATED]  Different algorithms for ilu0 are discussed in this section. 5.7.2.1. cusparse<t>csrilu02_numericBoost() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , float * boost_val ) cusparseStatus_t cusparseDcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , double * boost_val ) cusparseStatus_t cusparseCcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , cuComplex * boost_val ) cusparseStatus_t cusparseZcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , cuDoubleComplex * boost_val ) The user can use a boost value to replace a numerical value in incomplete LU factorization. The tol is used to determine a numerical zero, and the boost_val is used to replace a numerical zero. The behavior is if tol >= fabs(A(j,j)) , then A(j,j)=boost_val . To enable a boost value, the user has to set parameter enable_boost to 1 before calling csrilu02() . To disable a boost value, the user can call csrilu02_numericBoost() again with parameter enable_boost=0 . If enable_boost=0 , tol and boost_val are ignored. Both tol and boost_val can be in the host memory or device memory. The user can set the proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context info structure initialized using cusparseCreateCsrilu02Info() enable_boost disable boost by enable_boost=0 ; otherwise, boost is enabled tol tolerance to determine a numerical zero boost_val boost value to replace a numerical zero See cusparseStatus_t for the description of the return status. 5.7.2.2. cusparse<t>csrilu02_bufferSize() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrilu02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , float * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseDcsrilu02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , double * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseCcsrilu02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , int * pBufferSizeInBytes ) cusparseStatus_t cusparseZcsrilu02_bufferSize ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , int * pBufferSizeInBytes ) This function returns size of the buffer used in computing the incomplete-LU factorization with \\(0\\) fill-in and no pivoting: \\(A \\approx LU\\) A is an m×m sparse matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA . The buffer size depends on the dimension m and nnz , the number of nonzeros of the matrix. If the user changes the matrix, it is necessary to call csrilu02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m number of rows and columns of matrix A . nnz number of nonzeros of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . Output info record internal states based on different algorithms pBufferSizeInBytes number of bytes of the buffer used in csrilu02_analysis() and csrilu02() See cusparseStatus_t for the description of the return status. 5.7.2.3. cusparse<t>csrilu02_analysis() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrilu02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDcsrilu02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCcsrilu02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZcsrilu02_analysis ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the analysis phase of the incomplete-LU factorization with \\(0\\) fill-in and no pivoting: \\(A \\approx LU\\) A is an m×m sparse matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA . This function requires the buffer size returned by csrilu02_bufferSize() . The address of pBuffer must be a multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. Function csrilu02_analysis() reports a structural zero and computes level information stored in the opaque structure info . The level information can extract more parallelism during incomplete LU factorization; however csrilu02() can be done without level information. To disable level information, the user must specify the policy of csrilu02() as CUSPARSE_SOLVE_POLICY_NO_LEVEL . It is the user’s choice whether to call csrilu02() if csrilu02_analysis() reports a structural zero. In this case, the user can still call csrilu02() , which will return a numerical zero at the same position as the structural zero. However the result is meaningless. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. m number of rows and columns of matrix A . nnz number of nonzeros of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . info structure initialized using cusparseCreateCsrilu02Info() . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user, the size is returned by csrilu02_bufferSize() . Output info Structure filled with information collected during the analysis phase (that should be passed to the solve phase unchanged). See cusparseStatus_t for the description of the return status. 5.7.2.4. cusparse<t>csrilu02() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrilu02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , float * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDcsrilu02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , double * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCcsrilu02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuComplex * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZcsrilu02 ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , cuDoubleComplex * csrValA_valM , const int * csrRowPtrA , const int * csrColIndA , csrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the solve phase of the incomplete-LU factorization with \\(0\\) fill-in and no pivoting: \\(A \\approx LU\\) A is an m×m sparse matrix that is defined in CSR storage format by the three arrays csrValA_valM , csrRowPtrA , and csrColIndA . This function requires a buffer size returned by csrilu02_bufferSize() . The address of pBuffer must be a multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL . The fill mode and diagonal type are ignored. Although csrilu02() can be done without level information, the user still needs to be aware of consistency. If csrilu02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , csrilu02() can be run with or without levels. On the other hand, if csrilu02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , csrilu02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Function csrilu02() reports the first numerical zero, including a structural zero. The user must call cusparseXcsrilu02_zeroPivot() to know where the numerical zero is. For example, suppose A is a real m × m matrix, the following code solves precondition system M*y = x where M is the product of LU factors L and U . // Suppose that A is m x m sparse matrix represented by CSR format, // Assumption: // - handle is already created by cusparseCreate(), // - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory, // - d_x is right hand side vector on device memory, // - d_y is solution vector on device memory. // - d_z is intermediate result on device memory. cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; cusparseMatDescr_t descr_U = 0 ; csrilu02Info_t info_M = 0 ; csrsv2Info_t info_L = 0 ; csrsv2Info_t info_U = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_U ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1. ; const cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_U = CUSPARSE_SOLVE_POLICY_USE_LEVEL ; const cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE ; const cusparseOperation_t trans_U = CUSPARSE_OPERATION_NON_TRANSPOSE ; // step 1: create a descriptor which contains // - matrix M is base-1 // - matrix L is base-1 // - matrix L is lower triangular // - matrix L has unit diagonal // - matrix U is base-1 // - matrix U is upper triangular // - matrix U has non-unit diagonal cusparseCreateMatDescr ( & descr_M ); cusparseSetMatIndexBase ( descr_M , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_M , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseCreateMatDescr ( & descr_L ); cusparseSetMatIndexBase ( descr_L , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_L , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseSetMatFillMode ( descr_L , CUSPARSE_FILL_MODE_LOWER ); cusparseSetMatDiagType ( descr_L , CUSPARSE_DIAG_TYPE_UNIT ); cusparseCreateMatDescr ( & descr_U ); cusparseSetMatIndexBase ( descr_U , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_U , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseSetMatFillMode ( descr_U , CUSPARSE_FILL_MODE_UPPER ); cusparseSetMatDiagType ( descr_U , CUSPARSE_DIAG_TYPE_NON_UNIT ); // step 2: create a empty info structure // we need one info for csrilu02 and two info's for csrsv2 cusparseCreateCsrilu02Info ( & info_M ); cusparseCreateCsrsv2Info ( & info_L ); cusparseCreateCsrsv2Info ( & info_U ); // step 3: query how much memory used in csrilu02 and csrsv2, and allocate the buffer cusparseDcsrilu02_bufferSize ( handle , m , nnz , descr_M , d_csrVal , d_csrRowPtr , d_csrColInd , info_M , & pBufferSize_M ); cusparseDcsrsv2_bufferSize ( handle , trans_L , m , nnz , descr_L , d_csrVal , d_csrRowPtr , d_csrColInd , info_L , & pBufferSize_L ); cusparseDcsrsv2_bufferSize ( handle , trans_U , m , nnz , descr_U , d_csrVal , d_csrRowPtr , d_csrColInd , info_U , & pBufferSize_U ); pBufferSize = max ( pBufferSize_M , max ( pBufferSize_L , pBufferSize_U )); // pBuffer returned by cudaMalloc is automatically aligned to 128 bytes. cudaMalloc (( void ** ) & pBuffer , pBufferSize ); // step 4: perform analysis of incomplete Cholesky on M //         perform analysis of triangular solve on L //         perform analysis of triangular solve on U // The lower(upper) triangular part of M has the same sparsity pattern as L(U), // we can do analysis of csrilu0 and csrsv2 simultaneously. cusparseDcsrilu02_analysis ( handle , m , nnz , descr_M , d_csrVal , d_csrRowPtr , d_csrColInd , info_M , policy_M , pBuffer ); status = cusparseXcsrilu02_zeroPivot ( handle , info_M , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"A(%d,%d) is missing \\n \" , structural_zero , structural_zero ); } cusparseDcsrsv2_analysis ( handle , trans_L , m , nnz , descr_L , d_csrVal , d_csrRowPtr , d_csrColInd , info_L , policy_L , pBuffer ); cusparseDcsrsv2_analysis ( handle , trans_U , m , nnz , descr_U , d_csrVal , d_csrRowPtr , d_csrColInd , info_U , policy_U , pBuffer ); // step 5: M = L * U cusparseDcsrilu02 ( handle , m , nnz , descr_M , d_csrVal , d_csrRowPtr , d_csrColInd , info_M , policy_M , pBuffer ); status = cusparseXcsrilu02_zeroPivot ( handle , info_M , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"U(%d,%d) is zero \\n \" , numerical_zero , numerical_zero ); } // step 6: solve L*z = x cusparseDcsrsv2_solve ( handle , trans_L , m , nnz , & alpha , descr_L , // replace with cusparseSpSV d_csrVal , d_csrRowPtr , d_csrColInd , info_L , d_x , d_z , policy_L , pBuffer ); // step 7: solve U*y = z cusparseDcsrsv2_solve ( handle , trans_U , m , nnz , & alpha , descr_U , // replace with cusparseSpSV d_csrVal , d_csrRowPtr , d_csrColInd , info_U , d_z , d_y , policy_U , pBuffer ); // step 6: free resources cudaFree ( pBuffer ); cusparseDestroyMatDescr ( descr_M ); cusparseDestroyMatDescr ( descr_L ); cusparseDestroyMatDescr ( descr_U ); cusparseDestroyCsrilu02Info ( info_M ); cusparseDestroyCsrsv2Info ( info_L ); cusparseDestroyCsrsv2Info ( info_U ); cusparseDestroy ( handle ); The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m number of rows and columns of matrix A . nnz number of nonzeros of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA_valM <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged). policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is returned by csrilu02_bufferSize() . Output csrValA_valM <type> matrix containing the incomplete-LU lower and upper triangular factors. See cusparseStatus_t for the description of the return status. 5.7.2.5. cusparseXcsrilu02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXcsrilu02_zeroPivot ( cusparseHandle_t handle , csrilu02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero; otherwise, position=-1 . The position can be 0-based or 1-based, the same as the matrix. Function cusparseXcsrilu02_zeroPivot() is a blocking call. It calls cudaDeviceSynchronize( ) to make sure all previous kernels are done. The position can be in the host memory or device memory. The user can set proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle Handle to the cuSPARSE library context. info info contains structural zero or numerical zero if the user already called csrilu02_analysis() or csrilu02() . Output position If no structural or numerical zero, position is -1; otherwise if A(j,j) is missing or U(j,j) is zero, position=j . See cusparseStatus_t for the description of the return status. 5.7.2.6. cusparse<t>bsrilu02_numericBoost() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrilu02_numericBoost ( cusparseHandle_t handle , bsrilu02Info_t info , int enable_boost , double * tol , float * boost_val ) cusparseStatus_t cusparseDbsrilu02_numericBoost ( cusparseHandle_t handle , bsrilu02Info_t info , int enable_boost , double * tol , double * boost_val ) cusparseStatus_t cusparseCbsrilu02_numericBoost ( cusparseHandle_t handle , bsrilu02Info_t info , int enable_boost , double * tol , cuComplex * boost_val ) cusparseStatus_t cusparseZbsrilu02_numericBoost ( cusparseHandle_t handle , bsrilu02Info_t info , int enable_boost , double * tol , cuDoubleComplex * boost_val ) The user can use a boost value to replace a numerical value in incomplete LU factorization. Parameter tol is used to determine a numerical zero, and boost_val is used to replace a numerical zero. The behavior is as follows: if tol >= fabs(A(j,j)) , then reset each diagonal element of block A(j,j) by boost_val . To enable a boost value, the user sets parameter enable_boost to 1 before calling bsrilu02() . To disable the boost value, the user can call bsrilu02_numericBoost() with parameter enable_boost=0 . If enable_boost=0 , tol and boost_val are ignored. Both tol and boost_val can be in host memory or device memory. The user can set the proper mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. info structure initialized using cusparseCreateBsrilu02Info() . enable_boost disable boost by setting enable_boost=0 . Otherwise, boost is enabled. tol tolerance to determine a numerical zero. boost_val boost value to replace a numerical zero. See cusparseStatus_t for the description of the return status. 5.7.2.7. cusparse<t>bsrilu02_bufferSize() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrilu02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , int * pBufferSizeInBytes ); cusparseStatus_t cusparseDbsrilu02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , int * pBufferSizeInBytes ); cusparseStatus_t cusparseCbsrilu02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , int * pBufferSizeInBytes ); cusparseStatus_t cusparseZbsrilu02_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , int * pBufferSizeInBytes ); This function returns the size of the buffer used in computing the incomplete-LU factorization with 0 fill-in and no pivoting. \\(A \\approx LU\\) A is an (mb*blockDim)*(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA . The buffer size depends on the dimensions of mb , blockDim , and the number of nonzero blocks of the matrix nnzb . If the user changes the matrix, it is necessary to call bsrilu02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur. Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows and columns of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A, larger than zero. Output info record internal states based on different algorithms. pBufferSizeInBytes number of bytes of the buffer used in bsrilu02_analysis() and bsrilu02() . Status Returned CUSPARSE_STATUS_SUCCESS the operation completed successfully. CUSPARSE_STATUS_NOT_INITIALIZED the library was not initialized. CUSPARSE_STATUS_ALLOC_FAILED the resources could not be allocated. CUSPARSE_STATUS_INVALID_VALUE invalid parameters were passed ( mb,nnzb<=0 ), base index is not 0 or 1. CUSPARSE_STATUS_ARCH_MISMATCH the device only supports compute capability 2.0 and above. CUSPARSE_STATUS_INTERNAL_ERROR an internal operation failed. CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED the matrix type is not supported. 5.7.2.8. cusparse<t>bsrilu02_analysis() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrilu02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrilu02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsrilu02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsrilu02_analysis ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descrA , cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the analysis phase of the incomplete-LU factorization with 0 fill-in and no pivoting. \\(A \\approx LU\\) A is an (mb*blockDim)×(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA . The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. This function requires a buffer size returned by bsrilu02_bufferSize() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsrilu02_analysis() reports a structural zero and computes level information stored in the opaque structure info . The level information can extract more parallelism during incomplete LU factorization. However bsrilu02() can be done without level information. To disable level information, the user needs to specify the parameter policy of bsrilu02[_analysis| ] as CUSPARSE_SOLVE_POLICY_NO_LEVEL . Function bsrilu02_analysis() always reports the first structural zero, even with parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL . The user must call cusparseXbsrilu02_zeroPivot() to know where the structural zero is. It is the user’s choice whether to call bsrilu02() if bsrilu02_analysis() reports a structural zero. In this case, the user can still call bsrilu02() , which will return a numerical zero at the same position as the structural zero. However the result is meaningless. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows and block columns of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A, larger than zero. info structure initialized using cusparseCreateBsrilu02Info() . policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user, the size is returned by bsrilu02_bufferSize() . Output info structure filled with information collected during the analysis phase (that should be passed to the solve phase unchanged) See cusparseStatus_t for the description of the return status. 5.7.2.9. cusparse<t>bsrilu02() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSbsrilu02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descry , float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseDbsrilu02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descry , double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseCbsrilu02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descry , cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) cusparseStatus_t cusparseZbsrilu02 ( cusparseHandle_t handle , cusparseDirection_t dirA , int mb , int nnzb , const cusparseMatDescr_t descry , cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , bsrilu02Info_t info , cusparseSolvePolicy_t policy , void * pBuffer ) This function performs the solve phase of the incomplete-LU factorization with 0 fill-in and no pivoting. \\(A \\approx LU\\) A is an (mb*blockDim)×(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA . The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW . The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored. Function bsrilu02() supports an arbitrary blockDim . This function requires a buffer size returned by bsrilu02_bufferSize() . The address of pBuffer must be a multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. Although bsrilu02() can be used without level information, the user must be aware of consistency. If bsrilu02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrilu02() can be run with or without levels. On the other hand, if bsrilu02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrilu02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Function bsrilu02() has the same behavior as csrilu02() . That is, bsr2csr(bsrilu02(A)) = csrilu02(bsr2csr(A)) . The numerical zero of csrilu02() means there exists some zero U(j,j) . The numerical zero of bsrilu02() means there exists some block U(j,j) that is not invertible. Function bsrilu02 reports the first numerical zero, including a structural zero. The user must call cusparseXbsrilu02_zeroPivot() to know where the numerical zero is. For example, suppose A is a real m-by-m matrix where m=mb*blockDim . The following code solves precondition system M*y = x , where M is the product of LU factors L and U . // Suppose that A is m x m sparse matrix represented by BSR format, // The number of block rows/columns is mb, and // the number of nonzero blocks is nnzb. // Assumption: // - handle is already created by cusparseCreate(), // - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory, // - d_x is right hand side vector on device memory. // - d_y is solution vector on device memory. // - d_z is intermediate result on device memory. // - d_x, d_y and d_z are of size m. cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; cusparseMatDescr_t descr_U = 0 ; bsrilu02Info_t info_M = 0 ; bsrsv2Info_t info_L = 0 ; bsrsv2Info_t info_U = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_U ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1. ; const cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL ; const cusparseSolvePolicy_t policy_U = CUSPARSE_SOLVE_POLICY_USE_LEVEL ; const cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE ; const cusparseOperation_t trans_U = CUSPARSE_OPERATION_NON_TRANSPOSE ; const cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; // step 1: create a descriptor which contains // - matrix M is base-1 // - matrix L is base-1 // - matrix L is lower triangular // - matrix L has unit diagonal // - matrix U is base-1 // - matrix U is upper triangular // - matrix U has non-unit diagonal cusparseCreateMatDescr ( & descr_M ); cusparseSetMatIndexBase ( descr_M , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_M , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseCreateMatDescr ( & descr_L ); cusparseSetMatIndexBase ( descr_L , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_L , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseSetMatFillMode ( descr_L , CUSPARSE_FILL_MODE_LOWER ); cusparseSetMatDiagType ( descr_L , CUSPARSE_DIAG_TYPE_UNIT ); cusparseCreateMatDescr ( & descr_U ); cusparseSetMatIndexBase ( descr_U , CUSPARSE_INDEX_BASE_ONE ); cusparseSetMatType ( descr_U , CUSPARSE_MATRIX_TYPE_GENERAL ); cusparseSetMatFillMode ( descr_U , CUSPARSE_FILL_MODE_UPPER ); cusparseSetMatDiagType ( descr_U , CUSPARSE_DIAG_TYPE_NON_UNIT ); // step 2: create a empty info structure // we need one info for bsrilu02 and two info's for bsrsv2 cusparseCreateBsrilu02Info ( & info_M ); cusparseCreateBsrsv2Info ( & info_L ); cusparseCreateBsrsv2Info ( & info_U ); // step 3: query how much memory used in bsrilu02 and bsrsv2, and allocate the buffer cusparseDbsrilu02_bufferSize ( handle , dir , mb , nnzb , descr_M , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_M , & pBufferSize_M ); cusparseDbsrsv2_bufferSize ( handle , dir , trans_L , mb , nnzb , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_L , & pBufferSize_L ); cusparseDbsrsv2_bufferSize ( handle , dir , trans_U , mb , nnzb , descr_U , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_U , & pBufferSize_U ); pBufferSize = max ( pBufferSize_M , max ( pBufferSize_L , pBufferSize_U )); // pBuffer returned by cudaMalloc is automatically aligned to 128 bytes. cudaMalloc (( void ** ) & pBuffer , pBufferSize ); // step 4: perform analysis of incomplete LU factorization on M //         perform analysis of triangular solve on L //         perform analysis of triangular solve on U // The lower(upper) triangular part of M has the same sparsity pattern as L(U), // we can do analysis of bsrilu0 and bsrsv2 simultaneously. cusparseDbsrilu02_analysis ( handle , dir , mb , nnzb , descr_M , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_M , policy_M , pBuffer ); status = cusparseXbsrilu02_zeroPivot ( handle , info_M , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == statuss ){ printf ( \"A(%d,%d) is missing \\n \" , structural_zero , structural_zero ); } cusparseDbsrsv2_analysis ( handle , dir , trans_L , mb , nnzb , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_L , policy_L , pBuffer ); cusparseDbsrsv2_analysis ( handle , dir , trans_U , mb , nnzb , descr_U , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_U , policy_U , pBuffer ); // step 5: M = L * U cusparseDbsrilu02 ( handle , dir , mb , nnzb , descr_M , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_M , policy_M , pBuffer ); status = cusparseXbsrilu02_zeroPivot ( handle , info_M , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == statuss ){ printf ( \"block U(%d,%d) is not invertible \\n \" , numerical_zero , numerical_zero ); } // step 6: solve L*z = x cusparseDbsrsv2_solve ( handle , dir , trans_L , mb , nnzb , & alpha , descr_L , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_L , d_x , d_z , policy_L , pBuffer ); // step 7: solve U*y = z cusparseDbsrsv2_solve ( handle , dir , trans_U , mb , nnzb , & alpha , descr_U , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info_U , d_z , d_y , policy_U , pBuffer ); // step 6: free resources cudaFree ( pBuffer ); cusparseDestroyMatDescr ( descr_M ); cusparseDestroyMatDescr ( descr_L ); cusparseDestroyMatDescr ( descr_U ); cusparseDestroyBsrilu02Info ( info_M ); cusparseDestroyBsrsv2Info ( info_L ); cusparseDestroyBsrsv2Info ( info_U ); cusparseDestroy ( handle ); The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dirA storage format of blocks: either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows and block columns of matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A . bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one. bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A; must be larger than zero. info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged). policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL . pBuffer buffer allocated by the user; the size is returned by bsrilu02_bufferSize() . Output bsrValA <type> matrix containing the incomplete-LU lower and upper triangular factors See cusparseStatus_t for the description of the return status. 5.7.2.10. cusparseXbsrilu02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrilu02_zeroPivot ( cusparseHandle_t handle , bsrilu02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero (the block is not invertible). Otherwise position=-1 . The position can be 0-based or 1-based, the same as the matrix. Function cusparseXbsrilu02_zeroPivot() is a blocking call. It calls cudaDeviceSynchronize() to make sure all previous kernels are done. The position can be in the host memory or device memory. The user can set proper the mode with cusparseSetPointerMode() . The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. info info contains structural zero or numerical zero if the user already called bsrilu02_analysis() or bsrilu02() . Output position if no structural or numerical zero, position is -1; otherwise if A(j,j) is missing or U(j,j) is not invertible, position=j . See cusparseStatus_t for the description of the return status. 5.7.3. Tridiagonal Solve  Different algorithms for tridiagonal solve are discussed in this section. 5.7.3.1. cusparse<t>gtsv2_buffSizeExt()  cusparseStatus_t cusparseSgtsv2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const float * dl , const float * d , const float * du , const float * B , int ldb , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseDgtsv2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const double * dl , const double * d , const double * du , const double * B , int ldb , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseCgtsv2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const cuComplex * dl , const cuComplex * d , const cuComplex * du , const cuComplex * B , int ldb , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseZgtsv2_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , const cuDoubleComplex * B , int ldb , size_t * bufferSizeInBytes ) This function returns the size of the buffer used in gtsv2 which computes the solution of a tridiagonal linear system with multiple right-hand sides. \\(A \\ast X = B\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B . Notice that solution X overwrites right-hand-side matrix B on exit. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m the size of the linear system (must be ≥ 3). n number of right-hand sides, columns of matrix B . dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The last element of each upper diagonal must be zero. B <type> dense right-hand-side array of dimensions (ldb, n) . ldb leading dimension of B (that is ≥ \\(\\max\\text{(1,\\ m))}\\) . Output pBufferSizeInBytes number of bytes of the buffer used in the gtsv2 . See cusparseStatus_t for the description of the return status. 5.7.3.2. cusparse<t>gtsv2()  cusparseStatus_t cusparseSgtsv2 ( cusparseHandle_t handle , int m , int n , const float * dl , const float * d , const float * du , float * B , int ldb , void * pBuffer ) cusparseStatus_t cusparseDgtsv2 ( cusparseHandle_t handle , int m , int n , const double * dl , const double * d , const double * du , double * B , int ldb , void * pBuffer ) cusparseStatus_t cusparseCgtsv2 ( cusparseHandle_t handle , int m , int n , const cuComplex * dl , const cuComplex * d , const cuComplex * du , cuComplex * B , int ldb , void * pBuffer ) cusparseStatus_t cusparseZgtsv2 ( cusparseHandle_t handle , int m , int n , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , cuDoubleComplex * B , int ldb , void * pBuffer ) This function computes the solution of a tridiagonal linear system with multiple right-hand sides: \\(A \\ast X = B\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B . Notice that solution X overwrites right-hand-side matrix B on exit. Assuming A is of size m and base-1, dl , d and du are defined by the following formula: dl(i) := A(i, i-1) for i=1,2,...,m The first element of dl is out-of-bound ( dl(1) := A(1,0) ), so dl(1) = 0 . d(i) = A(i,i) for i=1,2,...,m du(i) = A(i,i+1) for i=1,2,...,m The last element of du is out-of-bound ( du(m) := A(m,m+1) ), so du(m) = 0 . The routine does perform pivoting, which usually results in more accurate and more stable results than cusparse<t>gtsv_nopivot() or cusparse<t>gtsv2_nopivot() at the expense of some execution time. This function requires a buffer size returned by gtsv2_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m the size of the linear system (must be ≥ 3). n number of right-hand sides, columns of matrix B . dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The last element of each upper diagonal must be zero. B <type> dense right-hand-side array of dimensions (ldb, n) . ldb leading dimension of B (that is ≥ \\(\\max\\text{(1,\\ m))}\\) . pBuffer buffer allocated by the user, the size is return by gtsv2_bufferSizeExt . Output B <type> dense solution array of dimensions (ldb, n) . See cusparseStatus_t for the description of the return status. 5.7.3.3. cusparse<t>gtsv2_nopivot_bufferSizeExt()  cusparseStatus_t cusparseSgtsv2_nopivot_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const float * dl , const float * d , const float * du , const float * B , int ldb , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseDgtsv2_nopivot_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const double * dl , const double * d , const double * du , const double * B , int ldb , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseCgtsv2_nopivot_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const cuComplex * dl , const cuComplex * d , const cuComplex * du , const cuComplex * B , int ldb , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseZgtsv2_nopivot_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , const cuDoubleComplex * B , int ldb , size_t * bufferSizeInBytes ) This function returns the size of the buffer used in gtsv2_nopivot which computes the solution of a tridiagonal linear system with multiple right-hand sides. \\(A \\ast X = B\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B . Notice that solution X overwrites right-hand-side matrix B on exit. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m the size of the linear system (must be ≥ 3). n number of right-hand sides, columns of matrix B . dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The last element of each upper diagonal must be zero. B <type> dense right-hand-side array of dimensions (ldb, n) . ldb leading dimension of B . (that is ≥ \\(\\max\\text{(1,\\ m))}\\) . Output pBufferSizeInBytes number of bytes of the buffer used in the gtsv2_nopivot . See cusparseStatus_t for the description of the return status. 5.7.3.4. cusparse<t>gtsv2_nopivot()  cusparseStatus_t cusparseSgtsv2_nopivot ( cusparseHandle_t handle , int m , int n , const float * dl , const float * d , const float * du , float * B , int ldb , void * pBuffer ) cusparseStatus_t cusparseDgtsv2_nopivot ( cusparseHandle_t handle , int m , int n , const double * dl , const double * d , const double * du , double * B , int ldb , void * pBuffer ) cusparseStatus_t cusparseCgtsv2_nopivot ( cusparseHandle_t handle , int m , int n , const cuComplex * dl , const cuComplex * d , const cuComplex * du , cuComplex * B , int ldb , void * pBuffer ) cusparseStatus_t cusparseZgtsv2_nopivot ( cusparseHandle_t handle , int m , int n , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , cuDoubleComplex * B , int ldb , void * pBuffer ) This function computes the solution of a tridiagonal linear system with multiple right-hand sides: \\(A \\ast X = B\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B . Notice that solution X overwrites right-hand-side matrix B on exit. The routine does not perform any pivoting and uses a combination of the Cyclic Reduction (CR) and the Parallel Cyclic Reduction (PCR) algorithms to find the solution. It achieves better performance when m is a power of 2. This function requires a buffer size returned by gtsv2_nopivot_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. m the size of the linear system (must be ≥ 3). n number of right-hand sides, columns of matrix B . dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The last element of each upper diagonal must be zero. B <type> dense right-hand-side array of dimensions (ldb, n) . ldb leading dimension of B . (that is ≥ \\(\\max\\text{(1,\\ m))}\\) . pBuffer buffer allocated by the user, the size is return by gtsv2_nopivot_bufferSizeExt . Output B <type> dense solution array of dimensions (ldb, n) . See cusparseStatus_t for the description of the return status. 5.7.4. Batched Tridiagonal Solve  Different algorithms for batched tridiagonal solve are discussed in this section. 5.7.4.1. cusparse<t>gtsv2StridedBatch_bufferSizeExt()  cusparseStatus_t cusparseSgtsv2StridedBatch_bufferSizeExt ( cusparseHandle_t handle , int m , const float * dl , const float * d , const float * du , const float * x , int batchCount , int batchStride , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseDgtsv2StridedBatch_bufferSizeExt ( cusparseHandle_t handle , int m , const double * dl , const double * d , const double * du , const double * x , int batchCount , int batchStride , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseCgtsv2StridedBatch_bufferSizeExt ( cusparseHandle_t handle , int m , const cuComplex * dl , const cuComplex * d , const cuComplex * du , const cuComplex * x , int batchCount , int batchStride , size_t * bufferSizeInBytes ) cusparseStatus_t cusparseZgtsv2StridedBatch_bufferSizeExt ( cusparseHandle_t handle , int m , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , const cuDoubleComplex * x , int batchCount , int batchStride , size_t * bufferSizeInBytes ) This function returns the size of the buffer used in gtsv2StridedBatch which computes the solution of multiple tridiagonal linear systems for i =0,…, batchCount : \\(A^{(i)} \\ast \\text{y}^{(i)} = \\text{x}^{(i)}\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix X . Notice that solution Y overwrites right-hand-side matrix X on exit. The different matrices are assumed to be of the same size and are stored with a fixed batchStride in memory. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. n the size of the linear system (must be ≥ 3). dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The lower diagonal \\(dl^{(i)}\\) that corresponds to the i th linear system starts at location dl+batchStride×i in memory. Also, the first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. The main diagonal \\(d^{(i)}\\) that corresponds to the i th linear system starts at location d+batchStride×i in memory. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The upper diagonal \\(du^{(i)}\\) that corresponds to the i th linear system starts at location du+batchStride×i in memory. Also, the last element of each upper diagonal must be zero. x <type> dense array that contains the right-hand-side of the tri-diagonal linear system. The right-hand-side \\(x^{(i)}\\) that corresponds to the i th linear system starts at location x+batchStride×i in memory. batchCount number of systems to solve. batchStride stride (number of elements) that separates the vectors of every system (must be at least m ). Output pBufferSizeInBytes number of bytes of the buffer used in the gtsv2StridedBatch . See cusparseStatus_t for the description of the return status. 5.7.4.2. cusparse<t>gtsv2StridedBatch()  cusparseStatus_t cusparseSgtsv2StridedBatch ( cusparseHandle_t handle , int m , const float * dl , const float * d , const float * du , float * x , int batchCount , int batchStride , void * pBuffer ) cusparseStatus_t cusparseDgtsv2StridedBatch ( cusparseHandle_t handle , int m , const double * dl , const double * d , const double * du , double * x , int batchCount , int batchStride , void * pBuffer ) cusparseStatus_t cusparseCgtsv2StridedBatch ( cusparseHandle_t handle , int m , const cuComplex * dl , const cuComplex * d , const cuComplex * du , cuComplex * x , int batchCount , int batchStride , void * pBuffer ) cusparseStatus_t cusparseZgtsv2StridedBatch ( cusparseHandle_t handle , int m , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , cuDoubleComplex * x , int batchCount , int batchStride , void * pBuffer ) This function computes the solution of multiple tridiagonal linear systems for i =0,…, batchCount : \\(A^{(i)} \\ast \\text{y}^{(i)} = \\text{x}^{(i)}\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix X . Notice that solution Y overwrites right-hand-side matrix X on exit. The different matrices are assumed to be of the same size and are stored with a fixed batchStride in memory. The routine does not perform any pivoting and uses a combination of the Cyclic Reduction (CR) and the Parallel Cyclic Reduction (PCR) algorithms to find the solution. It achieves better performance when m is a power of 2. This function requires a buffer size returned by gtsv2StridedBatch_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. n the size of the linear system (must be ≥ 3). dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The lower diagonal \\(dl^{(i)}\\) that corresponds to the i th linear system starts at location dl+batchStride×i in memory. Also, the first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. The main diagonal \\(d^{(i)}\\) that corresponds to the i th linear system starts at location d+batchStride×i in memory. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The upper diagonal \\(du^{(i)}\\) that corresponds to the i th linear system starts at location du+batchStride×i in memory. Also, the last element of each upper diagonal must be zero. x <type> dense array that contains the right-hand-side of the tri-diagonal linear system. The right-hand-side \\(x^{(i)}\\) that corresponds to the i th linear system starts at location x+batchStride×i in memory. batchCount number of systems to solve. batchStride stride (number of elements) that separates the vectors of every system (must be at least n ). pBuffer buffer allocated by the user, the size is return by gtsv2StridedBatch_bufferSizeExt . Output x <type> dense array that contains the solution of the tri-diagonal linear system. The solution \\(x^{(i)}\\) that corresponds to the i th linear system starts at location x+batchStride×i in memory. See cusparseStatus_t for the description of the return status. 5.7.4.3. cusparse<t>gtsvInterleavedBatch()  cusparseStatus_t cusparseSgtsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const float * dl , const float * d , const float * du , const float * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDgtsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const double * dl , const double * d , const double * du , const double * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseCgtsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const cuComplex * dl , const cuComplex * d , const cuComplex * du , const cuComplex * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseZgtsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , const cuDoubleComplex * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseSgtsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , float * dl , float * d , float * du , float * x , int batchCount , void * pBuffer ) cusparseStatus_t cusparseDgtsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , double * dl , double * d , double * du , double * x , int batchCount , void * pBuffer ) cusparseStatus_t cusparseCgtsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , cuComplex * dl , cuComplex * d , cuComplex * du , cuComplex * x , int batchCount , void * pBuffer ) cusparseStatus_t cusparseZgtsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , cuDoubleComplex * dl , cuDoubleComplex * d , cuDoubleComplex * du , cuDoubleComplex * x , int batchCount , void * pBuffer ) This function computes the solution of multiple tridiagonal linear systems for i =0,…, batchCount : \\(A^{(i)} \\ast \\text{x}^{(i)} = \\text{b}^{(i)}\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B . Notice that solution X overwrites right-hand-side matrix B on exit. Assuming A is of size m and base-1, dl , d and du are defined by the following formula: dl(i) := A(i, i-1) for i=1,2,...,m The first element of dl is out-of-bound ( dl(1) := A(1,0) ), so dl(1) = 0 . d(i) = A(i,i) for i=1,2,...,m du(i) = A(i,i+1) for i=1,2,...,m The last element of du is out-of-bound ( du(m) := A(m,m+1) ), so du(m) = 0 . The data layout is different from gtsvStridedBatch which aggregates all matrices one after another. Instead, gtsvInterleavedBatch gathers different matrices of the same element in a continous manner. If dl is regarded as a 2-D array of size m-by-batchCount , dl(:,j) to store j-th matrix. gtsvStridedBatch uses column-major while gtsvInterleavedBatch uses row-major. The routine provides three different algorithms, selected by parameter algo . The first algorithm is cuThomas provided by Barcelona Supercomputing Center . The second algorithm is LU with partial pivoting and last algorithm is QR. From stability perspective, cuThomas is not numerically stable because it does not have pivoting. LU with partial pivoting and QR are stable. From performance perspective, LU with partial pivoting and QR is about 10% to 20% slower than cuThomas. This function requires a buffer size returned by gtsvInterleavedBatch_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. If the user prepares aggregate format, one can use cublasXgeam to get interleaved format. However such transformation takes time comparable to solver itself. To reach best performance, the user must prepare interleaved format explicitly. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. algo algo = 0: cuThomas (unstable algorithm); algo = 1: LU with pivoting (stable algorithm); algo = 2: QR (stable algorithm) m the size of the linear system. dl <type> dense array containing the lower diagonal of the tri-diagonal linear system. The first element of each lower diagonal must be zero. d <type> dense array containing the main diagonal of the tri-diagonal linear system. du <type> dense array containing the upper diagonal of the tri-diagonal linear system. The last element of each upper diagonal must be zero. x <type> dense right-hand-side array of dimensions (batchCount, n) . pBuffer buffer allocated by the user, the size is return by gtsvInterleavedBatch_bufferSizeExt . Output x <type> dense solution array of dimensions (batchCount, n) . See cusparseStatus_t for the description of the return status. 5.7.5. Batched Pentadiagonal Solve  Different algorithms for batched pentadiagonal solve are discussed in this section. 5.7.5.1. cusparse<t>gpsvInterleavedBatch()  cusparseStatus_t cusparseSgpsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const float * ds , const float * dl , const float * d , const float * du , const float * dw , const float * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDgpsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const double * ds , const double * dl , const double * d , const double * du , const double * dw , const double * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseCgpsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const cuComplex * ds , const cuComplex * dl , const cuComplex * d , const cuComplex * du , const cuComplex * dw , const cuComplex * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseZgpsvInterleavedBatch_bufferSizeExt ( cusparseHandle_t handle , int algo , int m , const cuDoubleComplex * ds , const cuDoubleComplex * dl , const cuDoubleComplex * d , const cuDoubleComplex * du , const cuDoubleComplex * dw , const cuDoubleComplex * x , int batchCount , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseSgpsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , float * ds , float * dl , float * d , float * du , float * dw , float * x , int batchCount , void * pBuffer ) cusparseStatus_t cusparseDgpsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , double * ds , double * dl , double * d , double * du , double * dw , double * x , int batchCount , void * pBuffer ) cusparseStatus_t cusparseCgpsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , cuComplex * ds , cuComplex * dl , cuComplex * d , cuComplex * du , cuComplex * dw , cuComplex * x , int batchCount , void * pBuffer ) cusparseStatus_t cusparseZgpsvInterleavedBatch ( cusparseHandle_t handle , int algo , int m , cuDoubleComplex * ds , cuDoubleComplex * dl , cuDoubleComplex * d , cuDoubleComplex * du , cuDoubleComplex * dw , cuDoubleComplex * x , int batchCount , void * pBuffer ) This function computes the solution of multiple penta-diagonal linear systems for i =0,…, batchCount : \\(A^{(i)} \\ast \\text{x}^{(i)} = \\text{b}^{(i)}\\) The coefficient matrix A of each of these penta-diagonal linear system is defined with five vectors corresponding to its lower ( ds, dl ), main ( d ), and upper ( du, dw ) matrix diagonals; the right-hand sides are stored in the dense matrix B . Notice that solution X overwrites right-hand-side matrix B on exit. Assuming A is of size m and base-1, ds , dl , d , du and dw are defined by the following formula: ds(i) := A(i, i-2) for i=1,2,...,m The first two elements of ds is out-of-bound ( ds(1) := A(1,-1) , ds(2) := A(2,0) ), so ds(1) = 0 and ds(2) = 0 . dl(i) := A(i, i-1) for i=1,2,...,m The first element of dl is out-of-bound ( dl(1) := A(1,0) ), so dl(1) = 0 . d(i) = A(i,i) for i=1,2,...,m du(i) = A(i,i+1) for i=1,2,...,m The last element of du is out-of-bound ( du(m) := A(m,m+1) ), so du(m) = 0 . dw(i) = A(i,i+2) for i=1,2,...,m The last two elements of dw is out-of-bound ( dw(m-1) := A(m-1,m+1) , dw(m) := A(m,m+2) ), so dw(m-1) = 0 and dw(m) = 0 . The data layout is the same as gtsvStridedBatch . The routine is numerically stable because it uses QR to solve the linear system. This function requires a buffer size returned by gpsvInterleavedBatch_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If it is not, CUSPARSE_STATUS_INVALID_VALUE is returned. The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. algo only support algo = 0 (QR) m the size of the linear system. ds <type> dense array containing the lower diagonal (distance 2 to the diagonal) of the penta-diagonal linear system. The first two elements must be zero. dl <type> dense array containing the lower diagonal (distance 1 to the diagonal) of the penta-diagonal linear system. The first element must be zero. d <type> dense array containing the main diagonal of the penta-diagonal linear system. du <type> dense array containing the upper diagonal (distance 1 to the diagonal) of the penta-diagonal linear system. The last element must be zero. dw <type> dense array containing the upper diagonal (distance 2 to the diagonal) of the penta-diagonal linear system. The last two elements must be zero. x <type> dense right-hand-side array of dimensions (batchCount, n) . pBuffer buffer allocated by the user, the size is return by gpsvInterleavedBatch_bufferSizeExt . Output x <type> dense solution array of dimensions (batchCount, n) . See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSgpsvInterleavedBatch for a code example. 5.8. cuSPARSE Reorderings Reference  This chapter describes the reordering routines used to manipulate sparse matrices. 5.8.1. cusparse<t>csrcolor() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrcolor ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const float * fractionToColor , int * ncolors , int * coloring , int * reordering , cusparseColorInfo_t info ) cusparseStatus_t cusparseDcsrcolor ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const double * fractionToColor , int * ncolors , int * coloring , int * reordering , cusparseColorInfo_t info ) cusparseStatus_t cusparseCcsrcolor ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cuComplex * fractionToColor , int * ncolors , int * coloring , int * reordering , cusparseColorInfo_t info ) cusparseStatus_t cusparseZcsrcolor ( cusparseHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cuDoubleComplex * fractionToColor , int * ncolors , int * coloring , int * reordering , cusparseColorInfo_t info ) This function performs the coloring of the adjacency graph associated with the matrix A stored in CSR format. The coloring is an assignment of colors (integer numbers) to nodes, such that neighboring nodes have distinct colors. An approximate coloring algorithm is used in this routine, and is stopped when a certain percentage of nodes has been colored. The rest of the nodes are assigned distinct colors (an increasing sequence of integers numbers, starting from the last integer used previously). The last two auxiliary routines can be used to extract the resulting number of colors, their assignment and the associated reordering. The reordering is such that nodes that have been assigned the same color are reordered to be next to each other. The matrix A passed to this routine, must be stored as a general matrix and have a symmetric sparsity pattern. If the matrix is nonsymmetric the user should pass A+A^T as a parameter to this routine. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. m number of rows of matrix A . nnz number of nonzero elements of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA <type> array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A . csrRowPtrA integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A . fractionToColor fraction of nodes to be colored, which should be in the interval [0.0,1.0], for example 0.8 implies that 80 percent of nodes will be colored. info structure with information to be passed to the coloring. Output ncolors The number of distinct colors used (at most the size of the matrix, but likely much smaller). coloring The resulting coloring permutation reordering The resulting reordering permutation (untouched if NULL) See cusparseStatus_t for the description of the return status. 5.9. cuSPARSE Format Conversion Reference  This chapter describes the conversion routines between different sparse and dense storage formats. coosort , csrsort , cscsort , and csru2csr are sorting routines without malloc inside, the following table estimates the buffer size. routine buffer size maximum problem size if buffer is limited by 2GB coosort > 16*n bytes 125M csrsort or cscsort > 20*n bytes 100M csru2csr 'd' > 28*n bytes ; 'z' > 36*n bytes 71M for ‘d’ and 55M for ‘z’ 5.9.1. cusparse<t>bsr2csr()  cusparseStatus_t cusparseSbsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const cusparseMatDescr_t descrC , float * csrValC , int * csrRowPtrC , int * csrColIndC ) cusparseStatus_t cusparseDbsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const cusparseMatDescr_t descrC , double * csrValC , int * csrRowPtrC , int * csrColIndC ) cusparseStatus_t cusparseCbsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const cusparseMatDescr_t descrC , cuComplex * csrValC , int * csrRowPtrC , int * csrColIndC ) cusparseStatus_t cusparseZbsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int blockDim , const cusparseMatDescr_t descrC , cuDoubleComplex * csrValC , int * csrRowPtrC , int * csrColIndC ) This function converts a sparse matrix in BSR format that is defined by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ) into a sparse matrix in CSR format that is defined by arrays csrValC , csrRowPtrC , and csrColIndC . Let m(=mb*blockDim) be the number of rows of A and n(=nb*blockDim) be number of columns of A , then A and C are m*n sparse matrices. The BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) nonzero blocks, whereas the sparse matrix A contains nnz(=nnzb*blockDim*blockDim) elements. The user must allocate enough space for arrays csrRowPtrC , csrColIndC , and csrValC . The requirements are as follows: csrRowPtrC of m+1 elements csrValC of nnz elements csrColIndC of nnz elements The general procedure is as follows: // Given BSR format (bsrRowPtrA, bsrcolIndA, bsrValA) and // blocks of BSR format are stored in column-major order. cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; int m = mb * blockDim ; int nnzb = bsrRowPtrA [ mb ] - bsrRowPtrA [ 0 ]; // number of blocks int nnz = nnzb * blockDim * blockDim ; // number of elements cudaMalloc (( void ** ) & csrRowPtrC , sizeof ( int ) * ( m + 1 )); cudaMalloc (( void ** ) & csrColIndC , sizeof ( int ) * nnz ); cudaMalloc (( void ** ) & csrValC , sizeof ( float ) * nnz ); cusparseSbsr2csr ( handle , dir , mb , nb , descrA , bsrValA , bsrRowPtrA , bsrColIndA , blockDim , descrC , csrValC , csrRowPtrC , csrColIndC ); The routine requires no extra storage The routine supports asynchronous execution if blockDim != 1 or the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if blockDim != 1 or the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows of sparse matrix A . nb number of block columns of sparse matrix A . descrA the descriptor of matrix A . bsrValA <type> array of nnzb*blockDim*blockDim nonzero elements of matrix A . bsrRowPtrA integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix A . bsrColIndA integer array of nnzb column indices of the nonzero blocks of matrix A . blockDim block dimension of sparse matrix A . descrC the descriptor of matrix C . Output csrValC <type> array of nnz(=csrRowPtrC[m]-csrRowPtrC[0]) nonzero elements of matrix C . csrRowPtrC integer array of m+1 elements that contains the start of every row and the end of the last row plus one of matrix C . csrColIndC integer array of nnz column indices of the nonzero elements of matrix C . See cusparseStatus_t for the description of the return status. 5.9.2. cusparse<t>gebsr2gebsc()  cusparseStatus_t cusparseSgebsr2gebsc_bufferSize ( cusparseHandle_t handle , int mb , int nb , int nnzb , const float * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseDgebsr2gebsc_bufferSize ( cusparseHandle_t handle , int mb , int nb , int nnzb , const double * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseCgebsr2gebsc_bufferSize ( cusparseHandle_t handle , int mb , int nb , int nnzb , const cuComplex * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseZgebsr2gebsc_bufferSize ( cusparseHandle_t handle , int mb , int nb , int nnzb , const cuDoubleComplex * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseSgebsr2gebsc ( cusparseHandle_t handle , int mb , int nb , int nnzb , const float * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , float * bscVal , int * bscRowInd , int * bscColPtr , cusparseAction_t copyValues , cusparseIndexBase_t baseIdx , void * pBuffer ) cusparseStatus_t cusparseDgebsr2gebsc ( cusparseHandle_t handle , int mb , int nb , int nnzb , const double * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , double * bscVal , int * bscRowInd , int * bscColPtr , cusparseAction_t copyValues , cusparseIndexBase_t baseIdx , void * pBuffer ) cusparseStatus_t cusparseCgebsr2gebsc ( cusparseHandle_t handle , int mb , int nb , int nnzb , const cuComplex * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , cuComplex * bscVal , int * bscRowInd , int * bscColPtr , cusparseAction_t copyValues , cusparseIndexBase_t baseIdx , void * pBuffer ) cusparseStatus_t cusparseZgebsr2gebsc ( cusparseHandle_t handle , int mb , int nb , int nnzb , const cuDoubleComplex * bsrVal , const int * bsrRowPtr , const int * bsrColInd , int rowBlockDim , int colBlockDim , cuDoubleComplex * bscVal , int * bscRowInd , int * bscColPtr , cusparseAction_t copyValues , cusparseIndexBase_t baseIdx , void * pBuffer ) This function can be seen as the same as csr2csc() when each block of size rowBlockDim*colBlockDim is regarded as a scalar. This sparsity pattern of the result matrix can also be seen as the transpose of the original sparse matrix, but the memory layout of a block does not change. The user must call gebsr2gebsc_bufferSize() to determine the size of the buffer required by gebsr2gebsc() , allocate the buffer, and pass the buffer pointer to gebsr2gebsc() . The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. mb number of block rows of sparse matrix A . nb number of block columns of sparse matrix A . nnzb number of nonzero blocks of matrix A . bsrVal <type> array of nnzb*rowBlockDim*colBlockDim nonzero elements of matrix A . bsrRowPtr integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one. bsrColInd integer array of nnzb column indices of the non-zero blocks of matrix A . rowBlockDim number of rows within a block of A . colBlockDim number of columns within a block of A . copyValues CUSPARSE_ACTION_SYMBOLIC or CUSPARSE_ACTION_NUMERIC . baseIdx CUSPARSE_INDEX_BASE_ZERO or CUSPARSE_INDEX_BASE_ONE . pBufferSize host pointer containing number of bytes of the buffer used in gebsr2gebsc() . pBuffer buffer allocated by the user; the size is return by gebsr2gebsc_bufferSize() . Output bscVal <type> array of nnzb*rowBlockDim*colBlockDim non-zero elements of matrix A . It is only filled-in if copyValues is set to CUSPARSE_ACTION_NUMERIC . bscRowInd integer array of nnzb row indices of the non-zero blocks of matrix A . bscColPtr integer array of nb+1 elements that contains the start of every block column and the end of the last block column plus one. See cusparseStatus_t for the description of the return status. 5.9.3. cusparse<t>gebsr2gebsr()  cusparseStatus_t cusparseSgebsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , int rowBlockDimC , int colBlockDimC , int * pBufferSize ) cusparseStatus_t cusparseDgebsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , int rowBlockDimC , int colBlockDimC , int * pBufferSize ) cusparseStatus_t cusparseCgebsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , int rowBlockDimC , int colBlockDimC , int * pBufferSize ) cusparseStatus_t cusparseZgebsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , int rowBlockDimC , int colBlockDimC , int * pBufferSize ) cusparseStatus_t cusparseXgebsr2gebsrNnz ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , const cusparseMatDescr_t descrC , int * bsrRowPtrC , int rowBlockDimC , int colBlockDimC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseSgebsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , const cusparseMatDescr_t descrC , float * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDimC , int colBlockDimC , void * pBuffer ) cusparseStatus_t cusparseDgebsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , const cusparseMatDescr_t descrC , double * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDimC , int colBlockDimC , void * pBuffer ) cusparseStatus_t cusparseCgebsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , const cusparseMatDescr_t descrC , cuComplex * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDimC , int colBlockDimC , void * pBuffer ) cusparseStatus_t cusparseZgebsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , int nnzb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDimA , int colBlockDimA , const cusparseMatDescr_t descrC , cuDoubleComplex * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDimC , int colBlockDimC , void * pBuffer ) This function converts a sparse matrix in general BSR format that is defined by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA into a sparse matrix in another general BSR format that is defined by arrays bsrValC , bsrRowPtrC , and bsrColIndC . If rowBlockDimA=1 and colBlockDimA=1 , cusparse[S|D|C|Z]gebsr2gebsr() is the same as cusparse[S|D|C|Z]csr2gebsr() . If rowBlockDimC=1 and colBlockDimC=1 , cusparse[S|D|C|Z]gebsr2gebsr() is the same as cusparse[S|D|C|Z]gebsr2csr() . A is an m*n sparse matrix where m(=mb*rowBlockDim) is the number of rows of A , and n(=nb*colBlockDim) is the number of columns of A . The general BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) nonzero blocks. The matrix C is also general BSR format with a different block size, rowBlockDimC*colBlockDimC . If m is not a multiple of rowBlockDimC , or n is not a multiple of colBlockDimC , zeros are filled in. The number of block rows of C is mc(=(m+rowBlockDimC-1)/rowBlockDimC) . The number of block rows of C is nc(=(n+colBlockDimC-1)/colBlockDimC) . The number of nonzero blocks of C is nnzc . The implementation adopts a two-step approach to do the conversion. First, the user allocates bsrRowPtrC of mc+1 elements and uses function cusparseXgebsr2gebsrNnz() to determine the number of nonzero block columns per block row of matrix C . Second, the user gathers nnzc (number of non-zero block columns of matrix C ) from either (nnzc=*nnzTotalDevHostPtr) or (nnzc=bsrRowPtrC[mc]-bsrRowPtrC[0]) and allocates bsrValC of nnzc*rowBlockDimC*colBlockDimC elements and bsrColIndC of nnzc integers. Finally the function cusparse[S|D|C|Z]gebsr2gebsr() is called to complete the conversion. The user must call gebsr2gebsr_bufferSize() to know the size of the buffer required by gebsr2gebsr() , allocate the buffer, and pass the buffer pointer to gebsr2gebsr() . The general procedure is as follows: // Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and // blocks of BSR format are stored in column-major order. cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; int base , nnzc ; int m = mb * rowBlockDimA ; int n = nb * colBlockDimA ; int mc = ( m + rowBlockDimC -1 ) / rowBlockDimC ; int nc = ( n + colBlockDimC -1 ) / colBlockDimC ; int bufferSize ; void * pBuffer ; cusparseSgebsr2gebsr_bufferSize ( handle , dir , mb , nb , nnzb , descrA , bsrValA , bsrRowPtrA , bsrColIndA , rowBlockDimA , colBlockDimA , rowBlockDimC , colBlockDimC , & bufferSize ); cudaMalloc (( void ** ) & pBuffer , bufferSize ); cudaMalloc (( void ** ) & bsrRowPtrC , sizeof ( int ) * ( mc + 1 )); // nnzTotalDevHostPtr points to host memory int * nnzTotalDevHostPtr = & nnzc ; cusparseXgebsr2gebsrNnz ( handle , dir , mb , nb , nnzb , descrA , bsrRowPtrA , bsrColIndA , rowBlockDimA , colBlockDimA , descrC , bsrRowPtrC , rowBlockDimC , colBlockDimC , nnzTotalDevHostPtr , pBuffer ); if ( NULL != nnzTotalDevHostPtr ){ nnzc = * nnzTotalDevHostPtr ; } else { cudaMemcpy ( & nnzc , bsrRowPtrC + mc , sizeof ( int ), cudaMemcpyDeviceToHost ); cudaMemcpy ( & base , bsrRowPtrC , sizeof ( int ), cudaMemcpyDeviceToHost ); nnzc -= base ; } cudaMalloc (( void ** ) & bsrColIndC , sizeof ( int ) * nnzc ); cudaMalloc (( void ** ) & bsrValC , sizeof ( float ) * ( rowBlockDimC * colBlockDimC ) * nnzc ); cusparseSgebsr2gebsr ( handle , dir , mb , nb , nnzb , descrA , bsrValA , bsrRowPtrA , bsrColIndA , rowBlockDimA , colBlockDimA , descrC , bsrValC , bsrRowPtrC , bsrColIndC , rowBlockDimC , colBlockDimC , pBuffer ); The routines require no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routines do not support CUDA graph capture Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows of sparse matrix A . nb number of block columns of sparse matrix A . nnzb number of nonzero blocks of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb*rowBlockDimA*colBlockDimA non-zero elements of matrix A . bsrRowPtrA integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix A . bsrColIndA integer array of nnzb column indices of the non-zero blocks of matrix A . rowBlockDimA number of rows within a block of A . colBlockDimA number of columns within a block of A . descrC the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . rowBlockDimC number of rows within a block of C . colBlockDimC number of columns within a block of C . pBufferSize host pointer containing number of bytes of the buffer used in gebsr2gebsr() . pBuffer buffer allocated by the user; the size is return by gebsr2gebsr_bufferSize() . Output bsrValC <type> array of nnzc*rowBlockDimC*colBlockDimC non-zero elements of matrix C . bsrRowPtrC integer array of mc+1 elements that contains the start of every block row and the end of the last block row plus one of matrix C . bsrColIndC integer array of nnzc block column indices of the nonzero blocks of matrix C . nnzTotalDevHostPtr total number of nonzero blocks of C . *nnzTotalDevHostPtr is the same as bsrRowPtrC[mc]-bsrRowPtrC[0] . See cusparseStatus_t for the description of the return status. 5.9.4. cusparse<t>gebsr2csr()  cusparseStatus_t cusparseSgebsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const float * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDim , int colBlockDim , const cusparseMatDescr_t descrC , float * csrValC , int * csrRowPtrC , int * csrColIndC ) cusparseStatus_t cusparseDgebsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const double * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDim , int colBlockDim , const cusparseMatDescr_t descrC , double * csrValC , int * csrRowPtrC , int * csrColIndC ) cusparseStatus_t cusparseCgebsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const cuComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDim , int colBlockDim , const cusparseMatDescr_t descrC , cuComplex * csrValC , int * csrRowPtrC , int * csrColIndC ) cusparseStatus_t cusparseZgebsr2csr ( cusparseHandle_t handle , cusparseDirection_t dir , int mb , int nb , const cusparseMatDescr_t descrA , const cuDoubleComplex * bsrValA , const int * bsrRowPtrA , const int * bsrColIndA , int rowBlockDim , int colBlockDim , const cusparseMatDescr_t descrC , cuDoubleComplex * csrValC , int * csrRowPtrC , int * csrColIndC ) This function converts a sparse matrix in general BSR format that is defined by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA into a sparse matrix in CSR format that is defined by arrays csrValC , csrRowPtrC , and csrColIndC . Let m(=mb*rowBlockDim) be number of rows of A and n(=nb*colBlockDim) be number of columns of A , then A and C are m*n sparse matrices. The general BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) non-zero blocks, whereas sparse matrix A contains nnz(=nnzb*rowBlockDim*colBlockDim) elements. The user must allocate enough space for arrays csrRowPtrC , csrColIndC , and csrValC . The requirements are as follows: csrRowPtrC of m+1 elements csrValC of nnz elements csrColIndC of nnz elements The general procedure is as follows: // Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and // blocks of BSR format are stored in column-major order. cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; int m = mb * rowBlockDim ; int n = nb * colBlockDim ; int nnzb = bsrRowPtrA [ mb ] - bsrRowPtrA [ 0 ]; // number of blocks int nnz = nnzb * rowBlockDim * colBlockDim ; // number of elements cudaMalloc (( void ** ) & csrRowPtrC , sizeof ( int ) * ( m + 1 )); cudaMalloc (( void ** ) & csrColIndC , sizeof ( int ) * nnz ); cudaMalloc (( void ** ) & csrValC , sizeof ( float ) * nnz ); cusparseSgebsr2csr ( handle , dir , mb , nb , descrA , bsrValA , bsrRowPtrA , bsrColIndA , rowBlockDim , colBlockDim , descrC , csrValC , csrRowPtrC , csrColIndC ); The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . mb number of block rows of sparse matrix A . nb number of block columns of sparse matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . bsrValA <type> array of nnzb*rowBlockDim*colBlockDim non-zero elements of matrix A . bsrRowPtrA integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix A . bsrColIndA integer array of nnzb column indices of the non-zero blocks of matrix A . rowBlockDim number of rows within a block of A . colBlockDim number of columns within a block of A . descrC the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . Output csrValC <type> array of nnz non-zero elements of matrix C . csrRowPtrC integer array of m+1 elements that contains the start of every row and the end of the last row plus one of matrix C . csrColIndC integer array of nnz column indices of the non-zero elements of matrix C . See cusparseStatus_t for the description of the return status. 5.9.5. cusparse<t>csr2gebsr()  cusparseStatus_t cusparseScsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseDcsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseCcsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseZcsr2gebsr_bufferSize ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , int rowBlockDim , int colBlockDim , int * pBufferSize ) cusparseStatus_t cusparseXcsr2gebsrNnz ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const int * csrRowPtrA , const int * csrColIndA , const cusparseMatDescr_t descrC , int * bsrRowPtrC , int rowBlockDim , int colBlockDim , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseScsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cusparseMatDescr_t descrC , float * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDim , int colBlockDim , void * pBuffer ) cusparseStatus_t cusparseDcsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cusparseMatDescr_t descrC , double * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDim , int colBlockDim , void * pBuffer ) cusparseStatus_t cusparseCcsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cusparseMatDescr_t descrC , cuComplex * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDim , int colBlockDim , void * pBuffer ) cusparseStatus_t cusparseZcsr2gebsr ( cusparseHandle_t handle , cusparseDirection_t dir , int m , int n , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cusparseMatDescr_t descrC , cuDoubleComplex * bsrValC , int * bsrRowPtrC , int * bsrColIndC , int rowBlockDim , int colBlockDim , void * pBuffer ) This function converts a sparse matrix A in CSR format (that is defined by arrays csrValA , csrRowPtrA , and csrColIndA ) into a sparse matrix C in general BSR format (that is defined by the three arrays bsrValC , bsrRowPtrC , and bsrColIndC ). The matrix A is a m*n sparse matrix and matrix C is a (mb*rowBlockDim)*(nb*colBlockDim) sparse matrix, where mb(=(m+rowBlockDim-1)/rowBlockDim) is the number of block rows of C , and nb(=(n+colBlockDim-1)/colBlockDim) is the number of block columns of C . The block of C is of size rowBlockDim*colBlockDim . If m is not multiple of rowBlockDim or n is not multiple of colBlockDim , zeros are filled in. The implementation adopts a two-step approach to do the conversion. First, the user allocates bsrRowPtrC of mb+1 elements and uses function cusparseXcsr2gebsrNnz() to determine the number of nonzero block columns per block row. Second, the user gathers nnzb (number of nonzero block columns of matrix C ) from either (nnzb=*nnzTotalDevHostPtr) or (nnzb=bsrRowPtrC[mb]-bsrRowPtrC[0]) and allocates bsrValC of nnzb*rowBlockDim*colBlockDim elements and bsrColIndC of nnzb integers. Finally function cusparse[S|D|C|Z]csr2gebsr() is called to complete the conversion. The user must obtain the size of the buffer required by csr2gebsr() by calling csr2gebsr_bufferSize() , allocate the buffer, and pass the buffer pointer to csr2gebsr() . The general procedure is as follows: // Given CSR format (csrRowPtrA, csrColIndA, csrValA) and // blocks of BSR format are stored in column-major order. cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; int base , nnzb ; int mb = ( m + rowBlockDim -1 ) / rowBlockDim ; int nb = ( n + colBlockDim -1 ) / colBlockDim ; int bufferSize ; void * pBuffer ; cusparseScsr2gebsr_bufferSize ( handle , dir , m , n , descrA , csrValA , csrRowPtrA , csrColIndA , rowBlockDim , colBlockDim , & bufferSize ); cudaMalloc (( void ** ) & pBuffer , bufferSize ); cudaMalloc (( void ** ) & bsrRowPtrC , sizeof ( int ) * ( mb + 1 )); // nnzTotalDevHostPtr points to host memory int * nnzTotalDevHostPtr = & nnzb ; cusparseXcsr2gebsrNnz ( handle , dir , m , n , descrA , csrRowPtrA , csrColIndA , descrC , bsrRowPtrC , rowBlockDim , colBlockDim , nnzTotalDevHostPtr , pBuffer ); if ( NULL != nnzTotalDevHostPtr ){ nnzb = * nnzTotalDevHostPtr ; } else { cudaMemcpy ( & nnzb , bsrRowPtrC + mb , sizeof ( int ), cudaMemcpyDeviceToHost ); cudaMemcpy ( & base , bsrRowPtrC , sizeof ( int ), cudaMemcpyDeviceToHost ); nnzb -= base ; } cudaMalloc (( void ** ) & bsrColIndC , sizeof ( int ) * nnzb ); cudaMalloc (( void ** ) & bsrValC , sizeof ( float ) * ( rowBlockDim * colBlockDim ) * nnzb ); cusparseScsr2gebsr ( handle , dir , m , n , descrA , csrValA , csrRowPtrA , csrColIndA , descrC , bsrValC , bsrRowPtrC , bsrColIndC , rowBlockDim , colBlockDim , pBuffer ); The routine cusparseXcsr2gebsrNnz() has the following properties: The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparse<t>csr2gebsr() has the following properties: The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN . m number of rows of sparse matrix A . n number of columns of sparse matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA <type> array of nnz nonzero elements of matrix A . csrRowPtrA integer array of m+1 elements that contains the start of every row and the end of the last row plus one of matrix A . csrColIndA integer array of nnz column indices of the nonzero elements of matrix A . descrC the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . rowBlockDim number of rows within a block of C . colBlockDim number of columns within a block of C . pBuffer buffer allocated by the user, the size is return by csr2gebsr_bufferSize() . Output bsrValC <type> array of nnzb*rowBlockDim*colBlockDim nonzero elements of matrix C . bsrRowPtrC integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix C . bsrColIndC integer array of nnzb column indices of the nonzero blocks of matrix C . nnzTotalDevHostPtr total number of nonzero blocks of matrix C . Pointer nnzTotalDevHostPtr can point to a device memory or host memory. See cusparseStatus_t for the description of the return status. 5.9.6. cusparse<t>coo2csr()  cusparseStatus_t cusparseXcoo2csr ( cusparseHandle_t handle , const int * cooRowInd , int nnz , int m , int * csrRowPtr , cusparseIndexBase_t idxBase ) This function converts the array containing the uncompressed row indices (corresponding to COO format) into an array of compressed row pointers (corresponding to CSR format). It can also be used to convert the array containing the uncompressed column indices (corresponding to COO format) into an array of column pointers (corresponding to CSC format). The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. cooRowInd integer array of nnz uncompressed row indices. nnz number of non-zeros of the sparse matrix (that is also the length of array cooRowInd ). m number of rows of matrix A . idxBase CUSPARSE_INDEX_BASE_ZERO or CUSPARSE_INDEX_BASE_ONE . Output csrRowPtr integer array of m+1 elements that contains the start of every row and the end of the last row plus one. See cusparseStatus_t for the description of the return status. 5.9.7. cusparse<t>csr2coo()  cusparseStatus_t cusparseXcsr2coo ( cusparseHandle_t handle , const int * csrRowPtr , int nnz , int m , int * cooRowInd , cusparseIndexBase_t idxBase ) This function converts the array containing the compressed row pointers (corresponding to CSR format) into an array of uncompressed row indices (corresponding to COO format). It can also be used to convert the array containing the compressed column indices (corresponding to CSC format) into an array of uncompressed column indices (corresponding to COO format). The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context. csrRowPtr integer array of m+1 elements that contains the start of every row and the end of the last row plus one. nnz number of nonzeros of the sparse matrix (that is also the length of array cooRowInd ). m number of rows of matrix A . idxBase CUSPARSE_INDEX_BASE_ZERO or CUSPARSE_INDEX_BASE_ONE . Output cooRowInd integer array of nnz uncompressed row indices. See cusparseStatus_t for the description of the return status. 5.9.8. cusparseCsr2cscEx2()  cusparseStatus_t cusparseCsr2cscEx2_bufferSize ( cusparseHandle_t handle , int m , int n , int nnz , const void * csrVal , const int * csrRowPtr , const int * csrColInd , void * cscVal , int * cscColPtr , int * cscRowInd , cudaDataType valType , cusparseAction_t copyValues , cusparseIndexBase_t idxBase , cusparseCsr2CscAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseCsr2cscEx2 ( cusparseHandle_t handle , int m , int n , int nnz , const void * csrVal , const int * csrRowPtr , const int * csrColInd , void * cscVal , int * cscColPtr , int * cscRowInd , cudaDataType valType , cusparseAction_t copyValues , cusparseIndexBase_t idxBase , cusparseCsr2CscAlg_t alg , void * buffer ) This function converts a sparse matrix in CSR format (that is defined by the three arrays csrVal , csrRowPtr , and csrColInd ) into a sparse matrix in CSC format (that is defined by arrays cscVal , cscRowInd , and cscColPtr ). The resulting matrix can also be seen as the transpose of the original sparse matrix. Notice that this routine can also be used to convert a matrix in CSC format into a matrix in CSR format. The routine requires extra storage proportional to the number of nonzero values nnz . It provides in output always the same matrix. It is executed asynchronously with respect to the host, and it may return control to the application on the host before the result is ready. The function cusparseCsr2cscEx2_bufferSize() returns the size of the workspace needed by cusparseCsr2cscEx2() . User needs to allocate a buffer of this size and give that buffer to cusparseCsr2cscEx2() as an argument. If nnz == 0 , then csrColInd , csrVal , cscVal , and cscRowInd could have NULL value. In this case, cscColPtr is set to idxBase for all values. If m == 0 or n == 0 , the pointers are not checked and the routine returns CUSPARSE_STATUS_SUCCESS . Input handle Handle to the cuSPARSE library context m Number of rows of the CSR input matrix; number of columns of the CSC ouput matrix n Number of columns of the CSR input matrix; number of rows of the CSC ouput matrix nnz Number of nonzero elements of the CSR and CSC matrices csrVal Value array of size nnz of the CSR matrix; of same type as valType csrRowPtr Integer array of size m + 1 that containes the CSR row offsets csrColInd Integer array of size nnz that containes the CSR column indices cscVal Value array of size nnz of the CSC matrix; of same type as valType cscColPtr Integer array of size n + 1 that containes the CSC column offsets cscRowInd Integer array of size nnz that containes the CSC row indices valType Value type for both CSR and CSC matrices copyValues CUSPARSE_ACTION_SYMBOLIC or CUSPARSE_ACTION_NUMERIC idxBase Index base CUSPARSE_INDEX_BASE_ZERO or CUSPARSE_INDEX_BASE_ONE alg Algorithm implementation. see cusparseCsr2CscAlg_t for possible values. bufferSize Number of bytes of workspace needed by cusparseCsr2cscEx2() buffer Pointer to workspace buffer cusparseCsr2cscEx2() supports the following data types: X / Y CUDA_R_8I CUDA_R_16F CUDA_R_16BF CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F cusparseCsr2cscEx2() supports the following algorithms ( cusparseCsr2CscAlg_t ): Algorithm Notes CUSPARSE_CSR2CSC_ALG_DEFAULT , CUSPARSE_CSR2CSC_ALG1 Default algorithm Action Notes CUSPARSE_ACTION_SYMBOLIC Compute the “structure” of the CSC output matrix (offset, row indices) CUSPARSE_ACTION_NUMERIC Compute the “structure” of the CSC output matrix and copy the values cusparseCsr2cscEx2() has the following properties: The routine requires no extra storage The routine supports asynchronous execution cusparseCsr2cscEx2() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. 5.9.9. cusparse<t>nnz()  cusparseStatus_t cusparseSnnz ( cusparseHandle_t handle , cusparseDirection_t dirA , int m , int n , const cusparseMatDescr_t descrA , const float * A , int lda , int * nnzPerRowColumn , int * nnzTotalDevHostPtr ) cusparseStatus_t cusparseDnnz ( cusparseHandle_t handle , cusparseDirection_t dirA , int m , int n , const cusparseMatDescr_t descrA , const double * A , int lda , int * nnzPerRowColumn , int * nnzTotalDevHostPtr ) cusparseStatus_t cusparseCnnz ( cusparseHandle_t handle , cusparseDirection_t dirA , int m , int n , const cusparseMatDescr_t descrA , const cuComplex * A , int lda , int * nnzPerRowColumn , int * nnzTotalDevHostPtr ) cusparseStatus_t cusparseZnnz ( cusparseHandle_t handle , cusparseDirection_t dirA , int m , int n , const cusparseMatDescr_t descrA , const cuDoubleComplex * A , int lda , int * nnzPerRowColumn , int * nnzTotalDevHostPtr ) This function computes the number of nonzero elements per row or column and the total number of nonzero elements in a dense matrix. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. dirA direction that specifies whether to count nonzero elements by CUSPARSE_DIRECTION_ROW or by CUSPARSE_DIRECTION_COLUMN . m number of rows of matrix A . n number of columns of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . A array of dimensions (lda, n) . lda leading dimension of dense array A . Output nnzPerRowColumn array of size m or n containing the number of nonzero elements per row or column, respectively. nnzTotalDevHostPtr total number of nonzero elements in device or host memory. See cusparseStatus_t for the description of the return status. 5.9.10. cusparseCreateIdentityPermutation() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateIdentityPermutation ( cusparseHandle_t handle , int n , int * p ); This function creates an identity map. The output parameter p represents such map by p = 0:1:(n-1) . This function is typically used with coosort , csrsort , cscsort . The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context. n host size of the map. Output parameter device or host description p device integer array of dimensions n . See cusparseStatus_t for the description of the return status. 5.9.11. cusparseXcoosort()  cusparseStatus_t cusparseXcoosort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * cooRows , const int * cooCols , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcoosortByRow ( cusparseHandle_t handle , int m , int n , int nnz , int * cooRows , int * cooCols , int * P , void * pBuffer ) cusparseStatus_t cusparseXcoosortByColumn ( cusparseHandle_t handle , int m , int n , int nnz , int * cooRows , int * cooCols , int * P , void * pBuffer ); This function sorts COO format. The sorting is in-place. Also the user can sort by row or sort by column. A is an m×n sparse matrix that is defined in COO storage format by the three arrays cooVals , cooRows , and cooCols . There is no assumption for the base index of the matrix. coosort uses stable sort on signed integer, so the value of cooRows or cooCols can be negative. This function coosort() requires buffer size returned by coosort_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. The parameter P is both input and output. If the user wants to compute sorted cooVal , P must be set as 0:1:(nnz-1) before coosort() , and after coosort() , new sorted value array satisfies cooVal_sorted = cooVal(P) . Remark: the dimension m and n are not used. If the user does not know the value of m or n , just passes a value positive. This usually happens if the user only reads a COO array first and needs to decide the dimension m or n later. The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . nnz host number of nonzero elements of matrix A . cooRows device integer array of nnz unsorted row indices of A . cooCols device integer array of nnz unsorted column indices of A . P device integer array of nnz unsorted map indices. To construct cooVal , the user has to set P=0:1:(nnz-1) . pBuffer device buffer allocated by the user; the size is returned by coosort_bufferSizeExt() . Output parameter device or host description cooRows device integer array of nnz sorted row indices of A . cooCols device integer array of nnz sorted column indices of A . P device integer array of nnz sorted map indices. pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status Please visit cuSPARSE Library Samples - cusparseXcoosortByRow for a code example. 5.9.12. cusparseXcsrsort()  cusparseStatus_t cusparseXcsrsort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * csrRowPtr , const int * csrColInd , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcsrsort ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const int * csrRowPtr , int * csrColInd , int * P , void * pBuffer ) This function sorts CSR format. The stable sorting is in-place. The matrix type is regarded as CUSPARSE_MATRIX_TYPE_GENERAL implicitly. In other words, any symmetric property is ignored. This function csrsort() requires buffer size returned by csrsort_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. The parameter P is both input and output. If the user wants to compute sorted csrVal , P must be set as 0:1:(nnz-1) before csrsort() , and after csrsort() , new sorted value array satisfies csrVal_sorted = csrVal(P) . The general procedure is as follows: // A is a 3x3 sparse matrix, base-0 //     | 1 2 3 | // A = | 4 5 6 | //     | 7 8 9 | const int m = 3 ; const int n = 3 ; const int nnz = 9 ; csrRowPtr [ m + 1 ] = { 0 , 3 , 6 , 9 }; // on device csrColInd [ nnz ] = { 2 , 1 , 0 , 0 , 2 , 1 , 1 , 2 , 0 }; // on device csrVal [ nnz ] = { 3 , 2 , 1 , 4 , 6 , 5 , 8 , 9 , 7 }; // on device size_t pBufferSizeInBytes = 0 ; void * pBuffer = NULL ; int * P = NULL ; // step 1: allocate buffer cusparseXcsrsort_bufferSizeExt ( handle , m , n , nnz , csrRowPtr , csrColInd , & pBufferSizeInBytes ); cudaMalloc ( & pBuffer , sizeof ( char ) * pBufferSizeInBytes ); // step 2: setup permutation vector P to identity cudaMalloc ( ( void ** ) & P , sizeof ( int ) * nnz ); cusparseCreateIdentityPermutation ( handle , nnz , P ); // step 3: sort CSR format cusparseXcsrsort ( handle , m , n , nnz , descrA , csrRowPtr , csrColInd , P , pBuffer ); // step 4: gather sorted csrVal cusparseDgthr ( handle , nnz , csrVal , csrVal_sorted , P , CUSPARSE_INDEX_BASE_ZERO ); The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . nnz host number of nonzero elements of matrix A . csrRowsPtr device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColInd device integer array of nnz unsorted column indices of A . P device integer array of nnz unsorted map indices. To construct csrVal , the user has to set P=0:1:(nnz-1) . pBuffer device buffer allocated by the user; the size is returned by csrsort_bufferSizeExt() . Output parameter device or host description csrColInd device integer array of nnz sorted column indices of A . P device integer array of nnz sorted map indices. pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.13. cusparseXcscsort()  cusparseStatus_t cusparseXcscsort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * cscColPtr , const int * cscRowInd , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcscsort ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const int * cscColPtr , int * cscRowInd , int * P , void * pBuffer ) This function sorts CSC format. The stable sorting is in-place. The matrix type is regarded as CUSPARSE_MATRIX_TYPE_GENERAL implicitly. In other words, any symmetric property is ignored. This function cscsort() requires buffer size returned by cscsort_bufferSizeExt() . The address of pBuffer must be multiple of 128 bytes. If not, CUSPARSE_STATUS_INVALID_VALUE is returned. The parameter P is both input and output. If the user wants to compute sorted cscVal , P must be set as 0:1:(nnz-1) before cscsort() , and after cscsort() , new sorted value array satisfies cscVal_sorted = cscVal(P) . The general procedure is as follows: // A is a 3x3 sparse matrix, base-0 //     | 1 2  | // A = | 4 0  | //     | 0 8  | const int m = 3 ; const int n = 2 ; const int nnz = 4 ; cscColPtr [ n + 1 ] = { 0 , 2 , 4 }; // on device cscRowInd [ nnz ] = { 1 , 0 , 2 , 0 }; // on device cscVal [ nnz ] = { 4.0 , 1.0 , 8.0 , 2.0 }; // on device size_t pBufferSizeInBytes = 0 ; void * pBuffer = NULL ; int * P = NULL ; // step 1: allocate buffer cusparseXcscsort_bufferSizeExt ( handle , m , n , nnz , cscColPtr , cscRowInd , & pBufferSizeInBytes ); cudaMalloc ( & pBuffer , sizeof ( char ) * pBufferSizeInBytes ); // step 2: setup permutation vector P to identity cudaMalloc ( ( void ** ) & P , sizeof ( int ) * nnz ); cusparseCreateIdentityPermutation ( handle , nnz , P ); // step 3: sort CSC format cusparseXcscsort ( handle , m , n , nnz , descrA , cscColPtr , cscRowInd , P , pBuffer ); // step 4: gather sorted cscVal cusparseDgthr ( handle , nnz , cscVal , cscVal_sorted , P , CUSPARSE_INDEX_BASE_ZERO ); The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . nnz host number of nonzero elements of matrix A . cscColPtr device integer array of n+1 elements that contains the start of every column and the end of the last column plus one. cscRowInd device integer array of nnz unsorted row indices of A . P device integer array of nnz unsorted map indices. To construct cscVal , the user has to set P=0:1:(nnz-1) . pBuffer device buffer allocated by the user; the size is returned by cscsort_bufferSizeExt() . Output parameter device or host description cscRowInd device integer array of nnz sorted row indices of A . P device integer array of nnz sorted map indices. pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.14. cusparseXcsru2csr() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsru2csrInfo ( csru2csrInfo_t * info ); cusparseStatus_t cusparseDestroyCsru2csrInfo ( csru2csrInfo_t info ); cusparseStatus_t cusparseScsru2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , float * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDcsru2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , double * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseCcsru2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , cuComplex * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseZcsru2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , cuDoubleComplex * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseScsru2csr ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , float * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseDcsru2csr ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , double * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseCcsru2csr ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , cuComplex * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseZcsru2csr ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , cuDoubleComplex * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseScsr2csru ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , float * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseDcsr2csru ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , double * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseCcsr2csru ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , cuComplex * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) cusparseStatus_t cusparseZcsr2csru ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , cuDoubleComplex * csrVal , const int * csrRowPtr , int * csrColInd , csru2csrInfo_t info , void * pBuffer ) This function transfers unsorted CSR format to CSR format, and vice versa. The operation is in-place. This function is a wrapper of csrsort and gthr . The usecase is the following scenario. If the user has a matrix A of CSR format which is unsorted, and implements his own code (which can be CPU or GPU kernel) based on this special order (for example, diagonal first, then lower triangle, then upper triangle), and wants to convert it to CSR format when calling CUSPARSE library, and then convert it back when doing something else on his/her kernel. For example, suppose the user wants to solve a linear system Ax=b by the following iterative scheme \\(x^{(k+1)} = x^{(k)} + L^{(-1)}*(b - Ax^{(k)})\\) The code heavily uses SpMV and triangular solve. Assume that the user has an in-house design of SpMV (Sparse Matrix-Vector multiplication) based on special order of A . However the user wants to use the cuSPARSE library for triangular solver. Then the following code can work. do     step 1: compute residual vector r = b - A x (k) by in-house SpMV     step 2: B := sort(A), and L is lower triangular part of B     (only sort A once and keep the permutation vector)     step 3: solve z = L (-1) * ( b - A x (k) ) by cusparseXcsrsv     step 4: add correction x (k+1) = x (k) + z     step 5: A := unsort(B)     (use permutation vector to get back the unsorted CSR) until convergence The requirements of step 2 and step 5 are In-place operation. The permutation vector P is hidden in an opaque structure. No cudaMalloc inside the conversion routine. Instead, the user has to provide the buffer explicitly. The conversion between unsorted CSR and sorted CSR may needs several times, but the function only generates the permutation vector P once. The function is based on csrsort , gather and scatter operations. The operation is called csru2csr , which means unsorted CSR to sorted CSR. Also we provide the inverse operation, called csr2csru . In order to keep the permutation vector invisible, we need an opaque structure called csru2csrInfo . Then two functions ( cusparseCreateCsru2csrInfo , cusparseDestroyCsru2csrInfo ) are used to initialize and to destroy the opaque structure. cusparse[S|D|C|Z]csru2csr_bufferSizeExt returns the size of the buffer. The permutation vector P is also allcated inside csru2csrInfo . The lifetime of the permutation vector is the same as the lifetime of csru2csrInfo . cusparse[S|D|C|Z]csru2csr performs forward transformation from unsorted CSR to sorted CSR. First call uses csrsort to generate the permutation vector P , and subsequent call uses P to do transformation. cusparse[S|D|C|Z]csr2csru performs backward transformation from sorted CSR to unsorted CSR. P is used to get unsorted form back. The routine cusparse<t>csru2csr() has the following properties: The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparse<t>csr2csru() has the following properties if pBuffer != NULL : The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture The following tables describe parameters of csr2csru_bufferSizeExt and csr2csru . Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . nnz host number of nonzero elements of matrix A . descrA host the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrVal device <type> array of nnz unsorted nonzero elements of matrix A . csrRowsPtr device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColInd device integer array of nnz unsorted column indices of A . info host opaque structure initialized using cusparseCreateCsru2csrInfo() . pBuffer device buffer allocated by the user; the size is returned by csru2csr_bufferSizeExt() . Output parameter device or host description csrVal device <type> array of nnz sorted nonzero elements of matrix A . csrColInd device integer array of nnz sorted column indices of A . pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.15. cusparseXpruneDense2csr() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseHpruneDense2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const __half * A , int lda , const __half * threshold , const cusparseMatDescr_t descrC , const __half * csrValC , const int * csrRowPtrC , const int * csrColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseSpruneDense2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const float * A , int lda , const float * threshold , const cusparseMatDescr_t descrC , const float * csrValC , const int * csrRowPtrC , const int * csrColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDpruneDense2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const double * A , int lda , const double * threshold , const cusparseMatDescr_t descrC , const double * csrValC , const int * csrRowPtrC , const int * csrColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseHpruneDense2csrNnz ( cusparseHandle_t handle , int m , int n , const __half * A , int lda , const __half * threshold , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseSpruneDense2csrNnz ( cusparseHandle_t handle , int m , int n , const float * A , int lda , const float * threshold , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseDpruneDense2csrNnz ( cusparseHandle_t handle , int m , int n , const double * A , int lda , const double * threshold , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseHpruneDense2csr ( cusparseHandle_t handle , int m , int n , const __half * A , int lda , const __half * threshold , const cusparseMatDescr_t descrC , __half * csrValC , const int * csrRowPtrC , int * csrColIndC , void * pBuffer ) cusparseStatus_t cusparseSpruneDense2csr ( cusparseHandle_t handle , int m , int n , const float * A , int lda , const float * threshold , const cusparseMatDescr_t descrC , float * csrValC , const int * csrRowPtrC , int * csrColIndC , void * pBuffer ) cusparseStatus_t cusparseDpruneDense2csr ( cusparseHandle_t handle , int m , int n , const double * A , int lda , const double * threshold , const cusparseMatDescr_t descrC , double * csrValC , const int * csrRowPtrC , int * csrColIndC , void * pBuffer ) This function prunes a dense matrix to a sparse matrix with CSR format. Given a dense matrix A and a non-negative value threshold , the function returns a sparse matrix C , defined by \\(\\begin{matrix}\n{{C(i,j)} = {A(i,j)}} & \\text{if\\ |A(i,j)|\\ >\\ threshold} \\\\\n\\end{matrix}\\) The implementation adopts a two-step approach to do the conversion. First, the user allocates csrRowPtrC of m+1 elements and uses function pruneDense2csrNnz() to determine the number of nonzeros columns per row. Second, the user gathers nnzC (number of nonzeros of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC[m]-csrRowPtrC[0]) and allocates csrValC of nnzC elements and csrColIndC of nnzC integers. Finally function pruneDense2csr() is called to complete the conversion. The user must obtain the size of the buffer required by pruneDense2csr() by calling pruneDense2csr_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneDense2csr() . Examples of prune chapter provides a simple example of pruneDense2csr() . The routine cusparse<t>pruneDense2csrNnz() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparse<t>DpruneDense2csr() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . A device array of dimension (lda, n). lda device leading dimension of A . It must be at least max(1, m). threshold host or device a value to drop the entries of A. threshold can point to a device memory or host memory. descrC host the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . pBuffer device buffer allocated by the user; the size is returned by pruneDense2csr_bufferSizeExt() . Output parameter device or host description nnzTotalDevHostPtr device or host total number of nonzero of matrix C . nnzTotalDevHostPtr can point to a device memory or host memory. csrValC device <type> array of nnzC nonzero elements of matrix C . csrRowsPtrC device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndC device integer array of nnzC column indices of C . pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.16. cusparseXpruneCsr2csr()  [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseHpruneCsr2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const __half * csrValA , const int * csrRowPtrA , const int * csrColIndA , const __half * threshold , const cusparseMatDescr_t descrC , const __half * csrValC , const int * csrRowPtrC , const int * csrColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseSpruneCsr2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const float * threshold , const cusparseMatDescr_t descrC , const float * csrValC , const int * csrRowPtrC , const int * csrColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDpruneCsr2csr_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const double * threshold , const cusparseMatDescr_t descrC , const double * csrValC , const int * csrRowPtrC , const int * csrColIndC , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseHpruneCsr2csrNnz ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const __half * csrValA , const int * csrRowPtrA , const int * csrColIndA , const __half * threshold , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseSpruneCsr2csrNnz ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const float * threshold , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseDpruneCsr2csrNnz ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const double * threshold , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , void * pBuffer ) cusparseStatus_t cusparseHpruneCsr2csr ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const __half * csrValA , const int * csrRowPtrA , const int * csrColIndA , const __half * threshold , const cusparseMatDescr_t descrC , __half * csrValC , const int * csrRowPtrC , int * csrColIndC , void * pBuffer ) cusparseStatus_t cusparseSpruneCsr2csr ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const float * threshold , const cusparseMatDescr_t descrC , float * csrValC , const int * csrRowPtrC , int * csrColIndC , void * pBuffer ) cusparseStatus_t cusparseDpruneCsr2csr ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const double * threshold , const cusparseMatDescr_t descrC , double * csrValC , const int * csrRowPtrC , int * csrColIndC , void * pBuffer ) This function prunes a sparse matrix to a sparse matrix with CSR format. Given a sparse matrix A and a non-negative value threshold , the function returns a sparse matrix C , defined by \\(\\begin{matrix}\n{{C(i,j)} = {A(i,j)}} & \\text{if\\ |A(i,j)|\\ >\\ threshold} \\\\\n\\end{matrix}\\) The implementation adopts a two-step approach to do the conversion. First, the user allocates csrRowPtrC of m+1 elements and uses function pruneCsr2csrNnz() to determine the number of nonzeros columns per row. Second, the user gathers nnzC (number of nonzeros of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC[m]-csrRowPtrC[0]) and allocates csrValC of nnzC elements and csrColIndC of nnzC integers. Finally function pruneCsr2csr() is called to complete the conversion. The user must obtain the size of the buffer required by pruneCsr2csr() by calling pruneCsr2csr_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneCsr2csr() . Examples of prune chapter provides a simple example of pruneCsr2csr() . The routine cusparse<t>pruneCsr2csrNnz() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparse<t>pruneCsr2csr() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . nnzA host number of nonzeros of matrix A . descrA host the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA device <type> array of nnzA nonzero elements of matrix A . csrRowsPtrA device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndA device integer array of nnzA column indices of A . threshold host or device a value to drop the entries of A. threshold can point to a device memory or host memory. descrC host the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . pBuffer device buffer allocated by the user; the size is returned by pruneCsr2csr_bufferSizeExt() . Output parameter device or host description nnzTotalDevHostPtr device or host total number of nonzero of matrix C . nnzTotalDevHostPtr can point to a device memory or host memory. csrValC device <type> array of nnzC nonzero elements of matrix C . csrRowsPtrC device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndC device integer array of nnzC column indices of C . pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.17. cusparseXpruneDense2csrPercentage()  [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseHpruneDense2csrByPercentage_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const __half * A , int lda , float percentage , const cusparseMatDescr_t descrC , const __half * csrValC , const int * csrRowPtrC , const int * csrColIndC , pruneInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseSpruneDense2csrByPercentage_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const float * A , int lda , float percentage , const cusparseMatDescr_t descrC , const float * csrValC , const int * csrRowPtrC , const int * csrColIndC , pruneInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDpruneDense2csrByPercentage_bufferSizeExt ( cusparseHandle_t handle , int m , int n , const double * A , int lda , float percentage , const cusparseMatDescr_t descrC , const double * csrValC , const int * csrRowPtrC , const int * csrColIndC , pruneInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseHpruneDense2csrNnzByPercentage ( cusparseHandle_t handle , int m , int n , const __half * A , int lda , float percentage , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseSpruneDense2csrNnzByPercentage ( cusparseHandle_t handle , int m , int n , const float * A , int lda , float percentage , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseDpruneDense2csrNnzByPercentage ( cusparseHandle_t handle , int m , int n , const double * A , int lda , float percentage , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseHpruneDense2csrByPercentage ( cusparseHandle_t handle , int m , int n , const __half * A , int lda , float percentage , const cusparseMatDescr_t descrC , __half * csrValC , const int * csrRowPtrC , int * csrColIndC , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseSpruneDense2csrByPercentage ( cusparseHandle_t handle , int m , int n , const float * A , int lda , float percentage , const cusparseMatDescr_t descrC , float * csrValC , const int * csrRowPtrC , int * csrColIndC , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseDpruneDense2csrByPercentage ( cusparseHandle_t handle , int m , int n , const double * A , int lda , float percentage , const cusparseMatDescr_t descrC , double * csrValC , const int * csrRowPtrC , int * csrColIndC , pruneInfo_t info , void * pBuffer ) This function prunes a dense matrix to a sparse matrix by percentage. Given a dense matrix A and a non-negative value percentage , the function computes sparse matrix C by the following three steps: Step 1: sort absolute value of A in ascending order. \\(\\begin{matrix}\n{key\\ :=\\ sort(\\ |A|\\ )} \\\\\n\\end{matrix}\\) Step 2: choose threshold by the parameter percentage \\(\\begin{matrix}\n{pos\\ =\\ ceil(m*n*(percentage/100))\\ -\\ 1} \\\\\n{pos\\ =\\ min(pos,\\ m*n-1)} \\\\\n{pos\\ =\\ max(pos,\\ 0)} \\\\\n{threshold\\ =\\ key\\lbrack pos\\rbrack} \\\\\n\\end{matrix}\\) Step 3: call pruneDense2csr() by with the parameter threshold . The implementation adopts a two-step approach to do the conversion. First, the user allocates csrRowPtrC of m+1 elements and uses function pruneDense2csrNnzByPercentage() to determine the number of nonzeros columns per row. Second, the user gathers nnzC (number of nonzeros of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC[m]-csrRowPtrC[0]) and allocates csrValC of nnzC elements and csrColIndC of nnzC integers. Finally function pruneDense2csrByPercentage() is called to complete the conversion. The user must obtain the size of the buffer required by pruneDense2csrByPercentage() by calling pruneDense2csrByPercentage_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneDense2csrByPercentage() . Remark 1: the value of percentage must be not greater than 100. Otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Remark 2: the zeros of A are not ignored. All entries are sorted, including zeros. This is different from pruneCsr2csrByPercentage() Examples of prune chapter provides a simple example of pruneDense2csrNnzByPercentage() . The routine cusparse<t>pruneDense2csrNnzByPercentage() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparse<t>pruneDense2csrByPercentage() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . A device array of dimension (lda, n). lda device leading dimension of A . It must be at least max(1, m). percentage host percentage <=100 and percentage >= 0 descrC host the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . pBuffer device buffer allocated by the user; the size is returned by pruneDense2csrByPercentage_bufferSizeExt() . Output parameter device or host description nnzTotalDevHostPtr device or host total number of nonzero of matrix C . nnzTotalDevHostPtr can point to a device memory or host memory. csrValC device <type> array of nnzC nonzero elements of matrix C . csrRowsPtrC device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndC device integer array of nnzC column indices of C . pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.18. cusparseXpruneCsr2csrByPercentage() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseHpruneCsr2csrByPercentage_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const __half * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , const __half * csrValC , const int * csrRowPtrC , const int * csrColIndC , pruneInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseSpruneCsr2csrByPercentage_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , const float * csrValC , const int * csrRowPtrC , const int * csrColIndC , pruneInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseDpruneCsr2csrByPercentage_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , const double * csrValC , const int * csrRowPtrC , const int * csrColIndC , pruneInfo_t info , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseHpruneCsr2csrNnzByPercentage ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const __half * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseSpruneCsr2csrNnzByPercentage ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseDpruneCsr2csrNnzByPercentage ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , int * csrRowPtrC , int * nnzTotalDevHostPtr , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseHpruneCsr2csrByPercentage ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const __half * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , __half * csrValC , const int * csrRowPtrC , int * csrColIndC , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseSpruneCsr2csrByPercentage ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , float * csrValC , const int * csrRowPtrC , int * csrColIndC , pruneInfo_t info , void * pBuffer ) cusparseStatus_t cusparseDpruneCsr2csrByPercentage ( cusparseHandle_t handle , int m , int n , int nnzA , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , float percentage , const cusparseMatDescr_t descrC , double * csrValC , const int * csrRowPtrC , int * csrColIndC , pruneInfo_t info , void * pBuffer ) This function prunes a sparse matrix to a sparse matrix by percentage. Given a sparse matrix A and a non-negative value percentage , the function computes sparse matrix C by the following three steps: Step 1: sort absolute value of A in ascending order. \\(\\begin{matrix}\n{key\\ :=\\ sort(\\ |csrValA|\\ )} \\\\\n\\end{matrix}\\) Step 2: choose threshold by the parameter percentage \\(\\begin{matrix}\n{pos\\ =\\ ceil(nnzA*(percentage/100))\\ -\\ 1} \\\\\n{pos\\ =\\ min(pos,\\ nnzA-1)} \\\\\n{pos\\ =\\ max(pos,\\ 0)} \\\\\n{threshold\\ =\\ key\\lbrack pos\\rbrack} \\\\\n\\end{matrix}\\) Step 3: call pruneCsr2csr() by with the parameter threshold . The implementation adopts a two-step approach to do the conversion. First, the user allocates csrRowPtrC of m+1 elements and uses function pruneCsr2csrNnzByPercentage() to determine the number of nonzeros columns per row. Second, the user gathers nnzC (number of nonzeros of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC[m]-csrRowPtrC[0]) and allocates csrValC of nnzC elements and csrColIndC of nnzC integers. Finally function pruneCsr2csrByPercentage() is called to complete the conversion. The user must obtain the size of the buffer required by pruneCsr2csrByPercentage() by calling pruneCsr2csrByPercentage_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneCsr2csrByPercentage() . Remark 1: the value of percentage must be not greater than 100. Otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned. Examples of prune chapter provides a simple example of pruneCsr2csrByPercentage() . The routine cusparse<t>pruneCsr2csrNnzByPercentage() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparse<t>pruneCsr2csrByPercentage() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context. m host number of rows of matrix A . n host number of columns of matrix A . nnzA host number of nonzeros of matrix A . descrA host the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA device <type> array of nnzA nonzero elements of matrix A . csrRowsPtrA device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndA device integer array of nnzA column indices of A . percentage host percentage <=100 and percentage >= 0 descrC host the descriptor of matrix C . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . pBuffer device buffer allocated by the user; the size is returned by pruneCsr2csrByPercentage_bufferSizeExt() . Output parameter device or host description nnzTotalDevHostPtr device or host total number of nonzero of matrix C . nnzTotalDevHostPtr can point to a device memory or host memory. csrValC device <type> array of nnzC nonzero elements of matrix C . csrRowsPtrC device integer array of m+1 elements that contains the start of every row and the end of the last row plus one. csrColIndC device integer array of nnzC column indices of C . pBufferSizeInBytes host number of bytes of the buffer. See cusparseStatus_t for the description of the return status. 5.9.19. cusparse<t>nnz_compress() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseSnnz_compress ( cusparseHandle_t handle , int m , const cusparseMatDescr_t descr , const float * csrValA , const int * csrRowPtrA , int * nnzPerRow , int * nnzC , float tol ) cusparseStatus_t cusparseDnnz_compress ( cusparseHandle_t handle , int m , const cusparseMatDescr_t descr , const double * csrValA , const int * csrRowPtrA , int * nnzPerRow , int * nnzC , double tol ) cusparseStatus_t cusparseCnnz_compress ( cusparseHandle_t handle , int m , const cusparseMatDescr_t descr , const cuComplex * csrValA , const int * csrRowPtrA , int * nnzPerRow , int * nnzC , cuComplex tol ) cusparseStatus_t cusparseZnnz_compress ( cusparseHandle_t handle , int m , const cusparseMatDescr_t descr , const cuDoubleComplex * csrValA , const int * csrRowPtrA , int * nnzPerRow , int * nnzC , cuDoubleComplex tol ) This function is the step one to convert from csr format to compressed csr format. Given a sparse matrix A and a non-negative value threshold, the function returns nnzPerRow(the number of nonzeros columns per row) and nnzC(the total number of nonzeros) of a sparse matrix C, defined by \\(\\begin{matrix}\n{{C(i,j)} = {A(i,j)}} & \\text{if\\ |A(i,j)|\\ >\\ threshold} \\\\\n\\end{matrix}\\) A key assumption for the cuComplex and cuDoubleComplex case is that this tolerance is given as the real part. For example tol = 1e-8 + 0*i and we extract cureal, that is the x component of this struct. This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context. m number of rows of matrix A . descrA the descriptor of matrix A . The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL . Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE . csrValA csr noncompressed values array csrRowPtrA the corresponding input noncompressed row pointer. tol non-negative tolerance to determine if a number less than or equal to it. Output nnzPerRow this array contains the number of elements whose absolute values are greater than tol per row. nnzC host/device pointer of the total number of elements whose absolute values are greater than tol. See cusparseStatus_t for the description of the return status. 6. cuSPARSE Generic APIs  The cuSPARSE Generic APIs allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (SpMV) and sparse matrix-matrix multiplication (SpMM), in a flexible way. The new APIs have the following capabilities and features: Set matrix data layouts, number of batches, and storage formats (for example, CSR, COO, and so on). Set input/output/compute data types. This also allows mixed data-type computation. Set types of sparse vector/matrix indices (e.g. 32-bit, 64-bit). Choose the algorithm for the computation. Guarantee external device memory for internal operations. Provide extensive consistency checks across input matrices and vectors. This includes the validation of sizes, data types, layout, allowed operations, etc. Provide constant descriptors for vector and matrix inputs to support const-safe interface and guarantee that the APIs do not modify their inputs. 6.1. Generic Types Reference  The cuSPARSE generic type references are described in this section. 6.1.1. cusparseFormat_t  This type indicates the format of the sparse matrix.\nSee cuSPARSE Storage Format for their description. Value Meaning CUSPARSE_FORMAT_COO The matrix is stored in Coordinate (COO) format organized in Structure of Arrays (SoA) layout CUSPARSE_FORMAT_CSR The matrix is stored in Compressed Sparse Row (CSR) format CUSPARSE_FORMAT_CSC The matrix is stored in Compressed Sparse Column (CSC) format CUSPARSE_FORMAT_BLOCKED_ELL The matrix is stored in Blocked-Ellpack (Blocked-ELL) format CUSPARSE_FORMAT_SLICED_ELL The matrix is stored in Sliced-Ellpack (Sliced-ELL) format CUSPARSE_FORMAT_BSR The matrix is stored in Block Sparse Row (BSR) format 6.1.2. cusparseOrder_t  This type indicates the memory layout of a dense matrix. Value Meaning CUSPARSE_ORDER_ROW The matrix is stored in row-major CUSPARSE_ORDER_COL The matrix is stored in column-major 6.1.3. cusparseIndexType_t  This type indicates the index type for representing the sparse matrix indices. Value Meaning CUSPARSE_INDEX_32I 32-bit signed integer [1, 2^31 - 1] CUSPARSE_INDEX_64I 64-bit signed integer [1, 2^63 - 1] 6.2. Dense Vector APIs  The cuSPARSE helper functions for dense vector descriptor are described in this section. See the Dense Vector Format section for the detailed description of the storage format. 6.2.1. cusparseCreateDnVec()  cusparseStatus_t cusparseCreateDnVec ( cusparseDnVecDescr_t * dnVecDescr , int64_t size , void * values , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstDnVec ( cusparseConstDnVecDescr_t * dnVecDescr , int64_t size , const void * values , cudaDataType valueType ) This function initializes the dense vector descriptor dnVecDescr . Param. Memory In/out Meaning dnVecDescr HOST OUT Dense vector descriptor size HOST IN Size of the dense vector values DEVICE IN Values of the dense vector. Array with size elements valueType HOST IN Enumerator specifying the datatype of values cusparseCreateDnVec() has the following constraints: values must be aligned to the size of the datatype specified by valueType . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.2.2. cusparseDestroyDnVec()  cusparseStatus_t cusparseDestroyDnVec ( cusparseConstDnVecDescr_t dnVecDescr ) // non-const descriptor supported This function releases the host memory allocated for the dense vector descriptor dnVecDescr . Param. Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor See cusparseStatus_t for the description of the return status. 6.2.3. cusparseDnVecGet()  cusparseStatus_t cusparseDnVecGet ( cusparseDnVecDescr_t dnVecDescr , int64_t * size , void ** values , cudaDataType * valueType ) cusparseStatus_t cusparseConstDnVecGet ( cusparseConstDnVecDescr_t dnVecDescr , int64_t * size , const void ** values , cudaDataType * valueType ) This function returns the fields of the dense vector descriptor dnVecDescr . Param. Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor size HOST OUT Size of the dense vector values DEVICE OUT Values of the dense vector. Array with nnz elements valueType HOST OUT Enumerator specifying the datatype of values See cusparseStatus_t for the description of the return status. 6.2.4. cusparseDnVecGetValues()  cusparseStatus_t cusparseDnVecGetValues ( cusparseDnVecDescr_t dnVecDescr , void ** values ) cusparseStatus_t cusparseConstDnVecGetValues ( cusparseConstDnVecDescr_t dnVecDescr , const void ** values ) This function returns the values field of the dense vector descriptor dnVecDescr . Param. Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor values DEVICE OUT Values of the dense vector See cusparseStatus_t for the description of the return status. 6.2.5. cusparseDnVecSetValues()  cusparseStatus_t cusparseDnVecSetValues ( cusparseDnVecDescr_t dnVecDescr , void * values ) This function set the values field of the dense vector descriptor dnVecDescr . Param. Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor values DEVICE IN Values of the dense vector. Array with size elements cusparseDnVecSetValues() has the following constraints: values must be aligned to the size of the datatype specified in dnVecDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.3. Sparse Vector APIs  The cuSPARSE helper functions for sparse vector descriptor are described in this section. See the Sparse Vector Format section for the detailed description of the storage format. 6.3.1. cusparseCreateSpVec()  cusparseStatus_t cusparseCreateSpVec ( cusparseSpVecDescr_t * spVecDescr , int64_t size , int64_t nnz , void * indices , void * values , cusparseIndexType_t idxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstSpVec ( cusparseConstSpVecDescr_t * spVecDescr , int64_t size , int64_t nnz , const void * indices , const void * values , cusparseIndexType_t idxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spVecDescr . Param. Memory In/out Meaning spVecDescr HOST OUT Sparse vector descriptor size HOST IN Size of the sparse vector nnz HOST IN Number of non-zero entries of the sparse vector indices DEVICE IN Indices of the sparse vector. Array with nnz elements values DEVICE IN Values of the sparse vector. Array with nnz elements idxType HOST IN Enumerator specifying the data type of indices idxBase HOST IN Enumerator specifying the the index base of indices valueType HOST IN Enumerator specifying the datatype of values cusparseCreateSpVec() has the following constraints: indices and values must be aligned to the size of the datatypes specified by idxType and valueType , respectively. See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.3.2. cusparseDestroySpVec()  cusparseStatus_t cusparseDestroySpVec ( cusparseConstSpVecDescr_t spVecDescr ) // non-const descriptor supported This function releases the host memory allocated for the sparse vector descriptor spVecDescr . Param. Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor See cusparseStatus_t for the description of the return status. 6.3.3. cusparseSpVecGet()  cusparseStatus_t cusparseSpVecGet ( cusparseSpVecDescr_t spVecDescr , int64_t * size , int64_t * nnz , void ** indices , void ** values , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstSpVecGet ( cusparseConstSpVecDescr_t spVecDescr , int64_t * size , int64_t * nnz , const void ** indices , const void ** values , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse vector descriptor spVecDescr . Param. Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor size HOST OUT Size of the sparse vector nnz HOST OUT Number of non-zero entries of the sparse vector indices DEVICE OUT Indices of the sparse vector. Array with nnz elements values DEVICE OUT Values of the sparse vector. Array with nnz elements idxType HOST OUT Enumerator specifying the data type of indices idxBase HOST OUT Enumerator specifying the the index base of indices valueType HOST OUT Enumerator specifying the datatype of values See cusparseStatus_t for the description of the return status. 6.3.4. cusparseSpVecGetIndexBase()  cusparseStatus_t cusparseSpVecGetIndexBase ( cusparseConstSpVecDescr_t spVecDescr , // non-const descriptor supported cusparseIndexBase_t * idxBase ) This function returns the idxBase field of the sparse vector descriptor spVecDescr . Param. Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor idxBase HOST OUT Enumerator specifying the the index base of indices See cusparseStatus_t for the description of the return status. 6.3.5. cusparseSpVecGetValues()  cusparseStatus_t cusparseSpVecGetValues ( cusparseSpVecDescr_t spVecDescr , void ** values ) cusparseStatus_t cusparseConstSpVecGetValues ( cusparseConstSpVecDescr_t spVecDescr , const void ** values ) This function returns the values field of the sparse vector descriptor spVecDescr . Param. Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor values DEVICE OUT Values of the sparse vector. Array with nnz elements See cusparseStatus_t for the description of the return status. 6.3.6. cusparseSpVecSetValues()  cusparseStatus_t cusparseSpVecSetValues ( cusparseSpVecDescr_t spVecDescr , void * values ) This function set the values field of the sparse vector descriptor spVecDescr . Param. Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor values DEVICE IN Values of the sparse vector. Array with nnz elements cusparseDnVecSetValues() has the following constraints: values must be aligned to the size of the datatype specified in spVecDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.4. Dense Matrix APIs  The cuSPARSE helper functions for dense matrix descriptor are described in this section. See the Dense Matrix Format section for the detailed description of the storage format. 6.4.1. cusparseCreateDnMat()  cusparseStatus_t cusparseCreateDnMat ( cusparseDnMatDescr_t * dnMatDescr , int64_t rows , int64_t cols , int64_t ld , void * values , cudaDataType valueType , cusparseOrder_t order ) cusparseStatus_t cusparseCreateConstDnMat ( cusparseConstDnMatDescr_t * dnMatDescr , int64_t rows , int64_t cols , int64_t ld , const void * values , cudaDataType valueType , cusparseOrder_t order ) The function initializes the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST OUT Dense matrix descriptor rows HOST IN Number of rows of the dense matrix cols HOST IN Number of columns of the dense matrix ld HOST IN Leading dimension of the dense matrix values DEVICE IN Values of the dense matrix. Array with size elements valueType HOST IN Enumerator specifying the datatype of values order HOST IN Enumerator specifying the memory layout of the dense matrix cusparseCreateDnMat() has the following constraints: values must be aligned to the size of the datatype specified by valueType . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.4.2. cusparseDestroyDnMat()  cusparseStatus_t cusparseDestroyDnMat ( cusparseConstDnMatDescr_t dnMatDescr ) // non-const descriptor supported This function releases the host memory allocated for the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor See cusparseStatus_t for the description of the return status. 6.4.3. cusparseDnMatGet()  cusparseStatus_t cusparseDnMatGet ( cusparseDnMatDescr_t dnMatDescr , int64_t * rows , int64_t * cols , int64_t * ld , void ** values , cudaDataType * type , cusparseOrder_t * order ) cusparseStatus_t cusparseConstDnMatGet ( cusparseConstDnMatDescr_t dnMatDescr , int64_t * rows , int64_t * cols , int64_t * ld , const void ** values , cudaDataType * type , cusparseOrder_t * order ) This function returns the fields of the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor rows HOST OUT Number of rows of the dense matrix cols HOST OUT Number of columns of the dense matrix ld HOST OUT Leading dimension of the dense matrix values DEVICE OUT Values of the dense matrix. Array with ld * cols elements valueType HOST OUT Enumerator specifying the datatype of values order HOST OUT Enumerator specifying the memory layout of the dense matrix See cusparseStatus_t for the description of the return status. 6.4.4. cusparseDnMatGetValues()  cusparseStatus_t cusparseDnMatGetValues ( cusparseDnMatDescr_t dnMatDescr , void ** values ) cusparseStatus_t cusparseConstDnMatGetValues ( cusparseConstDnMatDescr_t dnMatDescr , const void ** values ) This function returns the values field of the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor values DEVICE OUT Values of the dense matrix. Array with ld * cols elements See cusparseStatus_t for the description of the return status. 6.4.5. cusparseDnMatSetValues()  cusparseStatus_t cusparseDnMatSetValues ( cusparseDnMatDescr_t dnMatDescr , void * values ) This function sets the values field of the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor values DEVICE IN Values of the dense matrix. Array with ld * cols elements cusparseDnMatSetValues() has the following constraints: values must be aligned to the size of the datatype specified in dnMatDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.4.6. cusparseDnMatGetStridedBatch()  cusparseStatus_t cusparseDnMatGetStridedBatch ( cusparseConstDnMatDescr_t dnMatDescr , // non-const descriptor supported int * batchCount , int64_t * batchStride ) The function returns the number of batches and the batch stride of the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor batchCount HOST OUT Number of batches of the dense matrix batchStride HOST OUT Address offset between a matrix and the next one in the batch See cusparseStatus_t for the description of the return status. 6.4.7. cusparseDnMatSetStridedBatch()  cusparseStatus_t cusparseDnMatSetStridedBatch ( cusparseDnMatDescr_t dnMatDescr , int batchCount , int64_t batchStride ) The function sets the number of batches and the batch stride of the dense matrix descriptor dnMatDescr . Param. Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor batchCount HOST IN Number of batches of the dense matrix batchStride HOST IN Address offset between a matrix and the next one in the batch. batchStride ≥ ld * cols if the matrix uses column-major layout, batchStride ≥ ld * rows otherwise See cusparseStatus_t for the description of the return status. 6.5. Sparse Matrix APIs  The cuSPARSE helper functions for sparse matrix descriptor are described in this section. See the COO , CSR , CSC , SELL , BSR , Blocked-Ell sections for the detailed description of the storage formats. 6.5.1. Coordinate (COO)  6.5.1.1. cusparseCreateCoo()  cusparseStatus_t cusparseCreateCoo ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * cooRowInd , void * cooColInd , void * cooValues , cusparseIndexType_t cooIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCoo ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * cooRowInd , const void * cooColInd , const void * cooValues , cusparseIndexType_t cooIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the COO format (Structure of Arrays layout). Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix cooRowInd DEVICE IN Row indices of the sparse matrix. Array with nnz elements cooColInd DEVICE IN Column indices of the sparse matrix. Array with nnz elements cooValues DEVICE IN Values of the sparse matrix. Array with nnz elements cooIdxType HOST IN Data type of cooRowInd and cooColInd idxBase HOST IN Index base of cooRowInd and cooColInd valueType HOST IN Datatype of cooValues cusparseCreateCoo() has the following constraints: cooRowInd , cooColInd , and cooValues must be aligned to the size of the datatypes specified by cooIdxType , cooIdxType , and valueType . respectively. See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.1.2. cusparseCooGet()  cusparseStatus_t cusparseCooGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , void ** cooRowInd , void ** cooColInd , void ** cooValues , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstCooGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , const void ** cooRowInd , const void ** cooColInd , const void ** cooValues , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in COO format (Array of Structures layout). Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix cooRowInd DEVICE OUT Row indices of the sparse matrix. Array nnz elements cooColInd DEVICE OUT Column indices of the sparse matrix. Array nnz elements cooValues DEVICE OUT Values of the sparse matrix. Array nnz elements cooIdxType HOST OUT Data type of cooRowInd and cooColInd idxBase HOST OUT Index base of cooRowInd and cooColInd valueType HOST OUT Datatype of cooValues See cusparseStatus_t for the description of the return status. 6.5.1.3. cusparseCooSetPointers()  cusparseStatus_t cusparseCooSetPointers ( cusparseSpMatDescr_t spMatDescr , void * cooRows , void * cooColumns , void * cooValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor cooRows DEVICE IN Row indices of the sparse matrix. Array with nnz elements cooColumns DEVICE IN Column indices of the sparse matrix. Array with nnz elements cooValues DEVICE IN Values of the sparse matrix. Array with nnz elements cusparseCooSetPointers() has the following constraints: cooRows , cooColumns , and cooValues must be aligned to the size of their corresponding datatypes specified in spMatDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.1.4. cusparseCooSetStridedBatch()  cusparseStatus_t cusparseCooSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t batchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix batchStride HOST IN address offset between consecutive batches See cusparseStatus_t for the description of the return status. 6.5.2. Compressed Sparse Row (CSR)  6.5.2.1. cusparseCreateCsr()  cusparseStatus_t cusparseCreateCsr ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * csrRowOffsets , void * csrColInd , void * csrValues , cusparseIndexType_t csrRowOffsetsType , cusparseIndexType_t csrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCsr ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * csrRowOffsets , const void * csrColInd , const void * csrValues , cusparseIndexType_t csrRowOffsetsType , cusparseIndexType_t csrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the CSR format. Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix csrRowOffsets DEVICE IN Row offsets of the sparse matrix. Array with rows + 1 elements csrColInd DEVICE IN Column indices of the sparse matrix. Array with nnz elements csrValues DEVICE IN Values of the sparse matrix. Array with nnz elements csrRowOffsetsType HOST IN Data type of csrRowOffsets csrColIndType HOST IN Data type of csrColInd idxBase HOST IN Index base of csrRowOffsets and csrColInd valueType HOST IN Datatype of csrValues cusparseCreateCsr() has the following constraints: csrRowOffsets , csrColInd , and csrValues must be aligned to the size of the datatypes specified by csrRowOffsetsType , csrColIndType , and valueType , respectively. See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.2.2. cusparseCsrGet()  cusparseStatus_t cusparseCsrGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , void ** csrRowOffsets , void ** csrColInd , void ** csrValues , cusparseIndexType_t * csrRowOffsetsType , cusparseIndexType_t * csrColIndType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstCsrGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , const void ** csrRowOffsets , const void ** csrColInd , const void ** csrValues , cusparseIndexType_t * csrRowOffsetsType , cusparseIndexType_t * csrColIndType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in CSR format. Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix csrRowOffsets DEVICE OUT Row offsets of the sparse matrix. Array with rows + 1 elements csrColInd DEVICE OUT Column indices of the sparse matrix. Array with nnz elements csrValues DEVICE OUT Values of the sparse matrix. Array with nnz elements csrRowOffsetsType HOST OUT Data type of csrRowOffsets csrColIndType HOST OUT Data type of csrColInd idxBase HOST OUT Index base of csrRowOffsets and csrColInd valueType HOST OUT Datatype of csrValues See cusparseStatus_t for the description of the return status. 6.5.2.3. cusparseCsrSetPointers()  cusparseStatus_t cusparseCsrSetPointers ( cusparseSpMatDescr_t spMatDescr , void * csrRowOffsets , void * csrColInd , void * csrValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor csrRowOffsets DEVICE IN Row offsets of the sparse matrix. Array with rows + 1 elements csrColInd DEVICE IN Column indices of the sparse matrix. Array with nnz elements csrValues DEVICE IN Values of the sparse matrix. Array with nnz elements cusparseCsrSetPointers() has the following constraints: csrRowOffsets , csrColInd , and csrValues must be aligned to the size of their corresponding datatypes specified in spMatDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.2.4. cusparseCsrSetStridedBatch()  cusparseStatus_t cusparseCsrSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t offsetsBatchStride , int64_t columnsValuesBatchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix offsetsBatchStride HOST IN Address offset between consecutive batches for the row offset array columnsValuesBatchStride HOST IN Address offset between consecutive batches for the column and value arrays See cusparseStatus_t for the description of the return status. 6.5.3. Compressed Sparse Column (CSC)  6.5.3.1. cusparseCreateCsc()  cusparseStatus_t cusparseCreateCsc ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * cscColOffsets , void * cscRowInd , void * cscValues , cusparseIndexType_t cscColOffsetsType , cusparseIndexType_t cscRowIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCsc ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * cscColOffsets , const void * cscRowInd , const void * cscValues , cusparseIndexType_t cscColOffsetsType , cusparseIndexType_t cscRowIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the CSC format. Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix cscColOffsets DEVICE IN Column offsets of the sparse matrix. Array with cols + 1 elements cscRowInd DEVICE IN Row indices of the sparse matrix. Array with nnz elements cscValues DEVICE IN Values of the sparse matrix. Array with nnz elements cscColOffsetsType HOST IN Data type of cscColOffsets cscRowIndType HOST IN Data type of cscRowInd idxBase HOST IN Index base of cscColOffsets and cscRowInd valueType HOST IN Datatype of cscValues cusparseCreateCsc() has the following constraints: cscColOffsets , cscRowInd , and cscValues must be aligned to the size of the datatypes specified by cscColOffsetsType , cscRowIndType , and valueType , respectively. See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.3.2. cusparseCscGet()  cusparseStatus_t cusparseCscGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , void ** cscColOffsets , void ** cscRowInd , void ** cscValues , cusparseIndexType_t * cscColOffsetsType , cusparseIndexType_t * cscRowIndType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstCscGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , const void ** cscColOffsets , const void ** cscRowInd , const void ** cscValues , cusparseIndexType_t * cscColOffsetsType , cusparseIndexType_t * cscRowIndType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in CSC format. Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix cscColOffsets DEVICE OUT Col offsets of the sparse matrix. Array with cols + 1 elements cscRowInd DEVICE OUT Row indices of the sparse matrix. Array with nnz elements cscValues DEVICE OUT Values of the sparse matrix. Array with nnz elements cscColOffsetsType HOST OUT Data type of cscColOffsets cscRowIndType HOST OUT Data type of cscRowInd idxBase HOST OUT Index base of cscColOffsets and cscRowInd valueType HOST OUT Datatype of cscValues See cusparseStatus_t for the description of the return status. 6.5.3.3. cusparseCscSetPointers()  cusparseStatus_t cusparseCscSetPointers ( cusparseSpMatDescr_t spMatDescr , void * cscColOffsets , void * cscRowInd , void * cscValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor cscColOffsets DEVICE IN Col offsets of the sparse matrix. Array with cols + 1 elements cscRowInd DEVICE IN Row indices of the sparse matrix. Array with nnz elements cscValues DEVICE IN Values of the sparse matrix. Array with nnz elements cusparseCscSetPointers() has the following constraints: cscColOffsets , cscRowInd , and cscValues must be aligned to the size of their corresponding datatypes specified in spMatDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.4. Blocked-Ellpack (Blocked-ELL)  6.5.4.1. cusparseCreateBlockedEll()  cusparseStatus_t cusparseCreateBlockedEll ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t ellBlockSize , int64_t ellCols , void * ellColInd , void * ellValue , cusparseIndexType_t ellIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstBlockedEll ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t ellBlockSize , int64_t ellCols , const void * ellColInd , const void * ellValue , cusparseIndexType_t ellIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr for the Blocked-Ellpack (ELL) format. Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix ellBlockSize HOST IN Size of the ELL-Block ellCols HOST IN Actual number of columns of the Blocked-Ellpack format ( ellValue columns) ellColInd DEVICE IN Blocked-ELL Column indices. Array with [ellCols / ellBlockSize][rows / ellBlockSize] elements ellValue DEVICE IN Values of the sparse matrix. Array with rows * ellCols elements ellIdxType HOST IN Data type of ellColInd idxBase HOST IN Index base of ellColInd valueType HOST IN Data type of ellValue Blocked-ELL Column indices ( ellColInd ) are in the range [0, cols / ellBlockSize -1] . The array can contain -1 values for indicating empty blocks. See cusparseStatus_t for the description of the return status. 6.5.4.2. cusparseBlockedEllGet()  cusparseStatus_t cusparseBlockedEllGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * ellBlockSize , int64_t * ellCols , void ** ellColInd , void ** ellValue , cusparseIndexType_t * ellIdxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstBlockedEllGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * ellBlockSize , int64_t * ellCols , const void ** ellColInd , const void ** ellValue , cusparseIndexType_t * ellIdxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in Blocked-Ellpack (ELL) format. Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix ellBlockSize HOST OUT Size of the ELL-Block ellCols HOST OUT Actual number of columns of the Blocked-Ellpack format ellColInd DEVICE OUT Column indices for the ELL-Block. Array with [cols / ellBlockSize][rows / ellBlockSize] elements ellValue DEVICE OUT Values of the sparse matrix. Array with rows * ellCols elements ellIdxType HOST OUT Data type of ellColInd idxBase HOST OUT Index base of ellColInd valueType HOST OUT Datatype of ellValue See cusparseStatus_t for the description of the return status. 6.5.5. Sliced-Ellpack (SELL)  6.5.5.1. cusparseCreateSlicedEll()  cusparseStatus_t cusparseCreateSlicedEll ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , int64_t sellValuesSize , int64_t sliceSize , void * sellSliceOffsets , void * sellColInd , void * sellValues , cusparseIndexType_t sellSliceOffsetsType , cusparseIndexType_t sellColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstSlicedEll ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , int64_t sellValuesSize , int64_t sliceSize , const void * sellSliceOffsets , const void * sellColInd , const void * sellValues , cusparseIndexType_t sellSliceOffsetsType , cusparseIndexType_t sellColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr for the Sliced Ellpack (SELL) format. Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of nonzero elements in the sparse matrix sellValuesSize HOST IN Total number of elements in sellValues array (nonzero and padding) sliceSize HOST IN The number of rows per slice sellSliceOffsets DEVICE IN Slice offsets of the sparse matrix. Array of size \\(\\left \\lceil{\\frac{rows}{sliceSize}}\\right \\rceil + 1\\) sellColInd DEVICE IN Column indexes of the sparse matrix. Array of size sellValuesSize sellValues DEVICE IN Values of the sparse matrix. Array of size sellValuesSize elements sellSliceOffsetsType HOST IN Data type of sellSliceOffsets sellColIndType HOST IN Data type of sellColInd idxBase HOST IN Index base of sellColInd valueType HOST IN Data type of sellValues Note Sliced Ellpack Column array sellColInd contains -1 values for indicating padded entries. cusparseCreateSlicedEll() has the following constraints: sellSliceOffsets , sellColInd , and sellValues must be aligned to the size of the datatypes specified by sellSliceOffsetsType , sellColIndType , and valueType , respectively. See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.6. Block Sparse Row (BSR)  6.5.6.1. cusparseCreateBsr()  cusparseStatus_t cusparseCreateBsr ( cusparseSpMatDescr_t * spMatDescr , int64_t brows , int64_t bcols , int64_t bnnz , int64_t rowBlockSize , int64_t colBlockSize , void * bsrRowOffsets , void * bsrColInd , void * bsrValues , cusparseIndexType_t bsrRowOffsetsType , cusparseIndexType_t bsrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType , cusparseOrder_t order ) cusparseStatus_t cusparseCreateConstBsr ( cusparseConstSpMatDescr_t * spMatDescr , int64_t brows , int64_t bcols , int64_t bnnz , int64_t rowBlockSize , int64_t colBlockSize , const void * bsrRowOffsets , const void * bsrColInd , const void * bsrValues , cusparseIndexType_t bsrRowOffsetsType , cusparseIndexType_t bsrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType , cusparseOrder_t order ) This function initializes the sparse matrix descriptor spMatDescr for the Block Compressed Row (BSR) format. Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor brows HOST IN Number of block rows of the sparse matrix bcols HOST IN Number of block columns of the sparse matrix bnnz HOST IN Number of blocks of the sparse matrix rowBlockSize HOST IN Number of rows of each block colBlockSize HOST IN Number of columns of each block bsrRowOffsets DEVICE IN Block row offsets of the sparse matrix. Array of size brows + 1 bsrColInd DEVICE IN Block column indices of the sparse matrix. Array of size bnnz bsrValues DEVICE IN Values of the sparse matrix. Array of size bnnz * rowBlockSize * colBlockSize bsrRowOffsetsType HOST IN Data type of bsrRowOffsets bsrColIndType HOST IN Data type of bsrColInd idxBase HOST IN Base index of bsrRowOffsets and bsrColInd valueType HOST IN Datatype of bsrValues order HOST IN Enumerator specifying the memory layout of values in each block cusparseCreateBsr() has the following constraints: bsrRowOffsets , bsrColInd , and bsrValues must be aligned to the size of the datatypes specified by bsrRowOffsetsType , bsrColIndType , and valueType , respectively. See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.6.2. cusparseBsrSetStridedBatch()  cusparseStatus_t cusparseBsrSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t offsetsBatchStride , int64_t columnsBatchStride , int64_t valuesBatchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix offsetsBatchStride HOST IN Address offset between consecutive batches for the row offset array columnsBatchStride HOST IN Address offset between consecutive batches for the column array valuesBatchStride HOST IN Address offset between consecutive batches for the values array See cusparseStatus_t for the description of the return status. 6.5.7. All Sparse Formats  6.5.7.1. cusparseDestroySpMat()  cusparseStatus_t cusparseDestroySpMat ( cusparseConstSpMatDescr_t spMatDescr ) // non-const descriptor supported This function releases the host memory allocated for the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor See cusparseStatus_t for the description of the return status. 6.5.7.2. cusparseSpMatGetSize()  cusparseStatus_t cusparseSpMatGetSize ( cusparseConstSpMatDescr_t spMatDescr , // non-const descriptor supported int64_t * rows , int64_t * cols , int64_t * nnz ) This function returns the sizes of the sparse matrix spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.3. cusparseSpMatGetFormat()  cusparseStatus_t cusparseSpMatGetFormat ( cusparseConstSpMatDescr_t spMatDescr , // non-const descriptor supported cusparseFormat_t * format ) This function returns the format field of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor format HOST OUT Storage format of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.4. cusparseSpMatGetIndexBase()  cusparseStatus_t cusparseSpMatGetIndexBase ( cusparseConstSpMatDescr_t spMatDescr , // non-const descriptor supported cusparseIndexBase_t * idxBase ) This function returns the idxBase field of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor idxBase HOST OUT Index base of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.5. cusparseSpMatGetValues()  cusparseStatus_t cusparseSpMatGetValues ( cusparseSpMatDescr_t spMatDescr , void ** values ) cusparseStatus_t cusparseConstSpMatGetValues ( cusparseConstSpMatDescr_t spMatDescr , const void ** values ) This function returns the values field of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor values DEVICE OUT Values of the sparse matrix. Array with nnz elements See cusparseStatus_t for the description of the return status. 6.5.7.6. cusparseSpMatSetValues()  cusparseStatus_t cusparseSpMatSetValues ( cusparseSpMatDescr_t spMatDescr , void * values ) This function sets the values field of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor values DEVICE IN Values of the sparse matrix. Array with nnz elements cusparseSpMatSetValues() has the following constraints: values must be aligned to the size of its corresponding datatype specified in spMatDescr . See cudaDataType_t for the description of the datatypes. See cusparseStatus_t for the description of the return status. 6.5.7.7. cusparseSpMatGetStridedBatch()  cusparseStatus_t cusparseSpMatGetStridedBatch ( cusparseConstSpMatDescr_t spMatDescr , // non-const descriptor supported int * batchCount ) This function returns the batchCount field of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST OUT Number of batches of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.8. cusparseSpMatGetAttribute()  cusparseStatus_t cusparseSpMatGetAttribute ( cusparseConstSpMatDescr_t spMatDescr , // non-const descriptor supported cusparseSpMatAttribute_t attribute , void * data , size_t dataSize ) The function gets the attributes of the sparse matrix descriptor spMatDescr . Param. Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor attribute HOST IN Attribute enumerator data HOST OUT Attribute value dataSize HOST IN Size of the attribute in bytes for safety Attribute Meaning Possible Values CUSPARSE_SPMAT_FILL_MODE Indicates if the lower or upper part of a matrix is stored in sparse storage CUSPARSE_FILL_MODE_LOWER CUSPARSE_FILL_MODE_UPPER CUSPARSE_SPMAT_DIAG_TYPE Indicates if the matrix diagonal entries are unity CUSPARSE_DIAG_TYPE_NON_UNIT CUSPARSE_DIAG_TYPE_UNIT See cusparseStatus_t for the description of the return status. 6.5.7.9. cusparseSpMatSetAttribute()  cusparseStatus_t cusparseSpMatSetAttribute ( cusparseSpMatDescr_t spMatDescr , cusparseSpMatAttribute_t attribute , const void * data , size_t dataSize ) The function sets the attributes of the sparse matrix descriptor spMatDescr Param. Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor attribute HOST IN Attribute enumerator data HOST IN Attribute value dataSize HOST IN Size of the attribute in bytes for safety Attribute Meaning Possible Values CUSPARSE_SPMAT_FILL_MODE Indicates if the lower or upper part of a matrix is stored in sparse storage CUSPARSE_FILL_MODE_LOWER CUSPARSE_FILL_MODE_UPPER CUSPARSE_SPMAT_DIAG_TYPE Indicates if the matrix diagonal entries are unity CUSPARSE_DIAG_TYPE_NON_UNIT CUSPARSE_DIAG_TYPE_UNIT See cusparseStatus_t for the description of the return status. 6.6. Generic API Functions  6.6.1. cusparseAxpby()  cusparseStatus_t cusparseAxpby ( cusparseHandle_t handle , const void * alpha , cusparseConstSpVecDescr_t vecX , // non-const descriptor supported const void * beta , cusparseDnVecDescr_t vecY ) The function computes the sum of a sparse vector vecX and a dense vector vecY . \\(\\mathbf{Y} = \\alpha\\mathbf{X} + \\beta\\mathbf{Y}\\) In other words, for i = 0 to n -1 Y [ i ] = beta * Y [ i ] for i = 0 to nnz -1 Y [ X_indices [ i ]] += alpha * X_values [ i ] Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of compute type vecX HOST IN Sparse vector X beta HOST or DEVICE IN \\(\\beta\\) scalar used for multiplication of compute type vecY HOST IN/OUT Dense vector Y cusparseAxpby supports the following index type for representing the sparse vector vecX : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseAxpby supports the following data types: Uniform-precision computation: X / Y / compute CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: X / Y compute CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_C_16F CUDA_C_32F [DEPRECATED] CUDA_C_16BF [DEPRECATED] cusparseAxpby() has the following constraints: The arrays representing the sparse vector vecX must be aligned to 16 bytes cusparseAxpby() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run if the the sparse vector vecX indices are distinct The routine allows indices of vecX to be unsorted cusparseAxpby() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseAxpby for a code example. 6.6.2. cusparseGather()  cusparseStatus_t cusparseGather ( cusparseHandle_t handle , cusparseConstDnVecDescr_t vecY , // non-const descriptor supported cusparseSpVecDescr_t vecX ) The function gathers the elements of the dense vector vecY into the sparse vector vecX In other words, for i = 0 to nnz -1 X_values [ i ] = Y [ X_indices [ i ]] Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context vecX HOST OUT Sparse vector X vecY HOST IN Dense vector Y cusparseGather supports the following index type for representing the sparse vector vecX : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseGather supports the following data types: X / Y CUDA_R_16F CUDA_R_16BF CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F cusparseGather() has the following constraints: The arrays representing the sparse vector vecX must be aligned to 16 bytes cusparseGather() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run if the the sparse vector vecX indices are distinct The routine allows indices of vecX to be unsorted cusparseGather() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseGather for a code example. 6.6.3. cusparseScatter()  cusparseStatus_t cusparseScatter ( cusparseHandle_t handle , cusparseConstSpVecDescr_t vecX , // non-const descriptor supported cusparseDnVecDescr_t vecY ) The function scatters the elements of the sparse vector vecX into the dense vector vecY In other words, for i = 0 to nnz -1 Y [ X_indices [ i ]] = X_values [ i ] Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context vecX HOST IN Sparse vector X vecY HOST OUT Dense vector Y cusparseScatter supports the following index type for representing the sparse vector vecX : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseScatter supports the following data types: X / Y CUDA_R_8I CUDA_R_16F CUDA_R_16BF CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F cusparseScatter() has the following constraints: The arrays representing the sparse vector vecX must be aligned to 16 bytes cusparseScatter() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run if the the sparse vector vecX indices are distinct The routine allows indices of vecX to be unsorted cusparseScatter() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseScatter for a code example. 6.6.4. cusparseRot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseRot ( cusparseHandle_t handle , const void * c_coeff , const void * s_coeff , cusparseSpVecDescr_t vecX , cusparseDnVecDescr_t vecY ) The function computes the Givens rotation matrix \\(G = \\begin{bmatrix}\nc & s \\\\\n{- s} & c \\\\\n\\end{bmatrix}\\) to a sparse vecX and a dense vector vecY In other words, for i = 0 to nnz -1 Y [ X_indices [ i ]] = c * Y [ X_indices [ i ]] - s * X_values [ i ] X_values [ i ] = c * X_values [ i ] + s * Y [ X_indices [ i ]] Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context c_coeff HOST or DEVICE IN cosine element of the rotation matrix vecX HOST IN/OUT Sparse vector X s_coeff HOST or DEVICE IN sine element of the rotation matrix vecY HOST IN/OUT Dense vector Y cusparseRot supports the following index type for representing the sparse vector vecX : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseRot supports the following data types: Uniform-precision computation: X / Y / compute CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: X / Y compute CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_C_16F CUDA_C_32F [DEPRECATED] CUDA_C_16BF [DEPRECATED] cusparseRot() has the following constraints: The arrays representing the sparse vector vecX must be aligned to 16 bytes cusparseRot() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run if the the sparse vector vecX indices are distinct cusparseRot() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseRot for a code example. 6.6.5. cusparseSpVV()  cusparseStatus_t cusparseSpVV_bufferSize ( cusparseHandle_t handle , cusparseOperation_t opX , cusparseConstSpVecDescr_t vecX , // non-const descriptor supported cusparseConstDnVecDescr_t vecY , // non-const descriptor supported void * result , cudaDataType computeType , size_t * bufferSize ) cusparseStatus_t cusparseSpVV ( cusparseHandle_t handle , cusparseOperation_t opX , cusparseConstSpVecDescr_t vecX , // non-const descriptor supported cusparseConstDnVecDescr_t vecY , // non-const descriptor supported void * result , cudaDataType computeType , void * externalBuffer ) The function computes the inner dot product of a sparse vector vecX and a dense vector vecY \\(result = op\\left(\\mathbf{X}\\right) \\cdot \\mathbf{Y}\\) In other words, result = 0 ; for i = 0 to nnz -1 result += op ( X_values [ i ]) * Y [ X_indices [ i ]] \\(\\text{op}(X) = \\begin{cases}\nX & \\text{if op(X) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\n\\overline{X} & \\text{if op(X) == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) The function cusparseSpVV_bufferSize() returns the size of the workspace needed by cusparseSpVV() Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opX HOST IN Operation op(X) that is non-transpose or conjugate transpose vecX HOST IN Sparse vector X vecY HOST IN Dense vector Y result HOST or DEVICE OUT The resulting dot product computeType HOST IN Datatype in which the computation is executed bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpVV externalBuffer DEVICE IN Pointer to a workspace buffer of at least bufferSize bytes cusparseSpVV supports the following index type for representing the sparse vector vecX : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) The data types combinations currently supported for cusparseSpVV are listed below: Uniform-precision computation: X / Y / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: X / Y computeType / result Notes CUDA_R_8I CUDA_R_32I CUDA_R_8I CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_C_16F CUDA_C_32F [DEPRECATED] CUDA_C_16BF CUDA_C_32F [DEPRECATED] cusparseSpVV() has the following constraints: The arrays representing the sparse vector vecX must be aligned to 16 bytes cusparseSpVV() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run if the the sparse vector vecX indices are distinct The routine allows indices of vecX to be unsorted cusparseSpVV() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSpVV for a code example. 6.6.6. cusparseSpMV()  cusparseStatus_t cusparseSpMV_bufferSize ( cusparseHandle_t handle , cusparseOperation_t opA , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnVecDescr_t vecX , // non-const descriptor supported const void * beta , cusparseDnVecDescr_t vecY , cudaDataType computeType , cusparseSpMVAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseSpMV_preprocess ( cusparseHandle_t handle , cusparseOperation_t opA , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnVecDescr_t vecX , // non-const descriptor supported const void * beta , cusparseDnVecDescr_t vecY , cudaDataType computeType , cusparseSpMVAlg_t alg , void * externalBuffer ) cusparseStatus_t cusparseSpMV ( cusparseHandle_t handle , cusparseOperation_t opA , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnVecDescr_t vecX , // non-const descriptor supported const void * beta , cusparseDnVecDescr_t vecY , cudaDataType computeType , cusparseSpMVAlg_t alg , void * externalBuffer ) This function performs the multiplication of a sparse matrix matA and a dense vector vecX \\(\\mathbf{Y} = \\alpha op\\left( \\mathbf{A} \\right) \\cdot \\mathbf{X} + \\beta\\mathbf{Y}\\) where op(A) is a sparse matrix of size \\(m \\times k\\) X is a dense vector of size \\(k\\) Y is a dense vector of size \\(m\\) \\(\\alpha\\) and \\(\\beta\\) are scalars Also, for matrix A \\(\\text{op}(A) == \\begin{cases}\nA & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if op(A) ==CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) The function cusparseSpMV_bufferSize() returns the size of the workspace needed by cusparseSpMV_preprocess() and cusparseSpMV() Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A vecX HOST IN Dense vector X beta HOST or DEVICE IN \\(\\beta\\) scalar used for multiplication of type computeType vecY HOST IN/OUT Dense vector Y computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpMV externalBuffer DEVICE IN Pointer to a workspace buffer of at least bufferSize bytes The sparse matrix formats currently supported are listed below: CUSPARSE_FORMAT_COO CUSPARSE_FORMAT_CSR CUSPARSE_FORMAT_CSC CUSPARSE_FORMAT_SLICED_ELL cusparseSpMV supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseSpMV supports the following data types: Uniform-precision computation: A / X / Y / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: A / X Y computeType Notes CUDA_R_8I CUDA_R_32I CUDA_R_32I CUDA_R_8I CUDA_R_32F CUDA_R_32F CUDA_R_16F CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_16BF CUDA_R_16BF CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_16F CUDA_C_16F [DEPRECATED] CUDA_C_16BF CUDA_C_16BF [DEPRECATED] A X / Y / computeType CUDA_R_32F CUDA_R_64F Mixed Regular/Complex computation: A X / Y / computeType CUDA_R_32F CUDA_C_32F CUDA_R_64F CUDA_C_64F NOTE: CUDA_R_16F , CUDA_R_16BF , CUDA_C_16F , and CUDA_C_16BF data types always imply mixed-precision computation. cusparseSpMV() supports the following algorithms: Algorithm Notes CUSPARSE_SPMV_ALG_DEFAULT Default algorithm for any sparse matrix format. CUSPARSE_SPMV_COO_ALG1 Default algorithm for COO sparse matrix format. May produce slightly different results during different runs with the same input parameters. CUSPARSE_SPMV_COO_ALG2 Provides deterministic (bit-wise) results for each run. If opA != CUSPARSE_OPERATION_NON_TRANSPOSE , it is identical to CUSPARSE_SPMV_COO_ALG1 . CUSPARSE_SPMV_CSR_ALG1 Default algorithm for CSR/CSC sparse matrix format. May produce slightly different results during different runs with the same input parameters. CUSPARSE_SPMV_CSR_ALG2 Provides deterministic (bit-wise) results for each run. If opA != CUSPARSE_OPERATION_NON_TRANSPOSE , it is identical to CUSPARSE_SPMV_CSR_ALG1 . CUSPARSE_SPMV_SELL_ALG1 Default algorithm for Sliced Ellpack sparse matrix format. Provides deterministic (bit-wise) results for each run. Performance notes: CUSPARSE_SPMV_COO_ALG1 and CUSPARSE_SPMV_CSR_ALG1 provide higher performance than CUSPARSE_SPMV_COO_ALG2 and CUSPARSE_SPMV_CSR_ALG2 . In general, opA == CUSPARSE_OPERATION_NON_TRANSPOSE is 3x faster than opA != CUSPARSE_OPERATION_NON_TRANSPOSE . Using cusparseSpMV_preprocess() helps improve performance of cusparseSpMV() in CSR. It is beneficial when we need to run cusparseSpMV() multiple times with a same matrix ( cusparseSpMV_preprocess() is executed only once). cusparseSpMV() has the following properties: The routine requires extra storage for CSR/CSC format (all algorithms) and for COO format with CUSPARSE_SPMV_COO_ALG2 algorithm. Provides deterministic (bit-wise) results for each run only for CUSPARSE_SPMV_COO_ALG2 and CUSPARSE_SPMV_CSR_ALG2 algorithms, and opA == CUSPARSE_OPERATION_NON_TRANSPOSE . The routine supports asynchronous execution. compute-sanitizer could report false race conditions for this routine when beta == 0 . This is for optimization purposes and does not affect the correctness of the computation. The routine allows the indices of matA to be unsorted. cusparseSpMV() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSpMV CSR and cusparseSpMV COO for a code example. 6.6.7. cusparseSpSV()  cusparseStatus_t cusparseSpSV_createDescr ( cusparseSpSVDescr_t * spsvDescr ); cusparseStatus_t cusparseSpSV_destroyDescr ( cusparseSpSVDescr_t spsvDescr ); cusparseStatus_t cusparseSpSV_bufferSize ( cusparseHandle_t handle , cusparseOperation_t opA , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnVecDescr_t vecX , // non-const descriptor supported cusparseDnVecDescr_t vecY , cudaDataType computeType , cusparseSpSVAlg_t alg , cusparseSpSVDescr_t spsvDescr , size_t * bufferSize ) cusparseStatus_t cusparseSpSV_analysis ( cusparseHandle_t handle , cusparseOperation_t opA , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnVecDescr_t vecX , // non-const descriptor supported cusparseDnVecDescr_t vecY , cudaDataType computeType , cusparseSpSVAlg_t alg , cusparseSpSVDescr_t spsvDescr void * externalBuffer ) cusparseStatus_t cusparseSpSV_solve ( cusparseHandle_t handle , cusparseOperation_t opA , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnVecDescr_t vecX , // non-const descriptor supported cusparseDnVecDescr_t vecY , cudaDataType computeType , cusparseSpSVAlg_t alg , cusparseSpSVDescr_t spsvDescr ) cusparseStatus_t cusparseSpSV_updateMatrix ( cusparseHandle_t handle , cusparseSpSVDescr_t spsvDescr , void * newValues , cusparseSpSVUpdate_t updatePart ) The function solves a system of linear equations whose coefficients are represented in a sparse triangular matrix: \\(op\\left( \\mathbf{A} \\right) \\cdot \\mathbf{Y} = \\alpha\\mathbf{X}\\) where op(A) is a sparse square matrix of size \\(m \\times m\\) X is a dense vector of size \\(m\\) Y is a dense vector of size \\(m\\) \\(\\alpha\\) is a scalar Also, for matrix A \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if op(A) == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) The function cusparseSpSV_bufferSize() returns the size of the workspace needed by cusparseSpSV_analysis() and cusparseSpSV_solve() .\nThe function cusparseSpSV_analysis() performs the analysis phase, while cusparseSpSV_solve() executes the solve phase for a sparse triangular linear system.\nThe opaque data structure spsvDescr is used to share information among all functions.\nThe function cusparseSpSV_updateMatrix() updates spsvDescr with new matrix values. The routine supports arbitrary sparsity for the input matrix, but only the upper or lower triangular part is taken into account in the computation. NOTE: all parameters must be consistent across cusparseSpSV API calls and the matrix descriptions and externalBuffer must not be modified between cusparseSpSV_analysis() and cusparseSpSV_solve() . The function cusparseSpSV_updateMatrix() can be used to update the values on the sparse matrix stored inside the opaque data structure spsvDescr Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A vecX HOST IN Dense vector X vecY HOST IN/OUT Dense vector Y computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpSV_analysis() and cusparseSpSV_solve() externalBuffer DEVICE IN/OUT Pointer to a workspace buffer of at least bufferSize bytes. It is used by cusparseSpSV_analysis and cusparseSpSV_solve() spsvDescr HOST IN/OUT Opaque descriptor for storing internal data used across the three steps The sparse matrix formats currently supported are listed below: CUSPARSE_FORMAT_CSR CUSPARSE_FORMAT_COO CUSPARSE_FORMAT_SLICED_ELL The cusparseSpSV() supports the following shapes and properties: CUSPARSE_FILL_MODE_LOWER and CUSPARSE_FILL_MODE_UPPER fill modes CUSPARSE_DIAG_TYPE_NON_UNIT and CUSPARSE_DIAG_TYPE_UNIT diagonal types The fill mode and diagonal type can be set by cusparseSpMatSetAttribute() cusparseSpSV() supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseSpSV() supports the following data types: Uniform-precision computation: A / X / Y / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F cusparseSpSV() supports the following algorithms: Algorithm Notes CUSPARSE_SPSV_ALG_DEFAULT Default algorithm cusparseSpSV() has the following properties: The routine requires extra storage for the analysis phase which is proportional to number of non-zero entries of the sparse matrix Provides deterministic (bit-wise) results for each run for the solving phase cusparseSpSV_solve() The routine supports in-place operation The cusparseSpSV_solve() routine supports asynchronous execution cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines accept NULL for vecX and vecY The routine allows the indices of matA to be unsorted cusparseSpSV() supports the following optimizations : CUDA graph capture Hardware Memory Compression cusparseSpSV_updateMatrix() updates the sparse matrix after calling the analysis phase. This functions supports the following update strategies ( updatePart ): Strategy Notes CUSPARSE_SPSV_UPDATE_GENERAL Updates the sparse matrix values with values of newValues array CUSPARSE_SPSV_UPDATE_DIAGONAL Updates the diagonal part of the matrix with diagonal values stored in newValues array. That is, newValues has the new diagonal values only See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSpSV CSR and cuSPARSE Library Samples - cusparseSpSV COO for code examples. 6.6.8. cusparseSpMM()  cusparseStatus_t cusparseSpMM_bufferSize ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpMMAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseSpMM_preprocess ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpMMAlg_t alg , void * externalBuffer ) cusparseStatus_t cusparseSpMM ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpMMAlg_t alg , void * externalBuffer ) The function performs the multiplication of a sparse matrix matA and a dense matrix matB . \\(\\mathbf{C} = \\alpha op\\left( \\mathbf{A} \\right) \\cdot op\\left( \\mathbf{B} \\right) + \\beta\\mathbf{C}\\) where op(A) is a sparse matrix of size \\(m \\times k\\) op(B) is a dense matrix of size \\(k \\times n\\) C is a dense matrix of size \\(m \\times n\\) \\(\\alpha\\) and \\(\\beta\\) are scalars The routine can be also used to perform the multiplication of a dense matrix and a sparse matrix by switching the dense matrices layout: \\(\\begin{array}{l}\n\\left. \\mathbf{C}_{C} = \\mathbf{B}_{C} \\cdot \\mathbf{A} + \\beta\\mathbf{C}_{C}\\rightarrow \\right. \\\\\n{\\mathbf{C}_{R} = \\mathbf{A}^{T} \\cdot \\mathbf{B}_{R} + \\beta\\mathbf{C}_{R}} \\\\\n\\end{array}\\) where \\(\\mathbf{B}_{C}\\) , \\(\\mathbf{C}_{C}\\) indicate column-major layout, while \\(\\mathbf{B}_{R}\\) , \\(\\mathbf{C}_{R}\\) refer to row-major layout Also, for matrix A and B \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if op(A) == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) \\(\\text{op}(B) = \\begin{cases}\nB & \\text{if op(B) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nB^{T} & \\text{if op(B) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nB^{H} & \\text{if op(B) == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) When using the (conjugate) transpose of the sparse matrix A , this routine may produce slightly different results during different runs with the same input parameters. The function cusparseSpMM_bufferSize() returns the size of the workspace needed by cusparseSpMM() The function cusparseSpMM_preprocess() can be called before cusparseSpMM to speedup the actual computation. It is useful when cusparseSpMM is called multiple times with the same sparsity pattern ( matA ). The values of the matrices ( matA , matB , matC ) can change arbitrarily. It provides performance advantages is used with CUSPARSE_SPMM_CSR_ALG1 or CUSPARSE_SPMM_CSR_ALG3 . For all other formats and algorithms have no effect. Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A matB HOST IN Dense matrix B beta HOST or DEVICE IN \\(\\beta\\) scalar used for multiplication of type computeType matC HOST IN/OUT Dense matrix C computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpMM externalBuffer DEVICE IN Pointer to workspace buffer of at least bufferSize bytes cusparseSpMM supports the following sparse matrix formats: CUSPARSE_FORMAT_COO CUSPARSE_FORMAT_CSR CUSPARSE_FORMAT_CSC CUSPARSE_FORMAT_BSR CUSPARSE_FORMAT_BLOCKED_ELL (1) COO/CSR/CSC/BSR FORMATS cusparseSpMM supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseSpMM supports the following data types: Uniform-precision computation: A / B / C / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: A / B C computeType CUDA_R_8I CUDA_R_32I CUDA_R_32I CUDA_R_8I CUDA_R_32F CUDA_R_32F CUDA_R_16F CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_16BF CUDA_R_16BF CUDA_C_16F CUDA_C_16F CUDA_C_32F [DEPRECATED] CUDA_C_16BF CUDA_C_16BF [DEPRECATED] NOTE: CUDA_R_16F , CUDA_R_16BF , CUDA_C_16F , and CUDA_C_16BF data types always imply mixed-precision computation. cusparseSpMM supports the following algorithms: Algorithm Notes CUSPARSE_SPMM_ALG_DEFAULT Default algorithm for any sparse matrix format CUSPARSE_SPMM_COO_ALG1 Algorithm 1 for COO sparse matrix format May provide better performance for small number of nnz Provides the best performance with column-major layout It supports batched computation May produce slightly different results during different runs with the same input parameters CUSPARSE_SPMM_COO_ALG2 Algorithm 2 for COO sparse matrix format It provides deterministic result Provides the best performance with column-major layout In general, slower than Algorithm 1 It supports batched computation It requires additional memory If opA != CUSPARSE_OPERATION_NON_TRANSPOSE , it is identical to CUSPARSE_SPMM_COO_ALG1 CUSPARSE_SPMM_COO_ALG3 Algorithm 3 for COO sparse matrix format May provide better performance for large number of nnz May produce slightly different results during different runs with the same input parameters CUSPARSE_SPMM_COO_ALG4 Algorithm 4 for COO sparse matrix format Provides better performance with row-major layout It supports batched computation May produce slightly different results during different runs with the same input parameters CUSPARSE_SPMM_CSR_ALG1 Algorithm 1 for CSR/CSC sparse matrix format Provides the best performance with column-major layout It supports batched computation It requires additional memory May produce slightly different results during different runs with the same input parameters CUSPARSE_SPMM_CSR_ALG2 Algorithm 2 for CSR/CSC sparse matrix format Provides the best performance with row-major layout It supports batched computation It requires additional memory May produce slightly different results during different runs with the same input parameters CUSPARSE_SPMM_CSR_ALG3 Algorithm 3 for CSR/CSC sparse matrix format It provides deterministic result It requires additional memory It supports only opA == CUSPARSE_OPERATION_NON_TRANSPOSE It does not support opB == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE It does not support CUDA_C_16F and CUDA_C_16BF data types CUSPARSE_SPMM_BSR_ALG1 Algorithm 1 for BSR sparse matrix format It provides deterministic result It requires no additional memory It supports only opA == CUSPARSE_OPERATION_NON_TRANSPOSE It does not support CUDA_C_16F and CUDA_C_16BF data types It does not support column-major blocks in A Performance notes: Row-major layout provides higher performance than column-major CUSPARSE_SPMM_COO_ALG4 and CUSPARSE_SPMM_CSR_ALG2 should be used with row-major layout, while CUSPARSE_SPMM_COO_ALG1 , CUSPARSE_SPMM_COO_ALG2 , CUSPARSE_SPMM_COO_ALG3 , and CUSPARSE_SPMM_CSR_ALG1 with column-major layout For beta != 1 , most algorithms scale the output matrix before the main computation For n == 1 , the routine may use cusparseSpMV() cusparseSpMM() with all algorithms support the following batch modes except for CUSPARSE_SPMM_CSR_ALG3 : \\(C_{i} = A \\cdot B_{i}\\) \\(C_{i} = A_{i} \\cdot B\\) \\(C_{i} = A_{i} \\cdot B_{i}\\) The number of batches and their strides can be set by using cusparseCooSetStridedBatch , cusparseCsrSetStridedBatch , and cusparseDnMatSetStridedBatch . The maximum number of batches for cusparseSpMM() is 65,535. cusparseSpMM() has the following properties: The routine requires no extra storage for CUSPARSE_SPMM_COO_ALG1 , CUSPARSE_SPMM_COO_ALG3 , CUSPARSE_SPMM_COO_ALG4 , CUSPARSE_SPMM_BSR_ALG1 The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run only for CUSPARSE_SPMM_COO_ALG2 , CUSPARSE_SPMM_CSR_ALG3 , and CUSPARSE_SPMM_BSR_ALG1 algorithms compute-sanitizer could report false race conditions for this routine. This is for optimization purposes and does not affect the correctness of the computation The routine allows the indices of matA to be unsorted cusparseSpMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression Please visit cuSPARSE Library Samples - cusparseSpMM CSR and cusparseSpMM COO for a code example. For batched computation please visit cusparseSpMM CSR Batched and cusparseSpMM COO Batched . (2) BLOCKED-ELLPACK FORMAT cusparseSpMM supports the following data types for CUSPARSE_FORMAT_BLOCKED_ELL format and the following GPU architectures for exploiting NVIDIA Tensor Cores: A / B C computeType opB Compute Capability CUDA_R_16F CUDA_R_16F CUDA_R_16F N , T ≥ 70 CUDA_R_16F CUDA_R_16F CUDA_R_32F N , T ≥ 70 CUDA_R_16F CUDA_R_32F CUDA_R_32F N , T ≥ 70 CUDA_R_8I CUDA_R_32I CUDA_R_32I N column-major ≥ 75 T row-major CUDA_R_16BF CUDA_R_16BF CUDA_R_32F N , T ≥ 80 CUDA_R_16BF CUDA_R_32F CUDA_R_32F N , T ≥ 80 CUDA_R_32F CUDA_R_32F CUDA_R_32F N , T ≥ 80 CUDA_R_64F CUDA_R_64F CUDA_R_64F N , T ≥ 80 cusparseSpMM supports the following algorithms with CUSPARSE_FORMAT_BLOCKED_ELL format: Algorithm Notes CUSPARSE_SPMM_ALG_DEFAULT Default algorithm for any sparse matrix format CUSPARSE_SPMM_BLOCKED_ELL_ALG1 Default algorithm for Blocked-ELL format Performance notes: Blocked-ELL SpMM provides the best performance with Power-of-2 Block-Sizes. Large Block-Sizes (e.g. ≥ 64) provide the best performance. The function has the following limitations: The pointer mode must be equal to CUSPARSE_POINTER_MODE_HOST Only opA == CUSPARSE_OPERATION_NON_TRANSPOSE is supported. opB == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE is not supported. Only CUSPARSE_INDEX_32I is supported. Please visit cuSPARSE Library Samples - cusparseSpMM Blocked-ELL for a code example. See cusparseStatus_t for the description of the return status. 6.6.9. cusparseSpMMOp()  cusparseStatus_t CUSPARSEAPI cusparseSpMMOp_createPlan ( cusparseHandle_t handle , cusparseSpMMOpPlan_t * plan , cusparseOperation_t opA , cusparseOperation_t opB , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpMMOpAlg_t alg , const void * addOperationNvvmBuffer , size_t addOperationBufferSize , const void * mulOperationNvvmBuffer , size_t mulOperationBufferSize , const void * epilogueNvvmBuffer , size_t epilogueBufferSize , size_t * SpMMWorkspaceSize ) cusparseStatus_t cusparseSpMMOp_destroyPlan ( cusparseSpMMOpPlan_t plan ) cusparseStatus_t cusparseSpMMOp ( cusparseSpMMOpPlan_t plan , void * externalBuffer ) NOTE 1: NVRTC and nvJitLink are not currently available on Arm64 Android platforms. NOTE 2: The routine does not support Android and Tegra platforms except Judy (sm87). Experimental : The function performs the multiplication of a sparse matrix matA and a dense matrix matB with custom operators. \\({C^{\\prime}}_{ij} = \\text{epilogue}\\left( {\\sum_{k}^{\\oplus}{op\\left( A_{ik} \\right) \\otimes op\\left( B_{kj} \\right),C_{ij}}} \\right)\\) where op(A) is a sparse matrix of size \\(m \\times k\\) op(B) is a dense matrix of size \\(k \\times n\\) C is a dense matrix of size \\(m \\times n\\) \\(\\oplus\\) , \\(\\otimes\\) , and \\(\\text{epilogue}\\) are custom add , mul , and epilogue operators respectively. Also, for matrix A and B \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\n\\end{cases}\\) \\(\\text{op}(B) = \\begin{cases}\nB & {\\text{if op(}B\\text{) == CUSPARSE_OPERATION_NON_TRANSPOSE}} \\\\\nB^{T} & {\\text{if op(}B\\text{) == CUSPARSE_OPERATION_TRANSPOSE}} \\\\\n\\end{cases}\\) Only opA == CUSPARSE_OPERATION_NON_TRANSPOSE is currently supported The function cusparseSpMMOp_createPlan() returns the size of the workspace and the compiled kernel needed by cusparseSpMMOp() Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) matA HOST IN Sparse matrix A matB HOST IN Dense matrix B matC HOST IN/OUT Dense matrix C computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation addOperationNvvmBuffer HOST IN Pointer to the NVVM buffer containing the custom add operator addOperationBufferSize HOST IN Size in bytes of addOperationNvvmBuffer mulOperationNvvmBuffer HOST IN Pointer to the NVVM buffer containing the custom mul operator mulOperationBufferSize HOST IN Size in bytes of mulOperationNvvmBuffer epilogueNvvmBuffer HOST IN Pointer to the NVVM buffer containing the custom epilogue operator epilogueBufferSize HOST IN Size in bytes of epilogueNvvmBuffer SpMMWorkspaceSize HOST OUT Number of bytes of workspace needed by cusparseSpMMOp The operators must have the following signature and return type __device__ < computetype > add_op ( < computetype > value1 , < computetype > value2 ); __device__ < computetype > mul_op ( < computetype > value1 , < computetype > value2 ); __device__ < computetype > epilogue ( < computetype > value1 , < computetype > value2 ); <computetype> is one of float , double , cuComplex , cuDoubleComplex , or int , cusparseSpMMOp supports the following sparse matrix formats: CUSPARSE_FORMAT_CSR cusparseSpMMOp supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseSpMMOp supports the following data types: Uniform-precision computation: A / B / C / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: A / B C computeType CUDA_R_8I CUDA_R_32I CUDA_R_32I CUDA_R_8I CUDA_R_32F CUDA_R_32F CUDA_R_16F CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_16BF CUDA_R_16BF cusparseSpMMOp supports the following algorithms: Algorithm Notes CUSPARSE_SPMM_OP_ALG_DEFAULT Default algorithm for any sparse matrix format Performance notes: Row-major layout provides higher performance than column-major. cusparseSpMMOp() has the following properties: The routine requires extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run The routine allows the indices of matA to be unsorted cusparseSpMMOp() supports the following optimizations : CUDA graph capture Hardware Memory Compression Please visit cuSPARSE Library Samples - cusparseSpMMOp See cusparseStatus_t for the description of the return status. 6.6.10. cusparseSpSM()  cusparseStatus_t cusparseSpSM_createDescr ( cusparseSpSMDescr_t * spsmDescr ); cusparseStatus_t cusparseSpSM_destroyDescr ( cusparseSpSMDescr_t spsmDescr ); cusparseStatus_t cusparseSpSM_bufferSize ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpSMAlg_t alg , cusparseSpSMDescr_t spsmDescr , size_t * bufferSize ) cusparseStatus_t cusparseSpSM_analysis ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpSMAlg_t alg , cusparseSpSMDescr_t spsmDescr , void * externalBuffer ) cusparseStatus_t cusparseSpSM_solve ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported cusparseDnMatDescr_t matC , cudaDataType computeType , cusparseSpSMAlg_t alg , cusparseSpSMDescr_t spsmDescr ) cusparseStatus_t cusparseSpSM_updateMatrix ( cusparseHandle_t handle , cusparseSpSMDescr_t spsmDescr , void * newValues , cusparseSpSMUpdate_t updatePart ) The function solves a system of linear equations whose coefficients are represented in a sparse triangular matrix: \\(op\\left( \\mathbf{A} \\right) \\cdot \\mathbf{C} = \\mathbf{\\alpha}op\\left( \\mathbf{B} \\right)\\) where op(A) is a sparse square matrix of size \\(m \\times m\\) op(B) is a dense matrix of size \\(m \\times n\\) C is a dense matrix of size \\(m \\times n\\) \\(\\alpha\\) is a scalar Also, for matrix A \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\nA^{H} & \\text{if op(A) == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\\n\\end{cases}\\) \\(\\text{op}(B) = \\begin{cases}\nB & \\text{if op(B) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nB^{T} & \\text{if op(B) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\n& \\text{ } \\\\\n\\end{cases}\\) The function cusparseSpSM_bufferSize() returns the size of the workspace needed by cusparseSpSM_analysis() and cusparseSpSM_solve() .\nThe function cusparseSpSM_analysis() performs the analysis phase, while cusparseSpSM_solve() executes the solve phase for a sparse triangular linear system.\nThe opaque data structure spsmDescr is used to share information among all functions.\nThe function cusparseSpSM_updateMatrix() updates spsmDescr with new matrix values. The routine supports arbitrary sparsity for the input matrix, but only the upper or lower triangular part is taken into account in the computation. cusparseSpSM_bufferSize() requires a buffer size for the analysis phase which is proportional to number of non-zero entries of the sparse matrix The externalBuffer is stored into spsmDescr and used by cusparseSpSM_solve() . For this reason, the device memory buffer must be deallocated only after cusparseSpSM_solve() NOTE: all parameters must be consistent across cusparseSpSM API calls and the matrix descriptions and externalBuffer must not be modified between cusparseSpSM_analysis() and cusparseSpSM_solve() Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A matB HOST IN Dense matrix B matC HOST IN/OUT Dense matrix C computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpSM_analysis() and cusparseSpSM_solve() externalBuffer DEVICE IN/OUT Pointer to a workspace buffer of at least bufferSize bytes. It is used by cusparseSpSM_analysis and cusparseSpSM_solve() spsmDescr HOST IN/OUT Opaque descriptor for storing internal data used across the three steps The sparse matrix formats currently supported are listed below: CUSPARSE_FORMAT_CSR CUSPARSE_FORMAT_COO The cusparseSpSM() supports the following shapes and properties: CUSPARSE_FILL_MODE_LOWER and CUSPARSE_FILL_MODE_UPPER fill modes CUSPARSE_DIAG_TYPE_NON_UNIT and CUSPARSE_DIAG_TYPE_UNIT diagonal types The fill mode and diagonal type can be set by cusparseSpMatSetAttribute() cusparseSpSM() supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseSpSM() supports the following data types: Uniform-precision computation: A / B / C / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F cusparseSpSM() supports the following algorithms: Algorithm Notes CUSPARSE_SPSM_ALG_DEFAULT Default algorithm cusparseSpSM() has the following properties: The routine requires no extra storage Provides deterministic (bit-wise) results for each run for the solving phase cusparseSpSM_solve() The cusparseSpSM_solve() routine supports asynchronous execution The routine supports in-place operation. The same device pointer must be provided to the values parameter of the dense matrices matB and matC . All other dense matrix descriptor parameters (e.g., order ) can be set independently cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines accept descriptors of NULL values for matB and matC . These two routines do not accept NULL descriptors The routine allows the indices of matA to be unsorted cusparseSpSM() supports the following optimizations : CUDA graph capture Hardware Memory Compression cusparseSpSM_updateMatrix() updates the sparse matrix after calling the analysis phase. This functions supports the following update strategies ( updatePart ): Strategy Notes CUSPARSE_SPSM_UPDATE_GENERAL Updates the sparse matrix values with values of newValues array CUSPARSE_SPSM_UPDATE_DIAGONAL Updates the diagonal part of the matrix with diagonal values stored in newValues array. That is, newValues has the new diagonal values only See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSpSM CSR and cuSPARSE Library Samples - cusparseSpSM COO for code examples. 6.6.11. cusparseSDDMM()  cusparseStatus_t cusparseSDDMM_bufferSize ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstDnMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSDDMMAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseSDDMM_preprocess ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstDnMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSDDMMAlg_t alg , void * externalBuffer ) cusparseStatus_t cusparseSDDMM ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstDnMatDescr_t matA , // non-const descriptor supported cusparseConstDnMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSDDMMAlg_t alg , void * externalBuffer ) This function performs the multiplication of matA and matB , followed by an element-wise multiplication with the sparsity pattern of matC . Formally, it performs the following operation: \\(\\mathbf{C} = \\alpha({op}(\\mathbf{A}) \\cdot {op}(\\mathbf{B})) \\circ {spy}(\\mathbf{C}) + \\beta\\mathbf{C}\\) where op(A) is a dense matrix of size \\(m \\times k\\) op(B) is a dense matrix of size \\(k \\times n\\) C is a sparse matrix of size \\(m \\times n\\) \\(\\alpha\\) and \\(\\beta\\) are scalars \\(\\circ\\) denotes the Hadamard (entry-wise) matrix product, and \\({spy}\\left( \\mathbf{C} \\right)\\) is the structural sparsity pattern matrix of C defined as: \\({spy}(\\mathbf{C})_{ij} = \\begin{cases}\n1 & {\\text{if}\\,\\mathbf{C}_{ij}\\,\\text{is an entry stored in}\\,\\texttt{matC}} \\\\\n0 & \\text{otherwise} \\\\\n\\end{cases}.\\) Also, for matrix A and B \\(\\text{op}(A) = \\begin{cases}\nA & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nA^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\n\\end{cases}\\) \\(\\text{op}(B) = \\begin{cases}\nB & \\text{if op(B) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\\nB^{T} & \\text{if op(B) == CUSPARSE_OPERATION_TRANSPOSE} \\\\\n\\end{cases}\\) The function cusparseSDDMM_bufferSize() returns the size of the workspace needed by cusparseSDDMM or cusparseSDDMM_preprocess . The function cusparseSDDMM_preprocess() can be called before cusparseSDDMM to speedup the actual computation. It is useful when cusparseSDDMM is called multiple times with the same sparsity pattern ( matC ). The values of the dense matrices ( matA , matB ) can change arbitrarily. Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Dense matrix matA matB HOST IN Dense matrix matB beta HOST or DEVICE IN \\(\\beta\\) scalar used for multiplication of type computeType matC HOST IN/OUT Sparse matrix matC computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSDDMM externalBuffer DEVICE IN Pointer to a workspace buffer of at least bufferSize bytes Currently supported sparse matrix formats: CUSPARSE_FORMAT_CSR CUSPARSE_FORMAT_BSR cusparseSDDMM() supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) The data types combinations currently supported for cusparseSDDMM are listed below: Uniform-precision computation: A / X / Y / computeType CUDA_R_32F CUDA_R_64F CUDA_C_32F CUDA_C_64F Mixed-precision computation: A / B C computeType CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16F CUDA_R_16F cusparseSDDMM for CUSPARSE_FORMAT_BSR also supports the following mixed-precision computation: A / B C computeType CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF NOTE: CUDA_R_16F , CUDA_R_16BF data types always imply mixed-precision computation. cusparseSDDMM() for CUSPASRE_FORMAT_BSR supports block sizes of 2, 4, 8, 16, 32, 64 and 128. cusparseSDDMM() supports the following algorithms: Algorithm Notes CUSPARSE_SDDMM_ALG_DEFAULT Default algorithm. It supports batched computation. Performance notes: cuspaseSDDMM() for CUSPARSE_FORMAT_CSR provides the best performance when matA and matB satisfy: matA : matA is in row-major order and opA is CUSPARSE_OPERATION_NON_TRANSPOSE , or matA is in col-major order and opA is not CUSPARSE_OPERATION_NON_TRANSPOSE matB : matB is in col-major order and opB is CUSPARSE_OPERATION_NON_TRANSPOSE , or matB is in row-major order and opB is not CUSPARSE_OPERATION_NON_TRANSPOSE cuspaseSDDMM() for CUSPARSE_FORMAT_BSR provides the best performance when matA and matB satisfy: matA : matA is in row-major order and opA is CUSPARSE_OPERATION_NON_TRANSPOSE , or matA is in col-major order and opA is not CUSPARSE_OPERATION_NON_TRANSPOSE matB : matB is in row-major order and opB is CUSPARSE_OPERATION_NON_TRANSPOSE , or matB is in col-major order and opB is not CUSPARSE_OPERATION_NON_TRANSPOSE cusparseSDDMM() supports the following batch modes: \\(C_{i} = (A \\cdot B) \\circ C_{i}\\) \\(C_{i} = \\left( A_{i} \\cdot B \\right) \\circ C_{i}\\) \\(C_{i} = \\left( A \\cdot B_{i} \\right) \\circ C_{i}\\) \\(C_{i} = \\left( A_{i} \\cdot B_{i} \\right) \\circ C_{i}\\) The number of batches and their strides can be set by using cusparseCsrSetStridedBatch and cusparseDnMatSetStridedBatch . The maximum number of batches for cusparseSDDMM() is 65,535. cusparseSDDMM() has the following properties: The routine requires no extra storage Provides deterministic (bit-wise) results for each run The routine supports asynchronous execution The routine allows the indices of matC to be unsorted cusparseSDDMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSDDMM for a code example. For batched computation please visit cusparseSDDMM CSR Batched . 6.6.12. cusparseSpGEMM()  cusparseStatus_t cusparseSpGEMM_createDescr ( cusparseSpGEMMDescr_t * descr ) cusparseStatus_t cusparseSpGEMM_destroyDescr ( cusparseSpGEMMDescr_t descr ) cusparseStatus_t cusparseSpGEMM_workEstimation ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstSpMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr , size_t * bufferSize1 , void * externalBuffer1 ) cusparseStatus_t cusparseSpGEMM_getNumProducts ( cusparseSpGEMMDescr_t spgemmDescr , int64_t * num_prods ) cusparseStatus_t cusparseSpGEMM_estimateMemory ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstSpMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr , float chunk_fraction , size_t * bufferSize3 , void * externalBuffer3 , size_t * bufferSize2 ) cusparseStatus_t cusparseSpGEMM_compute ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstSpMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr , size_t * bufferSize2 , void * externalBuffer2 ) cusparseStatus_t cusparseSpGEMM_copy ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseConstSpMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr ) This function performs the multiplication of two sparse matrices matA and matB . \\(\\mathbf{C^{\\prime}} = \\alpha op\\left( \\mathbf{A} \\right) \\cdot op\\left( \\mathbf{B} \\right) + \\beta\\mathbf{C}\\) where \\(\\alpha,\\) \\(\\beta\\) are scalars, and \\(\\mathbf{C},\\) \\(\\mathbf{C^{\\prime}}\\) have the same sparsity pattern. The functions cusparseSpGEMM_workEstimation() , cusparseSpGEMM_estimateMemory() , and cusparseSpGEMM_compute() are used for both determining the buffer size and performing the actual computation. Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication matA HOST IN Sparse matrix A matB HOST IN Sparse matrix B beta HOST or DEVICE IN \\(\\beta\\) scalar used for multiplication matC HOST IN/OUT Sparse matrix C computeType HOST IN Enumerator specifying the datatype in which the computation is executed alg HOST IN Enumerator specifying the algorithm for the computation spgemmDescr HOST IN/OUT Opaque descriptor for storing internal data used across the three steps num_prods HOST OUT Pointer to a 64-bit integer that stores the number of intermediate products calculated by cusparseSpGEMM_workEstimation chunk_fraction HOST IN The fraction of total intermediate products being computed in a chunk. Used by CUSPARSE_SPGEMM_ALG3 only. Value is in range (0,1]. bufferSize1 HOST IN/OUT Number of bytes of workspace requested by cusparseSpGEMM_workEstimation bufferSize2 HOST IN/OUT Number of bytes of workspace requested by cusparseSpGEMM_compute bufferSize3 HOST IN/OUT Number of bytes of workspace requested by cusparseSpGEMM_estimateMemory externalBuffer1 DEVICE IN Pointer to workspace buffer needed by cusparseSpGEMM_workEstimation and cusparseSpGEMM_compute externalBuffer2 DEVICE IN Pointer to workspace buffer needed by cusparseSpGEMM_compute and cusparseSpGEMM_copy externalBuffer3 DEVICE IN Pointer to workspace buffer needed by cusparseSpGEMM_estimateMemory Currently, the function has the following limitations: Only 32-bit indices CUSPARSE_INDEX_32I is supported Only CSR format CUSPARSE_FORMAT_CSR is supported Only opA , opB equal to CUSPARSE_OPERATION_NON_TRANSPOSE are supported The data types combinations currently supported for cusparseSpGEMM are listed below : Uniform-precision computation: A / B / C / computeType CUDA_R_16F [DEPRECATED] CUDA_R_16BF [DEPRECATED] CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F cusparseSpGEMM routine runs for the following algorithms: Algorithm Notes CUSPARSE_SPGEMM_DEFAULT Default algorithm. Currently, it is CUSPARSE_SPGEMM_ALG1 . CUSPARSE_SPGEMM_ALG1 Algorithm 1 Invokes cusparseSpGEMM_compute twice. The first invocation provides an upper bound of the memory required for the computation. The required memory is generally several times larger of the actual memory used. The user can provide an arbitrary buffer size bufferSize2 in the second invocation. If it is not sufficient, the routine will returns CUSPARSE_STATUS_INSUFFICIENT_RESOURCES status. Provides better performance than other algorithms. Provides deterministic (bit-wise) results for each run. CUSPARSE_SPGEMM_ALG2 Algorithm 2 Invokes cusparseSpGEMM_estimateMemory to get the amount of the memory required for the computation. Requires less memory for the computation than Algorithm 1. Performance is lower than Algorithm 1, higher than Algorithm 3. Provides deterministic (bit-wise) results for each run. CUSPARSE_SPGEMM_ALG3 Algorithm 3 Computes the intermediate products in chunks, one chunk at a time. Invokes cusparseSpGEMM_estimateMemory to get the amount of the memory required for the computation. The user can control the amount of required memory by changing the chunk size via chunk_fraction . The chunk size is a fraction of total intermediate products: chunk_fraction * (*num_prods) . Provides deterministic (bit-wise) results for each run. cusparseSpGEMM() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine allows the indices of matA and matB to be unsorted The routine guarantees the indices of matC to be sorted cusparseSpGEMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSpGEMM for a code example for CUSPARSE_SPGEMM_DEFAULT and CUSPARSE_SPGEMM_ALG1 , and cuSPARSE Library Samples - memory-optimzed cusparseSpGEMM for a code example for CUSPARSE_SPGEMM_ALG2 and CUSPARSE_SPGEMM_ALG3 . 6.6.13. cusparseSpGEMMreuse()  cusparseStatus_t cusparseSpGEMM_createDescr ( cusparseSpGEMMDescr_t * descr ) cusparseStatus_t cusparseSpGEMM_destroyDescr ( cusparseSpGEMMDescr_t descr ) cusparseStatus_t cusparseSpGEMMreuse_workEstimation ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , cusparseSpMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , // non-const descriptor supported cusparseSpMatDescr_t matC , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr , size_t * bufferSize1 , void * externalBuffer1 ) cusparseStatus_t cusparseSpGEMMreuse_nnz ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , cusparseSpMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , // non-const descriptor supported cusparseSpMatDescr_t matC , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr , size_t * bufferSize2 , void * externalBuffer2 , size_t * bufferSize3 , void * externalBuffer3 , size_t * bufferSize4 , void * externalBuffer4 ) cusparseStatus_t CUSPARSEAPI cusparseSpGEMMreuse_copy ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , cusparseSpMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , // non-const descriptor supported cusparseSpMatDescr_t matC , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr , size_t * bufferSize5 , void * externalBuffer5 ) cusparseStatus_t CUSPARSEAPI cusparseSpGEMMreuse_compute ( cusparseHandle_t handle , cusparseOperation_t opA , cusparseOperation_t opB , const void * alpha , cusparseSpMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , // non-const descriptor supported const void * beta , cusparseSpMatDescr_t matC , cudaDataType computeType , cusparseSpGEMMAlg_t alg , cusparseSpGEMMDescr_t spgemmDescr ) This function performs the multiplication of two sparse matrices matA and matB where the structure of the output matrix matC can be reused for multiple computations with different values. \\(\\mathbf{C^{\\prime}} = \\alpha op\\left( \\mathbf{A} \\right) \\cdot op\\left( \\mathbf{B} \\right) + \\beta\\mathbf{C}\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars. The functions cusparseSpGEMMreuse_workEstimation() , cusparseSpGEMMreuse_nnz() , and cusparseSpGEMMreuse_copy() are used for determining the buffer size and performing the actual computation. Note: cusparseSpGEMMreuse() output CSR matrix ( matC ) is sorted by column indices. MEMORY REQUIREMENT: cusparseSpGEMMreuse requires to keep in memory all intermediate products to reuse the structure of the output matrix. On the other hand, the number of intermediate products is orders of magnitude higher than the number of non-zero entries in general. In order to minimize the memory requirements, the routine uses multiple buffers that can be deallocated after they are no more needed. If the number of intermediate product exceeds 2^31-1 , the routine will returns CUSPARSE_STATUS_INSUFFICIENT_RESOURCES status. Currently, the function has the following limitations: Only 32-bit indices CUSPARSE_INDEX_32I is supported Only CSR format CUSPARSE_FORMAT_CSR is supported Only opA , opB equal to CUSPARSE_OPERATION_NON_TRANSPOSE are supported The data types combinations currently supported for cusparseSpGEMMreuse are listed below. Uniform-precision computation: A / B / C / computeType CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F Mixed-precision computation: [DEPRECATED] A / B C computeType CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F cusparseSpGEMMreuse routine runs for the following algorithm: Algorithm Notes CUSPARSE_SPGEMM_DEFAULT CUSPARSE_SPGEMM_CSR_ALG_NONDETERMINITIC Default algorithm. Provides deterministic (bit-wise) structure for the output matrix for each run, while value computation is not deterministic. CUSPARSE_SPGEMM_CSR_ALG_DETERMINITIC Provides deterministic (bit-wise) structure for the output matrix and value computation for each run. cusparseSpGEMMreuse() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine allows the indices of matA and matB to be unsorted The routine guarantees the indices of matC to be sorted cusparseSpGEMMreuse() supports the following optimizations : CUDA graph capture Hardware Memory Compression Refer to cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSpGEMMreuse for a code example. 6.6.14. cusparseSparseToDense()  cusparseStatus_t cusparseSparseToDense_bufferSize ( cusparseHandle_t handle , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseDnMatDescr_t matB , cusparseSparseToDenseAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseSparseToDense ( cusparseHandle_t handle , cusparseConstSpMatDescr_t matA , // non-const descriptor supported cusparseDnMatDescr_t matB , cusparseSparseToDenseAlg_t alg , void * buffer ) The function converts the sparse matrix matA in CSR, CSC, or COO format into its dense representation matB . Blocked-ELL is not currently supported. The function cusparseSparseToDense_bufferSize() returns the size of the workspace needed by cusparseSparseToDense() . Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context matA HOST IN Sparse matrix A matB HOST OUT Dense matrix B alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSparseToDense() buffer DEVICE IN Pointer to workspace buffer cusparseSparseToDense() supports the following index type for representing the sparse matrix matA : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseSparseToDense() supports the following data types: A / B CUDA_R_8I CUDA_R_16F CUDA_R_16BF CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F cusparseSparse2Dense() supports the following algorithm: Algorithm Notes CUSPARSE_SPARSETODENSE_ALG_DEFAULT Default algorithm cusparseSparseToDense() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run The routine allows the indices of matA to be unsorted cusparseSparseToDense() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseSparseToDense for a code example. 6.6.15. cusparseDenseToSparse()  cusparseStatus_t cusparseDenseToSparse_bufferSize ( cusparseHandle_t handle , cusparseConstDnMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , cusparseDenseToSparseAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseDenseToSparse_analysis ( cusparseHandle_t handle , cusparseConstDnMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , cusparseDenseToSparseAlg_t alg , void * buffer ) cusparseStatus_t cusparseDenseToSparse_convert ( cusparseHandle_t handle , cusparseConstDnMatDescr_t matA , // non-const descriptor supported cusparseSpMatDescr_t matB , cusparseDenseToSparseAlg_t alg , void * buffer ) The function converts the dense matrix matA into a sparse matrix matB in CSR, CSC, COO, or Blocked-ELL format. The function cusparseDenseToSparse_bufferSize() returns the size of the workspace needed by cusparseDenseToSparse_analysis() . The function cusparseDenseToSparse_analysis() updates the number of non-zero elements in the sparse matrix descriptor matB . The user is responsible to allocate the memory required by the sparse matrix: Row/Column indices and value arrays for CSC and CSR respectively Row, column, value arrays for COO Column ( ellColInd ), value ( ellValue ) arrays for Blocked-ELL Finally, we call cusparseDenseToSparse_convert() for filling the arrays allocated in the previous step. Param. Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context matA HOST IN Dense matrix A matB HOST OUT Sparse matrix B alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseDenseToSparse_analysis() buffer DEVICE IN Pointer to workspace buffer cusparseDenseToSparse() supports the following index type for representing the sparse vector matB : 32-bit indices ( CUSPARSE_INDEX_32I ) 64-bit indices ( CUSPARSE_INDEX_64I ) cusparseDenseToSparse() supports the following data types: A / B CUDA_R_*8I CUDA_R_16F CUDA_R_16BF CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F cusparseDense2Sparse() supports the following algorithm: Algorithm Notes CUSPARSE_DENSETOSPARSE_ALG_DEFAULT Default algorithm cusparseDenseToSparse() has the following properties: The routine requires no extra storage The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run The routine does not guarantee the indices of matB to be sorted cusparseDenseToSparse() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status. Please visit cuSPARSE Library Samples - cusparseDenseToSparse (CSR) and cuSPARSE Library Samples - cusparseDenseToSparse (Blocked-ELL) for code examples. 7. cuSPARSE Fortran Bindings  The cuSPARSE library is implemented using the C-based CUDA toolchain, and it thus provides a C-style API that makes interfacing to applications written in C or C++ trivial. There are also many applications implemented in Fortran that would benefit from using cuSPARSE, and therefore a cuSPARSE Fortran interface has been developed. Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform and toolchain. In particular, differences may exist in the following areas: Symbol names (capitalization, name decoration) Argument passing (by value or reference) Passing of pointer arguments (size of the pointer) To provide maximum flexibility in addressing those differences, the cuSPARSE Fortran interface is provided in the form of wrapper functions, which are written in C and are located in the file cusparse_fortran.c . This file also contains a few additional wrapper functions (for cudaMalloc() , cudaMemset , and so on) that can be used to allocate memory on the GPU. The cuSPARSE Fortran wrapper code is provided as an example only and needs to be compiled into an application for it to call the cuSPARSE API functions. Providing this source code allows users to make any changes necessary for a particular platform and toolchain. The cuSPARSE Fortran wrapper code has been used to demonstrate interoperability with the compilers g95 0.91 (on 32-bit and 64-bit Linux) and g95 0.92 (on 32-bit and 64-bit Mac OS X). In order to use other compilers, users have to make any changes to the wrapper code that may be required. The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all cuSPARSE functions. To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using CUDA_MALLOC() and CUDA_FREE() ) and to copy data between GPU and CPU memory spaces (using the CUDA_MEMCPY() routines). The sample wrappers provided in cusparse_fortran.c map device pointers to the OS-dependent type size_t , which is 32 bits wide on 32-bit platforms and 64 bits wide on a 64-bit platforms. One approach to dealing with index arithmetic on device pointers in Fortran code is to use C-style macros and to use the C preprocessor to expand them. On Linux and Mac OS X, preprocessing can be done by using the option '-cpp' with g95 or gfortran. The function GET_SHIFTED_ADDRESS() , provided with the cuSPARSE Fortran wrappers, can also be used, as shown in example B. Example B shows the the C++ of example A implemented in Fortran 77 on the host. This example should be compiled with ARCH_64 defined as 1 on a 64-bit OS system and as undefined on a 32-bit OS system. For example, on g95 or gfortran, it can be done directly on the command line using the option -cpp -DARCH_64=1 . 7.1. Fortran Application  c     #define ARCH_64 0\nc     #define ARCH_64 1\n\n      program cusparse_fortran_example\n      implicit none\n      integer cuda_malloc\n      external cuda_free\n      integer cuda_memcpy_c2fort_int\n      integer cuda_memcpy_c2fort_real\n      integer cuda_memcpy_fort2c_int\n      integer cuda_memcpy_fort2c_real\n      integer cuda_memset\n      integer cusparse_create\n      external cusparse_destroy\n      integer cusparse_get_version\n      integer cusparse_create_mat_descr\n      external cusparse_destroy_mat_descr\n      integer cusparse_set_mat_type\n      integer cusparse_get_mat_type\n      integer cusparse_get_mat_fill_mode\n      integer cusparse_get_mat_diag_type\n      integer cusparse_set_mat_index_base\n      integer cusparse_get_mat_index_base\n      integer cusparse_xcoo2csr\n      integer cusparse_dsctr\n      integer cusparse_dcsrmv\n      integer cusparse_dcsrmm\n      external get_shifted_address\n#if ARCH_64\n      integer*8 handle\n      integer*8 descrA\n      integer*8 cooRowIndex\n      integer*8 cooColIndex\n      integer*8 cooVal\n      integer*8 xInd\n      integer*8 xVal\n      integer*8 y\n      integer*8 z\n      integer*8 csrRowPtr\n      integer*8 ynp1\n#else\n      integer*4 handle\n      integer*4 descrA\n      integer*4 cooRowIndex\n      integer*4 cooColIndex\n      integer*4 cooVal\n      integer*4 xInd\n      integer*4 xVal\n      integer*4 y\n      integer*4 z\n      integer*4 csrRowPtr\n      integer*4 ynp1\n#endif\n      integer status\n      integer cudaStat1,cudaStat2,cudaStat3\n      integer cudaStat4,cudaStat5,cudaStat6\n      integer n, nnz, nnz_vector\n      parameter (n=4, nnz=9, nnz_vector=3)\n      integer cooRowIndexHostPtr(nnz)\n      integer cooColIndexHostPtr(nnz)\n      real*8  cooValHostPtr(nnz)\n      integer xIndHostPtr(nnz_vector)\n      real*8  xValHostPtr(nnz_vector)\n      real*8  yHostPtr(2*n)\n      real*8  zHostPtr(2*(n+1))\n      integer i, j\n      integer version, mtype, fmode, dtype, ibase\n      real*8  dzero,dtwo,dthree,dfive\n      real*8  epsilon\n\n\n      write(*,*) \"testing fortran example\"\n\nc     predefined constants (need to be careful with them)\n      dzero = 0.0\n      dtwo  = 2.0\n      dthree= 3.0\n      dfive = 5.0\nc     create the following sparse test matrix in COO format\nc     (notice one-based indexing)\nc     |1.0     2.0 3.0|\nc     |    4.0        |\nc     |5.0     6.0 7.0|\nc     |    8.0     9.0|\n      cooRowIndexHostPtr(1)=1\n      cooColIndexHostPtr(1)=1\n      cooValHostPtr(1)     =1.0\n      cooRowIndexHostPtr(2)=1\n      cooColIndexHostPtr(2)=3\n      cooValHostPtr(2)     =2.0\n      cooRowIndexHostPtr(3)=1\n      cooColIndexHostPtr(3)=4\n      cooValHostPtr(3)     =3.0\n      cooRowIndexHostPtr(4)=2\n      cooColIndexHostPtr(4)=2\n      cooValHostPtr(4)     =4.0\n      cooRowIndexHostPtr(5)=3\n      cooColIndexHostPtr(5)=1\n      cooValHostPtr(5)     =5.0\n      cooRowIndexHostPtr(6)=3\n      cooColIndexHostPtr(6)=3\n      cooValHostPtr(6)     =6.0\n      cooRowIndexHostPtr(7)=3\n      cooColIndexHostPtr(7)=4\n      cooValHostPtr(7)     =7.0\n      cooRowIndexHostPtr(8)=4\n      cooColIndexHostPtr(8)=2\n      cooValHostPtr(8)     =8.0\n      cooRowIndexHostPtr(9)=4\n      cooColIndexHostPtr(9)=4\n      cooValHostPtr(9)     =9.0\nc     print the matrix\n      write(*,*) \"Input data:\"\n      do i=1,nnz\n         write(*,*) \"cooRowIndexHostPtr[\",i,\"]=\",cooRowIndexHostPtr(i)\n         write(*,*) \"cooColIndexHostPtr[\",i,\"]=\",cooColIndexHostPtr(i)\n         write(*,*) \"cooValHostPtr[\",     i,\"]=\",cooValHostPtr(i)\n      enddo\n\nc     create a sparse and dense vector\nc     xVal= [100.0 200.0 400.0]   (sparse)\nc     xInd= [0     1     3    ]\nc     y   = [10.0 20.0 30.0 40.0 | 50.0 60.0 70.0 80.0] (dense)\nc     (notice one-based indexing)\n      yHostPtr(1) = 10.0\n      yHostPtr(2) = 20.0\n      yHostPtr(3) = 30.0\n      yHostPtr(4) = 40.0\n      yHostPtr(5) = 50.0\n      yHostPtr(6) = 60.0\n      yHostPtr(7) = 70.0\n      yHostPtr(8) = 80.0\n      xIndHostPtr(1)=1\n      xValHostPtr(1)=100.0\n      xIndHostPtr(2)=2\n      xValHostPtr(2)=200.0\n      xIndHostPtr(3)=4\n      xValHostPtr(3)=400.0\nc     print the vectors\n      do j=1,2\n         do i=1,n\n            write(*,*) \"yHostPtr[\",i,\",\",j,\"]=\",yHostPtr(i+n*(j-1))\n         enddo\n      enddo\n      do i=1,nnz_vector\n         write(*,*) \"xIndHostPtr[\",i,\"]=\",xIndHostPtr(i)\n         write(*,*) \"xValHostPtr[\",i,\"]=\",xValHostPtr(i)\n      enddo\n\nc     allocate GPU memory and copy the matrix and vectors into it\nc     cudaSuccess=0\nc     cudaMemcpyHostToDevice=1\n      cudaStat1 = cuda_malloc(cooRowIndex,nnz*4)\n      cudaStat2 = cuda_malloc(cooColIndex,nnz*4)\n      cudaStat3 = cuda_malloc(cooVal,     nnz*8)\n      cudaStat4 = cuda_malloc(y,          2*n*8)\n      cudaStat5 = cuda_malloc(xInd,nnz_vector*4)\n      cudaStat6 = cuda_malloc(xVal,nnz_vector*8)\n      if ((cudaStat1 /= 0) .OR.\n     $    (cudaStat2 /= 0) .OR.\n     $    (cudaStat3 /= 0) .OR.\n     $    (cudaStat4 /= 0) .OR.\n     $    (cudaStat5 /= 0) .OR.\n     $    (cudaStat6 /= 0)) then\n         write(*,*) \"Device malloc failed\"\n         write(*,*) \"cudaStat1=\",cudaStat1\n         write(*,*) \"cudaStat2=\",cudaStat2\n         write(*,*) \"cudaStat3=\",cudaStat3\n         write(*,*) \"cudaStat4=\",cudaStat4\n         write(*,*) \"cudaStat5=\",cudaStat5\n         write(*,*) \"cudaStat6=\",cudaStat6\n         stop 2\n      endif\n      cudaStat1 = cuda_memcpy_fort2c_int(cooRowIndex,cooRowIndexHostPtr,\n     $                                   nnz*4,1)\n      cudaStat2 = cuda_memcpy_fort2c_int(cooColIndex,cooColIndexHostPtr,\n     $                                   nnz*4,1)\n      cudaStat3 = cuda_memcpy_fort2c_real(cooVal,    cooValHostPtr,\n     $                                    nnz*8,1)\n      cudaStat4 = cuda_memcpy_fort2c_real(y,      yHostPtr,\n     $                                    2*n*8,1)\n      cudaStat5 = cuda_memcpy_fort2c_int(xInd,       xIndHostPtr,\n     $                                   nnz_vector*4,1)\n      cudaStat6 = cuda_memcpy_fort2c_real(xVal,      xValHostPtr,\n     $                                    nnz_vector*8,1)\n      if ((cudaStat1 /= 0) .OR.\n     $    (cudaStat2 /= 0) .OR.\n     $    (cudaStat3 /= 0) .OR.\n     $    (cudaStat4 /= 0) .OR.\n     $    (cudaStat5 /= 0) .OR.\n     $    (cudaStat6 /= 0)) then\n         write(*,*) \"Memcpy from Host to Device failed\"\n         write(*,*) \"cudaStat1=\",cudaStat1\n         write(*,*) \"cudaStat2=\",cudaStat2\n         write(*,*) \"cudaStat3=\",cudaStat3\n         write(*,*) \"cudaStat4=\",cudaStat4\n         write(*,*) \"cudaStat5=\",cudaStat5\n         write(*,*) \"cudaStat6=\",cudaStat6\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         stop 1\n      endif\n\nc     initialize cusparse library\nc     CUSPARSE_STATUS_SUCCESS=0\n      status = cusparse_create(handle)\n      if (status /= 0) then\n         write(*,*) \"CUSPARSE Library initialization failed\"\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         stop 1\n      endif\nc     get version\nc     CUSPARSE_STATUS_SUCCESS=0\n      status = cusparse_get_version(handle,version)\n      if (status /= 0) then\n         write(*,*) \"CUSPARSE Library initialization failed\"\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cusparse_destroy(handle)\n         stop 1\n      endif\n      write(*,*) \"CUSPARSE Library version\",version\n\nc     create and setup the matrix descriptor\nc     CUSPARSE_STATUS_SUCCESS=0\nc     CUSPARSE_MATRIX_TYPE_GENERAL=0\nc     CUSPARSE_INDEX_BASE_ONE=1\n      status= cusparse_create_mat_descr(descrA)\n      if (status /= 0) then\n         write(*,*) \"Creating matrix descriptor failed\"\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cusparse_destroy(handle)\n         stop 1\n      endif\n      status = cusparse_set_mat_type(descrA,0)\n      status = cusparse_set_mat_index_base(descrA,1)\nc     print the matrix descriptor\n      mtype = cusparse_get_mat_type(descrA)\n      fmode = cusparse_get_mat_fill_mode(descrA)\n      dtype = cusparse_get_mat_diag_type(descrA)\n      ibase = cusparse_get_mat_index_base(descrA)\n      write (*,*) \"matrix descriptor:\"\n      write (*,*) \"t=\",mtype,\"m=\",fmode,\"d=\",dtype,\"b=\",ibase\n\nc     exercise conversion routines (convert matrix from COO 2 CSR format)\nc     cudaSuccess=0\nc     CUSPARSE_STATUS_SUCCESS=0\nc     CUSPARSE_INDEX_BASE_ONE=1\n      cudaStat1 = cuda_malloc(csrRowPtr,(n+1)*4)\n      if (cudaStat1 /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Device malloc failed (csrRowPtr)\"\n         stop 2\n      endif\n      status= cusparse_xcoo2csr(handle,cooRowIndex,nnz,n,\n     $                          csrRowPtr,1)\n      if (status /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Conversion from COO to CSR format failed\"\n         stop 1\n      endif\nc     csrRowPtr = [0 3 4 7 9]\n\nc     exercise Level 1 routines (scatter vector elements)\nc     CUSPARSE_STATUS_SUCCESS=0\nc     CUSPARSE_INDEX_BASE_ONE=1\n      call get_shifted_address(y,n*8,ynp1)\n      status= cusparse_dsctr(handle, nnz_vector, xVal, xInd,\n     $                       ynp1, 1)\n      if (status /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Scatter from sparse to dense vector failed\"\n         stop 1\n      endif\nc     y = [10 20 30 40 | 100 200 70 400]\n\nc     exercise Level 2 routines (csrmv)\nc     CUSPARSE_STATUS_SUCCESS=0\nc     CUSPARSE_OPERATION_NON_TRANSPOSE=0\n      status= cusparse_dcsrmv(handle, 0, n, n, nnz, dtwo,\n     $                       descrA, cooVal, csrRowPtr, cooColIndex,\n     $                       y, dthree, ynp1)\n      if (status /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Matrix-vector multiplication failed\"\n         stop 1\n      endif\n\nc     print intermediate results (y)\nc     y = [10 20 30 40 | 680 760 1230 2240]\nc     cudaSuccess=0\nc     cudaMemcpyDeviceToHost=2\n      cudaStat1 = cuda_memcpy_c2fort_real(yHostPtr, y, 2*n*8, 2)\n      if (cudaStat1 /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Memcpy from Device to Host failed\"\n         stop 1\n      endif\n      write(*,*) \"Intermediate results:\"\n      do j=1,2\n         do i=1,n\n             write(*,*) \"yHostPtr[\",i,\",\",j,\"]=\",yHostPtr(i+n*(j-1))\n         enddo\n      enddo\n\nc     exercise Level 3 routines (csrmm)\nc     cudaSuccess=0\nc     CUSPARSE_STATUS_SUCCESS=0\nc     CUSPARSE_OPERATION_NON_TRANSPOSE=0\n      cudaStat1 = cuda_malloc(z, 2*(n+1)*8)\n      if (cudaStat1 /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Device malloc failed (z)\"\n         stop 2\n      endif\n      cudaStat1 = cuda_memset(z, 0, 2*(n+1)*8)\n      if (cudaStat1 /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(z)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Memset on Device failed\"\n         stop 1\n      endif\n      status= cusparse_dcsrmm(handle, 0, n, 2, n, nnz, dfive,\n     $                        descrA, cooVal, csrRowPtr, cooColIndex,\n     $                        y, n, dzero, z, n+1)\n      if (status /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(z)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Matrix-matrix multiplication failed\"\n         stop 1\n      endif\n\nc     print final results (z)\nc     cudaSuccess=0\nc     cudaMemcpyDeviceToHost=2\n      cudaStat1 = cuda_memcpy_c2fort_real(zHostPtr, z, 2*(n+1)*8, 2)\n      if (cudaStat1 /= 0) then\n         call cuda_free(cooRowIndex)\n         call cuda_free(cooColIndex)\n         call cuda_free(cooVal)\n         call cuda_free(xInd)\n         call cuda_free(xVal)\n         call cuda_free(y)\n         call cuda_free(z)\n         call cuda_free(csrRowPtr)\n         call cusparse_destroy_mat_descr(descrA)\n         call cusparse_destroy(handle)\n         write(*,*) \"Memcpy from Device to Host failed\"\n         stop 1\n      endif\nc     z = [950 400 2550 2600 0 | 49300 15200 132300 131200 0]\n      write(*,*) \"Final results:\"\n      do j=1,2\n         do i=1,n+1\n            write(*,*) \"z[\",i,\",\",j,\"]=\",zHostPtr(i+(n+1)*(j-1))\n         enddo\n      enddo\n\nc     check the results\n      epsilon = 0.00000000000001\n      if ((DABS(zHostPtr(1) - 950.0)   .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(2) - 400.0)   .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(3) - 2550.0)  .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(4) - 2600.0)  .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(5) - 0.0)     .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(6) - 49300.0) .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(7) - 15200.0) .GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(8) - 132300.0).GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(9) - 131200.0).GT. epsilon)  .OR.\n     $    (DABS(zHostPtr(10) - 0.0)    .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(1) - 10.0)    .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(2) - 20.0)    .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(3) - 30.0)    .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(4) - 40.0)    .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(5) - 680.0)   .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(6) - 760.0)   .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(7) - 1230.0)  .GT. epsilon)  .OR.\n     $    (DABS(yHostPtr(8) - 2240.0)  .GT. epsilon)) then\n          write(*,*) \"fortran example test FAILED\"\n       else\n          write(*,*) \"fortran example test PASSED\"\n       endif\n\nc      deallocate GPU memory and exit\n       call cuda_free(cooRowIndex)\n       call cuda_free(cooColIndex)\n       call cuda_free(cooVal)\n       call cuda_free(xInd)\n       call cuda_free(xVal)\n       call cuda_free(y)\n       call cuda_free(z)\n       call cuda_free(csrRowPtr)\n       call cusparse_destroy_mat_descr(descrA)\n       call cusparse_destroy(handle)\n\n       stop 0\n       end 8. Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: The cusparse<t>gtsv implementation is derived from a version developed by Li-Wen Chang from the University of Illinois. The cusparse<t>gtsvInterleavedBatch adopts cuThomasBatch developed by Pedro Valero-Lara and Ivan Martínez-Pérez from Barcelona Supercomputing Center and BSC/UPC NVIDIA GPU Center of Excellence. This product includes {fmt} - A modern formatting library https://fmt.dev Copyright (c) 2012 - present, Victor Zverovich. 9. Bibliography  [1] N. Bell and M. Garland, “Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors” , Supercomputing, 2009. [2] R. Grimes, D. Kincaid, and D. Young, “ITPACK 2.0 User’s Guide”, Technical Report CNA-150, Center for Numerical Analysis, University of Texas, 1979. [3] M. Naumov, “Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS” , Technical Report and White Paper, 2011. [4] Pedro Valero-Lara, Ivan Martínez-Pérez, Raül Sirvent, Xavier Martorell, and Antonio J. Peña. NVIDIA GPUs Scalability to Solve Multiple (Batch) Tridiagonal Systems. Implementation of cuThomasBatch. In Parallel Processing and Applied Mathematics - 12th International Conference (PPAM), 2017. 10. Notices  10.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ada-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/ada-tuning-guide/index.html", "content_type": "text/html", "text": "NVIDIA Ada GPU Architecture Tuning Guide 1. NVIDIA Ada GPU Architecture Tuning Guide 1.1. NVIDIA Ada GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ada GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Improved Tensor Core Operations 1.4.1.3. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased L2 capacity 1.4.2.2. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Ada Tuning Guide » 1. NVIDIA Ada GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ada GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ada GPU Architecture. 1. NVIDIA Ada GPU Architecture Tuning Guide  1.1. NVIDIA Ada GPU Architecture  The NVIDIA ® Ada GPU architecture is NVIDIA’s latest architecture for CUDA ® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ada GPU Architecture. 1.4. NVIDIA Ada GPU Architecture Tuning  1.4.1. Streaming Multiprocessor  The NVIDIA Ada GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM is 48, remaining the same compared to compute capability 8.6 GPUs, and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 24. The shared memory capacity per SM is 100 KB. The maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on compute capability 8.6 GPUs without changes to their application. 1.4.1.2. Improved Tensor Core Operations  The NVIDIA Ada GPU architecture includes new Ada Fourth Generation Tensor Cores featuring the Hopper FP8 Transformer Engine. 1.4.1.3. Improved FP32 throughput  Devices of compute capability 8.9 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run as-is on 8.9, it is recommended to compile explicitly for 8.9 to benefit from the increased FP32 throughput. 1.4.2. Memory System  1.4.2.1. Increased L2 capacity  The NVIDIA Ada GPU architecture increases the capacity of the L2 cache to 98304 KB in AD102, 16x larger than GA102. The NVIDIA Ada GPU architecture allows CUDA users to control the persistence of data in the L2 cache. For more information on the persistence of data in the L2 cache, refer to the section on managing the L2 cache in the CUDA C++ Programming Guide . 1.4.2.2. Unified Shared Memory/L1/Texture Cache  NVIDIA Ada architecture features a unified L1 cache, texture cache, and shared memory similar to that of the NVIDIA Ampere architecture. The combined L1 cache capacity is 128 KB. In the NVIDIA Ada GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA Ada GPU architecture supports shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 2. Revision History  Version 1.0 Initial Public Release Added support for compute capability 8.9 1 Throughout this guide, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.0 and 8.6, NVIDIA Ada refers to devices of compute capability 8.9. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html", "content_type": "text/html", "text": "NVIDIA Hopper Tuning Guide 1. NVIDIA Hopper Tuning Guide 1.1. NVIDIA Hopper GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Hopper Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Tensor Memory Accelerator 1.4.1.3. Thread Block Clusters 1.4.1.4. Improved FP32 Throughput 1.4.1.5. Dynamic Programming Instructions 1.4.2. Memory System 1.4.2.1. High-Bandwidth Memory HBM3 Subsystem 1.4.2.2. Increased L2 Capacity 1.4.2.3. Inline Compression 1.4.2.4. Unified Shared Memory/L1/Texture Cache 1.4.3. Fourth-Generation NVLink 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Hopper Tuning Guide » 1. NVIDIA Hopper Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Hopper GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the Hopper GPU Architecture. 1. NVIDIA Hopper Tuning Guide  1.1. NVIDIA Hopper GPU Architecture  The NVIDIA® Hopper GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Hopper GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere GPU architecture and NVIDIA Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA H100 GPU without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Hopper GPU architecture’s features. 1 For further details on the programming features discussed in this guide, refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure that global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Hopper Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with NVIDIA Hopper. 1.4. NVIDIA Hopper Tuning  1.4.1. Streaming Multiprocessor  The NVIDIA Hopper Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM remains the same as in NVIDIA Ampere GPU architecture (that is, 64), and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 9.0 (that is, H100 GPUs). For devices of compute capability 9.0 (H100 GPUs), shared memory capacity per SM is 228 KB, a 39% increase compared to A100’s capacity of 164 KB. For devices of compute capability 9.0 (H100 GPUs), the maximum shared memory per thread block is 227 KB. For applications using Thread Block Clusters, it is always recommended to compute the occupancy using cudaOccupancyMaxActiveClusters and launch cluster-based kernels accordingly. Overall, developers can expect similar occupancy as on NVIDIA Ampere GPU architecture GPUs without changes to their application. 1.4.1.2. Tensor Memory Accelerator  The Hopper architecture builds on top of the asynchronous copies introduced by NVIDIA Ampere GPU architecture and provides a more sophisticated asynchronous copy engine: the Tensor Memory Accelerator (TMA). TMA allows applications to transfer 1D and up to 5D tensors between global memory and shared memory, in both directions, as well as between the shared memory regions of different SMs in the same cluster (refer to Thread Block Clusters ). Additionally, for writes from shared memory to global memory, it allows specifying element wise reduction operations such as add/min/max as well as bitwise and/or for most common data types. This has several advantages: Avoids using registers for moving data between the different memory spaces. Avoids using SM instructions for moving data: a single thread can issue large data movement instructions to the TMA unit. The whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually necessary. Enables users to write warp specialized codes, where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the SM. This feature will be exposed through cuda::memcpy_async along with the cuda::barrier and cuda::pipeline for synchronizing data movement. 1.4.1.3. Thread Block Clusters  NVIDIA Hopper Architecture adds a new optional level of hierarchy, Thread Block Clusters, that allows for further possibilities when parallelizing applications. A thread block can read from, write to, and perform atomics in shared memory of other thread blocks within its cluster. This is known as Distributed Shared Memory. As demonstrated in the CUDA C++ Programming Guide , there are applications that cannot fit required data within shared memory and must use global memory instead. Distributed shared memory can act as an intermediate step between these two options. Distributed Shared Memory can be used by an SM simultaneously with L2 cache accesses. This can benefit applications that need to communicate data between SMs by utilizing the combined bandwidth of both distributed shared memory and L2. In order to achieve best performance for accesses to Distributed Shared Memory,\naccess patterns to those described in the CUDA C++ Best Practices Guide for Global Memory should be used.\nSpecifically, accesses to Distributed Shared Memory should be coalesced\nand aligned to 32-byte segments, if possible.\nAccess patterns with non-unit stride should be avoided if possible,\nwhich can be achieved by using local shared memory,\nsimilar to what is shown in the CUDA C++ Best Practices Guide for Shared Memory . The maximum portable cluster size supported is 8; however, NVIDIA Hopper H100 GPU allows for a nonportable cluster size of 16 by opting in. Launching a kernel with a nonportable cluster size requires setting the cudaFuncAttributeNonPortableClusterSizeAllowed function attribute. Using larger cluster sizes may reduce the maximum number of active blocks across the GPU (refer to Occupancy ). 1.4.1.4. Improved FP32 Throughput  Devices of compute capability 9.0 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. 1.4.1.5. Dynamic Programming Instructions  The NVIDIA Hopper architecture adds support for new instructions to accelerate dynamic programming algorithms, such as the Smith-Waterman algorithm for sequence alignment in bioinformatics, and algorithms in graph theory, game theory, ML, and finance problems. The new instructions permit computation of max and min values among three operands, max and min operations yielding predicates, combined add operation with max or min, operating on signed and unsigned 32-bit int and 16-bit short2 types, and half2. All DPX instructions with 16-bit short types DPX instructions enable 128 operations per cycle per SM. 1.4.2. Memory System  1.4.2.1. High-Bandwidth Memory HBM3 Subsystem  The NVIDIA H100 GPU has support for HBM3 and HBM2e memory, with capacity up to 80 GB. GPUs HBM3 memory system supports up to 3 TB/s memory bandwidth, a 93% increase over the 1.55 TB/s on A100-40GB. 1.4.2.2. Increased L2 Capacity  The NVIDIA Hopper architecture increases the L2 cache capacity from 40 MB in the A100 GPU to 50 MB in the H100 GPU. Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased. The NVIDIA Hopper architecture allows CUDA users to control the persistence of data in L2 cache similar to the NVIDIA Ampere GPU Architecture. For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Inline Compression  The NVIDIA Hopper architecture allows CUDA compute kernels to benefit from the new inline compression (ILC). This feature can be applied to individual memory allocation, and the compressor automatically chooses between several possible compression algorithms, or none if there is no suitable pattern. In case compression can be used, this feature allows accessing global memory at significantly higher bandwidth than global memory bandwidth, since only compressed data needs to be transferred between global memory and SMs. However, the feature does not allow for reducing memory footprint: since compression is automatic, even if compression is active, the memory region will use the same footprint as if there was no compression. This is because underlying data may be changed by the user application and may not be compressible during the entire duration of the application. The feature is available through the CUDA driver API. See the CUDA C++ Programming Guide section on compressible memory : CUmemGenericAllocationHandle allocationHandle ; CUmemAllocationProp prop = {}; memset ( prop , 0 , sizeof ( CUmemAllocationProp )); prop -> type = CU_MEM_ALLOCATION_TYPE_PINNED ; prop -> location . type = CU_MEM_LOCATION_TYPE_DEVICE ; prop -> location . id = currentDevice ; prop -> allocFlags . compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; cuMemCreate ( & allocationHandle , size , & prop , 0 ); One can check whether compressible memory is available on the given device with: cuDeviceGetAttribute ( & compressionAvailable , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , currentDevice ) Note that this example code does not handle errors and compiling this code requires linking against the CUDA library ( libcuda.so ). 1.4.2.4. Unified Shared Memory/L1/Texture Cache  The NVIDIA H100 GPU based on compute capability 9.0 increases the maximum capacity of the combined L1 cache, texture cache, and shared memory to 256 KB, from 192 KB in NVIDIA Ampere Architecture, an increase of 33%. In the NVIDIA Hopper GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout) can be selected at runtime as in previous architectures such as NVIDIA Ampere Architecture and NVIDIA Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA H100 GPU supports shared memory capacities of 0, 8, 16, 32, 64, 100, 132, 164, 196 and 228 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, the H100 GPU enables a single thread block to address up to 227 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like the NVIDIA Ampere Architecture and NVIDIA Volta GPU architectures, the NVIDIA Hopper GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp before delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 1.4.3. Fourth-Generation NVLink  The fourth generation of NVIDIA’s high-speed NVLink interconnect is implemented in H100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features. The fourth-generation NVLink has the same bidirectional data rate of 50 GB/s per link. The total number of links available is increased to 18 in H100, compared to 12 in A100, yielding 900 GB/s bidirectional bandwidth compared to 600 GB/s for A100. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History  Version 1.0 Initial Public Release Added support for compute capability 9.0 1 Throughout this guide, NVIDIA Volta refers to devices of compute capability 7.0, NVIDIA Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x, and NVIDIA Hopper refers to devices of compute capability 9.0. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html", "content_type": "text/html", "text": "NVIDIA Ampere GPU Architecture Tuning Guide 1. NVIDIA Ampere GPU Architecture Tuning Guide 1.1. NVIDIA Ampere GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ampere GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier 1.4.1.4. Warp level support for Reduction Operations 1.4.1.5. Improved Tensor Core Operations 1.4.1.6. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased Memory Capacity and High Bandwidth Memory 1.4.2.2. Increased L2 capacity and L2 Residency Controls 1.4.2.3. Unified Shared Memory/L1/Texture Cache 1.4.3. Third Generation NVLink 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Ampere Tuning Guide » 1. NVIDIA Ampere GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ampere GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ampere GPU Architecture. 1. NVIDIA Ampere GPU Architecture Tuning Guide  1.1. NVIDIA Ampere GPU Architecture  The NVIDIA Ampere GPU architecture is NVIDIA’s latest architecture for CUDA compute applications. The NVIDIA Ampere GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as Turing and Volta, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA A100 GPU without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ampere GPU architecture’s features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ampere GPU Architecture. 1.4. NVIDIA Ampere GPU Architecture Tuning  1.4.1. Streaming Multiprocessor  The NVIDIA Ampere GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Volta and Turing. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM remains the same as in Volta (i.e., 64) for compute capability 8.0, while for compute capability 8.6 it is 48. Other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 8.0 (i.e., A100 GPUs) and 16 for GPUs with compute capability 8.6. For devices of compute capability 8.0 (i.e., A100 GPUs) shared memory capacity per SM is 164 KB, a 71% increase compared to V100’s capacity of 96 KB. For GPUs with compute capability 8.6, shared memory capacity per SM is 100 KB. For devices of compute capability 8.0 (i.e., A100 GPUs) the maximum shared memory per thread block is 163 KB. For GPUs with compute capability 8.6 maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on Volta without changes to their application. 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory  The NVIDIA Ampere GPU architecture adds hardware acceleration for copying data from global memory to shared memory. These copy instructions are asynchronous, with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the SM. These instructions also avoid using extra registers for memory copies and can also bypass the L1 cache. This new feature is exposed via the pipeline API in CUDA. For more information please refer to the section on Async Copy in the CUDA C++ Programming Guide . 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier  The NVIDIA Ampere GPU architecture adds hardware acceleration for a split arrive/wait barrier in shared memory. These barriers can be used to implement fine grained thread controls, producer-consumer computation pipeline and divergence code patterns in CUDA. These barriers can also be used alongside the asynchronous copy. For more information on the Arrive/Wait Barriers refer to the Arrive/Wait Barrier section in the CUDA C++ Programming Guide . 1.4.1.4. Warp level support for Reduction Operations  The NVIDIA Ampere GPU architecture adds native support for warp wide reduction operations for 32-bit signed and unsigned integer operands. The warp wide reduction operations support arithmetic add , min , and max operations on 32-bit signed and unsigned integers and bitwise and , or and xor operations on 32-bit unsigned integers. For more details on the new warp wide reduction operations refer to Warp Reduce Functions in the CUDA C++ Programming Guide . 1.4.1.5. Improved Tensor Core Operations  The NVIDIA Ampere GPU architecture includes new Third Generation Tensor Cores that are more powerful than the Tensor Cores used in Volta and Turing SMs. The new Tensor Cores use a larger base matrix size and add powerful new math modes including: Support for FP64 Tensor Core, using new DMMA instructions. Support for Bfloat16 Tensor Core, through HMMA instructions. BFloat16 format is especially effective for DL training scenarios. Bfloat16 provides 8-bit exponent i.e., same range as FP32, 7-bit mantissa and 1 sign-bit. Support for TF32 Tensor Core, through HMMA instructions. TF32 is a new 19-bit Tensor Core format that can be easily integrated into programs for more accurate DL training than 16-bit HMMA formats. TF32 provides 8-bit exponent, 10-bit mantissa and 1 sign-bit. Support for bitwise AND along with bitwise XOR which was introduced in Turing, through BMMA instructions. The following table presents the evolution of matrix instruction sizes and supported data types for Tensor Cores across different GPU architecture generations. Instruction GPU Architecture Input Matrix format Output Accumulator format Matrix Instruction Size (MxNxK) HMMA (16-bit precision) NVIDIA Volta Architecture FP16 FP16 / FP32 8x8x4 NVIDIA Turing Architecture FP16 FP16 / FP32 8x8x4 / 16x8x8 / 16x8x16 NVIDIA Ampere Architecture FP16 / BFloat16 FP16 / FP32\n(BFloat16 only supports FP32 as accumulator) 16x8x8 / 16x8x16 HMMA (19-bit precision) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture N/A N/A N/A NVIDIA Ampere Architecture TF32 (19-bits) FP32 16x8x4 IMMA (Integer MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture unsigned char/signed char (8-bit precision) int32 8x8x16 NVIDIA Ampere Architecture unsigned char/signed char (8-bit precision) int32 8x8x16 / 16x8x16 / 16x8x32 IMMA (Integer sub-byte MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture unsigned u4/signed u4 (4-bit precision) int32 8x8x32 NVIDIA Ampere Architecture unsigned u4/signed u4 (4-bit precision) int32 8x8x32 / 16x8x32 / 16x8x64 BMMA (Binary MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture single bit int32 8x8x128 NVIDIA Ampere Architecture single bit int32 8x8x128 / 16x8x128 / 16x8x256 DMMA (64-bit precision) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture N/A N/A N/A NVIDIA Ampere Architecture FP64 FP64 8x8x4 For more details on the new Tensor Core operations refer to the Warp Matrix Multiply section in the CUDA C++ Programming Guide . 1.4.1.6. Improved FP32 throughput  Devices of compute capability 8.6 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run as is on 8.6, it is recommended to compile explicitly for 8.6 to benefit from the increased FP32 throughput. 1.4.2. Memory System  1.4.2.1. Increased Memory Capacity and High Bandwidth Memory  The NVIDIA A100 GPU increases the HBM2 memory capacity from 32 GB in V100 GPU to 40 GB in A100 GPU. Along with the increased memory capacity, the bandwidth is increased by 72%, from 900 GB/s on Volta V100 to 1550 GB/s on A100. 1.4.2.2. Increased L2 capacity and L2 Residency Controls  The NVIDIA Ampere GPU architecture increases the capacity of the L2 cache to 40 MB in Tesla A100, which is 7x larger than Tesla V100. Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased. The NVIDIA Ampere GPU architecture allows CUDA users to control the persistence of data in L2 cache. For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Unified Shared Memory/L1/Texture Cache  The NVIDIA A100 GPU based on compute capability 8.0 increases the maximum capacity of the combined L1 cache, texture cache and shared memory to 192 KB, 50% larger than the L1 cache in NVIDIA V100 GPU. The combined L1 cache capacity for GPUs with compute capability 8.6 is 128 KB. In the NVIDIA Ampere GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures such as Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA A100 GPU supports shared memory capacity of 0, 8, 16, 32, 64, 100, 132 or 164 KB per SM. GPUs with compute capability 8.6 support shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, the A100 GPU enables a single thread block to address up to 163 KB of shared memory and GPUs with compute capability 8.6 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Volta, the NVIDIA Ampere GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to Volta L1 is improvement in terms of both latency and bandwidth. 1.4.3. Third Generation NVLink  The third generation of NVIDIA’s high-speed NVLink interconnect is implemented in A100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features. The third generation NVLink has the same bi-directional data rate of 50 GB/s per link, but uses half the number of signal pairs to achieve this bandwidth. Therefore, the total number of links available is increased to twelve in A100, versus six in V100, yielding 600 GB/s bidirectional bandwidth versus 300 GB/s for V100. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. In the NVIDIA Ampere GPU architecture remote NVLINK accesses go through a Link TLB on the remote GPU. This Link TLB has a reach of 64 GB to the remote GPU’s memory. Applications with remote random accesses may want to constrain the remotely accessed region to 64 GB for each peer GPU. 2. Revision History  Version 1.1 Initial Public Release Added support for compute capability 8.6 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, and NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/turing-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/turing-tuning-guide/index.html", "content_type": "text/html", "text": "Turing Tuning Guide 1. Turing Tuning Guide 1.1. NVIDIA Turing Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Turing Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Independent Thread Scheduling 1.4.1.3. Occupancy 1.4.1.4. Integer Arithmetic 1.4.2. Tensor Core Operations 1.4.3. Memory Throughput 1.4.3.1. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Turing Tuning Guide » 1. Turing Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Turing The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Turing Architecture. 1. Turing Tuning Guide  1.1. NVIDIA Turing Compute Architecture  Turing is NVIDIA’s latest architecture for CUDA compute applications. Turing retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Pascal and Volta, and applications that follow the best practices for those architectures should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Turing architectural features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Turing Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Turing. 1.4. Turing Tuning  1.4.1. Streaming Multiprocessor  The Turing Streaming Multiprocessor (SM) is based on the same major architecture (7.x) as Volta, and provides similar improvements over Pascal. 1.4.1.1. Instruction Scheduling  Each Turing SM includes 4 warp-scheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp, or by 16 warps per SM without any instuction-level parallelism. Like Volta, the Turing SM provides 64 FP32 cores, 64 INT32 cores and 8 improved mixed-precision Tensor Cores. Turing has a lower double precision throughput than Volta with only 2 FP64 cores. 1.4.1.2. Independent Thread Scheduling  The Turing architecture features the same Independent Thread Scheduling introduced with Volta. This enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures. When porting existing codes to Volta or Turing, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide . To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier. The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy  The maximum number of concurrent warps per SM is 32 on Turing (versus 64 on Volta). Other factors influencing warp occupancy remain otherwise similar: The register file size is 64k 32-bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 16. Shared memory capacity per SM is 64KB. Overall, developers can expect similar occupancy as on Pascal or Volta without changes to their application. 1.4.1.4. Integer Arithmetic  Similar to Volta, the Turing SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can interleave pointer arithmetic with floating-point computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations  Volta introduced Tensor Cores to accelerate matrix multiply operations on mixed precision floating point data. Turing adds acceleration for integer matrix multiply operations. The tensor cores are exposed as Warp-Level Matrix Operations in the CUDA 10 C++ API. The API provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use Tensor Cores from a CUDA-C++ program. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller matrix fragments. Each Tensor Core performs the matrix multiply-accumulate: D = A x B + C. The Tensor Cores support half precision matrix multiplication, where the matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be either FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition. CUDA 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the Tensor Cores on Volta or Turing with FP16 inputs. Any binary compiled for Volta will run on Turing, but Volta binaries using Tensor Cores will only be able to reach half of Turing’s Tensor Core peak performance. Recompiling the binary specifically for Turing would allow it to reach the peak performance. See the Turing Compatibility Guide for more information. Turing’s Tensor Core supports integer matrix multiply operations, which can operate on 8-bit, 4-bit and 1-bit integer inputs, with 32-bit integer accumulation. When operating on 8-bit inputs, CUDA exposes fragment sizes of 16x16x16, 32x8x16, and 8x32x16. For sub-byte operations the fragment sizes available are 8x8x32 for 4-bit inputs, or 8x8x128 for 1-bit inputs. See the CUDA C++ Programming Guide for more information. 1.4.3. Memory Throughput  1.4.3.1. Unified Shared Memory/L1/Texture Cache  Turing features a unified L1 / Shared Memory cache similar to the one introduced in Volta, but with a smaller size. The total size of the unified L1 / Shared Memory cache in Turing is 96 KB. The portion of the cache dedicated to shared memory or L1 (known as the carveout ) can be changed at runtime, either automatically by the driver, or manually using the cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . Turing supports two carveout configurations, either with 64 KB of shared memory and 32 KB of L1, or with 32 KB of shared memory and 64 KB of L1. Turing allows a single thread block to address the full 64 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Pascal and Volta, Turing combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. The state-of-the-art L1 cache in Volta and Turing offers lower latency, higher bandwidth, and higher capacity compared to the earlier architectures. Like Volta, Turing’s L1 can cache write operations (write-through). The result is that for many applications Volta and Turing narrow the performance gap between explicitly managed shared memory and direct access to device memory. Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to devices of compute capability 6.x, Volta refers to devices of compute capability 7.0, and Turing refers to devices of compute capability 7.5. 2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/volta-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/volta-tuning-guide/index.html", "content_type": "text/html", "text": "Volta Tuning Guide 1. Volta Tuning Guide 1.1. NVIDIA Volta Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Volta Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Independent Thread Scheduling 1.4.1.3. Occupancy 1.4.1.4. Integer Arithmetic 1.4.2. Tensor Core Operations 1.4.3. Memory Throughput 1.4.3.1. High Bandwidth Memory 1.4.3.2. Unified Shared Memory/L1/Texture Cache 1.4.4. Cooperative Groups 1.4.5. Multi-Process Service 1.4.6. NVLink Interconnect 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Volta Tuning Guide » 1. Volta Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Volta The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Volta Architecture. 1. Volta Tuning Guide  1.1. NVIDIA Volta Compute Architecture  Volta is NVIDIA’s latest architecture for CUDA compute applications. Volta retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell and Pascal, and applications that follow the best practices for those architectures should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Volta architectural features. 1 Volta architecture comprises a single variant: GV100. A detailed overview of the major improvements in GV100 over earlier NVIDIA architectures is provided in a white paper entitled NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU . For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Volta Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Volta. 1.4. Volta Tuning  1.4.1. Streaming Multiprocessor  The Volta Streaming Multiprocessor (SM) provides the following improvements over Pascal. 1.4.1.1. Instruction Scheduling  Each Volta SM includes 4 warp-scheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations are reduced to four clock cycles, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp. Many more warps are, of course, recommended to cover the much greater latency of memory transactions and control-flow operations. Similar to GP100, the GV100 SM provides 64 FP32 cores and 32 FP64 cores. The GV100 SM additionally includes 64 INT32 cores and 8 mixed-precision Tensor Cores. GV100 provides up to 84 SMs. 1.4.1.2. Independent Thread Scheduling  The Volta architecture introduces Independent Thread Scheduling among threads in a warp. This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures. When porting existing codes to Volta, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide . To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier. The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy  The maximum number of concurrent warps per SM remains the same as in Pascal (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size is 64k 32-bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 32. Shared memory capacity per SM is 96KB, similar to GP104, and a 50% increase compared to GP100. Overall, developers can expect similar occupancy as on Pascal without changes to their application. 1.4.1.4. Integer Arithmetic  Unlike Pascal GPUs, the GV100 SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can now interleave pointer arithmetic with floating-point computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations  Each Tensor Core performs the following operation: D = AxB + C, where A, B, C, and D are 4x4 matrices. The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition with the other intermediate products for a 4x4x4 matrix multiply. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller elements. The Volta tensor cores are exposed as Warp-Level Matrix Operations in the CUDA 9 C++ API. The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program. At the CUDA level, the warp-level interface assumes 16x16 size matrices spanning all 32 threads of the warp. See the CUDA C++ Programming Guide for more information. 1.4.3. Memory Throughput  1.4.3.1. High Bandwidth Memory  GV100 uses up to eight memory dies per HBM2 stack and four stacks, with a maximum of 32 GB of GPU memory. A faster and more efficient HBM2 implementation delivers up to 900 GB/s of peak memory bandwidth, compared to 732 GB/s for GP100. This combination of a new generation HBM2 memory, and a new generation memory controller, in Volta provides 1.5x delivered memory bandwidth, compared to Pascal GP100—and a greater than 95% memory bandwidth efficiency running many workloads. In order to hide the DRAM latencies at full HBM2 bandwidth more memory accesses must be kept in flight, compared to GPUs equipped with traditional GDDR5. This is accomplished by the large complement of SMs in GV100, which typically boost the number of concurrent threads, and thus the reads-in-flight, compared to previous architectures. Resource-constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread. 1.4.3.2. Unified Shared Memory/L1/Texture Cache  In Volta the L1 cache, texture cache, and shared memory are backed by a combined 128 KB data cache. As in previous architectures, the portion of the cache dedicated to shared memory (known as the carveout ) can be selected at runtime using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . Volta supports shared memory capacities of 0, 8, 16, 32, 64, or 96 KB per SM. A new feature, Volta enables a single thread block to address the full 96 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Pascal, Volta combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Volta increases the maximum capacity of the L1 cache to 128 KB, more than 7x larger than the GP100 L1. Another benefit of its union with shared memory, the Volta L1 improves in terms of both latency and bandwidth compared to Pascal. The result is that for many applications Volta narrows the performance gap between explicitly managed shared memory and direct access to device memory. Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 1.4.4. Cooperative Groups  The Volta architecture introduced Independent Thread Scheduling, which enables intra-warp synchronization patterns that were previously not possible. To efficiently express these new patterns, CUDA 9 introduces Cooperative Groups. This is an extension to the CUDA programming model for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions. See the CUDA C++ Programming Guide for more information. 1.4.5. Multi-Process Service  The Volta Multi-Process Service is significantly improved compared to previous architecutres, both in terms of performance and robustness. Intermediary software schedulers, used for MPS with previous architectures, have been replaced by hardware accelerated units within the GPU. MPS clients now submit tasks directly to the GPU work queues, significantly decreasing submission latency and increasing aggregate throughput. The limit on the number of MPS clients has also been increased by 3x to 48. Volta MPS also provides each client with an isolated address space, 3 and extends Unified Memory support for MPS applications. Volta MPS also provides control for clients to restrict each client to a fraction of the GPU execution resources. Developers can use this feature to reduce or eliminate head-of-line blocking where work from one MPS client overwhelms GPU execution resources and prevents other clients from making progress, and thus improve average latency and jitter accross the system. 1.4.6. NVLink Interconnect  NVLink is NVIDIA’s high-speed data interconnect. NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory. GV100 supports up to six NVLink connections with each connection carrying up to 50 GB/s of bi-directional bandwidth. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Added Cooperative Groups section. Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, and Volta refers to devices of compute capability 7.x. 2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction. 3 As with previous architectures, MPS does not provide fatal fault isolation between clients. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html", "content_type": "text/html", "text": "Pascal Tuning Guide 1. Pascal Tuning Guide 1.1. NVIDIA Pascal Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Pascal Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Occupancy 1.4.2. New Arithmetic Primitives 1.4.2.1. FP16 Arithmetic Support 1.4.2.2. INT8 Dot Product 1.4.3. Memory Throughput 1.4.3.1. High Bandwidth Memory 2 DRAM 1.4.3.2. Unified L1/Texture Cache 1.4.4. Atomic Memory Operations 1.4.5. Shared Memory 1.4.5.1. Shared Memory Capacity 1.4.5.2. Shared Memory Bandwidth 1.4.6. Inter-GPU Communication 1.4.6.1. NVLink Interconnect 1.4.6.2. GPUDirect RDMA Bandwidth 1.4.7. Compute Preemption 1.4.8. Unified Memory Improvements 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Pascal Tuning Guide » 1. Pascal Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Pascal The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture. 1. Pascal Tuning Guide  1.1. NVIDIA Pascal Compute Architecture  Pascal retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell, and applications that follow the best practices for those architectures should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Pascal architectural features. 1 Pascal architecture comprises two major variants: GP100 and GP104. 2 A detailed overview of the major improvements in GP100 and GP104 over earlier NVIDIA architectures are described in a pair of white papers entitled NVIDIA Tesla P100: The Most Advanced Datacenter Accelerator Ever Built for GP100 and NVIDIA GeForce GTX 1080: Gaming Perfected for GP104. For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . Some of the Pascal features described in this guide are specific to either GP100 or GP104, as noted; if not specified, features apply to both Pascal variants. 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Pascal Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Pascal. 1.4. Pascal Tuning  1.4.1. Streaming Multiprocessor  The Pascal Streaming Multiprocessor (SM) is in many respects similar to that of Maxwell. Pascal further improves the already excellent power efficiency provided by the Maxwell architecture through both an improved 16-nm FinFET manufacturing process and various architectural modifications. 1.4.1.1. Instruction Scheduling  Like Maxwell, Pascal employs a power-of-two number of CUDA Cores per partition. This simplifies scheduling, since each of the SM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width (32). Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores. GP100 and GP104 designs incorporate different numbers of CUDA Cores per SM. Like Maxwell, each GP104 SM provides four warp schedulers managing a total of 128 single-precision (FP32) and four double-precision (FP64) cores. A GP104 processor provides up to 20 SMs, and the similar GP102 design provides up to 30 SMs. By contrast GP100 provides smaller but more numerous SMs. Each GP100 provides up to 60 SMs. 3 Each SM contains two warp schedulers managing a total of 64 FP32 and 32 FP64 cores. The resulting 2:1 ratio of FP32 to FP64 cores aligns well with GP100’s new datapath configuration, allowing Pascal to process FP64 workloads more efficiently than Kepler GK210, the previous NVIDIA architecture to emphasize FP64 performance. 1.4.1.2. Occupancy  The maximum number of concurrent warps per SM remains the same as in Maxwell (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size (64k 32-bit registers) is the same as that of Maxwell. The maximum registers per thread, 255, matches that of Maxwell. As with previous architectures, experimentation should be used to determine the optimum balance of register spilling vs. occupancy, however. The maximum number of thread blocks per SM is 32, the same as Maxwell. Shared memory capacity per SM is 64KB for GP100 and 96KB for GP104. For comparison, Maxwell provided 96KB and up to 112KB of shared memory, respectively. But each GP100 SM contains fewer CUDA Cores, so the shared memory available per core actually increases on GP100. The maximum shared memory per block remains limited at 48KB as with prior architectures (see Shared Memory Capacity ). As such, developers can expect similar occupancy as on Maxwell without changes to their application. As a result of scheduling improvements relative to Kepler, warp occupancy requirements (i.e., available parallelism) needed for maximum device utilization are generally reduced. 1.4.2. New Arithmetic Primitives  1.4.2.1. FP16 Arithmetic Support  Pascal provides improved FP16 support for applications, like deep learning, that are tolerant of low floating-point precision. The half type is used to represent FP16 values on the device. As with Maxwell, FP16 storage can be used to reduce the required memory footprint and bandwidth compared to FP32 or FP64 storage. Pascal also adds support for native FP16 instructions. Peak FP16 throughput is attained by using a paired operation to perform two FP16 instructions per core simultaneously. To be eligible for the paired operation the operands must be stored in a half2 vector type. GP100 and GP104 provide different FP16 throughputs. GP100, designed with training deep neural networks in mind, provides FP16 throughput up to 2x that of FP32 arithmetic. On GP104, FP16 throughput is lower, 1/64th that of FP32. However, compensating for reduced FP16 throughput, GP104 provides additional high-throughput INT8 support not available in GP100. 1.4.2.2. INT8 Dot Product  GP104 provides specialized instructions for two-way and four-way integer dot products. These are well suited for accelerating Deep Learning inference workloads. The __dp4a intrinsic computes a dot product of four 8-bit integers with accumulation into a 32-bit integer. Similarly, __dp2a performs a two-element dot product between two 16-bit integers in one vector, and two 8-bit integers in another with accumulation into a 32-bit integer. Both instructions offer a throughput equal to that of FP32 arithmetic. 1.4.3. Memory Throughput  1.4.3.1. High Bandwidth Memory 2 DRAM  GP100 uses High Bandwidth Memory 2 (HBM2) for its DRAM. HBM2 memories are stacked on a single silicon package along with the GPU die. This allows much wider interfaces at similar power compared to traditional GDDR technology. GP100 is linked to up to four stacks of HBM2 and uses two 512-bit memory controllers for each stack. The effective width of the memory bus is then 4096 bits, a significant increase over the 384 bits in GM200. This allows a tremendous boost in peak bandwidth even at reduced memory clocks. Thus, the GP100 equipped Tesla P100 has a peak bandwidth of 732 GB/s with a modest 715 MHz memory clock. DRAM access latencies remain similar to those observed on Maxwell. In order to hide DRAM latencies at full HBM2 bandwidth, more memory accesses must be kept in flight compared to GPUs equipped with traditional GDDR5. Helpfully, the large complement of SMs in GP100 will typically boost the number of concurrent threads (and thus reads-in-flight) compared to previous architectures. Resource constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread. The GP100 GPU’s register files, shared memories, L1 and L2 caches, and DRAM are all protected by Single-Error Correct Double-Error Detect (SECDED) ECC code. When enabling ECC support on a Kepler GK210, the available DRAM would be reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled. HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection. 4 1.4.3.2. Unified L1/Texture Cache  Like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. By default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Kepler serviced loads at a granularity of 128B when L1 caching of global loads was enabled and 32B otherwise. On Pascal the data access unit is 32B regardless of whether global loads are cached in L1. So it is no longer necessary to turn off L1 caching in order to reduce wasted global memory transactions associated with uncoalesced accesses. Unlike Maxwell, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. The balance of occupancy versus spilling should therefore be re-evaluated to ensure best performance. Two new device attributes were added in CUDA Toolkit 6.0: globalL1CacheSupported and localL1CacheSupported . Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process. Note Enabling caching of globals in GP104 can affect occupancy. If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed. This situation is reported by the profiler. 1.4.4. Atomic Memory Operations  Like Maxwell, Pascal provides native shared memory atomic operations for 32-bit integer arithmetic, along with native 32 or 64-bit compare-and-swap (CAS). Developers coming from Kepler, where shared memory atomics were implemented in software using a lock/update/unlock sequence, should see a large performance improvement particularly for heavily contended shared-memory atomics. Pascal also extends atomic addition in global memory to function on FP64 data. The atomicAdd() function in CUDA has thus been generalized to support 32 and 64-bit integer and floating-point types. The rounding mode for all floating-point atomic operations is round-to-nearest-even in Pascal. As in previous generations FP32 atomicAdd() flushes denormalized values to zero. For GP100 atomic operations may target the memories of peer GPUs connected through NVLink. Peer-to-peer atomics over NVLink use the same API as atomics targeting global memory. GPUs connected via PCIE do not support this feature. Pascal GPUs provide support system-wide atomic operations targeting migratable allocations 5 If system-wide atomic visibility is desired, operations targeting migratable memory must specify a system scope by using the atomic[Op]_system() intrinsics 6 . Using the device-scope atomics (e.g. atomicAdd() ) on migratable memory remains valid, but enforces atomic visibility only within the local GPU. Note Given the potential for incorrect usage of atomic scopes, it is recommended that applications use compute-sanitizer to detect and eliminate errors. As implemented for Pascal, system-wide atomics are intended to allow developers to experiment with enhanced memory models. They are implemented in software and some care is required to achieve good performance. When an atomic targets a migratable address backed by a remote memory space, the local processor page-faults so that the kernel can migrate the appropriate memory page to local memory. Then the usual hardware instructions are used to execute the atomic. Since the page is now locally resident, subsequent atomics from the same processor will not result in additional page-faults. However, atomic updates from different processors can incur frequent page-faults. 1.4.5. Shared Memory  1.4.5.1. Shared Memory Capacity  For Kepler, shared memory and the L1 cache shared the same on-chip storage. Maxwell and Pascal, by contrast, provide dedicated space to the shared memory of each SM, since the functionality of the L1 and texture caches have been merged. This increases the shared memory space available per SM as compared to Kepler: GP100 offers 64 KB shared memory per SM, and GP104 provides 96 KB per SM. This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e.g., radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count. Applications no longer need to select a preference of the L1/shared split for optimal performance. Note Thread-blocks remain limited to 48 KB of shared memory. For maximum flexibility, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM. 1.4.5.2. Shared Memory Bandwidth  Kepler provided an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM for shared memory accesses of 8 or 16 bytes. However, applications could only benefit from this when storing these larger elements in shared memory (i.e., integers and fp32 values saw no benefit), and only when the developer explicitly opted in to the 8-byte bank mode via the API. To simplify this, Pascal follows Maxwell in returning to fixed four-byte banks. This allows all applications using shared memory to benefit from the higher bandwidth, without specifying any particular preference via the API. 1.4.6. Inter-GPU Communication  1.4.6.1. NVLink Interconnect  NVLink is NVIDIA’s new high-speed data interconnect. NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory. GP100 supports up to four NVLink connections with each connection carrying up to 40 GB/s of bi-directional bandwidth. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 1.4.6.2. GPUDirect RDMA Bandwidth  GPUDirect RDMA allows third party devices such as network interface cards (NICs) to directly access GPU memory. This eliminates unnecessary copy buffers, lowers CPU overhead, and significantly decreases the latency of MPI send/receive messages from/to GPU memory. Pascal doubles the delivered RDMA bandwidth when reading data from the source GPU memory and writing to the target NIC memory over PCIe. 1.4.7. Compute Preemption  Compute Preemption is a new feature specific to GP100. Compute Preemption allows compute tasks running on the GPU to be interrupted at instruction-level granularity. The execution context (registers, shared memory, etc.) are swapped to GPU DRAM so that another application can be swapped in and run. Compute preemption offers two key advantages for developers: Long-running kernels no longer need to be broken up into small timeslices to avoid an unresponsive graphical user interface or kernel timeouts when a GPU is used simultaneously for compute and graphics. Interactive kernel debugging on a single-GPU system is now possible. 1.4.8. Unified Memory Improvements  Pascal offers new hardware capabilities to extend Unified Memory (UM) support. An extended 49-bit virtual addressing space allows Pascal GPUs to address the full 48-bit virtual address space of modern CPUs as well as the memories of all GPUs in the system through a single virtual address space, not limited by the physical memory sizes of any one processor. Pascal GPUs also support memory page faulting. Page faulting allows applications to access the same managed memory allocations from both host and device without explicit synchronization. It also removes the need for the CUDA runtime to pre-synchronize all managed memory allocations before each kernel launch. Instead, when a kernel accesses a non-resident memory page, it faults, and the page can be migrated to the GPU memory on-demand, or mapped into the GPU address space for access over PCIe/NVLink interfaces. These features boost performance on Pascal for many typical UM workloads. In cases where the UM heuristics prove suboptimal, further tuning is possible through a set of migration hints that can be added to the source code. On supporting operating system platforms, any memory allocated with the default OS allocator (for example, malloc or new) can be accessed from both GPU and CPU code using the same pointer. In fact, all system virtual memory can be accessed from the GPU. On such systems, there is no need to explicitly allocate managed memory using cudaMallocManaged() . 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, and Pascal refers to device of compute capability 6.x. 2 The specific compute capabilities of GP100 and GP104 are 6.0 and 6.1, respectively. The GP102 architecture is similar to GP104. 3 The Tesla P100 has 56 SMs enabled. 4 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory. 5 Migratable, or Unified Memory (UM) , allocations are made with cudaMallocManaged() or, for systems with Heterogeneous Memory Management (HMM) support, malloc() . 6 Here [Op] would be one of Add , CAS , etc. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/maxwell-tuning-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/maxwell-tuning-guide/index.html", "content_type": "text/html", "text": "Maxwell Tuning Guide 1. Maxwell Tuning Guide 1.1. NVIDIA Maxwell Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Maxwell Tuning 1.4.1. SMM 1.4.1.1. Occupancy 1.4.1.2. Instruction Scheduling 1.4.1.3. Instruction Latencies 1.4.1.4. Instruction Throughput 1.4.2. Memory Throughput 1.4.2.1. Unified L1/Texture Cache 1.4.3. Shared Memory 1.4.3.1. Shared Memory Capacity 1.4.3.2. Shared Memory Bandwidth 1.4.3.3. Fast Shared Memory Atomics 1.4.4. Dynamic Parallelism 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Maxwell Tuning Guide » 1. Maxwell Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Maxwell The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Maxwell Architecture. 1. Maxwell Tuning Guide  1.1. NVIDIA Maxwell Compute Architecture  Maxwell is NVIDIA’s next-generation architecture for CUDA compute applications. Maxwell retains and extends the same CUDA programming model as in previous NVIDIA architectures such as Fermi and Kepler, and applications that follow the best practices for those architectures should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features. 1 Maxwell introduces an all-new design for the Streaming Multiprocessor ( SM ) that dramatically improves energy efficiency. Although the Kepler SMX design was extremely efficient for its generation, through its development, NVIDIA’s GPU architects saw an opportunity for another big leap forward in architectural efficiency; the Maxwell SM is the realization of that vision. Improvements to control logic partitioning, workload balancing, clock-gating granularity, compiler-based scheduling, number of instructions issued per clock cycle, and many other enhancements allow the Maxwell SM (also called SMM ) to far exceed Kepler SMX efficiency. The first Maxwell-based GPU is codenamed GM107 and is designed for use in power-limited environments like notebooks and small form factor (SFF) PCs. GM107 is described in a whitepaper entitled NVIDIA GeForce GTX 750 Ti: Featuring First-Generation Maxwell GPU Technology, Designed for Extreme Performance per Watt . 2 The first GPU using the second-generation Maxwell architecture is codenamed GM204 . Second-generation Maxwell GPUs retain the power efficiency of the earlier generation while delivering significantly higher performance. GM204 is described in a whitepaper entitled NVIDIA GeForce GTX 980: Featuring Maxwell, The Most Advanced GPU Ever Made . Compute programming features of GM204 are similar to those of GM107, except where explicitly noted in this guide. For details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Maxwell Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Maxwell. 1.4. Maxwell Tuning  1.4.1. SMM  The Maxwell Streaming Multiprocessor, SMM, is similar in many respects to the Kepler architecture’s SMX. The key enhancements of SMM over SMX are geared toward improving efficiency without requiring significant increases in available parallelism per SM from the application. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SMM remains the same as in SMX (i.e., 64), and factors influencing warp occupancy remain similar or improved over SMX: The register file size (64k 32-bit registers) is the same as that of SMX. The maximum registers per thread, 255, matches that of Kepler GK110. As with Kepler, experimentation should be used to determine the optimum balance of register spilling vs. occupancy, however. The maximum number of thread blocks per SM has been increased from 16 to 32. This should result in an automatic occupancy improvement for kernels with small thread blocks of 64 or fewer threads (shared memory and register file resource requirements permitting). Such kernels would have tended to under-utilize SMX, but less so SMM. Shared memory capacity is increased (see Shared Memory Capacity ). As such, developers can expect similar or improved occupancy on SMM without changes to their application. At the same time, warp occupancy requirements (i.e., available parallelism) for maximum device utilization are similar to or less than those of SMX (see Instruction Latencies ). 1.4.1.2. Instruction Scheduling  The number of CUDA Cores per SM has been reduced to a power of two, however with Maxwell’s improved execution efficiency, performance per SM is usually within 10% of Kepler performance, and the improved area efficiency of SMM means CUDA Cores per GPU will be substantially higher vs. comparable Fermi or Kepler chips. SMM retains the same number of instruction issue slots per clock and reduces arithmetic latencies compared to the Kepler design. As with SMX, each SMM has four warp schedulers. Unlike SMX, however, all SMM core functional units are assigned to a particular scheduler, with no shared units. Along with the selection of a power-of-two number of CUDA Cores per SM, which simplifies scheduling and reduces stall cycles, this partitioning of SM computational resources in SMM is a major component of the streamlined efficiency of SMM. The power-of-two number of CUDA Cores per partition simplifies scheduling, as each of SMM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width. Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores. 1.4.1.3. Instruction Latencies  Another major improvement of SMM is that dependent math latencies have been significantly reduced; a consequence of this is a further reduction of stall cycles, as the available warp-level parallelism (i.e., occupancy) on SMM should be equal to or greater than that of SMX (see Occupancy ), while at the same time each math operation takes less time to complete, improving utilization and throughput. 1.4.1.4. Instruction Throughput  The most significant changes to peak instruction throughputs in SMM are as follows: The change in number of CUDA Cores per SM brings with it a corresponding change in peak single-precision floating point operations per clock per SM. However, since the number of SMs is typically increased, the result is an increase in aggregate peak throughput; furthermore, the scheduling and latency improvements also discussed above make this peak easier to approach. The throughput of many integer operations including multiply, logical operations and shift is improved. In addition, there are now specialized integer instructions that can accelerate pointer arithmetic. These instructions are most efficient when data structures are a power of two in size. Note As was already the recommended best practice, signed arithmetic should be preferred over unsigned arithmetic wherever possible for best throughput on SMM. The C language standard places more restrictions on overflow behavior for unsigned math, limiting compiler optimization opportunities. 1.4.2. Memory Throughput  1.4.2.1. Unified L1/Texture Cache  Maxwell combines the functionality of the L1 and texture caches into a single unit. As with Kepler, global loads in Maxwell are cached in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. In a manner similar to Kepler GK110B, GM204 retains this behavior by default but also allows applications to opt-in to caching of global loads in its unified L1/Texture cache. The opt-in mechanism is the same as with GK110B: pass the -Xptxas -dlcm=ca flag to nvcc at compile time. Local loads also are cached in L2 only, which could increase the cost of register spilling if L1 local load hit rates were high with Kepler. The balance of occupancy versus spilling should therefore be reevaluated to ensure best performance. Especially given the improvements to arithmetic latencies, code built for Maxwell may benefit from somewhat lower occupancy (due to increased registers per thread) in exchange for lower spilling. The unified L1/texture cache acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Two new device attributes were added in CUDA Toolkit 6.0: globalL1CacheSupported and localL1CacheSupported . Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process. Note Enabling caching of globals in GM204 can affect occupancy. If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed. This situation is reported by the profiler. 1.4.3. Shared Memory  1.4.3.1. Shared Memory Capacity  With Fermi and Kepler, shared memory and the L1 cache shared the same on-chip storage. Maxwell, by contrast, provides dedicated space to the shared memory of each SMM, since the functionality of the L1 and texture caches have been merged in SMM. This increases the shared memory space available per SMM as compared to SMX: GM107 provides 64 KB shared memory per SMM, and GM204 further increases this to 96 KB shared memory per SMM. This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e.g., radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count. Applications no longer need to select a preference of the L1/shared split for optimal performance. For purposes of backward compatibility with Fermi and Kepler, applications may optionally continue to specify such a preference, but the preference will be ignored on Maxwell, with the full 64 KB per SMM always going to shared memory. Note While the per-SM shared memory capacity is increased in SMM, the per-thread-block limit remains 48 KB. For maximum flexibility on possible future GPUs, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block, which would for example allow at least two such thread blocks to fit per SMM. 1.4.3.2. Shared Memory Bandwidth  Kepler SMX introduced an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM over Fermi for shared memory accesses of 8 or 16 bytes. However, applications could only benefit from this when storing these larger elements in shared memory (i.e., integers and fp32 values saw no benefit), and only when the developer explicitly opted into the 8-byte bank mode via the API. To simplify this, Maxwell returns to the Fermi style of shared memory banking, where banks are always four bytes wide. Aggregate shared memory bandwidth across the chip remains comparable to that of corresponding Kepler chips, given increased SM count. In this way, all applications using shared memory can now benefit from the higher bandwidth, even when storing only four-byte items into shared memory and without specifying any particular preference via the API. 1.4.3.3. Fast Shared Memory Atomics  Kepler introduced a dramatically higher throughput for atomic operations to global memory as compared to Fermi. However, atomic operations to shared memory remained essentially unchanged: both architectures implemented shared memory atomics using a lock/update/unlock pattern that could be expensive in the case of high contention for updates to particular locations in shared memory. Maxwell improves upon this by implementing native shared memory atomic operations for 32-bit integers and native shared memory 32-bit and 64-bit compare-and-swap (CAS), which can be used to implement other atomic functions with reduced overhead compared to the Fermi and Kepler methods. Note Refer to the CUDA C++ Programming Guide for an example implementation of an fp64 atomicAdd() using atomicCAS() . 1.4.4. Dynamic Parallelism  GK110 introduced a new architectural feature called Dynamic Parallelism, which allows the GPU to create additional work for itself. A programming model enhancement leveraging this feature was introduced in CUDA 5.0 to enable kernels running on GK110 to launch additional kernels onto the same GPU. SMM brings Dynamic Parallelism into the mainstream by supporting it across the product line, even in lower-power chips such as GM107. This will benefit developers, as it means that applications will no longer need special-case algorithm implementations for high-end GPUs that differ from those usable in more power-constrained environments. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated for second-generation Maxwell (compute capability 5.2). Version 1.2 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Fermi refers to devices of compute capability 2.x, Kepler refers to devices of compute capability 3.x, and Maxwell refers to devices of compute capability 5.x. 2 The features of GM108 are similar to those of GM107. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ada-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/ada-compatibility-guide/index.html", "content_type": "text/html", "text": "NVIDIA Ada GPU Architecture Compatibility 1. NVIDIA Ada GPU Architecture Compatibility 1.1. About this Document 1.2. Application Compatibility on the NVIDIA Ada GPU Architecture 1.3. Compatibility between Ampere and Ada 1.4. Verifying Ada Compatibility for Existing Applications 1.4.1. Applications Built Using CUDA Toolkit 10.2 or Earlier 1.4.2. Applications Built Using CUDA Toolkit 11.0 through 11.7 1.4.3. Applications Built Using CUDA Toolkit 11.8 1.5. Building Applications with the NVIDIA Ada GPU Architecture Support 1.5.1. Building Applications Using CUDA Toolkit 10.x or Earlier 1.5.2. Building Applications Using CUDA Toolkit 11.0 through 11.7 1.5.3. Building Applications Using CUDA Toolkit 11.8 1.5.4. Independent Thread Scheduling Compatibility 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Ada Compatibility Guide » 1. NVIDIA Ada GPU Architecture Compatibility v12.5 | PDF | Archive NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Ada GPUs. 1. NVIDIA Ada GPU Architecture Compatibility  1.1. About this Document  This application note, NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications , is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ada Architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ada GPU architecture. 1.2. Application Compatibility on the NVIDIA Ada GPU Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 8.6 is supported to run on a GPU with compute capability 8.9; however, a cubin generated for compute capability 8.9 is not supported to run on a GPU with compute capability 8.6, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0. Kernels can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forward-compatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.x. Therefore, although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide . When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels should work as-is on the NVIDIA Ada architecture based GPUs. In such cases, rebuilding the application is not required. However, application binaries that do not include PTX (only include cubins) need to be rebuilt to run on the NVIDIA Ada architecture based GPUs. To know more about building compatible applications, read Building Applications with the NVIDIA Ada GPU Architecture Support . 1.3. Compatibility between Ampere and Ada  The NVIDIA Ada architecture is based on Ampere’s Instruction Set Architecture ISA 8.0, extending it with new instructions. As a consequence, any binary that runs on Ampere will be able to run on Ada (forward compatibility), but an Ada binary will not be able to run on Ampere. 1.4. Verifying Ada Compatibility for Existing Applications  The first step towards making a CUDA application compatible with the NVIDIA Ada GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX). The following sections explain how to accomplish this for an already built CUDA application. 1.4.1. Applications Built Using CUDA Toolkit 10.2 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ada architecture based GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch the application. With CUDA_FORCE_PTX_JIT=1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JIT-compiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not compatible with the NVIDIA Ada GPU architecture and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ada GPU architecture. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.4.2. Applications Built Using CUDA Toolkit 11.0 through 11.7  CUDA applications built using CUDA Toolkit 11.0 through 11.7 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native cubin (see Compatibility between Ampere and Ada ) or PTX format (see Applications Built Using CUDA Toolkit 10.2 or Earlier ), or both. 1.4.3. Applications Built Using CUDA Toolkit 11.8  CUDA applications built using CUDA Toolkit 11.8 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native or Ada-native cubin (see Compatibility between Ampere and Ada ), or PTX format (see Applications Built Using CUDA Toolkit 10.2 or Earlier ), or both. 1.5. Building Applications with the NVIDIA Ada GPU Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ada GPU architecture. Although it is sufficient to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels that do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. 1.5.1. Building Applications Using CUDA Toolkit 10.x or Earlier  The nvcc compiler included with versions 10.x (10.0, 10.1 and 10.2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7.x). When using CUDA Toolkit 10.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 10.0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9.x supports architectures up to _60 and _61). The final -gencode to generate PTX would also need to be updated. For further information and examples, see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX, or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.5.2. Building Applications Using CUDA Toolkit 11.0 through 11.7  The nvcc compiler included with versions 11.0 through 11.7 of the CUDA Toolkit can generate cubins native to the Ampere architecture (compute capability 8.0 and 8.6). When using CUDA Toolkit 11.0 through 11.7, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_86,code=sm_86\n  -gencode=arch=compute_86,code=compute_86\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_86,code=sm_86\n  -gencode=arch=compute_86,code=compute_86\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 11.0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10.x supports architectures up to _72 and _75). The final -gencode to generate PTX also needs to be updated. For further information and examples, see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX, or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.5.3. Building Applications Using CUDA Toolkit 11.8  With version 11.8 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ada GPU architecture (compute capability 8.9). When using CUDA Toolkit 11.8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_86,code=sm_86\n  -gencode=arch=compute_89,code=sm_89\n  -gencode=arch=compute_89,code=compute_89\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_86,code=sm_86\n  -gencode=arch=compute_89,code=sm_89\n  -gencode=arch=compute_89,code=compute_89\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX, or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.5.4. Independent Thread Scheduling Compatibility  NVIDIA GPUs since Volta architecture have Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.x in the CUDA C++ Programming Guide for details and corrective actions. To aid migration to the NVIDIA Ada GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -gencode=arch=compute_60,code=sm_89 ... 2. Revision History  Version 1.0 Initial public release. 1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/hopper-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/hopper-compatibility-guide/index.html", "content_type": "text/html", "text": "Hopper Architecture Compatibility 1. Hopper Architecture Compatibility 1.1. About this Document 1.2. Application Compatibility on Hopper Architecture 1.3. Verifying Hopper Compatibility for Existing Applications 1.3.1. Applications Built Using CUDA Toolkit 11.7 or Earlier 1.3.2. Applications Built Using CUDA Toolkit 11.8 1.4. Building Applications with Hopper Architecture Support 1.4.1. Building Applications Using CUDA Toolkit 11.7 or Earlier 1.4.2. Building Applications Using CUDA Toolkit 11.8 1.4.3. Independent Thread Scheduling Compatibility 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Hopper Compatibility Guide » 1. Hopper Architecture Compatibility v12.5 | PDF | Archive Hopper Compatibility Guide for CUDA Applications The guide to building CUDA applications for Hopper GPUs 1. Hopper Architecture Compatibility  1.1. About this Document  This application note, Hopper Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Hopper architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Hopper architecture. 1.2. Application Compatibility on Hopper Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 8.0 is supported to run on a GPU with compute capability 8.6, however a cubin generated for compute capability 8.6 is not supported to run on a GPU with compute capability 8.0, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0. Kernel can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forward-compatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.0. Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide . When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels, should work as-is on the Hopper GPUs. In such cases, rebuilding the application is not required. However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the Hopper GPUs. To know more about building compatible applications read Building Applications with Hopper Architecture Support Application binaries that include PTX version of kernels with architecture conditional features using sm_90a or compute_90a in order to take full advantage of Hopper GPU architecture, are not forward or backward compatible. 1.3. Verifying Hopper Compatibility for Existing Applications  The first step towards making a CUDA application compatible with Hopper architecture is to check if the application binary already contains compatible GPU code (at least the PTX). The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 11.7 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 11.7 are compatible with Hopper GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch the application. With CUDA_FORCE_PTX_JIT=1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JIT-compiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not Hopper architecture compatible and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is Hopper compatible. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.8  CUDA applications built using CUDA Toolkit 11.8 are compatible with Hopper architecture as long as they are built to include kernels in native cubin (compute capability 9.0) or PTX form or both. 1.4. Building Applications with Hopper Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the Hopper architecture. Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application 2 . Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. PTX code compiled to target architecture conditional features using sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible. 1.4.1. Building Applications Using CUDA Toolkit 11.7 or Earlier  The nvcc compiler included with version 11.7 or earlier (11.0-11.7) of the CUDA Toolkit can generate cubins native to the NVIDIA Ampere GPU architectures (compute capability 8.x). When using CUDA Toolkit 11.7 or earlier, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_80,code=compute_80\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_80,code=compute_80\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 11.0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10.x supports architectures up to sm_72 and sm_75). The final -gencode to generate PTX also needs to be updated. For further information and examples see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.2. Building Applications Using CUDA Toolkit 11.8  With versions 11.8 of the CUDA Toolkit, nvcc can generate cubin native to the Hopper architecture (compute capability 9.0). When using CUDA Toolkit 11.8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_90,code=sm_90\n  -gencode=arch=compute_90,code=compute_90\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_90,code=sm_90\n  -gencode=arch=compute_90,code=compute_90\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.3. Independent Thread Scheduling Compatibility  NVIDIA GPUs since Volta architecture have Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity 3 , this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.x in the CUDA C++ Programming Guide for details and corrective actions. To aid migration to the Hopper architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -gencode=arch=compute_60,code=sm_90 ... 2. Revision History  Version 1.0 Initial public release. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Just-in-time compilation. 2 Starting with CUDA toolkit 11.8, this default behavior can be changed with environment variable CUDA_MODULE_LOADING. See Environment Variables in the CUDA C++ Programming Guide for details. 3 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html", "content_type": "text/html", "text": "NVIDIA Ampere GPU Architecture Compatibility 1. NVIDIA Ampere GPU Architecture Compatibility 1.1. About this Document 1.2. Application Compatibility on the NVIDIA Ampere GPU Architecture 1.3. Verifying Ampere Compatibility for Existing Applications 1.3.1. Applications Built Using CUDA Toolkit 10.2 or Earlier 1.3.2. Applications Built Using CUDA Toolkit 11.0 1.4. Building Applications with the NVIDIA Ampere GPU Architecture Support 1.4.1. Building Applications Using CUDA Toolkit 10.x or Earlier 1.4.2. Building Applications Using CUDA Toolkit 11.0 1.4.3. Independent Thread Scheduling Compatibility 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Ampere Compatibility Guide » 1. NVIDIA Ampere GPU Architecture Compatibility v12.5 | PDF | Archive NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Ampere GPU Architecture. 1. NVIDIA Ampere GPU Architecture Compatibility  1.1. About this Document  This application note, NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ampere Architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ampere GPU architecture. 1.2. Application Compatibility on the NVIDIA Ampere GPU Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 7.0 is supported to run on a GPU with compute capability 7.5, however a cubin generated for compute capability 7.5 is not supported to run on a GPU with compute capability 7.0, and a cubin generated with compute capability 7.x is not supported to run on a GPU with compute capability 8.x. Kernel can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forward-compatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 7.x is supported to run on compute capability 7.x or any higher revision (major or minor), including compute capability 8.x. Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the Programming Guide. When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels, should work as-is on the NVIDIA Ampere architecture based GPUs. In such cases, rebuilding the application is not required. However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the NVIDIA Ampere architecture based GPUs. To know more about building compatible applications read Building Applications with the NVIDIA Ampere GPU Architecture Support . 1.3. Verifying Ampere Compatibility for Existing Applications  The first step towards making a CUDA application compatible with the NVIDIA Ampere GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX). The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 10.2 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ampere architecture based GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch the application. With CUDA_FORCE_PTX_JIT=1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JIT-compiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not compatible with the NVIDIA Ampere GPU architecture and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ampere GPU architecture. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.0  CUDA applications built using CUDA Toolkit 11.0 are compatible with the NVIDIA Ampere GPU architecture as long as they are built to include kernels in native cubin (compute capability 8.0) or PTX form or both. 1.4. Building Applications with the NVIDIA Ampere GPU Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ampere GPU architecture. Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. 1.4.1. Building Applications Using CUDA Toolkit 10.x or Earlier  The nvcc compiler included with versions 10.x (10.0, 10.1 and 10.2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7.x). When using CUDA Toolkit 10.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 10.0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9.x supports architectures up to _60 and _61). The final -gencode to generate PTX would also need to be update – for further information and examples see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.2. Building Applications Using CUDA Toolkit 11.0  With versions 11.0 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ampere GPU architecture (compute capability 8.0). When using CUDA Toolkit 11.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_80,code=compute_80\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_80,code=sm_80\n  -gencode=arch=compute_80,code=compute_80\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.3. Independent Thread Scheduling Compatibility  NVIDIA GPUs since Volta architecture have Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.0 in the Programming Guide for details and corrective actions. To aid migration to the NVIDIA Ampere GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -gencode=arch=compute_60,code=sm_80 ... 2. Revision History  Version 1.0 Initial public release. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html", "content_type": "text/html", "text": "Turing Compatibility 1. Turing Compatibility 1.1. About this Document 1.2. Application Compatibility on Turing 1.3. Compatibility between Volta and Turing 1.4. Verifying Turing Compatibility for Existing Applications 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.4.2. Applications Using CUDA Toolkit 9.x 1.4.3. Applications Using CUDA Toolkit 10.0 1.5. Building Applications with Turing Support 1.5.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.5.2. Applications Using CUDA Toolkit 9.x 1.5.3. Applications Using CUDA Toolkit 10.0 1.5.4. Independent Thread Scheduling Compatibility 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Turing Compatibility Guide » 1. Turing Compatibility v12.5 | PDF | Archive Turing Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Turing GPUs. 1. Turing Compatibility  1.1. About this Document  This application note, Turing Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Turing Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Turing. 1.2. Application Compatibility on Turing  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Turing-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Turing-compatible PTX or cubins. 1.3. Compatibility between Volta and Turing  The Turing architecture is based on Volta’s Instruction Set Architecture ISA 7.0, extending it with new instructions. As a consequence, any binary that runs on Volta will be able to run on Turing (forward compatibility), but a Turing binary will not be able to run on Volta. Please note that Volta kernels using more than 64KB of shared memory (via the explicit opt-in, see CUDA C++ Programming Guide ) will not be able to launch on Turing, as they would exceed Turing’s shared memory capacity. Most applications compiled for Volta should run efficiently on Turing, except if the application uses heavily the Tensor Cores, or if recompiling would allow use of new Turing-specific instructions. Volta’s Tensor Core instructions can only reach half of the peak performance on Turing. Recompiling explicitly for Turing is thus recommended. 1.4. Verifying Turing Compatibility for Existing Applications  The first step is to check that Turing-compatible device code (at least PTX) is compiled into the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Turing as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Turing compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.4.2. Applications Using CUDA Toolkit 9.x  CUDA applications built using CUDA Toolkit 9.x are compatible with Turing as long as they are built to include kernels in either Volta-native cubin format (see Compatibility between Volta and Turing ) or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ) or both. 1.4.3. Applications Using CUDA Toolkit 10.0  CUDA applications built using CUDA Toolkit 10.0 are compatible with Turing as long as they are built to include kernels in Volta-native or Turing-native cubin format (see Compatibility between Volta and Turing ), or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ), or both. 1.5. Building Applications with Turing Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Turing depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.5.1. Applications Using CUDA Toolkit 8.0 or Earlier  The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to Volta or Turing architecture. To allow support for Volta, Turing and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Turing devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Turing compatibility. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_61,code=compute_61\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_61,code=compute_61\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.5.2. Applications Using CUDA Toolkit 9.x  With versions 9.x of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7.0). When using CUDA Toolkit 9.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_70,code=compute_70\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_70,code=compute_70\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. Also, note that CUDA 9.0 removes support for compute capability 2.x (Fermi) devices. Any compute_2x and sm_2x flags need to be removed from your compiler commands. 1.5.3. Applications Using CUDA Toolkit 10.0  With version 10.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Turing architecture (compute capability 7.5). When using CUDA Toolkit 10.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.5.4. Independent Thread Scheduling Compatibility  The Volta and Turing architectures feature Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.0 in the CUDA C++ Programming Guide for details and corrective actions. To aid migration Volta and Turing developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -arch=compute_60 -code=sm_70 ... 2. Revision History  Version 1.0 Initial public release. Version 1.1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/volta-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/volta-compatibility-guide/index.html", "content_type": "text/html", "text": "Volta Compatibility 1. Volta Compatibility 1.1. About this Document 1.2. Application Compatibility on Volta 1.3. Verifying Volta Compatibility for Existing Applications 1.3.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.3.2. Applications Using CUDA Toolkit 9.0 1.4. Building Applications with Volta Support 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.4.2. Applications Using CUDA Toolkit 9.0 1.4.3. Independent Thread Scheduling Compatibility 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Volta Compatibility Guide » 1. Volta Compatibility v12.5 | PDF | Archive Volta Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Volta Architecture. 1. Volta Compatibility  1.1. About this Document  This application note, Volta Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Volta Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Volta. 1.2. Application Compatibility on Volta  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Volta-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Volta-compatible PTX or cubins. 1.3. Verifying Volta Compatibility for Existing Applications  The first step is to check that Volta-compatible device code (at least PTX) is compiled into the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 8.0 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Volta as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from http://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Volta compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 9.0  CUDA applications built using CUDA Toolkit 9.0 are compatible with Volta as long as they are built to include kernels in either Volta-native cubin format (see Building Applications with Volta Support ) or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ) or both. 1.4. Building Applications with Volta Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Volta depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier  The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to the Volta architecture. To allow support for Volta and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Volta devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Volta compatibility. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_61,code=compute_61\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_61,code=compute_61\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 9.0  With version 9.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7.0). When using CUDA Toolkit 9.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_70,code=compute_70\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_70,code=compute_70\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. Also, note that CUDA 9.0 removes support for compute capability 2.x (Fermi) devices. Any compute_2x and sm_2x flags need to be removed from your compiler commands. 1.4.3. Independent Thread Scheduling Compatibility  The Volta architecture introduces Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.0 in the CUDA C++ Programming Guide for details and corrective actions. To aid migration Volta developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -arch=compute_60 -code=sm_70 ... 2. Revision History  Version 1.0 Initial public release. Version 1.1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/pascal-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/pascal-compatibility-guide/index.html", "content_type": "text/html", "text": "Pascal Compatibility 1. Pascal Compatibility 1.1. About this Document 1.2. Application Compatibility on Pascal 1.3. Verifying Pascal Compatibility for Existing Applications 1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier 1.3.2. Applications Using CUDA Toolkit 8.0 1.4. Building Applications with Pascal Support 1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier 1.4.2. Applications Using CUDA Toolkit 8.0 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Pascal Compatibility Guide » 1. Pascal Compatibility v12.5 | PDF | Archive Pascal Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Pascal Architecture. 1. Pascal Compatibility  1.1. About this Document  This application note, Pascal Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Pascal Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Pascal. 1.2. Application Compatibility on Pascal  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Pascal-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Pascal-compatible PTX or cubins. 1.3. Verifying Pascal Compatibility for Existing Applications  The first step is to check that Pascal-compatible device code (at least PTX) is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 7.5 are compatible with Pascal as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Pascal compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 8.0  CUDA applications built using CUDA Toolkit 8.0 are compatible with Pascal as long as they are built to include kernels in either Pascal-native cubin format (see Building Applications with Pascal Support ) or PTX format (see Applications Using CUDA Toolkit 7.5 or Earlier ) or both. 1.4. Building Applications with Pascal Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Pascal depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier  The compilers included in CUDA Toolkit 7.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Kepler and Maxwell, but they cannot generate cubin files native to the Pascal architecture. To allow support for Pascal and future architectures when using version 7.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Kepler or Maxwell devices natively and on Pascal devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Pascal compatibility. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_52,code=compute_52\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_52,code=compute_52\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 8.0  With version 8.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Pascal architectures (compute capability 6.0 and 6.1). When using CUDA Toolkit 8.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_61,code=compute_61\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_61,code=sm_61\n  -gencode=arch=compute_61,code=compute_61\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 2. Revision History  Version 1.0 Initial public release. Version 1.1 Use CUDA C++ instead of CUDA C/C++ 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/maxwell-compatibility-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/maxwell-compatibility-guide/index.html", "content_type": "text/html", "text": "Maxwell Compatibility 1. Maxwell Compatibility 1.1. About this Document 1.2. Application Compatibility on Maxwell 1.3. Verifying Maxwell Compatibility for Existing Applications 1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier 1.3.2. Applications Using CUDA Toolkit 6.0 or Later 1.4. Building Applications with Maxwell Support 1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier 1.4.2. Applications Using CUDA Toolkit 6.0 or Later 2. Revision History 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Maxwell Compatibility Guide » 1. Maxwell Compatibility v12.5 | PDF | Archive Maxwell Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Maxwell Architecture. 1. Maxwell Compatibility  1.1. About this Document  This application note, Maxwell Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Maxwell Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Maxwell. 1.2. Application Compatibility on Maxwell  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Maxwell-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Maxwell-compatible PTX or cubins. 1.3. Verifying Maxwell Compatibility for Existing Applications  The first step is to check that Maxwell-compatible device code (at least PTX) is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 5.5 are compatible with Maxwell as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Maxwell compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 6.0 or Later  CUDA applications built using CUDA Toolkit 6.0 or Later 1 are compatible with Maxwell as long as they are built to include kernels in either Maxwell-native cubin format (see Building Applications with Maxwell Support ) or PTX format (see Applications Using CUDA Toolkit 5.5 or Earlier ) or both. 1.4. Building Applications with Maxwell Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Maxwell depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier  The compilers included in CUDA Toolkit 5.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Fermi and Kepler, but they cannot generate cubin files native to the Maxwell architecture. To allow support for Maxwell and future architectures when using version 5.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Fermi or Kepler devices natively and on Maxwell devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Maxwell compatibility. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_20,code=sm_20\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_35,code=compute_35\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_20,code=sm_20\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_35,code=compute_35\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 6.0 or Later  With version 6.0 of the CUDA Toolkit, nvcc can generate cubin files native to the first-generation Maxwell architecture (compute capability 5.0); CUDA Toolkit 6.5 and later further add native support for second-generation Maxwell devices (compute capability 5.2). When using CUDA Toolkit 6.x or Later, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\vs2010\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_20,code=sm_20\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_52,code=compute_52\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_20,code=sm_20\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_52,code=sm_52\n  -gencode=arch=compute_52,code=compute_52\n  -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 2. Revision History  Version 1.0 Initial public release. Version 1.1 Updated for second-generation Maxwell (compute capability 5.2). Version 1.2 Use CUDA C++ instead of CUDA C/C++. Updated CUDA Toolkit reference to 6.0 and Later. 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Future CUDA Toolkit version might deprecate support for the Maxwell Architecture. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html", "parent_url": "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html", "content_type": "text/html", "text": "PTX ISA 8.5 1. Introduction 1.1. Scalable Data-Parallel Computing using GPUs 1.2. Goals of PTX 1.3. PTX ISA Version 8.5 1.4. Document Structure 2. Programming Model 2.1. A Highly Multithreaded Coprocessor 2.2. Thread Hierarchy 2.2.1. Cooperative Thread Arrays 2.2.2. Cluster of Cooperative Thread Arrays 2.2.3. Grid of Clusters 2.3. Memory Hierarchy 3. PTX Machine Model 3.1. A Set of SIMT Multiprocessors 3.2. Independent Thread Scheduling 3.3. On-chip Shared Memory 4. Syntax 4.1. Source Format 4.2. Comments 4.3. Statements 4.3.1. Directive Statements 4.3.2. Instruction Statements 4.4. Identifiers 4.5. Constants 4.5.1. Integer Constants 4.5.2. Floating-Point Constants 4.5.3. Predicate Constants 4.5.4. Constant Expressions 4.5.5. Integer Constant Expression Evaluation 4.5.6. Summary of Constant Expression Evaluation Rules 5. State Spaces, Types, and Variables 5.1. State Spaces 5.1.1. Register State Space 5.1.2. Special Register State Space 5.1.3. Constant State Space 5.1.3.1. Banked Constant State Space (deprecated) 5.1.4. Global State Space 5.1.5. Local State Space 5.1.6. Parameter State Space 5.1.6.1. Kernel Function Parameters 5.1.6.2. Kernel Function Parameter Attributes 5.1.6.3. Kernel Parameter Attribute: .ptr 5.1.6.4. Device Function Parameters 5.1.7. Shared State Space 5.1.8. Texture State Space (deprecated) 5.2. Types 5.2.1. Fundamental Types 5.2.2. Restricted Use of Sub-Word Sizes 5.2.3. Alternate Floating-Point Data Formats 5.2.4. Packed Data Types 5.2.4.1. Packed Floating Point Data Types 5.2.4.2. Packed Integer Data Types 5.3. Texture Sampler and Surface Types 5.3.1. Texture and Surface Properties 5.3.2. Sampler Properties 5.3.3. Channel Data Type and Channel Order Fields 5.4. Variables 5.4.1. Variable Declarations 5.4.2. Vectors 5.4.3. Array Declarations 5.4.4. Initializers 5.4.5. Alignment 5.4.6. Parameterized Variable Names 5.4.7. Variable Attributes 5.4.8. Variable and Function Attribute Directive: .attribute 5.5. Tensors 5.5.1. Tensor Dimension, size and format 5.5.2. Tensor Access Modes 5.5.3. Tiled Mode 5.5.3.1. Bounding Box 5.5.3.2. Traversal-Stride 5.5.3.3. Out of Boundary Access 5.5.4. Im2col mode 5.5.4.1. Bounding Box 5.5.4.2. Traversal Stride 5.5.4.3. Out of Boundary Access 5.5.5. Interleave layout 5.5.6. Swizzling Modes 5.5.7. Tensor-map 6. Instruction Operands 6.1. Operand Type Information 6.2. Source Operands 6.3. Destination Operands 6.4. Using Addresses, Arrays, and Vectors 6.4.1. Addresses as Operands 6.4.1.1. Generic Addressing 6.4.2. Arrays as Operands 6.4.3. Vectors as Operands 6.4.4. Labels and Function Names as Operands 6.5. Type Conversion 6.5.1. Scalar Conversions 6.5.2. Rounding Modifiers 6.6. Operand Costs 7. Abstracting the ABI 7.1. Function Declarations and Definitions 7.1.1. Changes from PTX ISA Version 1.x 7.2. Variadic Functions 7.3. Alloca 8. Memory Consistency Model 8.1. Scope and applicability of the model 8.1.1. Limitations on atomicity at system scope 8.2. Memory operations 8.2.1. Overlap 8.2.2. Aliases 8.2.3. Multimem Addresses 8.2.4. Memory Operations on Vector Data Types 8.2.5. Memory Operations on Packed Data Types 8.2.6. Initialization 8.3. State spaces 8.4. Operation types 8.4.1. mmio Operation 8.5. Scope 8.6. Proxies 8.7. Morally strong operations 8.7.1. Conflict and Data-races 8.7.2. Limitations on Mixed-size Data-races 8.8. Release and Acquire Patterns 8.9. Ordering of memory operations 8.9.1. Program Order 8.9.1.1. Asynchronous Operations 8.9.2. Observation Order 8.9.3. Fence-SC Order 8.9.4. Memory synchronization 8.9.5. Causality Order 8.9.6. Coherence Order 8.9.7. Communication Order 8.10. Axioms 8.10.1. Coherence 8.10.2. Fence-SC 8.10.3. Atomicity 8.10.4. No Thin Air 8.10.5. Sequential Consistency Per Location 8.10.6. Causality 9. Instruction Set 9.1. Format and Semantics of Instruction Descriptions 9.2. PTX Instructions 9.3. Predicated Execution 9.3.1. Comparisons 9.3.1.1. Integer and Bit-Size Comparisons 9.3.1.2. Floating Point Comparisons 9.3.2. Manipulating Predicates 9.4. Type Information for Instructions and Operands 9.4.1. Operand Size Exceeding Instruction-Type Size 9.5. Divergence of Threads in Control Constructs 9.6. Semantics 9.6.1. Machine-Specific Semantics of 16-bit Code 9.7. Instructions 9.7.1. Integer Arithmetic Instructions 9.7.1.1. Integer Arithmetic Instructions: add 9.7.1.2. Integer Arithmetic Instructions: sub 9.7.1.3. Integer Arithmetic Instructions: mul 9.7.1.4. Integer Arithmetic Instructions: mad 9.7.1.5. Integer Arithmetic Instructions: mul24 9.7.1.6. Integer Arithmetic Instructions: mad24 9.7.1.7. Integer Arithmetic Instructions: sad 9.7.1.8. Integer Arithmetic Instructions: div 9.7.1.9. Integer Arithmetic Instructions: rem 9.7.1.10. Integer Arithmetic Instructions: abs 9.7.1.11. Integer Arithmetic Instructions: neg 9.7.1.12. Integer Arithmetic Instructions: min 9.7.1.13. Integer Arithmetic Instructions: max 9.7.1.14. Integer Arithmetic Instructions: popc 9.7.1.15. Integer Arithmetic Instructions: clz 9.7.1.16. Integer Arithmetic Instructions: bfind 9.7.1.17. Integer Arithmetic Instructions: fns 9.7.1.18. Integer Arithmetic Instructions: brev 9.7.1.19. Integer Arithmetic Instructions: bfe 9.7.1.20. Integer Arithmetic Instructions: bfi 9.7.1.21. Integer Arithmetic Instructions: szext 9.7.1.22. Integer Arithmetic Instructions: bmsk 9.7.1.23. Integer Arithmetic Instructions: dp4a 9.7.1.24. Integer Arithmetic Instructions: dp2a 9.7.2. Extended-Precision Integer Arithmetic Instructions 9.7.2.1. Extended-Precision Arithmetic Instructions: add.cc 9.7.2.2. Extended-Precision Arithmetic Instructions: addc 9.7.2.3. Extended-Precision Arithmetic Instructions: sub.cc 9.7.2.4. Extended-Precision Arithmetic Instructions: subc 9.7.2.5. Extended-Precision Arithmetic Instructions: mad.cc 9.7.2.6. Extended-Precision Arithmetic Instructions: madc 9.7.3. Floating-Point Instructions 9.7.3.1. Floating Point Instructions: testp 9.7.3.2. Floating Point Instructions: copysign 9.7.3.3. Floating Point Instructions: add 9.7.3.4. Floating Point Instructions: sub 9.7.3.5. Floating Point Instructions: mul 9.7.3.6. Floating Point Instructions: fma 9.7.3.7. Floating Point Instructions: mad 9.7.3.8. Floating Point Instructions: div 9.7.3.9. Floating Point Instructions: abs 9.7.3.10. Floating Point Instructions: neg 9.7.3.11. Floating Point Instructions: min 9.7.3.12. Floating Point Instructions: max 9.7.3.13. Floating Point Instructions: rcp 9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64 9.7.3.15. Floating Point Instructions: sqrt 9.7.3.16. Floating Point Instructions: rsqrt 9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64 9.7.3.18. Floating Point Instructions: sin 9.7.3.19. Floating Point Instructions: cos 9.7.3.20. Floating Point Instructions: lg2 9.7.3.21. Floating Point Instructions: ex2 9.7.3.22. Floating Point Instructions: tanh 9.7.4. Half Precision Floating-Point Instructions 9.7.4.1. Half Precision Floating Point Instructions: add 9.7.4.2. Half Precision Floating Point Instructions: sub 9.7.4.3. Half Precision Floating Point Instructions: mul 9.7.4.4. Half Precision Floating Point Instructions: fma 9.7.4.5. Half Precision Floating Point Instructions: neg 9.7.4.6. Half Precision Floating Point Instructions: abs 9.7.4.7. Half Precision Floating Point Instructions: min 9.7.4.8. Half Precision Floating Point Instructions: max 9.7.4.9. Half Precision Floating Point Instructions: tanh 9.7.4.10. Half Precision Floating Point Instructions: ex2 9.7.5. Comparison and Selection Instructions 9.7.5.1. Comparison and Selection Instructions: set 9.7.5.2. Comparison and Selection Instructions: setp 9.7.5.3. Comparison and Selection Instructions: selp 9.7.5.4. Comparison and Selection Instructions: slct 9.7.6. Half Precision Comparison Instructions 9.7.6.1. Half Precision Comparison Instructions: set 9.7.6.2. Half Precision Comparison Instructions: setp 9.7.7. Logic and Shift Instructions 9.7.7.1. Logic and Shift Instructions: and 9.7.7.2. Logic and Shift Instructions: or 9.7.7.3. Logic and Shift Instructions: xor 9.7.7.4. Logic and Shift Instructions: not 9.7.7.5. Logic and Shift Instructions: cnot 9.7.7.6. Logic and Shift Instructions: lop3 9.7.7.7. Logic and Shift Instructions: shf 9.7.7.8. Logic and Shift Instructions: shl 9.7.7.9. Logic and Shift Instructions: shr 9.7.8. Data Movement and Conversion Instructions 9.7.8.1. Cache Operators 9.7.8.2. Cache Eviction Priority Hints 9.7.8.3. Data Movement and Conversion Instructions: mov 9.7.8.4. Data Movement and Conversion Instructions: mov 9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated) 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync 9.7.8.7. Data Movement and Conversion Instructions: prmt 9.7.8.8. Data Movement and Conversion Instructions: ld 9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc 9.7.8.10. Data Movement and Conversion Instructions: ldu 9.7.8.11. Data Movement and Conversion Instructions: st 9.7.8.12. Data Movement and Conversion Instructions: st.async 9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red 9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu 9.7.8.15. Data Movement and Conversion Instructions: applypriority 9.7.8.16. Data Movement and Conversion Instructions: discard 9.7.8.17. Data Movement and Conversion Instructions: createpolicy 9.7.8.18. Data Movement and Conversion Instructions: isspacep 9.7.8.19. Data Movement and Conversion Instructions: cvta 9.7.8.20. Data Movement and Conversion Instructions: cvt 9.7.8.21. Data Movement and Conversion Instructions: cvt.pack 9.7.8.22. Data Movement and Conversion Instructions: mapa 9.7.8.23. Data Movement and Conversion Instructions: getctarank 9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copy 9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operations 9.7.8.24.2. Async Proxy 9.7.8.24.3. Data Movement and Conversion Instructions: cp.async 9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_group 9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all 9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulk 9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk 9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch 9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor 9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor 9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor 9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group 9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group 9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace 9.7.9. Texture Instructions 9.7.9.1. Texturing Modes 9.7.9.2. Mipmaps 9.7.9.3. Texture Instructions: tex 9.7.9.4. Texture Instructions: tld4 9.7.9.5. Texture Instructions: txq 9.7.9.6. Texture Instructions: istypep 9.7.10. Surface Instructions 9.7.10.1. Surface Instructions: suld 9.7.10.2. Surface Instructions: sust 9.7.10.3. Surface Instructions: sured 9.7.10.4. Surface Instructions: suq 9.7.11. Control Flow Instructions 9.7.11.1. Control Flow Instructions: {} 9.7.11.2. Control Flow Instructions: @ 9.7.11.3. Control Flow Instructions: bra 9.7.11.4. Control Flow Instructions: brx.idx 9.7.11.5. Control Flow Instructions: call 9.7.11.6. Control Flow Instructions: ret 9.7.11.7. Control Flow Instructions: exit 9.7.12. Parallel Synchronization and Communication Instructions 9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrier 9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.sync 9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.cluster 9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fence 9.7.12.5. Parallel Synchronization and Communication Instructions: atom 9.7.12.6. Parallel Synchronization and Communication Instructions: red 9.7.12.7. Parallel Synchronization and Communication Instructions: red.async 9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated) 9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync 9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync 9.7.12.11. Parallel Synchronization and Communication Instructions: activemask 9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync 9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol 9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync 9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier 9.7.12.15.1. Size and alignment of mbarrier object 9.7.12.15.2. Contents of the mbarrier object 9.7.12.15.3. Lifecycle of the mbarrier object 9.7.12.15.4. Phase of the mbarrier object 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object 9.7.12.15.5.1. expect-tx operation 9.7.12.15.5.2. complete-tx operation 9.7.12.15.6. Phase Completion of the mbarrier object 9.7.12.15.7. Arrive-on operation on mbarrier object 9.7.12.15.8. mbarrier support with shared memory 9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init 9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval 9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx 9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx 9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive 9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop 9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive 9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait 9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count 9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy 9.7.13. Warp Level Matrix Multiply-Accumulate Instructions 9.7.13.1. Matrix Shape 9.7.13.2. Matrix Data-types 9.7.13.3. Matrix multiply-accumulate operation using wmma instructions 9.7.13.3.1. Matrix Fragments for WMMA 9.7.13.3.2. Matrix Storage for WMMA 9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load 9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store 9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma 9.7.13.4. Matrix multiply-accumulate operation using mma instruction 9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type 9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point type 9.7.13.4.3. Matrix Fragments for mma.m8n8k16 9.7.13.4.4. Matrix Fragments for mma.m8n8k32 9.7.13.4.5. Matrix Fragments for mma.m8n8k128 9.7.13.4.6. Matrix Fragments for mma.m16n8k4 9.7.13.4.7. Matrix Fragments for mma.m16n8k8 9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type 9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type 9.7.13.4.10. Matrix Fragments for mma.m16n8k32 9.7.13.4.11. Matrix Fragments for mma.m16n8k64 9.7.13.4.12. Matrix Fragments for mma.m16n8k128 9.7.13.4.13. Matrix Fragments for mma.m16n8k256 9.7.13.4.14. Multiply-and-Accumulate Instruction: mma 9.7.13.4.15. Warp-level matrix load instruction: ldmatrix 9.7.13.4.16. Warp-level matrix store instruction: stmatrix 9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix 9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A 9.7.13.5.1. Sparse matrix storage 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types 9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types 9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type 9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type 9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type 9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 type 9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type 9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer type 9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata 9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions 9.7.14.1. Warpgroup 9.7.14.2. Matrix Shape 9.7.14.3. Matrix Data-types 9.7.14.4. Async Proxy 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction 9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts 9.7.14.5.1.1. Register Fragments 9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16 9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8 9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32 9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256 9.7.14.5.1.2. Shared Memory Matrix Layout 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16 9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8 9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32 9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256 9.7.14.5.1.2.5. Strides 9.7.14.5.1.2.6. Swizzling Modes 9.7.14.5.1.2.7. Matrix Descriptor Format 9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async 9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction 9.7.14.6.1. Sparse matrix storage 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32 9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16 9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64 9.7.14.6.3. Shared Memory Matrix Layout 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32 9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16 9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64 9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp 9.7.14.7. Asynchronous wgmma Proxy Operations 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence 9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group 9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group 9.7.15. Stack Manipulation Instructions 9.7.15.1. Stack Manipulation Instructions: stacksave 9.7.15.2. Stack Manipulation Instructions: stackrestore 9.7.15.3. Stack Manipulation Instructions: alloca 9.7.16. Video Instructions 9.7.16.1. Scalar Video Instructions 9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax 9.7.16.1.2. Scalar Video Instructions: vshl, vshr 9.7.16.1.3. Scalar Video Instructions: vmad 9.7.16.1.4. Scalar Video Instructions: vset 9.7.16.2. SIMD Video Instructions 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 9.7.16.2.2. SIMD Video Instructions: vset2 9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 9.7.16.2.4. SIMD Video Instructions: vset4 9.7.17. Miscellaneous Instructions 9.7.17.1. Miscellaneous Instructions: brkpt 9.7.17.2. Miscellaneous Instructions: nanosleep 9.7.17.3. Miscellaneous Instructions: pmevent 9.7.17.4. Miscellaneous Instructions: trap 9.7.17.5. Miscellaneous Instructions: setmaxnreg 10. Special Registers 10.1. Special Registers: %tid 10.2. Special Registers: %ntid 10.3. Special Registers: %laneid 10.4. Special Registers: %warpid 10.5. Special Registers: %nwarpid 10.6. Special Registers: %ctaid 10.7. Special Registers: %nctaid 10.8. Special Registers: %smid 10.9. Special Registers: %nsmid 10.10. Special Registers: %gridid 10.11. Special Registers: %is_explicit_cluster 10.12. Special Registers: %clusterid 10.13. Special Registers: %nclusterid 10.14. Special Registers: %cluster_ctaid 10.15. Special Registers: %cluster_nctaid 10.16. Special Registers: %cluster_ctarank 10.17. Special Registers: %cluster_nctarank 10.18. Special Registers: %lanemask_eq 10.19. Special Registers: %lanemask_le 10.20. Special Registers: %lanemask_lt 10.21. Special Registers: %lanemask_ge 10.22. Special Registers: %lanemask_gt 10.23. Special Registers: %clock, %clock_hi 10.24. Special Registers: %clock64 10.25. Special Registers: %pm0..%pm7 10.26. Special Registers: %pm0_64..%pm7_64 10.27. Special Registers: %envreg<32> 10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi 10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2> 10.30. Special Registers: %total_smem_size 10.31. Special Registers: %aggr_smem_size 10.32. Special Registers: %dynamic_smem_size 10.33. Special Registers: %current_graph_exec 11. Directives 11.1. PTX Module Directives 11.1.1. PTX Module Directives: .version 11.1.2. PTX Module Directives: .target 11.1.3. PTX Module Directives: .address_size 11.2. Specifying Kernel Entry Points and Functions 11.2.1. Kernel and Function Directives: .entry 11.2.2. Kernel and Function Directives: .func 11.2.3. Kernel and Function Directives: .alias 11.3. Control Flow Directives 11.3.1. Control Flow Directives: .branchtargets 11.3.2. Control Flow Directives: .calltargets 11.3.3. Control Flow Directives: .callprototype 11.4. Performance-Tuning Directives 11.4.1. Performance-Tuning Directives: .maxnreg 11.4.2. Performance-Tuning Directives: .maxntid 11.4.3. Performance-Tuning Directives: .reqntid 11.4.4. Performance-Tuning Directives: .minnctapersm 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated) 11.4.6. Performance-Tuning Directives: .noreturn 11.4.7. Performance-Tuning Directives: .pragma 11.5. Debugging Directives 11.5.1. Debugging Directives: @@dwarf 11.5.2. Debugging Directives: .section 11.5.3. Debugging Directives: .file 11.5.4. Debugging Directives: .loc 11.6. Linking Directives 11.6.1. Linking Directives: .extern 11.6.2. Linking Directives: .visible 11.6.3. Linking Directives: .weak 11.6.4. Linking Directives: .common 11.7. Cluster Dimension Directives 11.7.1. Cluster Dimension Directives: .reqnctapercluster 11.7.2. Cluster Dimension Directives: .explicitcluster 11.7.3. Cluster Dimension Directives: .maxclusterrank 12. Release Notes 12.1. Changes in PTX ISA Version 8.5 12.2. Changes in PTX ISA Version 8.4 12.3. Changes in PTX ISA Version 8.3 12.4. Changes in PTX ISA Version 8.2 12.5. Changes in PTX ISA Version 8.1 12.6. Changes in PTX ISA Version 8.0 12.7. Changes in PTX ISA Version 7.8 12.8. Changes in PTX ISA Version 7.7 12.9. Changes in PTX ISA Version 7.6 12.10. Changes in PTX ISA Version 7.5 12.11. Changes in PTX ISA Version 7.4 12.12. Changes in PTX ISA Version 7.3 12.13. Changes in PTX ISA Version 7.2 12.14. Changes in PTX ISA Version 7.1 12.15. Changes in PTX ISA Version 7.0 12.16. Changes in PTX ISA Version 6.5 12.17. Changes in PTX ISA Version 6.4 12.18. Changes in PTX ISA Version 6.3 12.19. Changes in PTX ISA Version 6.2 12.20. Changes in PTX ISA Version 6.1 12.21. Changes in PTX ISA Version 6.0 12.22. Changes in PTX ISA Version 5.0 12.23. Changes in PTX ISA Version 4.3 12.24. Changes in PTX ISA Version 4.2 12.25. Changes in PTX ISA Version 4.1 12.26. Changes in PTX ISA Version 4.0 12.27. Changes in PTX ISA Version 3.2 12.28. Changes in PTX ISA Version 3.1 12.29. Changes in PTX ISA Version 3.0 12.30. Changes in PTX ISA Version 2.3 12.31. Changes in PTX ISA Version 2.2 12.32. Changes in PTX ISA Version 2.1 12.33. Changes in PTX ISA Version 2.0 14. Descriptions of .pragma Strings 14.1. Pragma Strings: “nounroll” 14.2. Pragma Strings: “used_bytes_mask” 15. Notices 15.1. Notice 15.2. OpenCL 15.3. Trademarks PTX ISA » 1. Introduction v8.5 | PDF | Archive Parallel Thread Execution ISA Version 8.5 The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture). 1. Introduction  This document describes PTX, a low-level parallel thread execution virtual machine and instruction\nset architecture (ISA). PTX exposes the GPU as a data-parallel computing device . 1.1. Scalable Data-Parallel Computing using GPUs  Driven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable\nGPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous\ncomputational horsepower and very high memory bandwidth. The GPU is especially well-suited to\naddress problems that can be expressed as data-parallel computations - the same program is executed\non many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic\noperations to memory operations. Because the same program is executed for each data element, there\nis a lower requirement for sophisticated flow control; and because it is executed on many data\nelements and has high arithmetic intensity, the memory access latency can be hidden with\ncalculations instead of big data caches. Data-parallel processing maps data elements to parallel processing threads. Many applications that\nprocess large data sets can use a data-parallel programming model to speed up the computations. In\n3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and\nmedia processing applications such as post-processing of rendered images, video encoding and\ndecoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to\nparallel processing threads. In fact, many algorithms outside the field of image rendering and\nprocessing are accelerated by data-parallel processing, from general signal processing or physics\nsimulation to computational finance or computational biology. PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs\nare translated at install time to the target hardware instruction set. The PTX-to-GPU translator\nand driver enable NVIDIA GPUs to be used as programmable parallel computers. 1.2. Goals of PTX  PTX provides a stable programming model and instruction set for general purpose parallel\nprogramming. It is designed to be efficient on NVIDIA GPUs supporting the computation features\ndefined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA\nand C/C++ generate PTX instructions, which are optimized for and translated to native\ntarget-architecture instructions. The goals for PTX include the following: Provide a stable ISA that spans multiple GPU generations. Achieve performance in compiled applications comparable to native GPU performance. Provide a machine-independent ISA for C/C++ and other compilers to target. Provide a code distribution ISA for application and middleware developers. Provide a common source-level ISA for optimizing code generators and translators, which map PTX to\nspecific target machines. Facilitate hand-coding of libraries, performance kernels, and architecture tests. Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units. 1.3. PTX ISA Version 8.5  PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction. 1.4. Document Structure  The information in this document is organized into the following Chapters: Programming Model outlines the programming model. PTX Machine Model gives an overview of the PTX virtual machine model. Syntax describes the basic syntax of the PTX language. State Spaces, Types, and Variables describes\nstate spaces, types, and variable declarations. Instruction Operands describes instruction operands. Abstracting the ABI describes the function and call syntax,\ncalling convention, and PTX support for abstracting the Application Binary Interface (ABI) . Instruction Set describes the instruction set. Special Registers lists special registers. Directives lists the assembly directives supported in PTX. Release Notes provides release notes for PTX ISA versions 2.x and\nbeyond. References 754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008. http://ieeexplore.ieee.org/servlet/opac?punumber=4610933 The OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011. http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf CUDA Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA Dynamic Parallelism Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism CUDA Atomicity Requirements. https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity PTX Writers Guide to Interoperability. https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html 2. Programming Model  2.1. A Highly Multithreaded Coprocessor  The GPU is a compute device capable of executing a very large number of threads in parallel. It\noperates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive\nportions of applications running on the host are off-loaded onto the device. More precisely, a portion of an application that is executed many times, but independently on\ndifferent data, can be isolated into a kernel function that is executed on the GPU as many different\nthreads. To that effect, such a function is compiled to the PTX instruction set and the resulting\nkernel is translated at install time to the target GPU instruction set. 2.2. Thread Hierarchy  The batch of threads that executes a kernel is organized as a grid. A grid consists of either\ncooperative thread arrays or clusters of cooperative thread arrays as described in this section and\nillustrated in Figure 1 and Figure 2 . Cooperative thread arrays (CTAs) implement CUDA\nthread blocks and clusters implement CUDA thread block clusters. 2.2.1. Cooperative Thread Arrays  The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program\nspecifies the execution of a given thread of a parallel thread array. A cooperative thread array ,\nor CTA, is an array of threads that execute a kernel concurrently or in parallel. Threads within a CTA can communicate with each other. To coordinate the communication of the threads\nwithin the CTA, one can specify synchronization points where threads wait until all threads in the\nCTA have arrived. Each thread has a unique thread identifier within the CTA. Programs use a data parallel\ndecomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread\nuses its thread identifier to determine its assigned role, assign specific input and output\npositions, compute addresses, and select work to perform. The thread identifier is a three-element\nvector tid , (with elements tid.x , tid.y , and tid.z ) that specifies the thread’s\nposition within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the\nnumber of thread ids in that CTA dimension. Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements ntid.x , ntid.y , and ntid.z ). The vector ntid specifies the number of threads in each\nCTA dimension. Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps . A warp is a maximal subset of threads from a single CTA, such that the threads execute\nthe same instructions at the same time. Threads within a warp are sequentially numbered. The warp\nsize is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be\nable to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate\nconstant, WARP_SZ , which may be used in any instruction where an immediate operand is allowed. 2.2.2. Cluster of Cooperative Thread Arrays  Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate\nwith each other via shared memory. The executing CTA has to make sure that the shared memory of the\npeer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before\ncompleting the shared memory operation. Threads within the different CTAs in a cluster can synchronize and communicate with each other via\nshared memory. Cluster-wide barriers can be used to synchronize all the threads within the\ncluster. Each CTA in a cluster has a unique CTA identifier within its cluster\n( cluster_ctaid ). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter cluster_nctaid . Each CTA in the cluster also has a unique CTA identifier ( cluster_ctarank )\nacross all dimensions. The total number of CTAs across all the dimensions in the cluster is\nspecified by cluster_nctarank . Threads may read and use these values through predefined, read-only\nspecial registers %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank . Cluster level is applicable only on target architecture sm_90 or higher. Specifying cluster\nlevel during launch time is optional. If the user specifies the cluster dimensions at launch time\nthen it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster\nlaunch with default dimension 1x1x1. PTX provides read-only special register %is_explicit_cluster to differentiate between explicit and implicit cluster launch. 2.2.3. Grid of Clusters  There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a\ncluster can contain. However, clusters with CTAs that execute the same kernel can be batched\ntogether into a grid of clusters, so that the total number of threads that can be launched in a\nsingle kernel invocation is very large. This comes at the expense of reduced thread communication\nand synchronization, because threads in different clusters cannot communicate and synchronize with\neach other. Each cluster has a unique cluster identifier ( clusterid ) within a grid of clusters. Each grid of\nclusters has a 1D, 2D , or 3D shape specified by the parameter nclusterid . Each grid also has a\nunique temporal grid identifier ( gridid ). Threads may read and use these values through\npredefined, read-only special registers %tid , %ntid , %clusterid , %nclusterid , and %gridid . Each CTA has a unique identifier ( ctaid ) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape\nspecified by the parameter nctaid . Thread may use and read these values through predefined,\nread-only special registers %ctaid and %nctaid . Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs\nwhere cluster is optional level and is applicable only for target architectures sm_90 and\nhigher. Figure 1 shows a grid consisting of CTAs and Figure 2 shows a grid consisting of clusters. Grids may be launched with dependencies between one another - a grid may be a dependent grid and/or\na prerequisite grid. To understand how grid dependencies may be defined, refer to the section on CUDA Graphs in the Cuda Programming Guide . Figure 1 Grid with CTAs  Figure 2 Grid with clusters  A cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads\nthat execute the same kernel program. A grid is a set of clusters consisting of CTAs that\nexecute independently. 2.3. Memory Hierarchy  PTX threads may access data from multiple state spaces during their execution as illustrated by Figure 3 where cluster level is introduced from\ntarget architecture sm_90 onwards. Each thread has a private local memory. Each thread block\n(CTA) has a shared memory visible to all threads of the block and to all active blocks in the\ncluster and with the same lifetime as the block. Finally, all threads have access to the same global\nmemory. There are additional state spaces accessible by all threads: the constant, param, texture, and\nsurface state spaces.  Constant and texture memory are read-only; surface memory is readable and\nwritable. The global, constant, param, texture, and surface state spaces are optimized for different\nmemory usages. For example, texture memory offers different addressing modes as well as data\nfiltering for specific data formats. Note that texture and surface memory is cached, and within the\nsame kernel call, the cache is not kept coherent with respect to global memory writes and surface\nmemory writes, so any texture fetch or surface read to an address that has been written to via a\nglobal or a surface write in the same kernel call returns undefined data. In other words, a thread\ncan safely read some texture or surface memory location only if this memory location has been\nupdated by a previous kernel call or memory copy, but not if it has been previously updated by the\nsame thread or another thread from the same kernel call. The global, constant, and texture state spaces are persistent across kernel launches by the same\napplication. Both the host and the device maintain their own local memory, referred to as host memory and device memory , respectively. The device memory may be mapped and read or written by the host, or,\nfor more efficient transfer, copied from the host memory through optimized API calls that utilize\nthe device’s high-performance Direct Memory Access (DMA) engine. Figure 3 Memory Hierarchy  3. PTX Machine Model  3.1. A Set of SIMT Multiprocessors  The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming\nMultiprocessors (SMs) . When a host program invokes a kernel grid, the blocks of the grid are\nenumerated and distributed to multiprocessors with available execution capacity. The threads of a\nthread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are\nlaunched on the vacated multiprocessors. A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction\nunit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent\nthreads in hardware with zero scheduling overhead. It implements a single-instruction barrier\nsynchronization. Fast barrier synchronization together with lightweight thread creation and\nzero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for\nexample, a low granularity decomposition of problems by assigning one thread to each data element\n(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation). To manage hundreds of threads running several different programs, the multiprocessor employs an\narchitecture we call SIMT (single-instruction, multiple-thread) . The multiprocessor maps each\nthread to one scalar processor core, and each scalar thread executes independently with its own\ninstruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,\nand executes threads in groups of parallel threads called warps . (This term originates from\nweaving, the first parallel thread technology.) Individual threads composing a SIMT warp start\ntogether at the same program address but are otherwise free to branch and execute independently. When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that\nget scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp\ncontains threads of consecutive, increasing thread IDs with the first warp containing thread 0. At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues\nthe next instruction to the active threads of the warp. A warp executes one common instruction at a\ntime, so full efficiency is realized when all threads of a warp agree on their execution path. If\nthreads of a warp diverge via a data-dependent conditional branch, the warp serially executes each\nbranch path taken, disabling threads that are not on that path, and when all paths complete, the\nthreads converge back to the same execution path. Branch divergence occurs only within a warp;\ndifferent warps execute independently regardless of whether they are executing common or disjointed\ncode paths. SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a\nsingle instruction controls multiple processing elements. A key difference is that SIMD vector\norganizations expose the SIMD width to the software, whereas SIMT instructions specify the execution\nand branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables\nprogrammers to write thread-level parallel code for independent, scalar threads, as well as\ndata-parallel code for coordinated threads. For the purposes of correctness, the programmer can\nessentially ignore the SIMT behavior; however, substantial performance improvements can be realized\nby taking care that the code seldom requires threads in a warp to diverge. In practice, this is\nanalogous to the role of cache lines in traditional code: Cache line size can be safely ignored when\ndesigning for correctness but must be considered in the code structure when designing for peak\nperformance. Vector architectures, on the other hand, require the software to coalesce loads into\nvectors and manage divergence manually. How many blocks a multiprocessor can process at once depends on how many registers per thread and\nhow much shared memory per block are required for a given kernel since the multiprocessor’s\nregisters and shared memory are split among all the threads of the batch of blocks. If there are not\nenough registers or shared memory available per multiprocessor to process at least one block, the\nkernel will fail to launch. Figure 4 Hardware Model  A set of SIMT multiprocessors with on-chip shared memory. 3.2. Independent Thread Scheduling  On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads\nin the warp together with an active mask specifying the active threads of the warp. As a result,\nthreads from the same warp in divergent regions or different states of execution cannot signal each\nother or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or\nmutexes can easily lead to deadlock, depending on which warp the contending threads come from. Starting with the Volta architecture, Independent Thread Scheduling allows full concurrency\nbetween threads, regardless of warp. With Independent Thread Scheduling , the GPU maintains\nexecution state per thread, including a program counter and call stack, and can yield execution at a\nper-thread granularity, either to make better use of execution resources or to allow one thread to\nwait for data to be produced by another. A schedule optimizer determines how to group active threads\nfrom the same warp together into SIMT units. This retains the high throughput of SIMT execution as\nin prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at\nsub-warp granularity. Independent Thread Scheduling can lead to a rather different set of threads participating in the\nexecuted code than intended if the developer made assumptions about warp-synchronicity of previous\nhardware architectures. In particular, any warp-synchronous code (such as synchronization-free,\nintra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the\nsection on Compute Capability 7.x in the Cuda Programming Guide for further details. 3.3. On-chip Shared Memory  As illustrated by Figure 4 , each multiprocessor has\non-chip memory of the four following types: One set of local 32-bit registers per processor, A parallel data cache or shared memory that is shared by all scalar processor cores and is where\nthe shared memory space resides, A read-only constant cache that is shared by all scalar processor cores and speeds up reads from\nthe constant memory space, which is a read-only region of device memory, A read-only texture cache that is shared by all scalar processor cores and speeds up reads from\nthe texture memory space, which is a read-only region of device memory; each multiprocessor\naccesses the texture cache via a texture unit that implements the various addressing modes and\ndata filtering. The local and global memory spaces are read-write regions of device memory. 4. Syntax  PTX programs are a collection of text source modules (files). PTX source modules have an\nassembly-language style syntax with instruction operation codes and operands. Pseudo-operations\nspecify symbol and addressing management. The ptxas optimizing backend compiler optimizes and\nassembles PTX source modules to produce corresponding binary object files. 4.1. Source Format  Source modules are ASCII text. Lines are separated by the newline character ( \\n ). All whitespace characters are equivalent; whitespace is ignored except for its use in separating\ntokens in the language. The C preprocessor cpp may be used to process PTX source modules. Lines beginning with # are\npreprocessor directives. The following are common preprocessor directives: #include , #define , #if , #ifdef , #else , #endif , #line , #file C: A Reference Manual by Harbison and Steele provides a good description of the C preprocessor. PTX is case sensitive and uses lowercase for keywords. Each PTX module must begin with a .version directive specifying the PTX language version,\nfollowed by a .target directive specifying the target architecture assumed. See PTX Module\nDirectives for a more information on these directives. 4.2. Comments  Comments in PTX follow C/C++ syntax, using non-nested /* and */ for comments that may span\nmultiple lines, and using // to begin a comment that extends up to the next newline character,\nwhich terminates the current line. Comments cannot occur within character constants, string\nliterals, or within other comments. Comments in PTX are treated as whitespace. 4.3. Statements  A PTX statement is either a directive or an instruction. Statements begin with an optional label and\nend with a semicolon. Examples .reg     .b32 r1, r2;\n        .global  .f32  array[N];\n\nstart:  mov.b32   r1, %tid.x;\n        shl.b32   r1, r1, 2;          // shift thread id by 2 bits\n        ld.global.b32 r2, array[r1];  // thread[tid] gets array[tid]\n        add.f32   r2, r2, 0.5;        // add 1/2 4.3.1. Directive Statements  Directive keywords begin with a dot, so no conflict is possible with user-defined identifiers. The\ndirectives in PTX are listed in Table 1 and\ndescribed in State Spaces, Types, and Variables and Directives . Table 1 PTX Directives  .address_size .explicitcluster .maxnreg .section .alias .extern .maxntid .shared .align .file .minnctapersm .sreg .branchtargets .func .noreturn .target .callprototype .global .param .tex .calltargets .loc .pragma .version .common .local .reg .visible .const .maxclusterrank .reqnctapercluster .weak .entry .maxnctapersm .reqntid 4.3.2. Instruction Statements  Instructions are formed from an instruction opcode followed by a comma-separated list of zero or\nmore operands, and terminated with a semicolon. Operands may be register variables, constant\nexpressions, address expressions, or label names. Instructions have an optional guard predicate\nwhich controls conditional execution. The guard predicate follows the optional label and precedes\nthe opcode, and is written as @p , where p is a predicate register. The guard predicate may\nbe optionally negated, written as @!p . The destination operand is first, followed by source operands. Instruction keywords are listed in Table 2 . All instruction keywords are\nreserved tokens in PTX. Table 2 Reserved Instruction Keywords  abs discard min shf vadd activemask div mma shfl vadd2 add dp2a mov shl vadd4 addc dp4a movmatrix shr vavrg2 alloca elect mul sin vavrg4 and ex2 mul24 slct vmad applypriority exit multimem sqrt vmax atom fence nanosleep st vmax2 bar fma neg stackrestore vmax4 barrier fns not stacksave vmin bfe getctarank or stmatrix vmin2 bfi griddepcontrol pmevent sub vmin4 bfind isspacep popc subc vote bmsk istypep prefetch suld vset bra ld prefetchu suq vset2 brev ldmatrix prmt sured vset4 brkpt ldu rcp sust vshl brx lg2 red szext vshr call lop3 redux tanh vsub clz mad rem testp vsub2 cnot mad24 ret tex vsub4 copysign madc rsqrt tld4 wgmma cos mapa sad trap wmma cp match selp txq xor createpolicy max set vabsdiff cvt mbarrier setmaxnreg vabsdiff2 cvta membar setp vabsdiff4 4.4. Identifiers  User-defined identifiers follow extended C++ rules: they either start with a letter followed by zero\nor more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,\nor percentage character followed by one or more letters, digits, underscore, or dollar characters: followsym:   [a-zA-Z0-9_$]\nidentifier:  [a-zA-Z]{followsym}* | {[_$%]{followsym}+ PTX does not specify a maximum length for identifiers and suggests that all implementations support\na minimum length of at least 1024 characters. Many high-level languages such as C and C++ follow similar rules for identifier names, except that\nthe percentage sign is not allowed. PTX allows the percentage sign as the first character of an\nidentifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined\nvariable names and compiler-generated names. PTX predefines one constant and a small number of special registers that begin with the percentage\nsign, listed in Table 3 . Table 3 Predefined Identifiers  %clock %laneid %lanemask_gt %pm0, ..., %pm7 %clock64 %lanemask_eq %nctaid %smid %ctaid %lanemask_le %ntid %tid %envreg<32> %lanemask_lt %nsmid %warpid %gridid %lanemask_ge %nwarpid WARP_SZ 4.5. Constants  PTX supports integer and floating-point constants and constant expressions. These constants may be\nused in data initialization and as operands to instructions. Type checking rules remain the same for\ninteger, floating-point, and bit-size types. For predicate-type data and instructions, integer\nconstants are allowed and are interpreted as in C, i.e., zero values are False and non-zero\nvalues are True . 4.5.1. Integer Constants  Integer constants are 64-bits in size and are either signed or unsigned, i.e., every integer\nconstant has type .s64 or .u64 . The signed/unsigned nature of an integer constant is needed\nto correctly evaluate constant expressions containing operations such as division and ordered\ncomparisons, where the behavior of the operation depends on the operand types. When used in an\ninstruction or data initialization, each integer constant is converted to the appropriate size based\non the data or instruction type at its use. Integer literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax\nfollows that of C. Integer literals may be followed immediately by the letter U to indicate that\nthe literal is unsigned. hexadecimal literal:  0[xX]{hexdigit}+U?\noctal literal:        0{octal digit}+U?\nbinary literal:       0[bB]{bit}+U?\ndecimal literal       {nonzero-digit}{digit}*U? Integer literals are non-negative and have a type determined by their magnitude and optional type\nsuffix as follows: literals are signed ( .s64 ) unless the value cannot be fully represented in .s64 or the unsigned suffix is specified, in which case the literal is unsigned ( .u64 ). The predefined integer constant WARP_SZ specifies the number of threads per warp for the target\nplatform; to date, all target architectures have a WARP_SZ value of 32. 4.5.2. Floating-Point Constants  Floating-point constants are represented as 64-bit double-precision values, and all floating-point\nconstant expressions are evaluated using 64-bit double precision arithmetic. The only exception is\nthe 32-bit hex notation for expressing an exact single-precision floating-point value; such values\nretain their exact 32-bit single-precision value and may not be used in constant expressions. Each\n64-bit floating-point constant is converted to the appropriate floating-point size based on the data\nor instruction type at its use. Floating-point literals may be written with an optional decimal point and an optional signed\nexponent. Unlike C and C++, there is no suffix letter to specify size; literals are always\nrepresented in 64-bit double-precision format. PTX includes a second representation of floating-point constants for specifying the exact machine\nrepresentation using a hexadecimal constant. To specify IEEE 754 double-precision floating point\nvalues, the constant begins with 0d or 0D followed by 16 hex digits. To specify IEEE 754\nsingle-precision floating point values, the constant begins with 0f or 0F followed by 8 hex\ndigits. 0[fF]{hexdigit}{8}      // single-precision floating point\n0[dD]{hexdigit}{16}     // double-precision floating point Example mov.f32  $f3, 0F3f800000;       //  1.0 4.5.3. Predicate Constants  In PTX, integer constants may be used as predicates. For predicate-type data initializers and\ninstruction operands, integer constants are interpreted as in C, i.e., zero values are False and\nnon-zero values are True . 4.5.4. Constant Expressions  In PTX, constant expressions are formed using operators as in C and are evaluated using rules\nsimilar to those in C, but simplified by restricting types and sizes, removing most casts, and\ndefining full semantics to eliminate cases where expression evaluation in C is implementation\ndependent. Constant expressions are formed from constant literals, unary plus and minus, basic arithmetic\noperators (addition, subtraction, multiplication, division), comparison operators, the conditional\nternary operator ( ?: ), and parentheses. Integer constant expressions also allow unary logical\nnegation ( ! ), bitwise complement ( ~ ), remainder ( % ), shift operators ( << and >> ), bit-type operators ( & , | , and ^ ), and logical operators ( && , || ). Constant expressions in PTX do not support casts between integer and floating-point. Constant expressions are evaluated using the same operator precedence as\nin C. Table 4 gives operator precedence and\nassociativity. Operator precedence is highest for unary operators and decreases with each line in\nthe chart. Operators on the same line have the same precedence and are evaluated right-to-left for\nunary operators and left-to-right for binary operators. Table 4 Operator Precedence  Kind Operator Symbols Operator Names Associates Primary () parenthesis n/a Unary +- ! ~ plus, minus, negation, complement right (.s64) (.u64) casts right Binary */ % multiplication, division, remainder left +- addition, subtraction >> << shifts < > <= >= ordered comparisons == != equal, not equal & bitwise AND ^ bitwise XOR | bitwise OR && logical AND || logical OR Ternary ?: conditional right 4.5.5. Integer Constant Expression Evaluation  Integer constant expressions are evaluated at compile time according to a set of rules that\ndetermine the type (signed .s64 versus unsigned .u64 ) of each sub-expression. These rules\nare based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and\nbehavior is fully defined in all cases (specifically, for remainder and shift operators). Literals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a U suffix. For example: 42 , 0x1234 , 0123 are signed. 0xfabc123400000000 , 42U , 0x1234U are unsigned. Unary plus and minus preserve the type of the input operand. For example: +123 , -1 , -(-42) are signed. -1U , -0xfabc123400000000 are unsigned. Unary logical negation ( ! ) produces a signed result with value 0 or 1 . Unary bitwise complement ( ~ ) interprets the source operand as unsigned and produces an\nunsigned result. Some binary operators require normalization of source operands. This normalization is known as the usual arithmetic conversions and simply converts both operands to unsigned type if either\noperand is unsigned. Addition, subtraction, multiplication, and division perform the usual arithmetic conversions and\nproduce a result with the same type as the converted operands. That is, the operands and result\nare unsigned if either source operand is unsigned, and is otherwise signed. Remainder ( % ) interprets the operands as unsigned. Note that this differs from C, which allows\na negative divisor but defines the behavior to be implementation dependent. Left and right shift interpret the second operand as unsigned and produce a result with the same\ntype as the first operand. Note that the behavior of right-shift is determined by the type of the\nfirst operand: right shift of a signed value is arithmetic and preserves the sign, and right shift\nof an unsigned value is logical and shifts in a zero bit. AND ( & ), OR ( | ), and XOR ( ^ ) perform the usual arithmetic conversions and produce a\nresult with the same type as the converted operands. AND_OP ( && ), OR_OP ( || ), Equal ( == ), and Not_Equal ( != ) produce a signed\nresult. The result value is 0 or 1. Ordered comparisons ( < , <= , > , >= ) perform the usual arithmetic conversions on\nsource operands and produce a signed result. The result value is 0 or 1 . Casting of expressions to signed or unsigned is supported using ( .s64 ) and ( .u64 ) casts. For the conditional operator ( ? : ) , the first operand must be an integer, and the second\nand third operands are either both integers or both floating-point. The usual arithmetic\nconversions are performed on the second and third operands, and the result type is the same as the\nconverted type. 4.5.6. Summary of Constant Expression Evaluation Rules  Table 5 contains a summary of the constant expression evaluation rules. Table 5 Constant Expression Evaluation Rules  Kind Operator Operand Types Operand Interpretation Result Type Primary () any type same as source same as source constant literal n/a n/a .u64 , .s64 , or .f64 Unary +- any type same as source same as source ! integer zero or non-zero .s64 ~ integer .u64 .u64 Cast (.u64) integer .u64 .u64 (.s64) integer .s64 .s64 Binary +- * / .f64 .f64 .f64 integer use usual conversions converted type < > <= >= .f64 .f64 .s64 integer use usual conversions .s64 == != .f64 .f64 .s64 integer use usual conversions .s64 % integer .u64 .s64 >> << integer 1st unchanged, 2nd is .u64 same as 1st operand & | ^ integer .u64 .u64 && || integer zero or non-zero .s64 Ternary ?: int ? .f64 : .f64 same as sources .f64 int ? int : int use usual conversions converted type 5. State Spaces, Types, and Variables  While the specific resources available in a given target GPU will vary, the kinds of resources will\nbe common across platforms, and these resources are abstracted in PTX through state spaces and data\ntypes. 5.1. State Spaces  A state space is a storage area with particular characteristics. All variables reside in some state\nspace. The characteristics of a state space include its size, addressability, access speed, access\nrights, and level of sharing between threads. The state spaces defined in PTX are a byproduct of parallel programming and graphics\nprogramming. The list of state spaces is shown in Table 6 ,and\nproperties of state spaces are shown in Table 7 . Table 6 State Spaces  Name Description .reg Registers, fast. .sreg Special registers. Read-only; pre-defined; platform-specific. .const Shared, read-only memory. .global Global memory, shared by all threads. .local Local memory, private to each thread. .param Kernel parameters, defined per-grid; or Function or local parameters, defined per-thread. .shared Addressable memory, defined per CTA, accessible to all threads in the cluster\nthroughout the lifetime of the CTA that defines it. .tex Global texture memory (deprecated). Table 7 Properties of State Spaces  Name Addressable Initializable Access Sharing .reg No No R/W per-thread .sreg No No RO per-CTA .const Yes Yes 1 RO per-grid .global Yes Yes 1 R/W Context .local Yes No R/W per-thread .param (as input to kernel) Yes 2 No RO per-grid .param (used in functions) Restricted 3 No R/W per-thread .shared Yes No R/W per-cluster 5 .tex No 4 Yes, via driver RO Context Notes: 1 Variables in .const and .global state spaces are initialized to zero by default. 2 Accessible only via the ld.param{::entry} instruction. Address may be taken via mov instruction. 3 Accessible via ld.param{::func} and st.param{::func} instructions. Device function\ninput and return parameters may have their address taken via mov ; the parameter is then located\non the stack frame and its address is in the .local state space. 4 Accessible only via the tex instruction. 5 Visible to the owning CTA and other active CTAs in the cluster. 5.1.1. Register State Space  Registers ( .reg state space) are fast storage locations. The number of registers is limited, and\nwill vary from platform to platform. When the limit is exceeded, register variables will be spilled\nto memory, causing changes in performance. For each architecture, there is a recommended maximum\nnumber of registers to use (see the CUDA Programming Guide for details). Registers may be typed (signed integer, unsigned integer, floating point, predicate) or\nuntyped. Register size is restricted; aside from predicate registers which are 1-bit, scalar\nregisters have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of\n16-, 32-, 64-, or 128-bits. The most common use of 8-bit registers is with ld , st , and cvt instructions, or as elements of vector tuples. Registers differ from the other state spaces in that they are not fully addressable, i.e., it is not\npossible to refer to the address of a register. When compiling to use the Application Binary\nInterface (ABI), register variables are restricted to function scope and may not be declared at\nmodule scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg variables, the compiler silently disables use of the ABI. Registers may have alignment\nboundaries required by multi-word loads and stores. 5.1.2. Special Register State Space  The special register ( .sreg ) state space holds predefined, platform-specific registers, such as\ngrid, cluster, CTA, and thread parameters, clock counters, and performance monitoring registers. All\nspecial registers are predefined. 5.1.3. Constant State Space  The constant ( .const ) state space is a read-only memory initialized by the host. Constant memory\nis accessed with a ld.const instruction. Constant memory is restricted in size, currently\nlimited to 64 KB which can be used to hold statically-sized constant variables. There is an\nadditional 640 KB of constant memory, organized as ten independent 64 KB regions. The driver may\nallocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel\nfunction parameters. Since the ten regions are not contiguous, the driver must ensure that constant\nbuffers are allocated so that each buffer fits entirely within a 64 KB region and does not span a\nregion boundary. Statically-sized constant variables have an optional variable initializer; constant variables with\nno explicit initializer are initialized to zero by default. Constant buffers allocated by the driver\nare initialized by the host, and pointers to such buffers are passed to the kernel as\nparameters. See the description of kernel parameter attributes in Kernel Function Parameter\nAttributes for more details on passing pointers\nto constant buffers as kernel parameters. 5.1.3.1. Banked Constant State Space (deprecated)  Previous versions of PTX exposed constant memory as a set of eleven 64 KB banks, with explicit bank\nnumbers required for variable declaration and during access. Prior to PTX ISA version 2.2, the constant memory was organized into fixed size banks. There were\neleven 64 KB banks, and banks were specified using the .const[bank] modifier, where bank ranged from 0 to 10. If no bank number was given, bank zero was assumed. By convention, bank zero was used for all statically-sized constant variables. The remaining banks\nwere used to declare incomplete constant arrays (as in C, for example), where the size is not\nknown at compile time. For example, the declaration .extern .const[2] .b32 const_buffer[]; resulted in const_buffer pointing to the start of constant bank two. This pointer could then be\nused to access the entire 64 KB constant bank. Multiple incomplete array variables declared in the\nsame bank were aliased, with each pointing to the start address of the specified constant bank. To access data in contant banks 1 through 10, the bank number was required in the state space of the\nload instruction. For example, an incomplete array in bank 2 was accessed as follows: .extern .const[2] .b32 const_buffer[];\nld.const[2].b32  %r1, [const_buffer+4]; // load second word In PTX ISA version 2.2, we eliminated explicit banks and replaced the incomplete array\nrepresentation of driver-allocated constant buffers with kernel parameter attributes that allow\npointers to constant buffers to be passed as kernel parameters. 5.1.4. Global State Space  The global ( .global ) state space is memory that is accessible by all threads in a context. It is\nthe mechanism by which threads in different CTAs, clusters, and grids can communicate. Use ld.global , st.global , and atom.global to access global variables. Global variables have an optional variable initializer; global variables with no explicit\ninitializer are initialized to zero by default. 5.1.5. Local State Space  The local state space ( .local ) is private memory for each thread to keep its own data. It is\ntypically standard memory with cache. The size is limited, as it must be allocated on a per-thread\nbasis. Use ld.local and st.local to access local variables. When compiling to use the Application Binary Interface (ABI) , .local state-space variables\nmust be declared within function scope and are allocated on the stack. In implementations that do\nnot support a stack, all local memory variables are stored at fixed addresses, recursive function\ncalls are not supported, and .local variables may be declared at module scope. When compiling\nlegacy PTX code (ISA versions prior to 3.0) containing module-scoped .local variables, the\ncompiler silently disables use of the ABI. 5.1.6. Parameter State Space  The parameter ( .param ) state space is used (1) to pass input arguments from the host to the\nkernel, (2a) to declare formal input and return parameters for device functions called from within\nkernel execution, and (2b) to declare locally-scoped byte array variables that serve as function\ncall arguments, typically for passing large structures by value to a function. Kernel function\nparameters differ from device function parameters in terms of access and sharing (read-only versus\nread-write, per-kernel versus per-thread). Note that PTX ISA versions 1.x supports only kernel\nfunction parameters in .param space; device function parameters were previously restricted to the\nregister state space. The use of parameter state space for device function parameters was introduced\nin PTX ISA version 2.0 and requires target architecture sm_20 or higher. Additional sub-qualifiers ::entry or ::func can be specified on instructions with .param state space to indicate\nwhether the address refers to kernel function parameter or device function parameter. If no\nsub-qualifier is specified with the .param state space, then the default sub-qualifier is specific\nto and dependent on the exact instruction. For example, st.param is equivalent to st.param::func whereas isspacep.param is equivalent to isspacep.param::entry . Refer to the instruction\ndescription for more details on default sub-qualifier assumption. Note The location of parameter space is implementation specific. For example, in some implementations\nkernel parameters reside in global memory. No access protection is provided between parameter and\nglobal space in this case. Though the exact location of the kernel parameter space is\nimplementation specific, the kernel parameter space window is always contained within the global\nspace window. Similarly, function parameters are mapped to parameter passing registers and/or\nstack locations based on the function calling conventions of the Application Binary Interface\n(ABI) . Therefore, PTX code should make no assumptions about the relative locations or ordering\nof .param space variables. 5.1.6.1. Kernel Function Parameters  Each kernel function definition includes an optional list of parameters. These parameters are\naddressable, read-only variables declared in the .param state space. Values passed from the host\nto the kernel are accessed through these parameter variables using ld.param instructions. The\nkernel parameter variables are shared across all CTAs from all clusters within a grid. The address of a kernel parameter may be moved into a register using the mov instruction. The\nresulting address is in the .param state space and is accessed using ld.param instructions. Example .entry foo ( .param .b32 N, .param .align 8 .b8 buffer[64] )\n{\n    .reg .u32 %n;\n    .reg .f64 %d;\n\n    ld.param.u32 %n, [N];\n    ld.param.f64 %d, [buffer];\n    ... Example .entry bar ( .param .b32 len )\n{\n    .reg .u32 %ptr, %n;\n\n    mov.u32      %ptr, len;\n    ld.param.u32 %n, [%ptr];\n    ... Kernel function parameters may represent normal data values, or they may hold addresses to objects\nin constant, global, local, or shared state spaces. In the case of pointers, the compiler and\nruntime system need information about which parameters are pointers, and to which state space they\npoint. Kernel parameter attribute directives are used to provide this information at the PTX\nlevel. See Kernel Function Parameter Attributes for a description of kernel parameter attribute\ndirectives. Note The current implementation does not allow creation of generic pointers to constant variables\n( cvta.const ) in programs that have pointers to constant buffers passed as kernel parameters. 5.1.6.2. Kernel Function Parameter Attributes  Kernel function parameters may be declared with an optional .ptr attribute to indicate that a\nparameter is a pointer to memory, and also indicate the state space and alignment of the memory\nbeing pointed to. Kernel Parameter Attribute: .ptr describes the .ptr kernel parameter attribute. 5.1.6.3. Kernel Parameter Attribute: .ptr  .ptr Kernel parameter alignment attribute. Syntax .param .type .ptr .space .align N  varname\n.param .type .ptr        .align N  varname\n\n.space = { .const, .global, .local, .shared }; Description Used to specify the state space and, optionally, the alignment of memory pointed to by a pointer\ntype kernel parameter. The alignment value N , if present, must be a power of two. If no state\nspace is specified, the pointer is assumed to be a generic address pointing to one of const, global,\nlocal, or shared memory. If no alignment is specified, the memory pointed to is assumed to be\naligned to a 4 byte boundary. Spaces between .ptr , .space , and .align may be eliminated to improve readability. PTX ISA Notes Introduced in PTX ISA version 2.2. Support for generic addressing of .const space added in PTX ISA version 3.1. Target ISA Notes Supported on all target architectures. Examples .entry foo ( .param .u32 param1,\n             .param .u32 .ptr.global.align 16 param2,\n             .param .u32 .ptr.const.align 8 param3,\n             .param .u32 .ptr.align 16 param4  // generic address\n                                               // pointer\n) { .. } 5.1.6.4. Device Function Parameters  PTX ISA version 2.0 extended the use of parameter space to device function parameters. The most\ncommon use is for passing objects by value that do not fit within a PTX register, such as C\nstructures larger than 8 bytes. In this case, a byte array in parameter space is used. Typically,\nthe caller will declare a locally-scoped .param byte array variable that represents a flattened\nC structure or union. This will be passed by value to a callee, which declares a .param formal\nparameter having the same size and alignment as the passed argument. Example // pass object of type struct { double d; int y; };\n.func foo ( .reg .b32 N, .param .align 8 .b8 buffer[12] )\n{\n    .reg .f64 %d;\n    .reg .s32 %y;\n\n    ld.param.f64 %d, [buffer];\n    ld.param.s32 %y, [buffer+8];\n    ...\n}\n\n// code snippet from the caller\n// struct { double d; int y; } mystruct; is flattened, passed to foo\n    ...\n    .reg .f64 dbl;\n    .reg .s32 x;\n    .param .align 8 .b8 mystruct;\n    ...\n    st.param.f64 [mystruct+0], dbl;\n    st.param.s32 [mystruct+8], x;\n    call foo, (4, mystruct);\n    ... See the section on function call syntax for more details. Function input parameters may be read via ld.param and function return parameters may be written\nusing st.param ; it is illegal to write to an input parameter or read from a return parameter. Aside from passing structures by value, .param space is also required whenever a formal\nparameter has its address taken within the called function. In PTX, the address of a function input\nparameter may be moved into a register using the mov instruction. Note that the parameter will\nbe copied to the stack if necessary, and so the address will be in the .local state space and is\naccessed via ld.local and st.local instructions. It is not possible to use mov to get\nthe address of or a locally-scoped .param space variable. Starting PTX ISA version 6.0, it is\npossible to use mov instruction to get address of return parameter of device function. Example // pass array of up to eight floating-point values in buffer\n.func foo ( .param .b32 N, .param .b32 buffer[32] )\n{\n    .reg .u32  %n, %r;\n    .reg .f32  %f;\n    .reg .pred %p;\n\n    ld.param.u32 %n, [N];\n    mov.u32      %r, buffer;  // forces buffer to .local state space\nLoop:\n    setp.eq.u32  %p, %n, 0;\n@%p: bra         Done;\n    ld.local.f32 %f, [%r];\n    ...\n    add.u32      %r, %r, 4;\n    sub.u32      %n, %n, 1;\n    bra          Loop;\nDone:\n    ...\n} 5.1.7. Shared State Space  The shared ( .shared ) state space is a memory that is owned by an executing CTA and is accessible\nto the threads of all the CTAs within a cluster. An address in shared memory can be read and written\nby any thread in a CTA cluster. Additional sub-qualifiers ::cta or ::cluster can be specified on instructions with .shared state space to indicate whether the address belongs to the shared memory window of the\nexecuting CTA or of any CTA in the cluster respectively. The addresses in the .shared::cta window also fall within the .shared::cluster window. If no sub-qualifier is specified with the .shared state space, then it defaults to ::cta . For example, ld.shared is equivalent to ld.shared::cta . Variables declared in .shared state space refer to the memory addresses in the current\nCTA. Instruction mapa gives the .shared::cluster address of the corresponding variable in\nanother CTA in the cluster. Shared memory typically has some optimizations to support the sharing. One example is broadcast;\nwhere all threads read from the same address. Another is sequential access from sequential threads. 5.1.8. Texture State Space (deprecated)  The texture ( .tex ) state space is global memory accessed via the texture instruction. It is\nshared by all threads in a context. Texture memory is read-only and cached, so accesses to texture\nmemory are not coherent with global memory stores to the texture image. The GPU hardware has a fixed number of texture bindings that can be accessed within a single kernel\n(typically 128). The .tex directive will bind the named texture memory variable to a hardware\ntexture identifier, where texture identifiers are allocated sequentially beginning with\nzero. Multiple names may be bound to the same physical texture identifier. An error is generated if\nthe maximum number of physical resources is exceeded. The texture name must be of type .u32 or .u64 . Physical texture resources are allocated on a per-kernel granularity, and .tex variables are\nrequired to be defined in the global scope. Texture memory is read-only. A texture’s base address is assumed to be aligned to a 16 byte\nboundary. Example .tex .u32 tex_a;         // bound to physical texture 0\n.tex .u32 tex_c, tex_d;  // both bound to physical texture 1\n.tex .u32 tex_d;         // bound to physical texture 2\n.tex .u32 tex_f;         // bound to physical texture 3 Note Explicit declarations of variables in the texture state space is deprecated, and programs should\ninstead reference texture memory through variables of type .texref . The .tex directive is\nretained for backward compatibility, and variables declared in the .tex state space are\nequivalent to module-scoped .texref variables in the .global state space. For example, a legacy PTX definitions such as .tex .u32 tex_a; is equivalent to: .global .texref tex_a; See Texture Sampler and Surface Types for the\ndescription of the .texref type and Texture Instructions for its use in texture instructions. 5.2. Types  5.2.1. Fundamental Types  In PTX, the fundamental types reflect the native data types supported by the target architectures. A\nfundamental type specifies both a basic type and a size. Register variables are always of a\nfundamental type, and instructions operate on these types. The same type-size specifiers are used\nfor both variable definitions and for typing instructions, so their names are intentionally short. Table 8 lists the fundamental type specifiers for\neach basic type: Table 8 Fundamental Type Specifiers  Basic Type Fundamental Type Specifiers Signed integer .s8 , .s16 , .s32 , .s64 Unsigned integer .u8 , .u16 , .u32 , .u64 Floating-point .f16 , .f16x2 , .f32 , .f64 Bits (untyped) .b8 , .b16 , .b32 , .b64 , .b128 Predicate .pred Most instructions have one or more type specifiers, needed to fully specify instruction\nbehavior. Operand types and sizes are checked against instruction types for compatibility. Two fundamental types are compatible if they have the same basic type and are the same size. Signed\nand unsigned integer types are compatible if they have the same size. The bit-size type is\ncompatible with any fundamental type having the same size. In principle, all variables (aside from predicates) could be declared using only bit-size types, but\ntyped variables enhance program readability and allow for better operand type checking. 5.2.2. Restricted Use of Sub-Word Sizes  The .u8 , .s8 , and .b8 instruction types are restricted to ld , st , and cvt instructions. The .f16 floating-point type is allowed only in conversions to and from .f32 , .f64 types, in half precision floating point instructions and texture fetch instructions. The .f16x2 floating point type is allowed only in half precision floating point arithmetic\ninstructions and texture fetch instructions. For convenience, ld , st , and cvt instructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. 5.2.3. Alternate Floating-Point Data Formats  The fundamental floating-point types supported in PTX have implicit bit representations that\nindicate the number of bits used to store exponent and mantissa. For example, the .f16 type\nindicates 5 bits reserved for exponent and 10 bits reserved for mantissa. In addition to the\nfloating-point representations assumed by the fundamental types, PTX allows the following alternate\nfloating-point data formats: bf16 data format: This data format is a 16-bit floating point format with 8 bits for exponent and 7 bits for\nmantissa. A register variable containing bf16 data must be declared with .b16 type. e4m3 data format: This data format is an 8-bit floating point format with 4 bits for exponent and 3 bits for\nmantissa. The e4m3 encoding does not support infinity and NaN values are limited to 0x7f and 0xff . A register variable containing e4m3 value must be declared using\nbit-size type. e5m2 data format: This data format is an 8-bit floating point format with 5 bits for exponent and 2 bits for\nmantissa. A register variable containing e5m2 value must be declared using bit-size type. tf32 data format: This data format is a special 32-bit floating point format supported by the matrix\nmultiply-and-accumulate instructions, with the same range as .f32 and reduced precision (>=10\nbits). The internal layout of tf32 format is implementation defined. PTX facilitates\nconversion from single precision .f32 type to tf32 format. A register variable containing tf32 data must be declared with .b32 type. Alternate data formats cannot be used as fundamental types. They are supported as source or\ndestination formats by certain instructions. 5.2.4. Packed Data Types  Certain PTX instructions operate on two sets of inputs in parallel, and produce two outputs. Such\ninstructions can use the data stored in a packed format. PTX supports packing two values of the same\nscalar data type into a single, larger value. The packed value is considered as a value of a packed\ndata type . In this section we describe the packed data types supported in PTX. 5.2.4.1. Packed Floating Point Data Types  PTX supports the following four variants of packed floating point data types: .f16x2 packed type containing two .f16 floating point values. .bf16x2 packed type containing two .bf16 alternate floating point values. .e4m3x2 packed type containing two .e4m3 alternate floating point values. .e5m2x2 packed type containing two .e5m2 alternate floating point values. .f16x2 is supported as a fundamental type. .bf16x2 , .e4m3x2 and .e5m2x2 cannot be\nused as fundamental types - they are supported as instruction types on certain instructions. A\nregister variable containing .bf16x2 data must be declared with .b32 type. A register\nvariable containing .e4m3x2 or .e5m2x2 data must be declared with .b16 type. 5.2.4.2. Packed Integer Data Types  PTX supports two variants of packed integer data types: .u16x2 and .s16x2 . The packed data\ntype consists of two .u16 or .s16 values. A register variable containing .u16x2 or .s16x2 data must be declared with .b32 type. Packed integer data types cannot be used as\nfundamental types. They are supported as instruction types on certain instructions. 5.3. Texture Sampler and Surface Types  PTX includes built-in opaque types for defining texture, sampler, and surface descriptor\nvariables. These types have named fields similar to structures, but all information about layout,\nfield ordering, base address, and overall size is hidden to a PTX program, hence the term opaque . The use of these opaque types is limited to: Variable definition within global (module) scope and in kernel entry parameter lists. Static initialization of module-scope variables using comma-delimited static assignment\nexpressions for the named members of the type. Referencing textures, samplers, or surfaces via texture and surface load/store instructions\n( tex , suld , sust , sured ). Retrieving the value of a named member via query instructions ( txq , suq ). Creating pointers to opaque variables using mov , e.g., mov.u64 reg, opaque_var; . The\nresulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and\nde-referenced by texture and surface load, store, and query instructions, but the pointer cannot\notherwise be treated as an address, i.e., accessing the pointer with ld and st instructions, or performing pointer arithmetic will result in undefined results. Opaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque\nvariable. Note Indirect access to textures and surfaces using pointers to opaque variables is supported\nbeginning with PTX ISA version 3.1 and requires target sm_20 or later. Indirect access to textures is supported only in unified texture mode (see below). The three built-in types are .texref , .samplerref , and .surfref . For working with\ntextures and samplers, PTX has two modes of operation. In the unified mode, texture and sampler\ninformation is accessed through a single .texref handle. In the independent mode , texture and\nsampler information each have their own handle, allowing them to be defined separately and combined\nat the site of usage in the program. In independent mode, the fields of the .texref type that\ndescribe sampler properties are ignored, since these properties are defined by .samplerref variables. Table 9 and Table 10 list the named members\nof each type for unified and independent texture modes. These members and their values have\nprecise mappings to methods and values defined in the texture HW class as well as\nexposed values via the API. Table 9 Opaque Type Fields in Unified Texture Mode  Member .texref values .surfref values width in elements height in elements depth in elements channel_data_type enum type corresponding to source language API channel_order enum type corresponding to source language API normalized_coords 0 , 1 N/A filter_mode nearest , linear N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A array_size as number of textures in a texture\narray as number of surfaces in a surface array num_mipmap_levels as number of levels in a mipmapped\ntexture N/A num_samples as number of samples in a multi-sample\ntexture N/A memory_layout N/A 1 for linear memory layout; 0 otherwise 5.3.1. Texture and Surface Properties  Fields width , height , and depth specify the size of the texture or surface in number of\nelements in each dimension. The channel_data_type and channel_order fields specify these properties of the texture or\nsurface using enumeration types corresponding to the source language API. For example, see Channel\nData Type and Channel Order Fields for\nthe OpenCL enumeration types currently supported in PTX. 5.3.2. Sampler Properties  The normalized_coords field indicates whether the texture or surface uses normalized coordinates\nin the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is\nspecified, the default is set by the runtime system based on the source language. The filter_mode field specifies how the values returned by texture reads are computed based on\nthe input texture coordinates. The addr_mode_{0,1,2} fields define the addressing mode in each dimension, which determine how\nout-of-range coordinates are handled. See the CUDA C++ Programming Guide for more details of these properties. Table 10 Opaque Type Fields in Independent Texture Mode  Member .samplerref values .texref values .surfref values width N/A in elements height N/A in elements depth N/A in elements channel_data_type N/A enum type corresponding to source\nlanguage API channel_order N/A enum type corresponding to source\nlanguage AP normalized_coords N/A 0 , 1 N/A force_unnormalized_coords 0 , 1 N/A N/A filter_mode nearest , linear ignored N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A N/A array_size N/A as number of textures\nin a texture array as number of surfaces in\na surface array num_mipmap_levels N/A as number of levels\nin a mipmapped\ntexture N/A num_samples N/A as number of samples\nin a multi-sample\ntexture N/A memory_layout N/A N/A 1 for linear memory\nlayout; 0 otherwise In independent texture mode, the sampler properties are carried in an independent .samplerref variable, and these fields are disabled in the .texref variables. One additional sampler\nproperty, force_unnormalized_coords , is available in independent texture mode. The force_unnormalized_coords field is a property of .samplerref variables that allows the\nsampler to override the texture header normalized_coords property. This field is defined only in\nindependent texture mode. When True , the texture header setting is overridden and unnormalized\ncoordinates are used; when False , the texture header setting is used. The force_unnormalized_coords property is used in compiling OpenCL; in OpenCL, the property of\nnormalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are\nalways initialized with normalized_coords set to True, and the OpenCL sampler-based normalized_coords flag maps (negated) to the PTX-level force_unnormalized_coords flag. Variables using these types may be declared at module scope or within kernel entry parameter\nlists. At module scope, these variables must be in the .global state space. As kernel\nparameters, these variables are declared in the .param state space. Example .global .texref     my_texture_name;\n.global .samplerref my_sampler_name;\n.global .surfref    my_surface_name; When declared at module scope, the types may be initialized using a list of static expressions\nassigning values to the named members. Example .global .texref tex1;\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             }; 5.3.3. Channel Data Type and Channel Order Fields  The channel_data_type and channel_order fields have enumeration types corresponding to the\nsource language API. Currently, OpenCL is the only source language that defines these\nfields. Table 12 and Table 11 show the\nenumeration values defined in OpenCL version 1.0 for channel data type and channel order. Table 11 OpenCL 1.0 Channel Data Type Definition  CL_SNORM_INT8 0x10D0 CL_SNORM_INT16 0x10D1 CL_UNORM_INT8 0x10D2 CL_UNORM_INT16 0x10D3 CL_UNORM_SHORT_565 0x10D4 CL_UNORM_SHORT_555 0x10D5 CL_UNORM_INT_101010 0x10D6 CL_SIGNED_INT8 0x10D7 CL_SIGNED_INT16 0x10D8 CL_SIGNED_INT32 0x10D9 CL_UNSIGNED_INT8 0x10DA CL_UNSIGNED_INT16 0x10DB CL_UNSIGNED_INT32 0x10DC CL_HALF_FLOAT 0x10DD CL_FLOAT 0x10DE Table 12 OpenCL 1.0 Channel Order Definition  CL_R 0x10B0 CL_A 0x10B1 CL_RG 0x10B2 CL_RA 0x10B3 CL_RGB 0x10B4 CL_RGBA 0x10B5 CL_BGRA 0x10B6 CL_ARGB 0x10B7 CL_INTENSITY 0x10B8 CL_LUMINANCE 0x10B9 5.4. Variables  In PTX, a variable declaration describes both the variable’s type and its state space. In addition\nto fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays. 5.4.1. Variable Declarations  All storage for data is specified with variable declarations. Every variable must reside in one of\nthe state spaces enumerated in the previous section. A variable declaration names the space in which the variable resides, its type and size, its name,\nan optional array size, an optional initializer, and an optional fixed address for the variable. Predicate variables may only be declared in the register state space. Examples .global .u32 loc;\n.reg    .s32 i;\n.const  .f32 bias[] = {-1.0, 1.0};\n.global .u8  bg[4] = {0, 0, 0, 0};\n.reg    .v4 .f32 accel;\n.reg    .pred p, q, r; 5.4.2. Vectors  Limited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate\nfundamental type can be declared by prefixing the type with .v2 or .v4 . Vectors must be\nbased on a fundamental type, and they may reside in the register space. Vectors cannot exceed\n128-bits in length; for example, .v4 .f64 is not allowed. Three-element vectors may be\nhandled by using a .v4 vector, where the fourth element provides padding. This is a common case\nfor three-dimensional grids, textures, etc. Examples .global .v4 .f32 V;   // a length-4 vector of floats\n.shared .v2 .u16 uv;  // a length-2 vector of unsigned ints\n.global .v4 .b8  v;   // a length-4 vector of bytes By default, vector variables are aligned to a multiple of their overall size (vector length times\nbase-type size), to enable vector load and store instructions which require addresses aligned to a\nmultiple of the access size. 5.4.3. Array Declarations  Array declarations are provided to allow the programmer to reserve space. To declare an array, the\nvariable name is followed with dimensional declarations similar to fixed-size array declarations\nin C. The size of each dimension is a constant expression. Examples .local  .u16 kernel[19][19];\n.shared .u8  mailbox[128]; The size of the array specifies how many elements should be reserved. For the declaration of array kernel above, 19*19 = 361 halfwords are reserved, for a total of 722 bytes. When declared with an initializer, the first dimension of the array may be omitted. The size of the\nfirst array dimension is determined by the number of elements in the array initializer. Examples .global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n.global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} }; Array index has eight elements, and array offset is a 4x2 array. 5.4.4. Initializers  Declared variables may specify an initial value using a syntax similar to C/C++, where the variable\nname is followed by an equals sign and the initial value or values for the variable. A scalar takes\na single value, while vectors and arrays take nested lists of values inside of curly braces (the\nnesting matches the dimensionality of the declaration). As in C, array initializers may be incomplete, i.e., the number of initializer elements may be less\nthan the extent of the corresponding array dimension, with remaining array locations initialized to\nthe default value for the specified array type. Examples .const  .f32 vals[8] = { 0.33, 0.25, 0.125 };\n.global .s32 x[3][2] = { {1,2}, {3} }; is equivalent to .const  .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 };\n.global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} }; Currently, variable initialization is supported only for constant and global state spaces. Variables\nin constant and global state spaces with no explicit initializer are initialized to zero by\ndefault. Initializers are not allowed in external variable declarations. Variable names appearing in initializers represent the address of the variable; this can be used to\nstatically initialize a pointer to a variable. Initializers may also contain var+offset expressions, where offset is a byte offset added to the address of var . Only variables in .global or .const state spaces may be used in initializers. By default, the resulting\naddress is the offset in the variable’s state space (as is the case when taking the address of a\nvariable with a mov instruction). An operator, generic() , is provided to create a generic\naddress for variables used in initializers. Starting PTX ISA version 7.1, an operator mask() is provided, where mask is an integer\nimmediate. The only allowed expressions in the mask() operator are integer constant expression\nand symbol expression representing address of variable. The mask() operator extracts n consecutive bits from the expression used in initializers and inserts these bits at the lowest\nposition of the initialized variable. The number n and the starting position of the bits to be\nextracted is specified by the integer immediate mask . PTX ISA version 7.1 only supports\nextracting a single byte starting at byte boundary from the address of the variable. PTX ISA version\n7.3 supports Integer constant expression as an operand in the mask() operator. Supported values for mask are: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000,\n0xFF000000000000, 0xFF00000000000000. Examples .const  .u32 foo = 42;\n.global .u32 bar[] = { 2, 3, 5 };\n.global .u32 p1 = foo;          // offset of foo in .const space\n.global .u32 p2 = generic(foo); // generic address of foo\n\n// array of generic-address pointers to elements of bar\n.global .u32 parr[] = { generic(bar), generic(bar)+4,\ngeneric(bar)+8 };\n\n// examples using mask() operator are pruned for brevity\n.global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...};\n\n.global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...}\n\n.global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...}\n\n.global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...}\n\n// mask() operator with integer const expression\n.global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...}; Note PTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer. PTX 3.1 code should\neither include explicit generic() operators in initializers, use cvta.global to form\ngeneric addresses at runtime, or load from the non-generic address using ld.global . Device function names appearing in initializers represent the address of the first instruction in\nthe function; this can be used to initialize a table of function pointers to be used with indirect\ncalls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to\ninitialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch\nkernels from GPU. See the CUDA Dynamic Parallelism Programming Guide for details. Labels cannot be used in initializers. Variables that hold addresses of variables or functions should be of type .u8 or .u32 or .u64 . Type .u8 is allowed only if the mask() operator is used. Initializers are allowed for all types except .f16 , .f16x2 and .pred . Examples .global .s32 n = 10;\n.global .f32 blur_kernel[][3]\n               = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}};\n\n.global .u32 foo[] = { 2, 3, 5, 7, 9, 11 };\n.global .u64 ptr = generic(foo);   // generic address of foo[0]\n.global .u64 ptr = generic(foo)+8; // generic address of foo[2] 5.4.5. Alignment  Byte alignment of storage for all addressable variables can be specified in the variable\ndeclaration. Alignment is specified using an optional .align byte-count specifier immediately\nfollowing the state-space specifier. The variable will be aligned to an address which is an integer\nmultiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment\nspecifies the address alignment for the starting address of the entire array, not for individual\nelements. The default alignment for scalar and array variables is to a multiple of the base-type size. The\ndefault alignment for vector variables is to a multiple of the overall vector size. Examples // allocate array at 4-byte aligned address.  Elements are bytes.\n.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0}; Note that all PTX instructions that access memory require that the address be aligned to a multiple\nof the access size. The access size of a memory instruction is the total number of bytes accessed in\nmemory. For example, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4 bytes. 5.4.6. Parameterized Variable Names  Since PTX supports virtual registers, it is quite common for a compiler frontend to generate a large\nnumber of register names. Rather than require explicit declaration of every name, PTX supports a\nsyntax for creating a set of variables having a common prefix string appended with integer suffixes. For example, suppose a program uses a large number, say one hundred, of .b32 variables, named %r0 , %r1 , …, %r99 . These 100 register variables can be declared as follows: .reg .b32 %r<100>;    // declare %r0, %r1, ..., %r99 This shorthand syntax may be used with any of the fundamental types and with any state space, and\nmay be preceded by an alignment specifier. Array variables cannot be declared this way, nor are\ninitializers permitted. 5.4.7. Variable Attributes  Variables may be declared with an optional .attribute directive which allows specifying special\nattributes of variables. Keyword .attribute is followed by attribute specification inside\nparenthesis. Multiple attributes are separated by comma. Variable and Function Attribute Directive: .attribute describes the .attribute directive. 5.4.8. Variable and Function Attribute Directive: .attribute  .attribute Variable and function attributes Description Used to specify special attributes of a variable or a function. The following attributes are supported. .managed .managed attribute specifies that variable will be allocated at a location in unified virtual\nmemory environment where host and other devices in the system can reference the variable\ndirectly. This attribute can only be used with variables in .global state space. See the CUDA\nUVM-Lite Programming Guide for details. .unified .unified attribute specifies that function has the same memory address on the host and on\nother devices in the system. Integer constants uuid1 and uuid2 respectively specify upper\nand lower 64 bits of the unique identifier associated with the function or the variable. This\nattribute can only be used on device functions or on variables in the .global state\nspace. Variables with .unified attribute are read-only and must be loaded by specifying .unified qualifier on the address operand of ld instruction, otherwise the behavior is\nundefined. PTX ISA Notes Introduced in PTX ISA version 4.0. Support for function attributes introduced in PTX ISA version 8.0. Target ISA Notes .managed attribute requires sm_30 or higher. .unified attribute requires sm_90 or higher. Examples .global .attribute(.managed) .s32 g;\n.global .attribute(.managed) .u64 x;\n\n.global .attribute(.unified(19,95)) .f32 f;\n\n.func .attribute(.unified(0xAB, 0xCD)) bar() { ... } 5.5. Tensors  A tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following\nproperties: Dimensionality Dimension sizes across each dimension Individual element types Tensor stride across each dimension PTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: Copying data between global and shared memories Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma , mma and wgmma.mma_async instructions. PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure\nand treat the data in the shared memory as a linear data. 5.5.1. Tensor Dimension, size and format  Tensors can have dimensions: 1D, 2D, 3D, 4D or 5D. Each dimension has a size which represents the number of elements along the dimension. The elements\ncan have one the following types: Bit-sized type: .b32 , .b64 Integer: .u8 , .u16 , .u32 , .s32 , .u64 , .s64 Floating point and alternate floating point: .f16 , .bf16 , .tf32 , .f32 , .f64 (rounded to nearest even). Tensor can have padding at the end in each of the dimensions to provide alignment for the data in\nthe subsequent dimensions. Tensor stride can be used to specify the amount of padding in each\ndimension. 5.5.2. Tensor Access Modes  Tensor data can be accessed in two modes: Tiled mode: In tiled mode, the source multi-dimensional tensor layout is preserved at the destination. Im2col mode: In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns\nat the destination. Refer here for more details. 5.5.3. Tiled Mode  This section talks about how Tensor and Tensor access work in tiled mode. 5.5.3.1. Bounding Box  A tensor can be accessed in chunks known as Bounding Box . The Bounding Box has the same\ndimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple\nof 16 bytes. The address of the bounding Box must also be aligned to 16 bytes. Bounding Box has the following access properties: Bounding Box dimension sizes Out of boundary access mode Traversal strides The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the\nbounding box. Starting offset of the bounding box along with the rest of the bounding box\ninformation together are used to determine the elements which are to be accessed. 5.5.3.2. Traversal-Stride  While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies\nthe exact number of elements to be skipped. If no jump over is required, default value of 1 must be\nspecified. The traversal stride in dimension 0 can be used for the Interleave layout . For non-interleaved layout, the traversal stride in\ndimension 0 must always be 1. Figure 5 illustrates tensor, tensor size, tensor stride,\nBounding Box size and traversal stride. Figure 5 Tiled mode bounding box, tensor size and traversal stride  5.5.3.3. Out of Boundary Access  PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor\nboundary in any dimension. There are 2 modes: Zero fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to 0. OOB-NaN fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN\ncalled OOB-NaN . Figure 6 shows an example of the out of boundary access. Figure 6 Out of boundary access  5.5.4. Im2col mode  Im2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data\nis treated as a batch of images with the following properties: N : number of images in the batch D, H, W : size of a 3D image (depth, height and width) C: channels per image element The above properties are associated with 3D, 4D and 5D tensors as follows: Dimension N/D/H/W/C applicability 3D NWC 4D NHWC 5D NDHWC 5.5.4.1. Bounding Box  In im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are\nspecified by Pixels-per-Column and Channels-per-Pixel parameters as described below. The dimensionality of the Bounding Box is two less than the tensor dimensionality. The following properties describe how to access of the elements in im2col mode: Bounding-Box Lower-Corner Bounding-Box Upper-Corner Pixels-per-Column Channels-per-Pixel Bounding-box Lower-Corner and Bounding-box Upper-Corner specify the two opposite corners of the\nBounding Box in the DHW space. Bounding-box Lower-Corner specifies the corner with the smallest\ncoordinate and Bounding-box Upper-Corner specifies the corner with the largest coordinate. Bounding-box Upper- and Lower-Corners are 16-bit signed values whose limits varies across the\ndimensions and are as shown below: 3D 4D 5D Upper- / Lower- Corner sizes [-2 15 , 2 15 -1] [-2 7 , 2 7 -1] [-2 4 , 2 4 -1] Figure 7 and Figure 8 show the Upper-Corners and Lower-Corners. Figure 7 im2col mode bounding box example 1  Figure 8 im2col mode bounding box example 2  The Bounding-box Upper- and Lower- Corners specify only the boundaries and not the number of\nelements to be accessed. Pixels-per-Column specifies the number of elements to be accessed in the\nNDHW space. Channels-per-Pixel specifies the number of elements to access across the C dimension. The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different\ndimensions: Across N and C dimensions: specify the starting offsets along the dimension, similar to the tiled\nmode. Across DHW dimensions: specify the location of the convolution filter base in the tensor\nspace. The filter corner location must be within the bounding box. The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter\nbase coordinates to determine the starting location in the tensor space from where the elements are\naccessed. The size of the im2col offsets varies across the dimensions and their valid ranges are as shown\nbelow: 3D 4D 5D im2col offsets range [0, 2 16 -1] [0, 2 8 -1] [0, 2 5 -1] Following are some examples of the im2col mode accesses: Example 1 ( Figure 9 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1. tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 0 , 0 ) Figure 9 im2col mode example 1  Example 2 ( Figure 10 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = 0 Bounding - Box Lower - Corner H = 0 Bounding - Box Upper - Corner W = -2 Bounding - Box Upper - Corner H = -2 tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 2 , 2 ) Figure 10 im2col mode example 2  5.5.4.2. Traversal Stride  The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being\naccessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being\naccessed, in im2col mode. The number of elements traversed along the D, H and W dimensions is strided by the traversal stride\nfor that dimension. The following example with Figure 11 illustrates accesse with traversal-strides: Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 8 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Traversal Stride = 2 Pixels - per - Column = 32 channels - per - pixel = 16 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1. Tensor coordinates in the instruction = ( 7 , 7 , 5 , 0 ) Im2col offsets in the instruction : ( 1 , 1 ) Figure 11 im2col mode traversal stride example  5.5.4.3. Out of Boundary Access  In im2col mode, when the number of requested pixels in NDHW space specified by Pixels-per-Column exceeds the number of available pixels in the image batch then out-of-bounds access is performed. Similar to tiled mode, zero fill or OOB-NaN fill can be performed based on the Fill-Mode\nspecified. 5.5.5. Interleave layout  Tensor can be interleaved and the following interleave layouts are supported: No interleave (NDHWC) 8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel. 16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel. The C information is organized in slices where sequential C elements are grouped in 16 byte or 32\nbyte quantities. If the total number of channels is not a multiple of the number of channels per slice, then the last\nslice must be padded with zeros to make it complete 16B or 32B slice. Interleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D. 5.5.6. Swizzling Modes  The layout of the data in the shared memory can be different to that of global memory, for access\nperformance reasons. The following describes various swizzling modes: No swizzle mode: There is no swizzling in this mode and the destination data layout is exactly similar to the\nsource data layout. 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 … Pattern repeats … 32 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is\n256 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 … Pattern repeats … An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,\nwith the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in Figure 12 . Figure 12 32-byte swizzle mode example  Figure 13 shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1. Figure 13 32-byte swizzle mode fragments  Figure 14 shows the destination data layout with 32 byte swizzling. Figure 14 32-byte swizzle mode destination data layout  64 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is\n512 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 … Pattern repeats … An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 32 channels, is shown in Figure 15 . Figure 15 64-byte swizzle mode example  Each colored cell represents 8 channels. Figure 16 shows the source data layout. Figure 16 64-byte swizzle mode source data layout  Figure 17 shows the destination data layout with 64 byte swizzling. Figure 17 64-byte swizzle mode destination data layout  128 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is\n1024 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 4 5 6 7 0 1 2 3 5 4 7 6 1 0 3 2 6 7 4 5 2 3 0 1 … Pattern repeats … An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 64 channels, is shown in Figure 18 . Figure 18 128-byte swizzle mode example  Each colored cell represents 8 channels. Figure 19 shows the source data layout. Figure 19 128-byte swizzle mode source data layout  Figure 20 shows the destination data layout with 128 byte swizzling. Figure 20 128-byte swizzle mode destination data layout  5.5.7. Tensor-map  The tensor-map is a 128-byte opaque object either in .const space or .param (kernel function\nparameter) space or .global space which describes the tensor properties and the access properties\nof the tensor data described in previous sections. Tensor-Map can be created using CUDA APIs. Refer to CUDA programming guide for more details. 6. Instruction Operands  6.1. Operand Type Information  All operands in instructions have a known type from their declarations. Each operand type must be\ncompatible with the type determined by the instruction template and instruction type. There is no\nautomatic conversion between types. The bit-size type is compatible with every type having the same size. Integer types of a common size\nare compatible with each other. Operands having type different from but compatible with the\ninstruction type are silently cast to the instruction type. 6.2. Source Operands  The source operands are denoted in the instruction descriptions by the names a , b , and c . PTX describes a load-store machine, so operands for ALU instructions must all be in variables\ndeclared in the .reg register state space. For most operations, the sizes of the operands must\nbe consistent. The cvt (convert) instruction takes a variety of operand types and sizes, as its job is to\nconvert from nearly any data type to any other data type (and size). The ld , st , mov , and cvt instructions copy data from one location to\nanother. Instructions ld and st move data from/to addressable state spaces to/from\nregisters. The mov instruction copies data between registers. Most instructions have an optional predicate guard that controls conditional execution, and a few\ninstructions have additional predicate source operands. Predicate operands are denoted by the names p , q , r , s . 6.3. Destination Operands  PTX instructions that produce a single result store the result in the field denoted by d (for\ndestination) in the instruction descriptions. The result operand is a scalar or vector variable in\nthe register state space. 6.4. Using Addresses, Arrays, and Vectors  Using scalar variables as operands is straightforward. The interesting capabilities begin with\naddresses, arrays, and vectors. 6.4.1. Addresses as Operands  All the memory instructions take an address operand that specifies the memory location being\naccessed. This addressable operand is one of: [var] the name of an addressable variable var . [reg] an integer or bit-size type register reg containing a byte address. [reg+immOff] a sum of register reg containing a byte address plus a constant integer byte offset (signed, 32-bit). [var+immOff] a sum of address of addressable variable var containing a byte address plus a constant integer\nbyte offset (signed, 32-bit). [immAddr] an immediate absolute byte address (unsigned, 32-bit). var[immOff] an array element as described in Arrays as Operands . The register containing an address may be declared as a bit-size type or integer type. The access size of a memory instruction is the total number of bytes accessed in memory. For\nexample, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4\nbytes. The address must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined. For example, among other things, the access\nmay proceed by silently masking off low-order address bits to achieve proper rounding, or the\ninstruction may fault. The address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are\nzero-extended to the specified width as needed, and truncated if the register width exceeds the\nstate space address width for the target architecture. Address arithmetic is performed using integer arithmetic and logical instructions. Examples include\npointer arithmetic and pointer comparisons. All addresses and address computations are byte-based;\nthere is no support for C-style pointer arithmetic. The mov instruction can be used to move the address of a variable into a pointer. The address is\nan offset in the state space in which the variable is declared. Load and store operations move data\nbetween registers and locations in addressable state spaces. The syntax is similar to that used in\nmany assembly languages, where scalar variables are simply named and addresses are de-referenced by\nenclosing the address expression in square brackets. Address expressions include variable names,\naddress registers, address register plus byte offset, and immediate address expressions which\nevaluate at compile-time to a constant address. Here are a few examples: .shared .u16 x;\n.reg    .u16 r0;\n.global .v4 .f32 V;\n.reg    .v4 .f32 W;\n.const  .s32 tbl[256];\n.reg    .b32 p;\n.reg    .s32 q;\n\nld.shared.u16   r0,[x];\nld.global.v4.f32 W, [V];\nld.const.s32    q, [tbl+12];\nmov.u32         p, tbl; 6.4.1.1. Generic Addressing  If a memory instruction does not specify a state space, the operation is performed using generic\naddressing. The state spaces .const , Kernel Function Parameters ( .param ), .local and .shared are modeled as\nwindows within the generic address space. Each window is defined by a window base and a window size\nthat is equal to the size of the corresponding state space. A generic address maps to global memory unless it falls within the window for const , local , or shared memory. The Kernel\nFunction Parameters ( .param ) window is contained\nwithin the .global window. Within each window, a generic address maps to an address in the\nunderlying state space by subtracting the window base from the generic address. 6.4.2. Arrays as Operands  Arrays of all types can be declared, and the identifier becomes an address constant in the space\nwhere the array is declared. The size of the array is a constant in the program. Array elements can be accessed using an explicitly calculated byte address, or by indexing into the\narray using square-bracket notation. The expression within square brackets is either a constant\ninteger, a register variable, or a simple register with constant offset expression, where the\noffset is a constant expression that is either added or subtracted from a register variable. If more\ncomplicated indexing is desired, it must be written as an address calculation prior to use. Examples\nare: ld.global.u32  s, a[0];\nld.global.u32  s, a[N-1];\nmov.u32        s, a[1];  // move address of a[1] into s 6.4.3. Vectors as Operands  Vector operands are supported by a limited subset of instructions, which include mov , ld , st , atom , red and tex . Vectors may also be passed as arguments to called functions. Vector elements can be extracted from the vector with the suffixes .x , .y , .z and .w , as well as the typical color fields .r , .g , .b and .a . A brace-enclosed list is used for pattern matching to pull apart vectors. .reg .v4 .f32 V;\n.reg .f32     a, b, c, d;\n\nmov.v4.f32 {a,b,c,d}, V; Vector loads and stores can be used to implement wide loads and stores, which may improve memory\nperformance. The registers in the load/store operations can be a vector, or a brace-enclosed list of\nsimilarly typed scalars. Here are examples: ld.global.v4.f32  {a,b,c,d}, [addr+16];\nld.global.v2.u32  V2, [addr+8]; Elements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows: Ra = V.x = V.r\nRb = V.y = V.g\nRc = V.z = V.b\nRd = V.w = V.a 6.4.4. Labels and Function Names as Operands  Labels and function names can be used only in bra / brx.idx and call instructions\nrespectively. Function names can be used in mov instruction to get the address of the function\ninto a register, for use in an indirect call. Beginning in PTX ISA version 3.1, the mov instruction may be used to take the address of kernel\nfunctions, to be passed to a system call that initiates a kernel launch from the GPU. This feature\nis part of the support for CUDA Dynamic Parallelism. See the CUDA Dynamic Parallelism Programming\nGuide for details. 6.5. Type Conversion  All operands to all arithmetic, logic, and data movement instruction must be of the same type and\nsize, except for operations where changing the size and/or type is part of the definition of the\ninstruction. Operands of different sizes or types must be converted prior to the operation. 6.5.1. Scalar Conversions  Table 13 shows what\nprecision and format the cvt instruction uses given operands of differing types. For example, if a cvt.s32.u16 instruction is given a u16 source operand and s32 as a destination operand,\nthe u16 is zero-extended to s32 . Conversions to floating-point that are beyond the range of floating-point numbers are represented\nwith the maximum floating-point value (IEEE 754 Inf for f32 and f64 , and ~131,000 for f16 ). Table 13 Convert Instruction Precision and Format  Destination Format s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Source\nFormat s8 – sext sext sext – sext sext sext s2f s2f s2f s16 chop 1 – sext sext chop 1 – sext sext s2f s2f s2f s32 chop 1 chop 1 – sext chop 1 chop 1 – sext s2f s2f s2f s64 chop 1 chop 1 chop – chop 1 chop 1 chop – s2f s2f s2f u8 – zext zext zext – zext zext zext u2f u2f u2f u16 chop 1 – zext zext chop 1 – zext zext u2f u2f u2f u32 chop 1 chop 1 – zext chop 1 chop 1 – zext u2f u2f u2f u64 chop 1 chop 1 chop – chop 1 chop 1 chop – u2f u2f u2f f16 f2s f2s f2s f2s f2u f2u f2u f2u – f2f f2f f32 f2s f2s f2s f2s f2u f2u f2u f2u f2f – f2f f64 f2s f2s f2s f2s f2u f2u f2u f2u f2f f2f – Notes sext = sign-extend; zext = zero-extend; chop = keep only low bits that fit; s2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float; f2u = float-to-unsigned; f2f = float-to-float. 1 If the destination register is wider than the destination format, the result is extended to the\ndestination register width after chopping. The type of extension (sign or zero) is based on the\ndestination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then\nsign-extends to 32-bit. 6.5.2. Rounding Modifiers  Conversion instructions may specify a rounding modifier. In PTX, there are four integer rounding\nmodifiers and four floating-point rounding\nmodifiers. Table 14 and Table 15 summarize the rounding modifiers. Table 14 Floating-Point Rounding Modifiers  Modifier Description .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Table 15 Integer Rounding Modifiers  Modifier Description .rni round to nearest integer, choosing even integer if source is equidistant between two integers. .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity 6.6. Operand Costs  Operands from different state spaces affect the speed of an operation. Registers are fastest, while\nglobal memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first\nis to have multiple threads of execution so that the hardware can issue a memory operation and then\nswitch to other execution. Another way to hide latency is to issue the load instructions as early as\npossible, as execution is not blocked until the desired result is used in a subsequent (in time)\ninstruction. The register in a store operation is available much more\nquickly. Table 16 gives estimates of the\ncosts of using different kinds of memory. Table 16 Cost Estimates for Accessing State-Spaces  Space Time Notes Register 0 Shared 0 Constant 0 Amortized cost is low, first access is high Local > 100 clocks Parameter 0 Immediate 0 Global > 100 clocks Texture > 100 clocks Surface > 100 clocks 7. Abstracting the ABI  Rather than expose details of a particular calling convention, stack layout, and Application Binary\nInterface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI\nimplementations. In this section, we describe the features of PTX needed to achieve this hiding of\nthe ABI. These include syntax for function definitions, function calls, parameter passing, support\nfor variadic functions ( varargs ), and memory allocated on the stack ( alloca ). Refer to PTX Writers Guide to Interoperability for details on generating PTX compliant with\nApplication Binary Interface (ABI) for the CUDA ® architecture. 7.1. Function Declarations and Definitions  In PTX, functions are declared and defined using the .func directive. A function declaration specifies an optional list of return parameters, the function name, and an optional list of input\nparameters; together these specify the function’s interface, or prototype. A function definition specifies both the interface and the body of the function. A function must be declared or defined\nprior to being called. The simplest function has no parameters or return values, and is represented in PTX as follows: .func foo\n{\n    ...\n    ret;\n}\n\n    ...\n    call foo;\n    ... Here, execution of the call instruction transfers control to foo , implicitly saving the\nreturn address. Execution of the ret instruction within foo transfers control to the\ninstruction following the call. Scalar and vector base-type input and return parameters may be represented simply as register\nvariables. At the call, arguments may be register variables or constants, and return values may be\nplaced directly into register variables. The arguments and return variables at the call must have\ntype and size that match the callee’s corresponding formal parameters. Example .func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc )\n{\n    add.u32 %res, %ptr, %inc;\n    ret;\n}\n\n    ...\n    call (%r1), inc_ptr, (%r1,4);\n    ... When using the ABI, .reg state space parameters must be at least 32-bits in size. Subword scalar\nobjects in the source language should be promoted to 32-bit registers in PTX, or use .param state space byte arrays described next. Objects such as C structures and unions are flattened into registers or byte arrays in PTX and are\nrepresented using .param space memory. For example, consider the following C structure, passed\nby value to a function: struct {\n    double dbl;\n    char   c[4];\n}; In PTX, this structure will be flattened into a byte array. Since memory accesses are required to be\naligned to a multiple of the access size, the structure in this example will be a 12 byte array with\n8 byte alignment so that accesses to the .f64 field are aligned. The .param state space is\nused to pass the structure by value: Example .func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12])\n{\n    .reg .f64 f1;\n    .reg .b32 c1, c2, c3, c4;\n    ...\n    ld.param.f64 f1, [y+0];\n    ld.param.b8  c1, [y+8];\n    ld.param.b8  c2, [y+9];\n    ld.param.b8  c3, [y+10];\n    ld.param.b8  c4, [y+11];\n    ...\n    ... // computation using x,f1,c1,c2,c3,c4;\n}\n\n{\n     .param .b8 .align 8 py[12];\n     ...\n     st.param.b64 [py+ 0], %rd;\n     st.param.b8  [py+ 8], %rc1;\n     st.param.b8  [py+ 9], %rc2;\n     st.param.b8  [py+10], %rc1;\n     st.param.b8  [py+11], %rc2;\n     // scalar args in .reg space, byte array in .param space\n     call (%out), bar, (%x, py);\n     ... In this example, note that .param space variables are used in two ways. First, a .param variable y is used in function definition bar to represent a formal parameter. Second, a .param variable py is declared in the body of the calling function and used to set up the\nstructure being passed to bar. The following is a conceptual way to think about the .param state space use in device functions. For a caller, The .param state space is used to set values that will be passed to a called function and/or\nto receive return values from a called function. Typically, a .param byte array is used to\ncollect together fields of a structure being passed by value. For a callee, The .param state space is used to receive parameter values and/or pass return values back to\nthe caller. The following restrictions apply to parameter passing. For a caller, Arguments may be .param variables, .reg variables, or constants. In the case of .param space formal parameters that are byte arrays, the argument must also be\na .param space byte array with matching type, size, and alignment. A .param argument must\nbe declared within the local scope of the caller. In the case of .param space formal parameters that are base-type scalar or vector variables,\nthe corresponding argument may be either a .param or .reg space variable with matching\ntype and size, or a constant that can be represented in the type of the formal parameter. In the case of .reg space formal parameters, the corresponding argument may be either a .param or .reg space variable of matching type and size, or a constant that can be\nrepresented in the type of the formal parameter. In the case of .reg space formal parameters, the register must be at least 32-bits in size. All st.param instructions used for passing arguments to function call must immediately precede\nthe corresponding call instruction and ld.param instruction used for collecting return\nvalue must immediately follow the call instruction without any control flow\nalteration. st.param and ld.param instructions used for argument passing cannot be\npredicated. This enables compiler optimization and ensures that the .param variable does not\nconsume extra space in the caller’s frame beyond that needed by the ABI. The .param variable\nsimply allows a mapping to be made at the call site between data that may be in multiple\nlocations (e.g., structure being manipulated by caller is located in registers and memory) to\nsomething that can be passed as a parameter or return value to the callee. For a callee, Input and return parameters may be .param variables or .reg variables. Parameters in .param memory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes. Parameters in the .reg state space must be at least 32-bits in size. The .reg state space can be used to receive and return base-type scalar and vector values,\nincluding sub-word size objects when compiling in non-ABI mode. Supporting the .reg state\nspace provides legacy support. Note that the choice of .reg or .param state space for parameter passing has no impact on\nwhether the parameter is ultimately passed in physical registers or on the stack. The mapping of\nparameters to physical registers and stack locations depends on the ABI definition and the order,\nsize, and alignment of parameters. 7.1.1. Changes from PTX ISA Version 1.x  In PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no\nsupport for array parameters. Objects such as C structures were flattened and passed or returned\nusing multiple registers. PTX ISA version 1.x supports multiple return values for this purpose. Beginning with PTX ISA version 2.0, formal parameters may be in either .reg or .param state\nspace, and .param space parameters support arrays. For targets sm_20 or higher, PTX\nrestricts functions to a single return value, and a .param byte array should be used to return\nobjects that do not fit into a register. PTX continues to support multiple return registers for sm_1x targets. Note PTX implements a stack-based ABI only for targets sm_20 or higher. PTX ISA versions prior to 3.0 permitted variables in .reg and .local state spaces to be\ndefined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows\nmodule-scoped .reg and .local variables and restricts their use to within function\nscope. When compiling without use of the ABI, module-scoped .reg and .local variables are\nsupported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing\nmodule-scoped .reg or .local variables, the compiler silently disables use of the ABI. 7.2. Variadic Functions  Note Support for variadic functions which was unimplemented has been removed from the spec. PTX version 6.0 supports passing unsized array parameter to a function which can be used to\nimplement variadic functions. Refer to Kernel and Function Directives: .func for details 7.3. Alloca  PTX provides alloca instruction for allocating storage at runtime on the per-thread local memory\nstack. The allocated stack memory can be accessed with ld.local and st.local instructions\nusing the pointer returned by alloca . In order to facilitate deallocation of memory allocated with alloca , PTX provides two additional\ninstructions: stacksave which allows reading the value of stack pointer in a local variable, and stackrestore which can restore the stack pointer with the saved value. alloca , stacksave , and stackrestore instructions are described in Stack Manipulation\nInstructions . Preview Feature: Stack manipulation instructions alloca , stacksave and stackrestore are preview features\nin PTX ISA version 7.3. All details are subject to change with no guarantees of backward\ncompatibility on future PTX ISA versions or SM architectures. 8. Memory Consistency Model  In multi-threaded executions, the side-effects of memory operations performed by each thread become\nvisible to other threads in a partial and non-identical order. This means that any two operations\nmay appear to happen in no order, or in different orders, to different threads. The axioms\nintroduced by the memory consistency model specify exactly which contradictions are forbidden\nbetween the orders observed by different threads. In the absence of any constraint, each read operation returns the value committed by some write\noperation to the same memory location, including the initial write to that memory location. The\nmemory consistency model effectively constrains the set of such candidate writes from which a read\noperation can return a value. 8.1. Scope and applicability of the model  The constraints specified under this model apply to PTX programs with any PTX ISA version number,\nrunning on sm_70 or later architectures. The memory consistency model does not apply to texture (including ld.global.nc ) and surface\naccesses. 8.1.1. Limitations on atomicity at system scope  When communicating with the host CPU, certain strong operations with system scope may not be\nperformed atomically on some systems. For more details on atomicity guarantees to host memory, see\nthe CUDA Atomicity Requirements . 8.2. Memory operations  The fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state\nspace available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX\nstate space has a unique address relative to all threads that have access to the same state space. Each PTX memory instruction specifies an address operand and a data type. The address operand\ncontains a virtual address that gets converted to a physical address during memory access. The\nphysical address and the size of the data type together define a physical memory location, which is\nthe range of bytes starting from the physical address and extending up to the size of the data type\nin bytes. The memory consistency model specification uses the terms “address” or “memory address” to indicate\na virtual address, and the term “memory location” to indicate a physical memory location. Each PTX memory instruction also specifies the operation — either a read, a write or an atomic\nread-modify-write — to be performed on all the bytes in the corresponding memory location. 8.2.1. Overlap  Two memory locations are said to overlap when the starting address of one location is within the\nrange of bytes constituting the other location. Two memory operations are said to overlap when they\nspecify the same virtual address and the corresponding memory locations overlap. The overlap is said\nto be complete when both memory locations are identical, and it is said to be partial otherwise. 8.2.2. Aliases  Two distinct virtual addresses are said to be aliases if they map to the same memory location. 8.2.3. Multimem Addresses  A multimem address is a virtual address which points to multiple distinct memory locations across\ndevices. Only multimem. * operations are valid on multimem addresses. That is, the behavior of accessing\na multimem address in any other memory operation is undefined. 8.2.4. Memory Operations on Vector Data Types  The memory consistency model relates operations executed on memory locations with scalar data types,\nwhich have a maximum size and alignment of 64 bits. Memory operations with a vector data type are\nmodelled as a set of equivalent memory operations with a scalar data type, executed in an\nunspecified order on the elements in the vector. 8.2.5. Memory Operations on Packed Data Types  A packed data type consists of two values of the same scalar data type, as described in Packed Data\nTypes . These values are accessed in adjacent memory locations. A\nmemory operation on a packed data type is modelled as a pair of equivalent memory operations on the\nscalar data type, executed in an unspecified order on each element of the packed data. 8.2.6. Initialization  Each byte in memory is initialized by a hypothetical write W0 executed before starting any thread\nin the program. If the byte is included in a program variable, and that variable has an initial\nvalue, then W0 writes the corresponding initial value for that byte; else W0 is assumed to have\nwritten an unknown but constant value to the byte. 8.3. State spaces  The relations defined in the memory consistency model are independent of state spaces. In\nparticular, causality order closes over all memory operations across all the state spaces. But the\nside-effect of a memory operation in one state space can be observed directly only by operations\nthat also have access to the same state space. This further constrains the synchronizing effect of a\nmemory operation in addition to scope. For example, the synchronizing effect of the PTX instruction ld.relaxed.shared.sys is identical to that of ld.relaxed.shared.cluster , since no thread\noutside the same cluster can execute an operation that accesses the same memory location. 8.4. Operation types  For simplicity, the rest of the document refers to the following operation types, instead of\nmentioning specific instructions that give rise to them. Table 17 Operation Types  Operation Type Instruction/Operation atomic operation atom or red instruction. read operation All variants of ld instruction and atom instruction (but not red instruction). write operation All variants of st instruction, and atomic operations if they result\nin a write. memory operation A read or write operation. volatile operation An instruction with .volatile qualifier. acquire operation A memory operation with .acquire or .acq_rel qualifier. release operation A memory operation with .release or .acq_rel qualifier. mmio operation An ld or st instruction with .mmio qualifier. memory fence operation A membar , fence.sc or fence.acq_rel instruction. proxy fence operation A fence.proxy or a membar.proxy instruction. strong operation A memory fence operation, or a memory operation with a .relaxed , .acquire , .release , .acq_rel , .volatile , or .mmio qualifier. weak operation An ld or st instruction with a .weak qualifier. synchronizing operation A barrier instruction, fence operation, release operation or acquire operation. 8.4.1. mmio Operation  An mmio operation is a memory operation with .mmio qualifier specified. It is usually performed\non a memory location which is mapped to the control registers of peer I/O devices. It can also be\nused for communication between threads but has poor performance relative to non- mmio operations. The semantic meaning of mmio operations cannot be defined precisely as it is defined by the\nunderlying I/O device. For formal specification of semantics of mmio operation from Memory\nConsistency Model perspective, it is equivalent to the semantics of a strong operation. But it\nfollows a few implementation-specific properties, if it meets the CUDA atomicity requirements at\nthe specified scope: Writes are always performed and are never combined within the scope specified. Reads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any\ncache within the scope specified. As an exception, in some implementations, the surrounding locations may also be loaded. In such\ncases the amount of data loaded is implementation specific and varies between 32 and 128 bytes\nin size. 8.5. Scope  Each strong operation must specify a scope , which is the set of threads that may interact\ndirectly with that operation and establish any of the relations described in the memory consistency\nmodel. There are four scopes: Table 18 Scopes  Scope Description .cta The set of all threads executing in the same CTA as the current thread. .cluster The set of all threads executing in the same cluster as the current thread. .gpu The set of all threads in the current program executing on the same compute\ndevice as the current thread. This also includes other kernel grids invoked by\nthe host program on the same compute device. .sys The set of all threads in the current program, including all kernel grids\ninvoked by the host program on all compute devices, and all threads\nconstituting the host program itself. Note that the warp is not a scope ; the CTA is the smallest collection of threads that qualifies as\na scope in the memory consistency model. 8.6. Proxies  A memory proxy , or a proxy is an abstract label applied to a method of memory access. When two\nmemory operations use distinct methods of memory access, they are said to be different proxies . Memory operations as defined in Operation types use generic method of memory access, i.e. a generic proxy . Other operations such as textures and surfaces all\nuse distinct methods of memory access, also distinct from the generic method. A proxy fence is required to synchronize memory operations across different proxies . Although\nvirtual aliases use the generic method of memory access, since using distinct virtual addresses\nbehaves as if using different proxies , they require a proxy fence to establish memory ordering. 8.7. Morally strong operations  Two operations are said to be morally strong relative to each other if they satisfy all of the\nfollowing conditions: The operations are related in program order (i.e, they are both executed by the same thread),\nor each operation is strong and specifies a scope that includes the thread executing the\nother operation. Both operations are performed via the same proxy . If both are memory operations, then they overlap completely. Most (but not all) of the axioms in the memory consistency model depend on relations between morally strong operations. 8.7.1. Conflict and Data-races  Two overlapping memory operations are said to conflict when at least one of them is a write . Two conflicting memory operations are said to be in a data-race if they are not related in causality order and they are not morally strong . 8.7.2. Limitations on Mixed-size Data-races  A data-race between operations that overlap completely is called a uniform-size data-race ,\nwhile a data-race between operations that overlap partially is called a mixed-size data-race . The axioms in the memory consistency model do not apply if a PTX program contains one or more mixed-size data-races . But these axioms are sufficient to describe the behavior of a PTX program\nwith only uniform-size data-races . Atomicity of mixed-size RMW operations In any program with or without mixed-size data-races , the following property holds for every pair\nof overlapping atomic operations A1 and A2 such that each specifies a scope that includes the\nother: Either the read-modify-write operation specified by A1 is performed completely before A2 is\ninitiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2\noverlap partially or completely. 8.8. Release and Acquire Patterns  Some sequences of instructions give rise to patterns that participate in memory synchronization as\ndescribed later. The release pattern makes prior operations from the current thread 1 visible to some operations from other threads. The acquire pattern makes some operations from\nother threads visible to later operations from the current thread. A release pattern on a location M consists of one of the following: A release operation on M E.g.: st.release [M]; or atom.acq_rel [M]; or mbarrier.arrive.release [M]; Or a release operation on M followed by a strong write on M in program order E.g.: st.release [M] ; st.relaxed [M]; Or a memory fence followed by a strong write on M in program order E.g.: fence; st.relaxed [M]; Any memory synchronization established by a release pattern only affects operations occurring in program order before the first instruction in that pattern. An acquire pattern on a location M consists of one of the following: An acquire operation on M E.g.: ld.acquire [M]; or atom.acq_rel [M]; or mbarrier.test_wait.acquire [M]; Or a strong read on M followed by an acquire operation on M in program order E.g.: ld.relaxed [M]; ld.acquire [M]; Or a strong read on M followed by a memory fence in program order E.g.: ld.relaxed [M]; fence; Any memory synchronization established by an acquire pattern only affects operations occurring\nin program order after the last instruction in that pattern. 1 For both release and acquire patterns, this effect is further extended to operations in\nother threads through the transitive nature of causality order . 8.9. Ordering of memory operations  The sequence of operations performed by each thread is captured as program order while memory\nsynchronization across threads is captured as causality order . The visibility of the side-effects\nof memory operations to other memory operations is captured as communication order . The memory\nconsistency model defines contradictions that are disallowed between communication order on the one\nhand, and causality order and program order on the other. 8.9.1. Program Order  The program order relates all operations performed by a thread to the order in which a sequential\nprocessor will execute instructions in the corresponding PTX source. It is a transitive relation\nthat forms a total order over the operations performed by the thread, but does not relate operations\nfrom different threads. 8.9.1.1. Asynchronous Operations  Some PTX instructions (all variants of cp.async , cp.async.bulk , cp.reduce.async.bulk , wgmma.mma_async ) perform operations that are asynchronous to the thread that executed the\ninstruction. These asynchronous operations are ordered after prior instructions in the same thread\n(except in the case of wgmma.mma_async ), but they are not part of the program order for that\nthread. Instead, they provide weaker ordering guarantees as documented in the instruction\ndescription. For example, the loads and stores performed as part of a cp.async are ordered with respect to\neach other, but not to those of any other cp.async instructions initiated by the same thread,\nnor any other instruction subsequently issued by the thread with the exception of cp.async.commit_group or cp.async.mbarrier.arrive . The asynchronous mbarrier arrive-on operation\nperformed by a cp.async.mbarrier.arrive instruction is ordered with respect to the memory\noperations performed by all prior cp.async operations initiated by the same thread, but not to\nthose of any other instruction issued by the thread. The implicit mbarrier complete-tx operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk instructions is ordered only with respect to the memory operations performed by the same\nasynchronous instruction, and in particular it does not transitively establish ordering with respect\nto prior instructions from the issuing thread. 8.9.2. Observation Order  Observation order relates a write W to a read R through an optional sequence of atomic\nread-modify-write operations. A write W precedes a read R in observation order if: R and W are morally strong and R reads the value written by W, or For some atomic operation Z, W precedes Z and Z precedes R in observation order . 8.9.3. Fence-SC Order  The Fence-SC order is an acyclic partial order, determined at runtime, that relates every pair of morally strong fence.sc operations. 8.9.4. Memory synchronization  Synchronizing operations performed by different threads synchronize with each other at runtime as\ndescribed here. The effect of such synchronization is to establish causality order across threads. A fence.sc operation X synchronizes with a fence.sc operation Y if X precedes Y in the Fence-SC order. A bar{.cta}.sync or bar{.cta}.red or bar{.cta}.arrive operation synchronizes with a bar{.cta}.sync or bar{.cta}.red operation executed on the same barrier. A barrier.cluster.arrive operation synchronizes with a barrier.cluster.wait operation. A release pattern X synchronizes with an acquire pattern Y, if a write operation in X\nprecedes a read operation in Y in observation order , and the first operation in X and the\nlast operation in Y are morally strong . API synchronization A synchronizes relation can also be established by certain CUDA APIs. Completion of a task enqueued in a CUDA stream synchronizes with the start of the following\ntask in the same stream, if any. For purposes of the above, recording or waiting on a CUDA event in a stream, or causing a\ncross-stream barrier to be inserted due to cudaStreamLegacy , enqueues tasks in the associated\nstreams even if there are no direct side effects. An event record task synchronizes with\nmatching event wait tasks, and a barrier arrival task synchronizes with matching barrier wait\ntasks. Start of a CUDA kernel synchronizes with start of all threads in the kernel. End of all threads\nin a kernel synchronize with end of the kernel. Start of a CUDA graph synchronizes with start of all source nodes in the graph. Completion of\nall sink nodes in a CUDA graph synchronizes with completion of the graph. Completion of a graph\nnode synchronizes with start of all nodes with a direct dependency. Start of a CUDA API call to enqueue a task synchronizes with start of the task. Completion of the last task queued to a stream, if any, synchronizes with return from cudaStreamSynchronize . Completion of the most recently queued matching event record task, if\nany, synchronizes with return from cudaEventSynchronize . Synchronizing a CUDA device or\ncontext behaves as if synchronizing all streams in the context, including ones that have been\ndestroyed. Returning cudaSuccess from an API to query a CUDA handle, such as a stream or event, behaves\nthe same as return from the matching synchronization API. In addition to establishing a synchronizes relation, the CUDA API synchronization mechanisms above\nalso participate in proxy-preserved base causality order . 8.9.5. Causality Order  Causality order captures how memory operations become visible across threads through synchronizing\noperations. The axiom “Causality” uses this order to constrain the set of write operations from\nwhich a read operation may read a value. Relations in the causality order primarily consist of relations in Base causality order 1 , which is a transitive order, determined at runtime. Base causality order An operation X precedes an operation Y in base causality order if: X precedes Y in program order , or X synchronizes with Y, or For some operation Z, X precedes Z in program order and Z precedes Y in base causality order , or X precedes Z in base causality order and Z precedes Y in program order , or X precedes Z in base causality order and Z precedes Y in base causality order . Proxy-preserved base causality order A memory operation X precedes a memory operation Y in proxy-preserved base causality order if X\nprecedes Y in base causality order , and: X and Y are performed to the same address, using the generic proxy , or X and Y are performed to the same address, using the same proxy , and by the same thread block,\nor X and Y are aliases and there is an alias proxy fence along the base causality path from X\nto Y. Causality order Causality order combines base causality order with some non-transitive relations as follows: An operation X precedes an operation Y in causality order if: X precedes Y in proxy-preserved base causality order , or For some operation Z, X precedes Z in observation order, and Z precedes Y in proxy-preserved\nbase causality order . 1 The transitivity of base causality order accounts for the “cumulativity” of synchronizing\noperations. 8.9.6. Coherence Order  There exists a partial transitive order that relates overlapping write operations, determined at\nruntime, called the coherence order 1 . Two overlapping write operations are related in coherence order if they are morally strong or if they are related in causality order . Two overlapping writes are unrelated in coherence order if they are in a data-race , which gives\nrise to the partial nature of coherence order . 1 Coherence order cannot be observed directly since it consists entirely of write\noperations. It may be observed indirectly by its use in constraining the set of candidate\nwrites that a read operation may read from. 8.9.7. Communication Order  The communication order is a non-transitive order, determined at runtime, that relates write\noperations to other overlapping memory operations. A write W precedes an overlapping read R in communication order if R returns the value of any\nbyte that was written by W. A write W precedes a write W’ in communication order if W precedes W’ in coherence order . A read R precedes an overlapping write W in communication order if, for any byte accessed by\nboth R and W, R returns the value written by a write W’ that precedes W in coherence order . Communication order captures the visibility of memory operations — when a memory operation X1\nprecedes a memory operation X2 in communication order , X1 is said to be visible to X2. 8.10. Axioms  8.10.1. Coherence  If a write W precedes an overlapping write W’ in causality order , then W must precede W’ in coherence order . 8.10.2. Fence-SC  Fence-SC order cannot contradict causality order . For a pair of morally strong fence.sc operations F1 and F2, if F1 precedes F2 in causality order , then F1 must precede F2 in Fence-SC order. 8.10.3. Atomicity  Single-Copy Atomicity Conflicting morally strong operations are performed with single-copy atomicity . When a read R\nand a write W are morally strong , then the following two communications cannot both exist in the\nsame execution, for the set of bytes accessed by both R and W: R reads any byte from W. R reads any byte from any write W’ which precedes W in coherence order . Atomicity of read-modify-write (RMW) operations When an atomic operation A and a write W overlap and are morally strong , then the following\ntwo communications cannot both exist in the same execution, for the set of bytes accessed by both A\nand W: A reads any byte from a write W’ that precedes W in coherence order . A follows W in coherence order . Litmus Test 1: . global . u32 x = 0 ; T1 T2 A1 : atom . sys . inc . u32 % r0 , [ x ]; A2 : atom . sys . inc . u32 % r0 , [ x ]; FINAL STATE : x == 2 Atomicity is guaranteed when the operations are morally strong . Litmus Test 2: . global . u32 x = 0 ; T1 T2 (In a different CTA) A1 : atom . cta . inc . u32 % r0 , [ x ]; A2 : atom . gpu . inc . u32 % r0 , [ x ]; FINAL STATE : x == 1 OR x == 2 Atomicity is not guaranteed if the operations are not morally strong . 8.10.4. No Thin Air  Values may not appear “out of thin air”: an execution cannot speculatively produce a value in such a\nway that the speculation becomes self-satisfying through chains of instruction dependencies and\ninter-thread communication. This matches both programmer intuition and hardware reality, but is\nnecessary to state explicitly when performing formal analysis. Litmus Test: Load Buffering . global . u32 x = 0 ; . global . u32 y = 0 ; T1 T2 A1 : ld . global . u32 % r0 , [ x ]; B1 : st . global . u32 [ y ], % r0 ; A2 : ld . global . u32 % r1 , [ y ]; B2 : st . global . u32 [ x ], % r1 ; FINAL STATE : x == 0 AND y == 0 The litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of\nthin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a\nsecond variable, with the first and second variable exchanged between the threads. If each variable\nis initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1,\nthen values passing through the memory operations in this example form a cycle:\nA1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of\nthe memory operations in this example were to speculatively associate a different value with the\ncorresponding memory location, then such a speculation would become self-fulfilling, and hence\nforbidden. 8.10.5. Sequential Consistency Per Location  Within any set of overlapping memory operations that are pairwise morally strong , communication\norder cannot contradict program order , i.e., a concatenation of program order between overlapping operations and morally strong relations in communication order cannot result in a\ncycle. This ensures that each program slice of overlapping pairwise morally strong operations is\nstrictly sequentially-consistent . Litmus Test: CoRR . global . u32 x = 0 ; T1 T2 W1 : st . global . relaxed . sys . u32 [ x ], 1 ; R1 : ld . global . relaxed . sys . u32 % r0 , [ x ]; R2 : ld . global . relaxed . sys . u32 % r1 , [ x ]; IF % r0 == 1 THEN % r1 == 1 The litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee. A\nthread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence\nof) reads R1 and R2 on the same location x. No other writes are executed on x, except the one\nmodelling the initial value. The operations W1, R1 and R2 are pairwise morally strong . If R1 reads\nfrom W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial\nvalue of x instead, then this would form a sequence of morally-strong relations R2->W1->R1 in communication order that contradicts the program order R1->R2 in thread T2. Hence R2 cannot read\nthe initial value of x in such an execution. 8.10.6. Causality  Relations in communication order cannot contradict causality order . This constrains the set of\ncandidate write operations that a read operation may read from: If a read R precedes an overlapping write W in causality order , then R cannot read from W. If a write W precedes an overlapping read R in causality order , then for any byte accessed by\nboth R and W, R cannot read from any write W’ that precedes W in coherence order . Litmus Test: Message Passing . global . u32 data = 0 ; . global . u32 flag = 0 ; T1 T2 W1 : st . global . u32 [ data ], 1 ; F1 : fence . sys ; W2 : st . global . relaxed . sys . u32 [ flag ], 1 ; R1 : ld . global . relaxed . sys . u32 % r0 , [ flag ]; F2 : fence . sys ; R2 : ld . global . u32 % r1 , [ data ]; IF % r0 == 1 THEN % r1 == 1 The litmus test known as “MP” (Message Passing) represents the essence of typical synchronization\nalgorithms. A vast majority of useful programs can be reduced to sequenced applications of this\npattern. Thread T1 first writes to a data variable and then to a flag variable while a second thread T2 first\nreads from the flag variable and then from the data variable. The operations on the flag are morally strong and the memory operations in each thread are separated by a fence , and these fences are morally strong . If R1 observes W2, then the release pattern “F1; W2” synchronizes with the acquire pattern “R1;\nF2”. This establishes the causality order W1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom causality guarantees that R2 cannot read from any write that precedes W1 in coherence order . In the absence\nof any other writes in this example, R2 must read from W1. Litmus Test: CoWR // These addresses are aliases . global . u32 data_alias_1 ; . global . u32 data_alias_2 ; T1 W1 : st . global . u32 [ data_alias_1 ], 1 ; F1 : fence . proxy . alias ; R1 : ld . global . u32 % r1 , [ data_alias_2 ]; % r1 == 1 Virtual aliases require an alias proxy fence along the synchronization path. Litmus Test: Store Buffering The litmus test known as “SB” (Store Buffering) demonstrates the sequential consistency enforced\nby the fence.sc . A thread T1 writes to a first variable, and then reads the value of a second\nvariable, while a second thread T2 writes to the second variable and then reads the value of the\nfirst variable. The memory operations in each thread are separated by fence. sc instructions,\nand these fences are morally strong . . global . u32 x = 0 ; . global . u32 y = 0 ; T1 T2 W1 : st . global . u32 [ x ], 1 ; F1 : fence . sc . sys ; R1 : ld . global . u32 % r0 , [ y ]; W2 : st . global . u32 [ y ], 1 ; F2 : fence . sc . sys ; R2 : ld . global . u32 % r1 , [ x ]; % r0 == 1 OR % r1 == 1 In any execution, either F1 precedes F2 in Fence-SC order, or vice versa. If F1 precedes F2 in Fence-SC order, then F1 synchronizes with F2. This establishes the causality order in W1 -> F1\n-> F2 -> R2. Axiom causality ensures that R2 cannot read from any write that precedes W1 in coherence order . In the absence of any other write to that variable, R2 must read from\nW1. Similarly, in the case where F2 precedes F1 in Fence-SC order, R1 must read from W2. If each fence.sc in this example were replaced by a fence.acq_rel instruction, then this outcome is\nnot guaranteed. There may be an execution where the write from each thread remains unobserved from\nthe other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0”\nfor variables y and x respectively. 9. Instruction Set  9.1. Format and Semantics of Instruction Descriptions  This section describes each PTX instruction. In addition to the name and the format of the\ninstruction, the semantics are described, followed by some examples that attempt to show several\npossible instantiations of the instruction. 9.2. PTX Instructions  PTX instructions generally have from zero to four operands, plus an optional guard predicate\nappearing after an @ symbol to the left of the opcode : @p opcode; @p opcode a; @p opcode d, a; @p opcode d, a, b; @p opcode d, a, b, c; For instructions that create a result value, the d operand is the destination operand, while a , b , and c are source operands. The setp instruction writes two destination registers. We use a | symbol to separate\nmultiple destination registers. setp.lt.s32  p|q, a, b;  // p = (a < b); q = !(a < b); For some instructions the destination operand is optional. A bit bucket operand denoted with an\nunderscore ( _ ) may be used in place of a destination register. 9.3. Predicated Execution  In PTX, predicate registers are virtual and have .pred as the type specifier. So, predicate\nregisters can be declared as .reg .pred p, q, r; All instructions have an optional guard predicate which controls conditional execution of the\ninstruction. The syntax to specify conditional execution is to prefix an instruction with @{!}p ,\nwhere p is a predicate variable, optionally negated. Instructions without a guard predicate are\nexecuted unconditionally. Predicates are most commonly set as the result of a comparison performed by the setp instruction. As an example, consider the high-level code if (i < n)\n    j = j + 1; This can be written in PTX as setp.lt.s32  p, i, n;    // p = (i < n)\n@p    add.s32      j, j, 1;    // if i < n, add 1 to j To get a conditional branch or conditional function call, use a predicate to control the execution\nof the branch or call instructions. To implement the above example as a true conditional branch, the\nfollowing PTX instruction sequence might be used: setp.lt.s32  p, i, n;    // compare i to n\n@!p   bra  L1;                 // if False, branch over\n      add.s32      j, j, 1;\nL1:     ... 9.3.1. Comparisons  9.3.1.1. Integer and Bit-Size Comparisons  The signed integer comparisons are the traditional eq (equal), ne (not-equal), lt (less-than), le (less-than-or-equal), gt (greater-than), and ge (greater-than-or-equal). The unsigned comparisons are eq , ne , lo (lower), ls (lower-or-same), hi (higher), and hs (higher-or-same). The bit-size comparisons are eq and ne ; ordering comparisons are not defined for bit-size types. Table 19 shows the operators for signed integer, unsigned integer, and bit-size types. Table 19 Operators for Signed Integer, Unsigned Integer, and Bit-Size Types  Meaning Signed Operator Unsigned Operator Bit-Size Operator a == b eq eq eq a != b ne ne ne a < b lt lo n/a a <= b le ls n/a a > b gt hi n/a a >= b ge hs n/a 9.3.1.2. Floating Point Comparisons  The ordered floating-point comparisons are eq , ne , lt , le , gt , and ge . If\neither operand is NaN , the result is False . Table 20 lists the floating-point\ncomparison operators. Table 20 Floating-Point Comparison Operators  Meaning Floating-Point Operator a == b && !isNaN(a) && !isNaN(b) eq a != b && !isNaN(a) && !isNaN(b) ne a < b && !isNaN(a) && !isNaN(b) lt a <= b && !isNaN(a) && !isNaN(b) le a > b && !isNaN(a) && !isNaN(b) gt a >= b && !isNaN(a) && !isNaN(b) ge To aid comparison operations in the presence of NaN values, unordered floating-point comparisons\nare provided: equ , neu , ltu , leu , gtu , and geu . If both operands are numeric\nvalues (not NaN ), then the comparison has the same result as its ordered counterpart. If either\noperand is NaN , then the result of the comparison is True . Table 21 lists the floating-point\ncomparison operators accepting NaN values. Table 21 Floating-Point Comparison Operators Accepting NaN  Meaning Floating-Point Operator a == b || isNaN(a) || isNaN(b) equ a != b || isNaN(a) || isNaN(b) neu a < b || isNaN(a) || isNaN(b) ltu a <= b || isNaN(a) || isNaN(b) leu a > b || isNaN(a) || isNaN(b) gtu a >= b || isNaN(a) || isNaN(b) geu To test for NaN values, two operators num ( numeric ) and nan ( isNaN ) are\nprovided. num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Table 22 lists the\nfloating-point comparison operators testing for NaN values. Table 22 Floating-Point Comparison Operators Testing for NaN  Meaning Floating-Point Operator !isNaN(a) && !isNaN(b) num isNaN(a) || isNaN(b) nan 9.3.2. Manipulating Predicates  Predicate values may be computed and manipulated using the following instructions: and , or , xor , not , and mov . There is no direct conversion between predicates and integer values, and no direct way to load or\nstore predicate register values. However, setp can be used to generate a predicate from an\ninteger, and the predicate-based select ( selp ) instruction can be used to generate an integer\nvalue based on the value of a predicate; for example: selp.u32 %r1,1,0,%p;    // convert predicate to 32-bit value 9.4. Type Information for Instructions and Operands  Typed instructions must have a type-size modifier. For example, the add instruction requires\ntype and size information to properly perform the addition operation (signed, unsigned, float,\ndifferent sizes), and this information must be specified as a suffix to the opcode. Example .reg .u16 d, a, b;\n\nadd.u16 d, a, b;    // perform a 16-bit unsigned add Some instructions require multiple type-size modifiers, most notably the data conversion instruction cvt . It requires separate type-size modifiers for the result and source, and these are placed in\nthe same order as the operands. For example: .reg .u16 a;\n.reg .f32 d;\n\ncvt.f32.u16 d, a;   // convert 16-bit unsigned to 32-bit float In general, an operand’s type must agree with the corresponding instruction-type modifier. The rules\nfor operand and instruction type conformance are as follows: Bit-size types agree with any type of the same size. Signed and unsigned integer types agree provided they have the same size, and integer operands are\nsilently cast to the instruction type if needed. For example, an unsigned integer operand used in\na signed integer instruction will be treated as a signed integer by the instruction. Floating-point types agree only if they have the same size; i.e., they must match exactly. Table 23 summarizes these type\nchecking rules. Table 23 Type Checking Rules  Operand Type .bX .sX .uX .fX Instruction Type .bX okay okay okay okay .sX okay okay okay invalid .uX okay okay okay invalid .fX okay invalid invalid okay Note Some operands have their type and size defined independently from the instruction type-size. For\nexample, the shift amount operand for left and right shift instructions always has type .u32 ,\nwhile the remaining operands have their type and size determined by the instruction type. Example // 64-bit arithmetic right shift; shift amount 'b' is .u32\n    shr.s64 d,a,b; 9.4.1. Operand Size Exceeding Instruction-Type Size  For convenience, ld , st , and cvt instructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned)\ninstruction types; floating-point instruction types still require that the operand type-size matches\nexactly, unless the operand is of bit-size type. When a source operand has a size that exceeds the instruction-type size, the source data is\ntruncated (chopped) to the appropriate number of bits specified by the instruction type-size. Table 24 summarizes the relaxed type-checking rules for source operands. Note that some combinations may\nstill be invalid for a particular instruction; for example, the cvt instruction does not support .bX instruction types, so those rows are invalid for cvt . Table 24 Relaxed Type-checking Rules for Source Operands  Source Operand Type b8 b16 b32 b64 b128 s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Instruction Type b8 – chop chop chop chop – chop chop chop – chop chop chop chop chop chop b16 inv – chop chop chop inv – chop chop inv – chop chop – chop chop b32 inv inv – chop chop inv inv – chop inv inv – chop inv – chop b64 inv inv inv – chop inv inv inv – inv inv inv – inv inv – b128 inv inv inv inv – inv inv inv inv inv inv inv inv inv inv inv s8 – chop chop chop chop – chop chop chop – chop chop chop inv inv inv s16 inv – chop chop chop inv – chop chop inv – chop chop inv inv inv s32 inv inv – chop chop inv inv – chop inv inv – chop inv inv inv s64 inv inv inv – chop inv inv inv – inv inv inv – inv inv inv u8 – chop chop chop chop – chop chop chop – chop chop chop inv inv inv u16 inv – chop chop chop inv – chop chop inv – chop chop inv inv inv u32 inv inv – chop chop inv inv – chop inv inv – chop inv inv inv u64 inv inv inv – chop inv inv inv – inv inv inv – inv inv inv f16 inv – chop chop chop inv inv inv inv inv inv inv inv – inv inv f32 inv inv – chop chop inv inv inv inv inv inv inv inv inv – inv f64 inv inv inv – chop inv inv inv inv inv inv inv inv inv inv – Notes chop = keep only low bits that fit; “–” = allowed, but no conversion needed; inv = invalid, parse error. Source register size must be of equal or greater size than the instruction-type size. Bit-size source registers may be used with any appropriately-sized instruction type. The data are\ntruncated (“chopped”) to the instruction-type size and interpreted according to the instruction\ntype. Integer source registers may be used with any appropriately-sized bit-size or integer instruction\ntype. The data are truncated to the instruction-type size and interpreted according to the\ninstruction type. Floating-point source registers can only be used with bit-size or floating-point instruction types.\nWhen used with a narrower bit-size instruction type, the data are truncated. When used with a\nfloating-point instruction type, the size must match exactly. When a destination operand has a size that exceeds the instruction-type size, the destination data\nis zero- or sign-extended to the size of the destination register. If the corresponding instruction\ntype is signed integer, the data is sign-extended; otherwise, the data is zero-extended. Table 25 summarizes the relaxed type-checking rules for destination operands. Table 25 Relaxed Type-checking Rules for Destination Operands  Destination Operand Type b8 b16 b32 b64 b128 s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Instruction Type b8 – zext zext zext zext – zext zext zext – zext zext zext zext zext zext b16 inv – zext zext zext inv – zext zext inv – zext zext – zext zext b32 inv inv – zext zext inv inv – zext inv inv – zext inv – zext b64 inv inv inv – zext inv inv inv – inv inv inv – inv inv – b128 inv inv inv inv – inv inv inv inv inv inv inv inv inv inv inv s8 – sext sext sext sext – sext sext sext – sext sext sext inv inv inv s16 inv – sext sext sext inv – sext sext inv – sext sext inv inv inv s32 inv inv – sext sext inv inv – sext inv inv – sext inv inv inv s64 inv inv inv – sext inv inv inv – inv inv inv – inv inv inv u8 – zext zext zext zext – zext zext zext – zext zext zext inv inv inv u16 inv – zext zext zext inv – zext zext inv – zext zext inv inv inv u32 inv inv – zext zext inv inv – zext inv inv – zext inv inv inv u64 inv inv inv – zext inv inv inv – inv inv inv – inv inv inv f16 inv – zext zext zext inv inv inv inv inv inv inv inv – inv inv f32 inv inv – zext zext inv inv inv inv inv inv inv inv inv – inv f64 inv inv inv – zext inv inv inv inv inv inv inv inv inv inv – Notes sext = sign-extend; zext = zero-extend; “–” = allowed, but no conversion needed; inv = invalid, parse error. Destination register size must be of equal or greater size than the instruction-type size. Bit-size destination registers may be used with any appropriately-sized instruction type. The data\nare sign-extended to the destination register width for signed integer instruction types, and are\nzero-extended to the destination register width otherwise. Integer destination registers may be used with any appropriately-sized bit-size or integer\ninstruction type. The data are sign-extended to the destination register width for signed integer\ninstruction types, and are zero-extended to the destination register width for bit-size an d\nunsigned integer instruction types. Floating-point destination registers can only be used with bit-size or floating-point instruction\ntypes. When used with a narrower bit-size instruction type, the data are zero-extended. When used\nwith a floating-point instruction type, the size must match exactly. 9.5. Divergence of Threads in Control Constructs  Threads in a CTA execute together, at least in appearance, until they come to a conditional control\nconstruct such as a conditional branch, conditional function call, or conditional return. If threads\nexecute down different control flow paths, the threads are called divergent . If all of the threads\nact in unison and follow a single control flow path, the threads are called uniform . Both\nsituations occur often in programs. A CTA with divergent threads may have lower performance than a CTA with uniformly executing threads,\nso it is important to have divergent threads re-converge as soon as possible. All control constructs\nare assumed to be divergent points unless the control-flow instruction is marked as uniform, using\nthe .uni suffix. For divergent control flow, the optimizing code generator automatically\ndetermines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore\nthe issue of divergent threads, but has the opportunity to improve performance by marking branch\npoints as uniform when the compiler or author can guarantee that the branch point is non-divergent. 9.6. Semantics  The goal of the semantic description of an instruction is to describe the results in all cases in as\nsimple language as possible. The semantics are described using C, until C is not expressive enough. 9.6.1. Machine-Specific Semantics of 16-bit Code  A PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a\n32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit\ncomputations are promoted to 32-bit computations. This can lead to computational differences\nbetween code run on a 16-bit machine versus the same code run on a 32-bit machine, since the\npromoted computation may have bits in the high-order half-word of registers that are not present in\n16-bit physical registers. These extra precision bits can become visible at the application level,\nfor example, by a right-shift instruction. At the PTX language level, one solution would be to define semantics for 16-bit code that is\nconsistent with execution on a 16-bit data path. This approach introduces a performance penalty for\n16-bit code executing on a 32-bit data path, since the translated code would require many additional\nmasking instructions to suppress extra precision bits in the high-order half-word of 32-bit\nregisters. Rather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of\n16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce\nportable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at\nappropriate points in the program to guarantee portability of the code. However, for many\nperformance-critical applications, this is not desirable, and for many applications the difference\nin execution is preferable to limiting performance. 9.7. Instructions  All PTX instructions may be predicated. In the following descriptions, the optional guard predicate\nis omitted from the syntax. 9.7.1. Integer Arithmetic Instructions  Integer arithmetic instructions operate on the integer types in register and constant immediate\nforms. The integer arithmetic instructions are: add sub mul mad mul24 mad24 sad div rem abs neg min max popc clz bfind fns brev bfe bfi bmsk szext dp4a dp2a 9.7.1.1. Integer Arithmetic Instructions: add  add Add two values. Syntax add.type       d, a, b;\nadd{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64,\n          .u16x2, .s16x2 }; Description Performs addition and writes the resulting value into a destination register. For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce .u16x2 , .s16x2 result in\ndestination. Operands d , a and b have type .type . For instruction types .u16x2 , .s16x2 ,\noperands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = iA[i] + iB[i];\n    }\n} else {\n    d = a + b;\n} Notes Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type. PTX ISA Notes Introduced in PTX ISA version 1.0. add.u16x2 and add.s16x2 introduced in PTX ISA version 8.0. Target ISA Notes Supported on all target architectures. add.u16x2 and add.s16x2 require sm_90 or higher. Examples @p  add.u32     x,y,z;\n    add.sat.s32 c,c,1;\n    add.u16x2   u,v,w; 9.7.1.2. Integer Arithmetic Instructions: sub  sub Subtract one value from another. Syntax sub.type       d, a, b;\nsub{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Performs subtraction and writes the resulting value into a destination register. Semantics d = a - b; Notes Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples sub.s32 c,a,b; 9.7.1.3. Integer Arithmetic Instructions: mul  mul Multiply two values. Syntax mul.mode.type  d, a, b;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Compute the product of two values. Semantics t = a * b;\nn = bitwidth of type;\nd = t;            // for .wide\nd = t<2n-1..n>;   // for .hi variant\nd = t<n-1..0>;    // for .lo variant Notes The type of the operation represents the types of the a and b operands. If .hi or .lo is specified, then d is the same size as a and b , and either the upper or lower\nhalf of the result is written to the destination register. If .wide is specified, then d is\ntwice as wide as a and b to receive the full result of the multiplication. The .wide suffix is supported only for 16- and 32-bit integer types. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mul.wide.s16 fa,fxs,fys;   // 16*16 bits yields 32 bits\nmul.lo.s16 fa,fxs,fys;     // 16*16 bits, save only the low 16 bits\nmul.wide.s32 z,x,y;        // 32*32 bits, creates 64 bit result 9.7.1.4. Integer Arithmetic Instructions: mad  mad Multiply two values, optionally extract the high or low half of the intermediate result, and add a third value. Syntax mad.mode.type  d, a, b, c;\nmad.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Multiplies two values, optionally extracts the high or low half of the intermediate result, and adds\na third value. Writes the result into a destination register. Semantics t = a * b;\nn = bitwidth of type;\nd = t + c;           // for .wide\nd = t<2n-1..n> + c;  // for .hi variant\nd = t<n-1..0> + c;   // for .lo variant Notes The type of the operation represents the types of the a and b operands. If .hi or .lo is\nspecified, then d and c are the same size as a and b , and either the upper or lower\nhalf of the result is written to the destination register. If .wide is specified, then d and c are twice as wide as a and b to receive the result of the multiplication. The .wide suffix is supported only for 16-bit and 32-bit integer types. Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type in .hi mode. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples @p  mad.lo.s32 d,a,b,c;\n    mad.lo.s32 r,p,q,r; 9.7.1.5. Integer Arithmetic Instructions: mul24  mul24 Multiply two 24-bit integer values. Syntax mul24.mode.type  d, a, b;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and return either\nthe high or low 32-bits of the 48-bit result. Semantics t = a * b;\nd = t<47..16>;    // for .hi variant\nd = t<31..0>;     // for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits. mul24.hi performs a 24x24-bit multiply and returns the high 32 bits of the 48-bit result. mul24.lo performs a 24x24-bit multiply and returns the low 32 bits of the 48-bit result. All operands are of the same type and size. mul24.hi may be less efficient on machines without hardware support for 24-bit multiply. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mul24.lo.s32 d,a,b;   // low 32-bits of 24x24-bit signed multiply. 9.7.1.6. Integer Arithmetic Instructions: mad24  mad24 Multiply two 24-bit integer values and add a third value. Syntax mad24.mode.type  d, a, b, c;\nmad24.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and add a third,\n32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low\n32-bits of the 48-bit result. Semantics t = a * b;\nd = t<47..16> + c;   // for .hi variant\nd = t<31..0> + c;    // for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits. mad24.hi performs a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third\nvalue. mad24.lo performs a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third\nvalue. All operands are of the same type and size. Saturation modifier: .sat limits result of 32-bit signed addition to MININT..MAXINT (no overflow). Applies only to .s32 type in .hi mode. mad24.hi may be less efficient on machines without hardware support for 24-bit multiply. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mad24.lo.s32 d,a,b,c;   // low 32-bits of 24x24-bit signed multiply. 9.7.1.7. Integer Arithmetic Instructions: sad  sad Sum of absolute differences. Syntax sad.type  d, a, b, c;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Adds the absolute value of a-b to c and writes the resulting value into d . Semantics d = c + ((a<b) ? b-a : a-b); PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples sad.s32  d,a,b,c;\nsad.u32  d,a,b,d;  // running sum 9.7.1.8. Integer Arithmetic Instructions: div  div Divide one value by another. Syntax div.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Divides a by b , stores result in d . Semantics d = a / b; Notes Division by zero yields an unspecified, machine-specific value. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples div.s32  b,n,i; 9.7.1.9. Integer Arithmetic Instructions: rem  rem The remainder of integer division. Syntax rem.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Divides a by b , store the remainder in d . Semantics d = a % b; Notes The behavior for negative numbers is machine-dependent and depends on whether divide rounds towards\nzero or negative infinity. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples rem.s32  x,x,8;    // x = x%8; 9.7.1.10. Integer Arithmetic Instructions: abs  abs Absolute value. Syntax abs.type  d, a;\n\n.type = { .s16, .s32, .s64 }; Description Take the absolute value of a and store it in d . Semantics d = |a|; Notes Only for signed integers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples abs.s32  r0,a; 9.7.1.11. Integer Arithmetic Instructions: neg  neg Arithmetic negate. Syntax neg.type  d, a;\n\n.type = { .s16, .s32, .s64 }; Description Negate the sign of a and store the result in d . Semantics d = -a; Notes Only for signed integers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples neg.s32  r0,a; 9.7.1.12. Integer Arithmetic Instructions: min  min Find the minimum of two values. Syntax min.atype         d, a, b;\nmin{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 }; Description Store the minimum of a and b in d . For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce .u16x2 , .s16x2 result\nin destination. Operands d , a and b have the same type as the instruction type. For instruction types .u16x2 , .s16x2 , operands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] < iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a < b) ? a : b; // Integer (signed and unsigned)\n} Notes Signed and unsigned differ. Saturation modifier: min.relu.{s16x2, s32} clamps the result to 0 if negative. PTX ISA Notes Introduced in PTX ISA version 1.0. min.u16x2 , min{.relu}.s16x2 and min.relu.s32 introduced in PTX ISA version 8.0. Target ISA Notes Supported on all target architectures. min.u16x2 , min{.relu}.s16x2 and min.relu.s32 require sm_90 or higher. Examples min.s32  r0,a,b;\n@p  min.u16  h,i,j;\n    min.s16x2.relu u,v,w; 9.7.1.13. Integer Arithmetic Instructions: max  max Find the maximum of two values. Syntax max.atype         d, a, b;\nmax{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 }; Description Store the maximum of a and b in d . For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce .u16x2 , .s16x2 result\nin destination. Operands d , a and b have the same type as the instruction type. For instruction types .u16x2 , .s16x2 , operands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] > iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a > b) ? a : b; // Integer (signed and unsigned)\n} Notes Signed and unsigned differ. Saturation modifier: max.relu.{s16x2, s32} clamps the result to 0 if negative. PTX ISA Notes Introduced in PTX ISA version 1.0. max.u16x2 , max{.relu}.s16x2 and max.relu.s32 introduced in PTX ISA version 8.0. Target ISA Notes Supported on all target architectures. max.u16x2 , max{.relu}.s16x2 and max.relu.s32 require sm_90 or higher. Examples max.u32  d,a,b;\nmax.s32  q,q,0;\nmax.relu.s16x2 t,t,u; 9.7.1.14. Integer Arithmetic Instructions: popc  popc Population count. Syntax popc.type  d, a;\n\n.type = { .b32, .b64 }; Description Count the number of one bits in a and place the resulting population count in 32-bit\ndestination register d . Operand a has the instruction type and destination d has type .u32 . Semantics .u32  d = 0;\nwhile (a != 0) {\n   if (a & 0x1)  d++;\n   a = a >> 1;\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes popc requires sm_20 or higher. Examples popc.b32  d, a;\npopc.b64  cnt, X;  // cnt is .u32 9.7.1.15. Integer Arithmetic Instructions: clz  clz Count leading zeros. Syntax clz.type  d, a;\n\n.type = { .b32, .b64 }; Description Count the number of leading zeros in a starting with the most-significant bit and place the\nresult in 32-bit destination register d . Operand a has the instruction type, and destination d has type .u32 . For .b32 type, the number of leading zeros is between 0 and 32,\ninclusively. For .b64 type, the number of leading zeros is between 0 and 64, inclusively. Semantics .u32  d = 0;\nif (.type == .b32)   { max = 32; mask = 0x80000000; }\nelse                 { max = 64; mask = 0x8000000000000000; }\n\nwhile (d < max && (a&mask == 0) ) {\n    d++;\n    a = a << 1;\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes clz requires sm_20 or higher. Examples clz.b32  d, a;\nclz.b64  cnt, X;  // cnt is .u32 9.7.1.16. Integer Arithmetic Instructions: bfind  bfind Find most significant non-sign bit. Syntax bfind.type           d, a;\nbfind.shiftamt.type  d, a;\n\n.type = { .u32, .u64,\n          .s32, .s64 }; Description Find the bit position of the most significant non-sign bit in a and place the result in d . Operand a has the instruction type, and destination d has type .u32 . For unsigned\nintegers, bfind returns the bit position of the most significant 1 . For signed integers, bfind returns the bit position of the most significant 0 for negative inputs and the most\nsignificant 1 for non-negative inputs. If .shiftamt is specified, bfind returns the shift amount needed to left-shift the found bit\ninto the most-significant bit position. bfind returns 0xffffffff if no non-sign bit is found. Semantics msb = (.type==.u32 || .type==.s32) ? 31 : 63;\n// negate negative signed inputs\nif ( (.type==.s32 || .type==.s64) && (a & (1<<msb)) ) {\n    a = ~a;\n}\n.u32  d = 0xffffffff;\nfor (.s32 i=msb; i>=0; i--) {\n    if (a & (1<<i))  { d = i; break; }\n}\nif (.shiftamt && d != 0xffffffff)  { d = msb - d; } PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes bfind requires sm_20 or higher. Examples bfind.u32  d, a;\nbfind.shiftamt.s64  cnt, X;  // cnt is .u32 9.7.1.17. Integer Arithmetic Instructions: fns  fns Find the n-th set bit Syntax fns.b32 d, mask, base, offset; Description Given a 32-bit value mask and an integer value base (between 0 and 31), find the n-th (given\nby offset) set bit in mask from the base bit, and store the bit position in d . If not\nfound, store 0xffffffff in d . Operand mask has a 32-bit type. Operand base has .b32 , .u32 or .s32 type. Operand offset has .s32 type. Destination d has type .b32. Operand base must be <= 31, otherwise behavior is undefined. Semantics d = 0xffffffff;\nif (offset == 0) {\n    if (mask[base] == 1) {\n        d = base;\n    }\n} else {\n    pos = base;\n    count = |offset| - 1;\n    inc = (offset > 0) ? 1 : -1;\n\n    while ((pos >= 0) && (pos < 32)) {\n        if (mask[pos] == 1) {\n            if (count == 0) {\n              d = pos;\n              break;\n           } else {\n               count = count – 1;\n           }\n        }\n        pos = pos + inc;\n    }\n} PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes fns requires sm_30 or higher. Examples fns.b32 d, 0xaaaaaaaa, 3, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 3, -1;  // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, -1;  // d = 1 9.7.1.18. Integer Arithmetic Instructions: brev  brev Bit reverse. Syntax brev.type  d, a;\n\n.type = { .b32, .b64 }; Description Perform bitwise reversal of input. Semantics msb = (.type==.b32) ? 31 : 63;\n\nfor (i=0; i<=msb; i++) {\n    d[i] = a[msb-i];\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes brev requires sm_20 or higher. Examples brev.b32  d, a; 9.7.1.19. Integer Arithmetic Instructions: bfe  bfe Bit Field Extract. Syntax bfe.type  d, a, b, c;\n\n.type = { .u32, .u64,\n          .s32, .s64 }; Description Extract bit field from a and place the zero or sign-extended result in d . Source b gives\nthe bit field starting bit position, and source c gives the bit field length in bits. Operands a and d have the same type as the instruction type. Operands b and c are\ntype .u32 , but are restricted to the 8-bit value range 0..255 . The sign bit of the extracted field is defined as: .u32 , .u64 : zero .s32 , .s64 : msb of input a if the extracted field extends beyond the msb of a msb of extracted\nfield, otherwise If the bit field length is zero, the result is zero. The destination d is padded with the sign bit of the extracted field. If the start position is\nbeyond the msb of the input, the destination d is filled with the replicated sign bit of the\nextracted field. Semantics msb = (.type==.u32 || .type==.s32) ? 31 : 63;\npos = b & 0xff;  // pos restricted to 0..255 range\nlen = c & 0xff;  // len restricted to 0..255 range\n\nif (.type==.u32 || .type==.u64 || len==0)\n    sbit = 0;\nelse\n    sbit = a[min(pos+len-1,msb)];\n\nd = 0;\nfor (i=0; i<=msb; i++) {\n    d[i] = (i<len && pos+i<=msb) ? a[pos+i] : sbit;\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes bfe requires sm_20 or higher. Examples bfe.b32  d,a,start,len; 9.7.1.20. Integer Arithmetic Instructions: bfi  bfi Bit Field Insert. Syntax bfi.type  f, a, b, c, d;\n\n.type = { .b32, .b64 }; Description Align and insert a bit field from a into b , and place the result in f . Source c gives the starting bit position for the insertion, and source d gives the bit field length in\nbits. Operands a , b , and f have the same type as the instruction type. Operands c and d are type .u32 , but are restricted to the 8-bit value range 0..255 . If the bit field length is zero, the result is b . If the start position is beyond the msb of the input, the result is b . Semantics msb = (.type==.b32) ? 31 : 63;\npos = c & 0xff;  // pos restricted to 0..255 range\nlen = d & 0xff;  // len restricted to 0..255 range\n\nf = b;\nfor (i=0; i<len && pos+i<=msb; i++) {\n    f[pos+i] = a[i];\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes bfi requires sm_20 or higher. Examples bfi.b32  d,a,b,start,len; 9.7.1.21. Integer Arithmetic Instructions: szext  szext Sign-extend or Zero-extend. Syntax szext.mode.type  d, a, b;\n\n.mode = { .clamp, .wrap };\n.type = { .u32, .s32 }; Description Sign-extends or zero-extends an N-bit value from operand a where N is specified in operand b . The resulting value is stored in the destination operand d . For the .s32 instruction type, the value in a is treated as an N-bit signed value and the\nmost significant bit of this N-bit value is replicated up to bit 31. For the .u32 instruction\ntype, the value in a is treated as an N-bit unsigned number and is zero-extended to 32\nbits. Operand b is an unsigned 32-bit value. If the value of N is 0, then the result of szext is 0. If the value of N is 32 or higher, then\nthe result of szext depends upon the value of the .mode qualifier as follows: If .mode is .clamp , then the result is the same as the source operand a . If .mode is .wrap , then the result is computed using the wrapped value of N. Semantics b1        = b & 0x1f;\ntoo_large = (b >= 32 && .mode == .clamp) ? true : false;\nmask      = too_large ? 0 : (~0) << b1;\nsign_pos  = (b1 - 1) & 0x1f;\n\nif (b1 == 0 || too_large || .type != .s32) {\n    sign_bit = false;\n} else {\n    sign_bit = (a >> sign_pos) & 1;\n}\nd = (a & ~mask) | (sign_bit ? mask | 0); PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes szext requires sm_70 or higher. Examples szext.clamp.s32 rd, ra, rb;\nszext.wrap.u32  rd, 0xffffffff, 0; // Result is 0. 9.7.1.22. Integer Arithmetic Instructions: bmsk  bmsk Bit Field Mask. Syntax bmsk.mode.b32  d, a, b;\n\n.mode = { .clamp, .wrap }; Description Generates a 32-bit mask starting from the bit position specified in operand a , and of the width\nspecified in operand b . The generated bitmask is stored in the destination operand d . The resulting bitmask is 0 in the following cases: When the value of a is 32 or higher and .mode is .clamp . When either the specified value of b or the wrapped value of b (when .mode is\nspecified as .wrap ) is 0. Semantics a1    = a & 0x1f;\nmask0 = (~0) << a1;\nb1    = b & 0x1f;\nsum   = a1 + b1;\nmask1 = (~0) << sum;\n\nsum-overflow          = sum >= 32 ? true : false;\nbit-position-overflow = false;\nbit-width-overflow    = false;\n\nif (.mode == .clamp) {\n    if (a >= 32) {\n        bit-position-overflow = true;\n        mask0 = 0;\n    }\n    if (b >= 32) {\n        bit-width-overflow = true;\n    }\n}\n\nif (sum-overflow || bit-position-overflow || bit-width-overflow) {\n    mask1 = 0;\n} else if (b1 == 0) {\n    mask1 = ~0;\n}\nd = mask0 & ~mask1; Notes The bitmask width specified by operand b is limited to range 0..32 in .clamp mode and to\nrange 0..31 in .wrap mode. PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes bmsk requires sm_70 or higher. Examples bmsk.clamp.b32  rd, ra, rb;\nbmsk.wrap.b32   rd, 1, 2; // Creates a bitmask of 0x00000006. 9.7.1.23. Integer Arithmetic Instructions: dp4a  dp4a Four-way byte dot product-accumulate. Syntax dp4a.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 }; Description Four-way byte dot product which is accumulated in 32-bit result. Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product. Operand c has type .u32 if both .atype and .btype are .u32 else operand c has type .s32 . Semantics d = c;\n\n// Extract 4 bytes from a 32bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_4(a, .atype);\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nfor (i = 0; i < 4; ++i) {\n    d += Va[i] * Vb[i];\n} PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes Requires sm_61 or higher. Examples dp4a.u32.u32           d0, a0, b0, c0;\ndp4a.u32.s32           d1, a1, b1, c1; 9.7.1.24. Integer Arithmetic Instructions: dp2a  dp2a Two-way dot product-accumulate. Syntax dp2a.mode.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\n.mode = { .lo, .hi }; Description Two-way 16-bit to 8-bit dot product which is accumulated in 32-bit result. Operand a and b are 32-bit inputs. Operand a holds two 16-bits inputs in packed form and\noperand b holds 4 byte inputs in packed form for dot product. Depending on the .mode specified, either lower half or upper half of operand b will be used\nfor dot product. Operand c has type .u32 if both .atype and .btype are .u32 else operand c has type .s32 . Semantics d = c;\n// Extract two 16-bit values from a 32-bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_2(a, .atype);\n\n// Extract four 8-bit values from a 32-bit input and sign or zer extend\n// based on input type.\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nb_select = (.mode == .lo) ? 0 : 2;\n\nfor (i = 0; i < 2; ++i) {\n    d += Va[i] * Vb[b_select + i];\n} PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes Requires sm_61 or higher. Examples dp2a.lo.u32.u32           d0, a0, b0, c0;\ndp2a.hi.u32.s32           d1, a1, b1, c1; 9.7.2. Extended-Precision Integer Arithmetic Instructions  Instructions add.cc , addc , sub.cc , subc , mad.cc and madc reference an\nimplicitly specified condition code register ( CC ) having a single carry flag bit ( CC.CF )\nholding carry-in/carry-out or borrow-in/borrow-out. These instructions support extended-precision\ninteger addition, subtraction, and multiplication. No other instructions access the condition code,\nand there is no support for setting, clearing, or testing the condition code. The condition code\nregister is not preserved across calls and is mainly intended for use in straight-line code\nsequences for computing extended-precision integer addition, subtraction, and multiplication. The extended-precision arithmetic instructions are: add.cc , addc sub.cc , subc mad.cc , madc 9.7.2.1. Extended-Precision Arithmetic Instructions: add.cc  add.cc Add two values with carry-out. Syntax add.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer addition and writes the carry-out value into the condition code register. Semantics d = a + b; carry-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit add.cc introduced in PTX ISA version 1.2. 64-bit add.cc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit add.cc is supported on all target architectures. 64-bit add.cc requires sm_20 or higher. Examples @p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4; 9.7.2.2. Extended-Precision Arithmetic Instructions: addc  addc Add two values with carry-in and optional carry-out. Syntax addc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer addition with carry-in and optionally writes the carry-out value into the condition\ncode register. Semantics d = a + b + CC.CF; if .cc specified, carry-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit addc introduced in PTX ISA version 1.2. 64-bit addc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit addc is supported on all target architectures. 64-bit addc requires sm_20 or higher. Examples @p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4; 9.7.2.3. Extended-Precision Arithmetic Instructions: sub.cc  sub.cc Subtract one value from another, with borrow-out. Syntax sub.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer subtraction and writes the borrow-out value into the condition code register. Semantics d = a - b; borrow-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit sub.cc introduced in PTX ISA version 1.2. 64-bit sub.cc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit sub.cc is supported on all target architectures. 64-bit sub.cc requires sm_20 or higher. Examples @p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4; 9.7.2.4. Extended-Precision Arithmetic Instructions: subc  subc Subtract one value from another, with borrow-in and optional borrow-out. Syntax subc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Performs integer subtraction with borrow-in and optionally writes the borrow-out value into the\ncondition code register. Semantics d = a  - (b + CC.CF); if .cc specified, borrow-out written to CC.CF Notes No integer rounding modifiers. No saturation. Behavior is the same for unsigned and signed integers. PTX ISA Notes 32-bit subc introduced in PTX ISA version 1.2. 64-bit subc introduced in PTX ISA version 4.3. Target ISA Notes 32-bit subc is supported on all target architectures. 64-bit subc requires sm_20 or higher. Examples @p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4; 9.7.2.5. Extended-Precision Arithmetic Instructions: mad.cc  mad.cc Multiply two values, extract high or low half of result, and add a third value with carry-out. Syntax mad{.hi,.lo}.cc.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third\nvalue. Writes the result to the destination register and the carry-out from the addition into the\ncondition code register. Semantics t = a * b;\nd = t<63..32> + c;    // for .hi variant\nd = t<31..0> + c;     // for .lo variant carry-out from addition is written to CC.CF Notes Generally used in combination with madc and addc to implement extended-precision multi-word\nmultiplication. See madc for an example. PTX ISA Notes 32-bit mad.cc introduced in PTX ISA version 3.0. 64-bit mad.cc introduced in PTX ISA version 4.3. Target ISA Notes Requires target sm_20 or higher. Examples @p  mad.lo.cc.u32 d,a,b,c;\n    mad.lo.cc.u32 r,p,q,r; 9.7.2.6. Extended-Precision Arithmetic Instructions: madc  madc Multiply two values, extract high or low half of result, and add a third value with carry-in and\noptional carry-out. Syntax madc{.hi,.lo}{.cc}.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third value\nalong with carry-in. Writes the result to the destination register and optionally writes the\ncarry-out from the addition into the condition code register. Semantics t = a * b;\nd = t<63..32> + c + CC.CF;     // for .hi variant\nd = t<31..0> + c + CC.CF;      // for .lo variant if .cc specified, carry-out from addition is written to CC.CF Notes Generally used in combination with mad.cc and addc to implement extended-precision\nmulti-word multiplication. See example below. PTX ISA Notes 32-bit madc introduced in PTX ISA version 3.0. 64-bit madc introduced in PTX ISA version 4.3. Target ISA Notes Requires target sm_20 or higher. Examples // extended-precision multiply:  [r3,r2,r1,r0] = [r5,r4] * [r7,r6]\nmul.lo.u32     r0,r4,r6;      // r0=(r4*r6).[31:0], no carry-out\nmul.hi.u32     r1,r4,r6;      // r1=(r4*r6).[63:32], no carry-out\nmad.lo.cc.u32  r1,r5,r6,r1;   // r1+=(r5*r6).[31:0], may carry-out\nmadc.hi.u32    r2,r5,r6,0;    // r2 =(r5*r6).[63:32]+carry-in,\n                              // no carry-out\nmad.lo.cc.u32   r1,r4,r7,r1;  // r1+=(r4*r7).[31:0], may carry-out\nmadc.hi.cc.u32  r2,r4,r7,r2;  // r2+=(r4*r7).[63:32]+carry-in,\n                              // may carry-out\naddc.u32        r3,0,0;       // r3 = carry-in, no carry-out\nmad.lo.cc.u32   r2,r5,r7,r2;  // r2+=(r5*r7).[31:0], may carry-out\nmadc.hi.u32     r3,r5,r7,r3;  // r3+=(r5*r7).[63:32]+carry-in 9.7.3. Floating-Point Instructions  Floating-point instructions operate on .f32 and .f64 register operands and constant\nimmediate values. The floating-point instructions are: testp copysign add sub mul fma mad div abs neg min max rcp sqrt rsqrt sin cos lg2 ex2 tanh Instructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions\nsupport subnormal inputs and results. Single-precision instructions support subnormal inputs and\nresults by default for sm_20 and subsequent targets, and flush subnormal inputs and results to\nsign-preserving zero for sm_1x targets. The optional .ftz modifier on single-precision\ninstructions provides backward compatibility with sm_1x targets by flushing subnormal inputs and\nresults to sign-preserving zero regardless of the target architecture. Single-precision add , sub , mul , and mad support saturation of results to the range\n[0.0, 1.0], with NaN s being flushed to positive zero. NaN payloads are supported for\ndouble-precision instructions (except for rcp.approx.ftz.f64 and rsqrt.approx.ftz.f64 , which\nmaps input NaN s to a canonical NaN ). Single-precision instructions return an unspecified NaN . Note that future implementations may support NaN payloads for single-precision\ninstructions, so PTX programs should not rely on the specific single-precision NaN s being\ngenerated. Table 26 summarizes\nfloating-point instructions in PTX. Table 26 Summary of Floating-Point Instructions  Instruction .rn .rz .rm .rp .ftz .sat Notes {add,sub,mul}.rnd.f32 x x x x x x If no rounding modifier is specified,\ndefault is .rn and instructions may\nbe folded into a multiply-add. {add,sub,mul}.rnd.f64 x x x x n/a n/a If no rounding modifier is specified,\ndefault is .rn and instructions may\nbe folded into a multiply-add. mad.f32 n/a n/a n/a n/a x x .target sm_1x No rounding modifier. {mad,fma}.rnd.f32 x x x x x x .target sm_20 or higher mad.f32 and fma.f32 are the same. {mad,fma}.rnd.f64 x x x x n/a n/a mad.f64 and fma.f64 are the same. div.full.f32 n/a n/a n/a n/a x n/a No rounding modifier. {div,rcp,sqrt}.approx.f32 n/a n/a n/a n/a x n/a n/a rcp.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f32 x x x x x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f64 x x x x n/a n/a .target sm_20 or higher {abs,neg,min,max}.f32 n/a n/a n/a n/a x n/a {abs,neg,min,max}.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.f32 n/a n/a n/a n/a x n/a rsqrt.approx.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {sin,cos,lg2,ex2}.approx.f32 n/a n/a n/a n/a x n/a tanh.approx.f32 n/a n/a n/a n/a n/a n/a .target sm_75 or higher 9.7.3.1. Floating Point Instructions: testp  testp Test floating-point property. Syntax testp.op.type  p, a;  // result is .pred\n\n.op   = { .finite, .infinite,\n          .number, .notanumber,\n          .normal, .subnormal };\n.type = { .f32, .f64 }; Description testp tests common properties of floating-point numbers and returns a predicate value of 1 if True and 0 if False . testp.finite True if the input is not infinite or NaN testp.infinite True if the input is positive or negative infinity testp.number True if the input is not NaN testp.notanumber True if the input is NaN testp.normal True if the input is a normal number (not NaN , not infinity) testp.subnormal True if the input is a subnormal number (not NaN , not infinity) As a special case, positive and negative zero are considered normal numbers. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Examples testp.notanumber.f32  isnan, f0;\ntestp.infinite.f64    p, X; 9.7.3.2. Floating Point Instructions: copysign  copysign Copy sign of one input to another. Syntax copysign.type  d, a, b;\n\n.type = { .f32, .f64 }; Description Copy sign bit of a into value of b , and return the result as d . PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Examples copysign.f32  x, y, z;\ncopysign.f64  A, B, C; 9.7.3.3. Floating Point Instructions: add  add Add two values. Syntax add{.rnd}{.ftz}{.sat}.f32  d, a, b;\nadd{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Performs addition and writes the resulting value into a destination register. Semantics d = a + b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that an add instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An add instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / add sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. add.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x add.f64 supports subnormal numbers. add.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: add.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes add.f32 supported on all target architectures. add.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for add.f64 , requires sm_13 or higher. for add.f32 , requires sm_20 or higher. Examples @p  add.rz.ftz.f32  f1,f2,f3; 9.7.3.4. Floating Point Instructions: sub  sub Subtract one value from another. Syntax sub{.rnd}{.ftz}{.sat}.f32  d, a, b;\nsub{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Performs subtraction and writes the resulting value into a destination register. Semantics d = a - b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that a sub instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A sub instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / sub sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sub.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x sub.f64 supports subnormal numbers. sub.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: sub.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes sub.f32 supported on all target architectures. sub.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for sub.f64 , requires sm_13 or higher. for sub.f32 , requires sm_20 or higher. Examples sub.f32 c,a,b;\nsub.rn.ftz.f32  f1,f2,f3; 9.7.3.5. Floating Point Instructions: mul  mul Multiply two values. Syntax mul{.rnd}{.ftz}{.sat}.f32  d, a, b;\nmul{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Compute the product of two values. Semantics d = a * b; Notes For floating-point multiplication, all operands must be the same size. Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that a mul instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A mul instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul/add and mul/sub sequences with no rounding modifiers may be\noptimized to use fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. mul.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x mul.f64 supports subnormal numbers. mul.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mul.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes mul.f32 supported on all target architectures. mul.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for mul.f64 , requires sm_13 or higher. for mul.f32 , requires sm_20 or higher. Examples mul.ftz.f32 circumf,radius,pi  // a single-precision multiply 9.7.3.6. Floating Point Instructions: fma  fma Fused multiply-add. Syntax fma.rnd{.ftz}{.sat}.f32  d, a, b, c;\nfma.rnd.f64              d, a, b, c;\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition. Semantics d = a*b + c; Notes fma.f32 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by .rnd . fma.f64 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by .rnd . fma.f64 is the same as mad.f64 . Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. fma.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x fma.f64 supports subnormal numbers. fma.f32 is unimplemented for sm_1x targets. Saturation: fma.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes fma.f64 introduced in PTX ISA version 1.4. fma.f32 introduced in PTX ISA version 2.0. Target ISA Notes fma.f32 requires sm_20 or higher. fma.f64 requires sm_13 or higher. Examples fma.rn.ftz.f32  w,x,y,z;\n@p  fma.rn.f64      d,a,b,c; 9.7.3.7. Floating Point Instructions: mad  mad Multiply two values and add a third value. Syntax mad{.ftz}{.sat}.f32      d, a, b, c;    // .target sm_1x\nmad.rnd{.ftz}{.sat}.f32  d, a, b, c;    // .target sm_20\nmad.rnd.f64              d, a, b, c;    // .target sm_13 and higher\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Multiplies two values and adds a third, and then writes the resulting value into a destination\nregister. Semantics d = a*b + c; Notes For .target sm_20 and higher: mad.f32 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by .rnd . mad.f64 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by .rnd . mad.{f32,f64} is the same as fma.{f32,f64} . For .target sm_1x : mad.f32 computes the product of a and b at double precision, and then the mantissa is\ntruncated to 23 bits, but the exponent is preserved. Note that this is different from computing\nthe product with mul , where the mantissa can be rounded and the exponent will be clamped. The\nexception for mad.f32 is when c = +/-0.0 , mad.f32 is identical to the result computed\nusing separate mul and add instructions. When JIT-compiled for SM 2.0 devices, mad.f32 is\nimplemented as a fused multiply-add (i.e., fma.rn.ftz.f32 ). In this case, mad.f32 can\nproduce slightly different numeric results and backward compatibility is not guaranteed in this\ncase. mad.f64 computes the product of a and b to infinite precision and then adds c to\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by .rnd . Unlike mad.f32 , the treatment of subnormal\ninputs and output follows IEEE 754 standard. mad.f64 is the same as fma.f64 . Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. mad.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x mad.f64 supports subnormal numbers. mad.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mad.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 1.0. In PTX ISA versions 1.4 and later, a rounding modifier is required for mad.f64 . Legacy mad.f64 instructions having no rounding modifier will map to mad.rn.f64 . In PTX ISA versions 2.0 and later, a rounding modifier is required for mad.f32 for sm_20 and higher targets. Errata mad.f32 requires a rounding modifier for sm_20 and higher targets. However for PTX ISA\nversion 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently defaults\nto mad.rn.f32 . For PTX ISA version 3.1, ptxas generates a warning and defaults to mad.rn.f32 , and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later. Target ISA Notes mad.f32 supported on all target architectures. mad.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz , .rm , .rp for mad.f64 , requires sm_13 or higher. .rn , .rz , .rm , .rp for mad.f32 , requires sm_20 or higher. Examples @p  mad.f32  d,a,b,c; 9.7.3.8. Floating Point Instructions: div  div Divide one value by another. Syntax div.approx{.ftz}.f32  d, a, b;  // fast, approximate divide\ndiv.full{.ftz}.f32    d, a, b;  // full-range approximate divide\ndiv.rnd{.ftz}.f32     d, a, b;  // IEEE 754 compliant rounding\ndiv.rnd.f64           d, a, b;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Divides a by b , stores result in d . Semantics d = a / b; Notes Fast, approximate single-precision divides: div.approx.f32 implements a fast approximation to divide, computed as d = a * (1/b) . For |b| in [2 -126 , 2 126 ], the maximum ulp error is 2. For 2 126 < |b| < 2 128 , if a is infinity, div.approx.f32 returns NaN , otherwise it\nreturns 0. div.full.f32 implements a relatively fast, full-range approximation that scales operands to\nachieve better accuracy, but is not fully IEEE 754 compliant and does not support rounding\nmodifiers. The maximum ulp error is 2 across the full range of inputs. Subnormal inputs and results are flushed to sign-preserving zero. Fast, approximate division by\nzero creates a value of infinity (with same sign as a ). Divide with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. div.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x div.f64 supports subnormal numbers. div.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes div.f32 and div.f64 introduced in PTX ISA version 1.0. Explicit modifiers .approx , .full , .ftz , and rounding introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, one of .approx , .full , or .rnd is required. For PTX ISA versions 1.0 through 1.3, div.f32 defaults to div.approx.ftz.f32 , and div.f64 defaults to div.rn.f64 . Target ISA Notes div.approx.f32 and div.full.f32 supported on all target architectures. div.rnd.f32 requires sm_20 or higher. div.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32 . div.{rz,rm,rp}.f64 requires sm_20 or higher. Examples div.approx.ftz.f32  diam,circum,3.14159;\ndiv.full.ftz.f32    x, y, z;\ndiv.rn.f64          xd, yd, zd; 9.7.3.9. Floating Point Instructions: abs  abs Absolute value. Syntax abs{.ftz}.f32  d, a;\nabs.f64        d, a; Description Take the absolute value of a and store the result in d . Semantics d = |a|; Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. abs.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x abs.f64 supports subnormal numbers. abs.f32 flushes subnormal inputs and results to sign-preserving zero. For abs.f32 , NaN input yields unspecified NaN . For abs.f64 , NaN input is passed\nthrough unchanged. Future implementations may comply with the IEEE 754 standard by preserving\npayload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes abs.f32 supported on all target architectures. abs.f64 requires sm_13 or higher. Examples abs.ftz.f32  x,f0; 9.7.3.10. Floating Point Instructions: neg  neg Arithmetic negate. Syntax neg{.ftz}.f32  d, a;\nneg.f64        d, a; Description Negate the sign of a and store the result in d . Semantics d = -a; Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. neg.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x neg.f64 supports subnormal numbers. neg.f32 flushes subnormal inputs and results to sign-preserving zero. NaN inputs yield an unspecified NaN . Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes neg.f32 supported on all target architectures. neg.f64 requires sm_13 or higher. Examples neg.ftz.f32  x,f0; 9.7.3.11. Floating Point Instructions: min  min Find the minimum of two values. Syntax min{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmin.f64                            d, a, b; Description Store the minimum of a and b in d . If .NaN modifier is specified, then the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the minimum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of min is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (.xorsign) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n    if (.abs) {\n        a = |a|;\n        b = |b|;\n   }\n}\nif (isNaN(a) && isNaN(b))                 d = NaN;\nelse if (.NaN && (isNaN(a) || isNaN(b)))  d = NaN;\nelse if (isNaN(a))                        d = b;\nelse if (isNaN(b))                        d = a;\nelse                                      d = (a < b) ? a : b;\nif (.xorsign && !isNaN(d)) {\n    setSignBit(d, xorsign);\n} Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. min.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x min.f64 supports subnormal numbers. min.f32 flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 1.0. min.NaN introduced in PTX ISA version 7.0. min.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes min.f32 supported on all target architectures. min.f64 requires sm_13 or higher. min.NaN requires sm_80 or higher. min.xorsign.abs requires sm_86 or higher. Examples @p  min.ftz.f32  z,z,x;\n    min.f64      a,b,c;\n    // fp32 min with .NaN\n    min.NaN.f32  f0,f1,f2;\n    // fp32 min with .xorsign.abs\n    min.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.12. Floating Point Instructions: max  max Find the maximum of two values. Syntax max{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmax.f64                            d, a, b; Description Store the maximum of a and b in d . If .NaN modifier is specified, the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the maximum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of max is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (.xorsign) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n    if (.abs) {\n        a = |a|;\n        b = |b|;\n    }\n}\nif (isNaN(a) && isNaN(b))                 d = NaN;\nelse if (.NaN && (isNaN(a) || isNaN(b)))  d = NaN;\nelse if (isNaN(a))                        d = b;\nelse if (isNaN(b))                        d = a;\nelse                                      d = (a > b) ? a : b;\nif (.xorsign && !isNaN(d)) {\n    setSignBit(d, xorsign);\n} Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. max.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x max.f64 supports subnormal numbers. max.f32 flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 1.0. max.NaN introduced in PTX ISA version 7.0. max.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes max.f32 supported on all target architectures. max.f64 requires sm_13 or higher. max.NaN requires sm_80 or higher. max.xorsign.abs requires sm_86 or higher. Examples max.ftz.f32  f0,f1,f2;\nmax.f64      a,b,c;\n// fp32 max with .NaN\nmax.NaN.f32  f0,f1,f2;\n// fp32 max with .xorsign.abs\nmax.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.13. Floating Point Instructions: rcp  rcp Take the reciprocal of a value. Syntax rcp.approx{.ftz}.f32  d, a;  // fast, approximate reciprocal\nrcp.rnd{.ftz}.f32     d, a;  // IEEE 754 compliant rounding\nrcp.rnd.f64           d, a;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Compute 1/a , store result in d . Semantics d = 1 / a; Notes Fast, approximate single-precision reciprocal: rcp.approx.f32 implements a fast approximation to reciprocal. The maximum absolute error is 2 -23.0 over the range 1.0-2.0. Input Result -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Reciprocal with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. rcp.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x rcp.f64 supports subnormal numbers. rcp.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes rcp.f32 and rcp.f64 introduced in PTX ISA version 1.0. rcp.rn.f64 and explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. General rounding modifiers were\nadded in PTX ISA version 2.0. For PTX ISA version 1.4 and later, one of .approx or .rnd is required. For PTX ISA versions 1.0 through 1.3, rcp.f32 defaults to rcp.approx.ftz.f32 , and rcp.f64 defaults to rcp.rn.f64 . Target ISA Notes rcp.approx.f32 supported on all target architectures. rcp.rnd.f32 requires sm_20 or higher. rcp.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32. rcp.{rz,rm,rp}.f64 requires sm_20 or higher. Examples rcp.approx.ftz.f32  ri,r;\nrcp.rn.ftz.f32      xi,x;\nrcp.rn.f64          xi,x; 9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64  rcp.approx.ftz.f64 Compute a fast, gross approximation to the reciprocal of a value. Syntax rcp.approx.ftz.f64  d, a; Description Compute a fast, gross approximation to the reciprocal as follows: extract the most-significant 32 bits of .f64 operand a in 1.11.20 IEEE floating-point\nformat (i.e., ignore the least-significant 32 bits of a ), compute an approximate .f64 reciprocal of this value using the most-significant 20 bits of\nthe mantissa of operand a , place the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits\nof destination d ,and zero the least significant 32 mantissa bits of .f64 destination d . Semantics tmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / tmp;\nd[31:0] = 0x00000000; Notes rcp.approx.ftz.f64 implements a fast, gross approximation to reciprocal. Input a[63:32] Result d[63:32] -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 . Subnormal inputs and results are flushed to sign-preserving zero. PTX ISA Notes rcp.approx.ftz.f64 introduced in PTX ISA version 2.1. Target ISA Notes rcp.approx.ftz.f64 requires sm_20 or higher. Examples rcp.ftz.f64  xi,x; 9.7.3.15. Floating Point Instructions: sqrt  sqrt Take the square root of a value. Syntax sqrt.approx{.ftz}.f32  d, a; // fast, approximate square root\nsqrt.rnd{.ftz}.f32     d, a; // IEEE 754 compliant rounding\nsqrt.rnd.f64           d, a; // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp }; Description Compute sqrt( a ) and store the result in d . Semantics d = sqrt(a); Notes sqrt.approx.f32 implements a fast approximation to square root. Input Result -Inf NaN -normal NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf +Inf NaN NaN Square root with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x sqrt.f64 supports subnormal numbers. sqrt.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes sqrt.f32 and sqrt.f64 introduced in PTX ISA version 1.0. sqrt.rn.f64 and explicit\nmodifiers .approx and .ftz were introduced in PTX ISA version 1.4. General rounding\nmodifiers were added in PTX ISA version 2.0. For PTX ISA version 1.4 and later, one of .approx or .rnd is required. For PTX ISA versions 1.0 through 1.3, sqrt.f32 defaults to sqrt.approx.ftz.f32 , and sqrt.f64 defaults to sqrt.rn.f64 . Target ISA Notes sqrt.approx.f32 supported on all target architectures. sqrt.rnd.f32 requires sm_20 or higher. sqrt.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32 . sqrt.{rz,rm,rp}.f64 requires sm_20 or higher. Examples sqrt.approx.ftz.f32  r,x;\nsqrt.rn.ftz.f32      r,x;\nsqrt.rn.f64          r,x; 9.7.3.16. Floating Point Instructions: rsqrt  rsqrt Take the reciprocal of the square root of a value. Syntax rsqrt.approx{.ftz}.f32  d, a;\nrsqrt.approx.f64        d, a; Description Compute 1/sqrt(a) and store the result in d . Semantics d = 1/sqrt(a); Notes rsqrt.approx implements an approximation to the reciprocal square root. Input Result -Inf NaN -normal NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN The maximum absolute error for rsqrt.f32 is 2 -22.4 over the range 1.0-4.0. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. rsqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x rsqrt.f64 supports subnormal numbers. rsqrt.f32 flushes subnormal inputs and results to sign-preserving zero. Note that rsqrt.approx.f64 is emulated in software and are relatively slow. PTX ISA Notes rsqrt.f32 and rsqrt.f64 were introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, rsqrt.f32 defaults to rsqrt.approx.ftz.f32 , and rsqrt.f64 defaults to rsqrt.approx.f64 . Target ISA Notes rsqrt.f32 supported on all target architectures. rsqrt.f64 requires sm_13 or higher. Examples rsqrt.approx.ftz.f32  isr, x;\nrsqrt.approx.f64      ISR, X; 9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64  rsqrt.approx.ftz.f64 Compute an approximation of the square root reciprocal of a value. Syntax rsqrt.approx.ftz.f64 d, a; Description Compute a double-precision ( .f64 ) approximation of the square root reciprocal of a value. The\nleast significant 32 bits of the double-precision ( .f64 ) destination d are all zeros. Semantics tmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / sqrt(tmp);\nd[31:0] = 0x00000000; Notes rsqrt.approx.ftz.f64 implements a fast approximation of the square root reciprocal of a value. Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 . Subnormal inputs and results are flushed to sign-preserving zero. PTX ISA Notes rsqrt.approx.ftz.f64 introduced in PTX ISA version 4.0. Target ISA Notes rsqrt.approx.ftz.f64 requires sm_20 or higher. Examples rsqrt.approx.ftz.f64 xi,x; 9.7.3.18. Floating Point Instructions: sin  sin Find the sine of a value. Syntax sin.approx{.ftz}.f32  d, a; Description Find the sine of the angle a (in radians). Semantics d = sin(a); Notes sin.approx.f32 implements a fast approximation to sine. Input Result -Inf NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sin.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes sin.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, sin.f32 defaults to sin.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples sin.approx.ftz.f32  sa, a; 9.7.3.19. Floating Point Instructions: cos  cos Find the cosine of a value. Syntax cos.approx{.ftz}.f32  d, a; Description Find the cosine of the angle a (in radians). Semantics d = cos(a); Notes cos.approx.f32 implements a fast approximation to cosine. Input Result -Inf NaN -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. cos.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes cos.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, cos.f32 defaults to cos.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples cos.approx.ftz.f32  ca, a; 9.7.3.20. Floating Point Instructions: lg2  lg2 Find the base-2 logarithm of a value. Syntax lg2.approx{.ftz}.f32  d, a; Description Determine the log 2 of a . Semantics d = log(a) / log(2); Notes lg2.approx.f32 implements a fast approximation to log 2 (a). Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 -Inf +subnormal -Inf +Inf +Inf NaN NaN The maximum absolute error is 2 -22.6 for mantissa. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. lg2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes lg2.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, lg2.f32 defaults to lg2.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples lg2.approx.ftz.f32  la, a; 9.7.3.21. Floating Point Instructions: ex2  ex2 Find the base-2 exponential of a value. Syntax ex2.approx{.ftz}.f32  d, a; Description Raise 2 to the power a . Semantics d = 2 ^ a; Notes ex2.approx.f32 implements a fast approximation to 2 a . Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN The maximum absolute error is 2 -22.5 for fraction in the primary range. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. ex2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes ex2.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, ex2.f32 defaults to ex2.approx.ftz.f32 . Target ISA Notes Supported on all target architectures. Examples ex2.approx.ftz.f32  xa, a; 9.7.3.22. Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.f32 d, a; Description Take hyperbolic tangent value of a . The operands d and a are of type .f32 . Semantics d = tanh(a); Notes tanh.approx.f32 implements a fast approximation to FP32 hyperbolic-tangent. Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -subnormal Same as input -0.0 -0.0 +0.0 +0.0 +subnormal Same as input +Inf 1.0 NaN NaN The subnormal numbers are supported. Note The subnormal inputs gets passed through to the output since the value of tanh(x) for small\nvalues of x is approximately the same as x . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_75 or higher. Examples tanh.approx.f32 sa, a; 9.7.4. Half Precision Floating-Point Instructions  Half precision floating-point instructions operate on .f16 and .f16x2 register operands. The\nhalf precision floating-point instructions are: add sub mul fma neg abs min max tanh ex2 Half-precision add , sub , mul , and fma support saturation of results to the range\n[0.0, 1.0], with NaN s being flushed to positive zero. Half-precision instructions return an\nunspecified NaN . 9.7.4.1. Half Precision Floating Point Instructions: add  add Add two values. Syntax add{.rnd}{.ftz}{.sat}.f16   d, a, b;\nadd{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nadd{.rnd}.bf16   d, a, b;\nadd{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn }; Description Performs addition and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce .f16x2 or .bf16x2 result\nin destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type,\noperands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a + b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] + fB[i];\n    }\n} Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even The default value of rounding modifier is .rn . Note that an add instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An add instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / add sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: By default, subnormal numbers are supported. add.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: add.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 4.2. add{.rnd}.bf16 and add{.rnd}.bf16x2 introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. add{.rnd}.bf16 and add{.rnd}.bf16x2 requires sm_90 or higher. Examples // scalar f16 additions\nadd.f16        d0, a0, b0;\nadd.rn.f16     d1, a1, b1;\nadd.bf16       bd0, ba0, bb0;\nadd.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 addition\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nadd.f16x2  p3, p1, p2;   // SIMD f16x2 addition\n\n// SIMD bf16 addition\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nadd.bf16x2  p6, p4, p5;       // SIMD bf16x2 addition\n\n// SIMD fp16 addition\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nadd.f16x2       f2, f0, f1;     // SIMD f16x2 addition\n\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nadd.bf16x2      f5, f3, f4;      // SIMD bf16x2 addition 9.7.4.2. Half Precision Floating Point Instructions: sub  sub Subtract two values. Syntax sub{.rnd}{.ftz}{.sat}.f16   d, a, b;\nsub{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nsub{.rnd}.bf16   d, a, b;\nsub{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn }; Description Performs subtraction and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then subtracted in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type,\noperands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a - b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] - fB[i];\n    }\n} Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even The default value of rounding modifier is .rn . Note that a sub instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A sub instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / sub sequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device. Subnormal numbers: By default, subnormal numbers are supported. sub.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: sub.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 4.2. sub{.rnd}.bf16 and sub{.rnd}.bf16x2 introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. sub{.rnd}.bf16 and sub{.rnd}.bf16x2 requires sm_90 or higher. Examples // scalar f16 subtractions\nsub.f16        d0, a0, b0;\nsub.rn.f16     d1, a1, b1;\nsub.bf16       bd0, ba0, bb0;\nsub.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 subtraction\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nsub.f16x2  p3, p1, p2;   // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nsub.bf16x2  p6, p4, p5;       // SIMD bf16x2 subtraction\n\n// SIMD fp16 subtraction\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nsub.f16x2       f2, f0, f1;     // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nsub.bf16x2      f5, f3, f4;      // SIMD bf16x2 subtraction 9.7.4.3. Half Precision Floating Point Instructions: mul  mul Multiply two values. Syntax mul{.rnd}{.ftz}{.sat}.f16   d, a, b;\nmul{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nmul{.rnd}.bf16   d, a, b;\nmul{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn }; Description Performs multiplication and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then multiplied in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type,\noperands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a * b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i];\n    }\n} Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even The default value of rounding modifier is .rn . Note that a mul instruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A mul instruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular, mul / add and mul/sub sequences with no rounding modifiers may\nbe optimized to use fused-multiply-add instructions on the target device. Subnormal numbers: By default, subnormal numbers are supported. mul.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mul.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . PTX ISA Notes Introduced in PTX ISA version 4.2. mul{.rnd}.bf16 and mul{.rnd}.bf16x2 introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. mul{.rnd}.bf16 and mul{.rnd}.bf16x2 requires sm_90 or higher. Examples // scalar f16 multiplications\nmul.f16        d0, a0, b0;\nmul.rn.f16     d1, a1, b1;\nmul.bf16       bd0, ba0, bb0;\nmul.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 multiplication\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nmul.f16x2  p3, p1, p2;   // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nmul.bf16x2  p6, p4, p5;       // SIMD bf16x2 multiplication\n\n// SIMD fp16 multiplication\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nmul.f16x2       f2, f0, f1;     // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nmul.bf16x2      f5, f3, f4;      // SIMD bf16x2 multiplication 9.7.4.4. Half Precision Floating Point Instructions: fma  fma Fused multiply-add Syntax fma.rnd{.ftz}{.sat}.f16     d, a, b, c;\nfma.rnd{.ftz}{.sat}.f16x2   d, a, b, c;\nfma.rnd{.ftz}.relu.f16      d, a, b, c;\nfma.rnd{.ftz}.relu.f16x2    d, a, b, c;\nfma.rnd{.relu}.bf16         d, a, b, c;\nfma.rnd{.relu}.bf16x2       d, a, b, c;\nfma.rnd.oob.{relu}.type     d, a, b, c;\n\n.rnd = { .rn }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source\noperands. Half-word operands are then operated in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a , b and c have .f16 or .b16 type. For .f16x2 instruction type, operands d , a , b and c have .b32 type. For .bf16 instruction type, operands d , a , b and c have .b16 type. For .bf16x2 instruction type, operands d , a , b and c have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = a * b + c;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    fC[0] = c[0:15];\n    fC[1] = c[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i] + fC[i];\n    }\n} Notes Rounding modifiers (default is .rn ): .rn mantissa LSB rounds to nearest even Subnormal numbers: By default, subnormal numbers are supported. fma.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: fma.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . fma.relu.{f16, f16x2, bf16, bf16x2} clamps the result to 0 if negative. NaN result is\nconverted to canonical NaN . Out Of Bounds modifier: fma.oob.{f16, f16x2, bf16, bf16x2} clamps the result to 0 if either of the operands\nis OOB NaN (defined under Tensors ) value. The test for the special NaN value\nand resultant forcing of the result to +0.0 is performed independently for each of the\ntwo SIMD operations. PTX ISA Notes Introduced in PTX ISA version 4.2. fma.relu.{f16, f16x2} and fma{.relu}.{bf16, bf16x2} introduced in PTX ISA version 7.0. Support for modifier .oob introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_53 or higher. fma.relu.{f16, f16x2} and fma{.relu}.{bf16, bf16x2} require sm_80 or higher. fma{.oob}.{f16, f16x2, bf16, bf16x2} requires sm_90 or higher. Examples // scalar f16 fused multiply-add\nfma.rn.f16         d0, a0, b0, c0;\nfma.rn.f16         d1, a1, b1, c1;\nfma.rn.relu.f16    d1, a1, b1, c1;\nfma.rn.oob.f16      d1, a1, b1, c1;\nfma.rn.oob.relu.f16 d1, a1, b1, c1;\n\n// scalar bf16 fused multiply-add\nfma.rn.bf16        d1, a1, b1, c1;\nfma.rn.relu.bf16   d1, a1, b1, c1;\nfma.rn.oob.bf16       d1, a1, b1, c1;\nfma.rn.oob.relu.bf16  d1, a1, b1, c1;\n\n// SIMD f16 fused multiply-add\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1}; // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3}; // pack two f16 to 32bit f16x2\nfma.rn.f16x2  p3, p1, p2, p2;   // SIMD f16x2 fused multiply-add\nfma.rn.relu.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.f16x2 p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier and relu saturation mode\n\n// SIMD fp16 fused multiply-add\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nfma.rn.f16x2    f2, f0, f1, f1; // SIMD f16x2 fused multiply-add\n\n// SIMD bf16 fused multiply-add\nfma.rn.bf16x2       f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add\nfma.rn.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier and relu saturation mode 9.7.4.5. Half Precision Floating Point Instructions: neg  neg Arithmetic negate. Syntax neg{.ftz}.f16    d, a;\nneg{.ftz}.f16x2  d, a;\nneg.bf16         d, a;\nneg.bf16x2       d, a; Description Negate the sign of a and store the result in d . For .f16x2 and .bf16x2 instruction type, forms input vector by extracting half word values\nfrom the source operand. Half-word operands are then negated in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .b32 type. For .bf16 instruction\ntype, operands d and a have .b16 type. For .bf16x2 instruction type, operands d and a have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = -a;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = -fA[i];\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. neg.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. NaN inputs yield an unspecified NaN . Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 6.0. neg.bf16 and neg.bf16x2 introduced in PTX ISA 7.0. Target ISA Notes Requires sm_53 or higher. neg.bf16 and neg.bf16x2 requires architecture sm_80 or higher. Examples neg.ftz.f16  x,f0;\nneg.bf16     x,b0;\nneg.bf16x2   x1,b1; 9.7.4.6. Half Precision Floating Point Instructions: abs  abs Absolute value Syntax abs{.ftz}.f16    d, a;\nabs{.ftz}.f16x2  d, a;\nabs.bf16         d, a;\nabs.bf16x2       d, a; Description Take absolute value of a and store the result in d . For .f16x2 and .bf16x2 instruction type, forms input vector by extracting half word values\nfrom the source operand. Absolute values of half-word operands are then computed in parallel to\nproduce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction\ntype, operands d and a have .b32 type. Semantics if (type == f16 || type == bf16) {\n    d = |a|;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = |fA[i]|;\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. abs.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. NaN inputs yield an unspecified NaN . Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit. PTX ISA Notes Introduced in PTX ISA version 6.5. abs.bf16 and abs.bf16x2 introduced in PTX ISA 7.0. Target ISA Notes Requires sm_53 or higher. abs.bf16 and abs.bf16x2 requires architecture sm_80 or higher. Examples abs.ftz.f16  x,f0;\nabs.bf16     x,b0;\nabs.bf16x2   x1,b1; 9.7.4.7. Half Precision Floating Point Instructions: min  min Find the minimum of two values. Syntax min{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmin{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16x2         d, a, b; Description Store the minimum of a and b in d . For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction\ntype, operands d and a have .b32 type. If .NaN modifier is specified, then the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the minimum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of min is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a < b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n               fA[i] = |fA[i]|;\n               fB[i] = |fB[i]|;\n           }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] < fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(d[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. min.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 7.0. min.xorsign introduced in PTX ISA version 7.2. Target ISA Notes Requires sm_80 or higher. min.xorsign.abs support requires sm_86 or higher. Examples min.ftz.f16       h0,h1,h2;\nmin.f16x2         b0,b1,b2;\n// SIMD fp16 min with .NaN\nmin.NaN.f16x2     b0,b1,b2;\nmin.bf16          h0, h1, h2;\n// SIMD bf16 min with NaN\nmin.NaN.bf16x2    b0, b1, b2;\n// scalar bf16 min with xorsign.abs\nmin.xorsign.abs.bf16 Rd, Ra, Rb 9.7.4.8. Half Precision Floating Point Instructions: max  max Find the maximum of two values. Syntax max{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmax{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16x2         d, a, b; Description Store the maximum of a and b in d . For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction\ntype, operands d and a have .b32 type. If .NaN modifier is specified, the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the maximum of\nabsolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the\nsign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign\nbit of both inputs before applying .abs operation. If the result of max is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a > b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n                fA[i] = |fA[i]|;\n                fB[i] = |fB[i]|;\n            }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] > fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(fA[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n} Notes Subnormal numbers: By default, subnormal numbers are supported. max.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 7.0. max.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes Requires sm_80 or higher. max.xorsign.abs support requires sm_86 or higher. Examples max.ftz.f16       h0,h1,h2;\nmax.f16x2         b0,b1,b2;\n// SIMD fp16 max with NaN\nmax.NaN.f16x2     b0,b1,b2;\n// scalar f16 max with xorsign.abs\nmax.xorsign.abs.f16 Rd, Ra, Rb;\nmax.bf16          h0, h1, h2;\n// scalar bf16 max and NaN\nmax.NaN.bf16x2    b0, b1, b2;\n// SIMD bf16 max with xorsign.abs\nmax.xorsign.abs.bf16x2 Rd, Ra, Rb; 9.7.4.9. Half Precision Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.type d, a;\n\n.type = {.f16, .f16x2, .bf16, .bf16x2} Description Take hyperbolic tangent value of a . The type of operands d and a are as specified by .type . For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a .f16x2 or .bf16x2 . Semantics if (.type == .f16 || .type == .bf16) {\n  d = tanh(a)\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = tanh(fA[0])\n  d[1] = tanh(fA[1])\n} Notes tanh.approx.{f16, f16x2, bf16, bf16x2} implements an approximate hyperbolic tangent in the\ntarget format. Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -0.0 -0.0 +0.0 +0.0 +Inf 1.0 NaN NaN The maximum absolute error for .f16 type is 2-10.987. The maximum absolute error for .bf16 type is 2-8. The subnormal numbers are supported. PTX ISA Notes Introduced in PTX ISA version 7.0. tanh.approx.{bf16/bf16x2} introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. tanh.approx.{bf16/bf16x2} requires sm_90 or higher. Examples tanh.approx.f16    h1, h0;\ntanh.approx.f16x2  hd1, hd0;\ntanh.approx.bf16   b1, b0;\ntanh.approx.bf16x2 hb1, hb0; 9.7.4.10. Half Precision Floating Point Instructions: ex2  ex2 Find the base-2 exponent of input. Syntax ex2.approx.atype     d, a;\nex2.approx.ftz.btype d, a;\n\n.atype = { .f16,  .f16x2}\n.btype = { .bf16, .bf16x2} Description Raise 2 to the power a . The type of operands d and a are as specified by .type . For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a .f16x2 or .bf16x2 . Semantics if (.type == .f16 || .type == .bf16) {\n  d = 2 ^ a\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = 2 ^ fA[0]\n  d[1] = 2 ^ fA[1]\n} Notes ex2.approx.{f16, f16x2, bf16, bf16x2} implement a fast approximation to 2 a . For the .f16 type, subnormal inputs are supported. ex2.approx.ftz.bf16 flushes subnormal\ninputs and results to sign-preserving zero. Results of ex2.approx.ftz.bf16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN Results of ex2.approx.f16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -0.0 +1.0 +0.0 +1.0 +Inf +Inf NaN NaN The maximum relative error for .f16 type is 2-9.9. The maximum relative error for .bf16 type\nis 2-7. PTX ISA Notes Introduced in PTX ISA version 7.0. ex2.approx.ftz.{bf16/bf16x2} introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. ex2.approx.ftz.{bf16/bf16x2} requires sm_90 or higher. Examples ex2.approx.f16         h1, h0;\nex2.approx.f16x2       hd1, hd0;\nex2.approx.ftz.bf16    b1, b2;\nex2.approx.ftz.bf16x2  hb1, hb2; 9.7.5. Comparison and Selection Instructions  The comparison select instructions are: set setp selp slct As with single-precision floating-point instructions, the set , setp , and slct instructions support subnormal numbers for sm_20 and higher targets and flush single-precision\nsubnormal inputs to sign-preserving zero for sm_1x targets. The optional .ftz modifier\nprovides backward compatibility with sm_1x targets by flushing subnormal inputs and results to\nsign-preserving zero regardless of the target architecture. 9.7.5.1. Comparison and Selection Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator. Syntax set.CmpOp{.ftz}.dtype.stype         d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.stype  d, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.dtype  = { .u32, .s32, .f32 };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 }; Description Compares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. If this result is True , 1.0f is written for floating-point\ndestination types, and 0xffffffff is written for integer destination types. Otherwise, 0x00000000 is written. Operand d has type .dtype ; operands a and b have type .stype ; operand c has\ntype .pred . Semantics t = (a CmpOp b) ? 1 : 0;\nif (isFloat(dtype))\n    d = BoolOp(t, c) ? 1.0f : 0x00000000;\nelse\n    d = BoolOp(t, c) ? 0xffffffff : 0x00000000; Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge . For unsigned values, the comparison operators lo , ls , hi , and hs for lower,\nlower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge ,\nrespectively. The untyped, bit-size comparisons are eq and ne . Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: sm_20+ By default, subnormal numbers are supported. set.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero. sm_1x set.dtype.f64 supports subnormal numbers. set.dtype.f32 flushes subnormal inputs to sign-preserving zero. Modifier .ftz applies only to .f32 comparisons. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes set with .f64 source type requires sm_13 or higher. Examples @p  set.lt.and.f32.s32  d,a,b,r;\n    set.eq.u32.u32      d,i,n; 9.7.5.2. Comparison and Selection Instructions: setp  setp Compare two numeric values with a relational operator, and (optionally) combine this result with a\npredicate value by applying a Boolean operator. Syntax setp.CmpOp{.ftz}.type         p[|q], a, b;\nsetp.CmpOp.BoolOp{.ftz}.type  p[|q], a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.type   = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 }; Description Compares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the first destination operand. A related value computed using\nthe complement of the compare result is written to the second destination operand. Applies to all numeric types. Operands a and b have type .type ; operands p , q ,\nand c have type .pred . The sink symbol ‘_’ may be used in place of any one of the\ndestination operands. Semantics t = (a CmpOp b) ? 1 : 0;\np = BoolOp(t, c);\nq = BoolOp(!t, c); Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge . For unsigned values, the comparison operators lo , ls , hi , and hs for lower,\nlower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge ,\nrespectively. The untyped, bit-size comparisons are eq and ne . Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: sm_20+ By default, subnormal numbers are supported. setp.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero. sm_1x setp.dtype.f64 supports subnormal numbers. setp.dtype.f32 flushes subnormal inputs to sign-preserving zero. Modifier .ftz applies only to .f32 comparisons. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes setp with .f64 source type requires sm_13 or higher. Examples setp.lt.and.s32  p|q,a,b,r;\n@q  setp.eq.u32      p,i,n; 9.7.5.3. Comparison and Selection Instructions: selp  selp Select between source operands, based on the value of the predicate source operand. Syntax selp.type d, a, b, c;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 }; Description Conditional selection. If c is True , a is stored in d , b otherwise. Operands d , a , and b must be of the same type. Operand c is a predicate. Semantics d = (c == 1) ? a : b; PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes selp.f64 requires sm_13 or higher. Examples selp.s32  r0,r,g,p;\n@q  selp.f32  f0,t,x,xp; 9.7.5.4. Comparison and Selection Instructions: slct  slct Select one source operand, based on the sign of the third operand. Syntax slct.dtype.s32        d, a, b, c;\nslct{.ftz}.dtype.f32  d, a, b, c;\n\n.dtype = { .b16, .b32, .b64,\n           .u16, .u32, .u64,\n           .s16, .s32, .s64,\n                 .f32, .f64 }; Description Conditional selection. If c ≥ 0, a is stored in d , otherwise b is stored in d . Operands d , a , and b are treated as a bitsize type of the same width as the first\ninstruction type; operand c must match the second instruction type ( .s32 or .f32 ). The\nselected input is copied to the output without modification. Semantics d = (c >= 0) ? a : b; Floating Point Notes For .f32 comparisons, negative zero equals zero. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. slct.ftz.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and\noperand a is selected. sm_1x slct.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand a is selected. Modifier .ftz applies only to .f32 comparisons. If operand c is NaN , the comparison is unordered and operand b is selected. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes slct.f64 requires sm_13 or higher. Examples slct.u32.s32  x, y, z, val;\nslct.ftz.u64.f32  A, B, C, fval; 9.7.6. Half Precision Comparison Instructions  The comparison instructions are: set setp 9.7.6.1. Half Precision Comparison Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator. Syntax set.CmpOp{.ftz}.f16.stype            d, a, b;\nset.CmpOp.BoolOp{.ftz}.f16.stype     d, a, b, {!}c;\n\nset.CmpOp.bf16.stype                 d, a, b;\nset.CmpOp.BoolOp.bf16.stype          d, a, b, {!}c;\n\nset.CmpOp{.ftz}.dtype.f16            d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16     d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp.dtype.bf16                 d, a, b;\nset.CmpOp.BoolOp.dtype.bf16          d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp{.ftz}.dtype.f16x2          d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16x2   d, a, b, {!}c;\n.dtype  = { .f16x2, .u32, .s32}\n\nset.CmpOp.dtype.bf16x2               d, a, b;\nset.CmpOp.BoolOp.dtype.bf16x2        d, a, b, {!}c;\n.dtype  = { .bf16x2, .u32, .s32}\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n            .f16, .f32, .f64}; Description Compares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. Result of this computation is written in destination register in the following way: If result is True , 0xffffffff is written for destination types .u32 / .s32 . 0xffff is written for destination types .u16 / .s16 . 1.0 in target precision floating point format is written for destination type .f16 , .bf16 . If result is False , 0x0 is written for all integer destination types. 0.0 in target precision floating point format is written for destination type .f16 , .bf16 . If the source type is .f16x2 or .bf16x2 then result of individual operations are packed in\nthe 32-bit destination operand. Operand c has type .pred . Semantics if (stype == .f16x2 || stype == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0]   = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1]   = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    if (dtype == .f16x2 || stype == .bf16x2) {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 1.0 : 0.0;\n        }\n    } else {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 0xffff : 0;\n        }\n    }\n} else if (dtype == .f16 || stype == .bf16) {\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? 1.0 : 0.0;\n} else  { // Integer destination type\n    trueVal = (isU16(dtype) || isS16(dtype)) ?  0xffff : 0xffffffff;\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? trueVal : 0;\n} Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: By default, subnormal numbers are supported. When .ftz modifier is specified then subnormal inputs and results are flushed to sign\npreserving zero. PTX ISA Notes Introduced in PTX ISA version 4.2. set.{u16, u32, s16, s32}.f16 and set.{u32, s32}.f16x2 are introduced in PTX ISA version 6.5. set.{u16, u32, s16, s32}.bf16 , set.{u32, s32, bf16x2}.bf16x2 , set.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64} are introduced in PTX ISA version\n7.8. Target ISA Notes Requires sm_53 or higher. set.{u16, u32, s16, s32}.bf16 , set.{u32, s32, bf16x2}.bf16x2 , set.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64} require sm_90 or higher. Examples set.lt.and.f16.f16  d,a,b,r;\nset.eq.f16x2.f16x2  d,i,n;\nset.eq.u32.f16x2    d,i,n;\nset.lt.and.u16.f16  d,a,b,r;\nset.ltu.or.bf16.f16    d,u,v,s;\nset.equ.bf16x2.bf16x2  d,j,m;\nset.geu.s32.bf16x2     d,j,m;\nset.num.xor.s32.bf16   d,u,v,s; 9.7.6.2. Half Precision Comparison Instructions: setp  setp Compare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator. Syntax setp.CmpOp{.ftz}.f16           p, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16    p, a, b, {!}c;\n\nsetp.CmpOp{.ftz}.f16x2         p|q, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16x2  p|q, a, b, {!}c;\n\nsetp.CmpOp.bf16                p, a, b;\nsetp.CmpOp.BoolOp.bf16         p, a, b, {!}c;\n\nsetp.CmpOp.bf16x2              p|q, a, b;\nsetp.CmpOp.BoolOp.bf16x2       p|q, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor }; Description Compares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the destination operand. Operand c , p and q has type .pred . For instruction type .f16 , operands a and b have type .b16 or .f16 . For instruction type .f16x2 , operands a and b have type .b32 . For instruction type .bf16 , operands a and b have type .b16 . For instruction type .bf16x2 , operands a and b have type .b32 . Semantics if (type == .f16 || type == .bf16) {\n     t = (a CmpOp b) ? 1 : 0;\n     p = BoolOp(t, c);\n} else if (type == .f16x2 || type == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    p = BoolOp(t[0], c);\n    q = BoolOp(t[1], c);\n} Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . If either operand is NaN , the result is False . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either\noperand is NaN , then the result of these comparisons is True . num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Subnormal numbers: By default, subnormal numbers are supported. setp.ftz.{f16,f16x2} flushes subnormal inputs to sign-preserving zero. PTX ISA Notes Introduced in PTX ISA version 4.2. setp.{bf16/bf16x2} introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_53 or higher. setp.{bf16/bf16x2} requires sm_90 or higher. Examples setp.lt.and.f16x2  p|q,a,b,r;\n@q  setp.eq.f16    p,i,n;\n\nsetp.gt.or.bf16x2  u|v,c,d,s;\n@q  setp.eq.bf16   u,j,m; 9.7.7. Logic and Shift Instructions  The logic and shift instructions are fundamentally untyped, performing bit-wise operations on\noperands of any type, provided the operands are of the same size. This permits bit-wise operations\non floating point values without having to define a union to access the bits. Instructions and , or , xor , and not also operate on predicates. The logical shift instructions are: and or xor not cnot lop3 shf shl shr 9.7.7.1. Logic and Shift Instructions: and  and Bitwise AND. Syntax and.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Compute the bit-wise and operation for the bits in a and b . Semantics d = a & b; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicate registers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples and.b32  x,q,r;\nand.b32  sign,fpvalue,0x80000000; 9.7.7.2. Logic and Shift Instructions: or  or Biwise OR. Syntax or.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Compute the bit-wise or operation for the bits in a and b . Semantics d = a | b; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicate registers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples or.b32  mask mask,0x00010001\nor.pred  p,q,r; 9.7.7.3. Logic and Shift Instructions: xor  xor Bitwise exclusive-OR (inequality). Syntax xor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Compute the bit-wise exclusive-or operation for the bits in a and b . Semantics d = a ^ b; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicate registers. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples xor.b32  d,q,r;\nxor.b16  d,x,0x0001; 9.7.7.4. Logic and Shift Instructions: not  not Bitwise negation; one’s complement. Syntax not.type d, a;\n\n.type = { .pred, .b16, .b32, .b64 }; Description Invert the bits in a . Semantics d = ~a; Notes The size of the operands must match, but not necessarily the type. Allowed types include predicates. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples not.b32  mask,mask;\nnot.pred  p,q; 9.7.7.5. Logic and Shift Instructions: cnot  cnot C/C++ style logical negation. Syntax cnot.type d, a;\n\n.type = { .b16, .b32, .b64 }; Description Compute the logical negation using C/C++ semantics. Semantics d = (a==0) ? 1 : 0; Notes The size of the operands must match, but not necessarily the type. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples cnot.b32 d,a; 9.7.7.6. Logic and Shift Instructions: lop3  lop3 Arbitrary logical operation on 3 inputs. Syntax lop3.b32 d, a, b, c, immLut;\nlop3.BoolOp.b32 d|p, a, b, c, immLut, q;\n\n.BoolOp   = { .or , .and }; Description Compute bitwise logical operation on inputs a , b , c and store the result in destination d . Optionally, .BoolOp can be specified to compute the predicate result p by performing a\nBoolean operation on the destination operand d with the predicate q in the following manner: p = (d != 0) BoolOp q; The sink symbol ‘_’ may be used in place of the destination operand d when .BoolOp qualifier\nis specified. The logical operation is defined by a look-up table which, for 3 inputs, can be represented as an\n8-bit value specified by operand immLut as described below. immLut is an integer constant\nthat can take values from 0 to 255, thereby allowing up to 256 distinct logical operations on inputs a , b , c . For a logical operation F(a, b, c) the value of immLut can be computed by applying the same\noperation to three predefined constant values as follows: ta = 0xF0;\ntb = 0xCC;\ntc = 0xAA;\n\nimmLut = F(ta, tb, tc); Examples: If F = (a & b & c);\nimmLut = 0xF0 & 0xCC & 0xAA = 0x80\n\nIf F = (a | b | c);\nimmLut = 0xF0 | 0xCC | 0xAA = 0xFE\n\nIf F = (a & b & ~c);\nimmLut = 0xF0 & 0xCC & (~0xAA) = 0x40\n\nIf F = ((a & b | c) ^ a);\nimmLut = (0xF0 & 0xCC | 0xAA) ^ 0xF0 = 0x1A The following table illustrates computation of immLut for various logical operations: ta tb tc Oper 0 (False) Oper 1 (ta & tb & tc) Oper 2 (ta & tb & ~tc) … Oper 254 (ta | tb | tc) Oper 255 (True) 0 0 0 0 0 0 … 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 immLut 0x0 0x80 0x40 … 0xFE 0xFF Semantics F = GetFunctionFromTable(immLut); // returns the function corresponding to immLut value\nd = F(a, b, c);\nif (BoolOp specified) {\n    p = (d != 0) BoolOp q;\n} PTX ISA Notes Introduced in PTX ISA version 4.3. Support for .BoolOp qualifier introduced in PTX ISA version 8.2. Target ISA Notes Requires sm_50 or higher. Qualifier .BoolOp requires sm_70 or higher. Examples lop3.b32       d, a, b, c, 0x40;\nlop3.or.b32  d|p, a, b, c, 0x3f, q;\nlop3.and.b32 _|p, a, b, c, 0x3f, q; 9.7.7.7. Logic and Shift Instructions: shf  shf Funnel shift. Syntax shf.l.mode.b32  d, a, b, c;  // left shift\nshf.r.mode.b32  d, a, b, c;  // right shift\n\n.mode = { .clamp, .wrap }; Description Shift the 64-bit value formed by concatenating operands a and b left or right by the amount\nspecified by the unsigned 32-bit value in c . Operand b holds bits 63:32 and operand a\nholds bits 31:0 of the 64-bit source value. The source is shifted left or right by the clamped\nor wrapped value in c . For shf.l , the most-significant 32-bits of the result are written\ninto d ; for shf.r , the least-significant 32-bits of the result are written into d . Semantics u32  n = (.mode == .clamp) ? min(c, 32) : c & 0x1f;\nswitch (shf.dir) {  // shift concatenation of [b, a]\n    case shf.l:     // extract 32 msbs\n           u32  d = (b << n)      | (a >> (32-n));\n    case shf.r:     // extract 32 lsbs\n           u32  d = (b << (32-n)) | (a >> n);\n} Notes Use funnel shift for multi-word shift operations and for rotate operations. The shift amount is\nlimited to the range 0..32 in clamp mode and 0..31 in wrap mode, so shifting multi-word\nvalues by distances greater than 32 requires first moving 32-bit words, then using shf to shift\nthe remaining 0..31 distance. To shift data sizes greater than 64 bits to the right, use repeated shf.r instructions applied\nto adjacent words, operating from least-significant word towards most-significant word. At each\nstep, a single word of the shifted result is computed. The most-significant word of the result is\ncomputed using a shr.{u32,s32} instruction, which zero or sign fills based on the instruction\ntype. To shift data sizes greater than 64 bits to the left, use repeated shf.l instructions applied to\nadjacent words, operating from most-significant word towards least-significant word. At each step, a\nsingle word of the shifted result is computed. The least-significant word of the result is computed\nusing a shl instruction. Use funnel shift to perform 32-bit left or right rotate by supplying the same value for source\narguments a and b . PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Requires sm_32 or higher. Example shf.l.clamp.b32  r3,r1,r0,16;\n\n// 128-bit left shift; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] << n\nshf.l.clamp.b32  r7,r2,r3,n;\nshf.l.clamp.b32  r6,r1,r2,n;\nshf.l.clamp.b32  r5,r0,r1,n;\nshl.b32          r4,r0,n;\n\n// 128-bit right shift, arithmetic; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] >> n\nshf.r.clamp.b32  r4,r0,r1,n;\nshf.r.clamp.b32  r5,r1,r2,n;\nshf.r.clamp.b32  r6,r2,r3,n;\nshr.s32          r7,r3,n;     // result is sign-extended\n\nshf.r.clamp.b32  r1,r0,r0,n;  // rotate right by n; n < 32\nshf.l.clamp.b32  r1,r0,r0,n;  // rotate left by n; n < 32\n\n// extract 32-bits from [r1,r0] starting at position n < 32\nshf.r.clamp.b32  r0,r0,r1,n; 9.7.7.8. Logic and Shift Instructions: shl  shl Shift bits left, zero-fill on right. Syntax shl.type d, a, b;\n\n.type = { .b16, .b32, .b64 }; Description Shift a left by the amount specified by unsigned 32-bit value in b . Semantics d = a << b; Notes Shift amounts greater than the register width N are clamped to N . The sizes of the destination and first source operand must match, but not necessarily the type. The b operand must be a 32-bit value, regardless of the instruction type. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Example shl.b32  q,a,2; 9.7.7.9. Logic and Shift Instructions: shr  shr Shift bits right, sign or zero-fill on left. Syntax shr.type d, a, b;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64 }; Description Shift a right by the amount specified by unsigned 32-bit value in b . Signed shifts fill with\nthe sign bit, unsigned and untyped shifts fill with 0 . Semantics d = a >> b; Notes Shift amounts greater than the register width N are clamped to N . The sizes of the destination and first source operand must match, but not necessarily the type. The b operand must be a 32-bit value, regardless of the instruction type. Bit-size types are included for symmetry with shl . PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Example shr.u16  c,a,2;\nshr.s32  i,i,1;\nshr.b16  k,i,j; 9.7.8. Data Movement and Conversion Instructions  These instructions copy data from place to place, and from state space to state space, possibly\nconverting it from one format to another. mov , ld , ldu , and st operate on both\nscalar and vector types. The isspacep instruction is provided to query whether a generic address\nfalls within a particular state space window. The cvta instruction converts addresses between generic and const , global , local , or shared state spaces. Instructions ld , st , suld , and sust support optional cache operations. The Data Movement and Conversion Instructions are: mov shfl.sync prmt ld ldu st st.async multimem.ld_reduce , multimem.st , multimem.red prefetch , prefetchu isspacep cvta cvt cvt.pack cp.async cp.async.commit_group cp.async.wait_group , cp.async.wait_all cp.async.bulk cp.reduce.async.bulk cp.async.bulk.prefetch cp.async.bulk.tensor cp.reduce.async.bulk.tensor cp.async.bulk.prefetch.tensor cp.async.bulk.commit_group cp.async.bulk.wait_group tensormap.replace 9.7.8.1. Cache Operators  PTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache\noperators require a target architecture of sm_20 or higher. Cache operators on load or store instructions are treated as performance hints only. The use of a\ncache operator on an ld or st instruction does not change the memory consistency behavior of\nthe program. For sm_20 and higher, the cache operators have the following definitions and behavior. Table 27 Cache Operators for Memory Load Instructions  Operator Meaning .ca Cache at all levels, likely to be accessed again. The default load instruction cache operation is ld.ca, which allocates cache lines in all\nlevels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but\nmultiple L1 caches are not coherent for global data. If one thread stores to global memory\nvia one L1 cache, and a second thread loads that address via a second L1 cache with ld.ca ,\nthe second thread may get stale L1 cache data, rather than the data stored by the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of parallel threads.\nStores by the first grid program are then correctly fetched by the second grid program issuing\ndefault ld.ca loads cached in L1. .cg Cache at global level (cache in L2 and below, not L1). Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2\ncache. .cs Cache streaming, likely to be accessed once. The ld.cs load cached streaming operation allocates global lines with evict-first policy in\nL1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or\ntwice. When ld.cs is applied to a Local window address, it performs the ld.lu operation. .lu Last use. The compiler/programmer may use ld.lu when restoring spilled registers and popping function\nstack frames to avoid needless write-backs of lines that will not be used again. The ld.lu instruction performs a load cached streaming operation ( ld.cs ) on global addresses. .cv Don’t cache and fetch again (consider cached system memory lines stale, fetch again). The ld.cv load operation applied to a global System Memory address invalidates (discards) a\nmatching L2 line and re-fetches the line on each new load. Table 28 Cache Operators for Memory Store Instructions  Operator Meaning .wb Cache write-back all coherent levels. The default store instruction cache operation is st.wb , which writes back cache lines of\ncoherent cache levels with normal eviction policy. If one thread stores to global memory, bypassing its L1 cache, and a second thread in a\ndifferent SM later loads from that address via a different L1 cache with ld.ca , the second\nthread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored\nby the first thread. The driver must invalidate global L1 cache lines between dependent grids of thread arrays.\nStores by the first grid program are then correctly missed in L1 and fetched by the second grid\nprogram issuing default ld.ca loads. .cg Cache at global level (cache in L2 and below, not L1). Use st.cg to cache global store data only globally, bypassing the L1 cache, and cache only\nin the L2 cache. .cs Cache streaming, likely to be accessed once. The st.cs store cached-streaming operation allocates cache lines with evict-first policy to\nlimit cache pollution by streaming output data. .wt Cache write-through (to system memory). The st.wt store write-through operation applied to a global System Memory address writes\nthrough the L2 cache. 9.7.8.2. Cache Eviction Priority Hints  PTX ISA version 7.4 adds optional cache eviction priority hints on load and store\ninstructions. Cache eviction priority requires target architecture sm_70 or higher. Cache eviction priority on load or store instructions is treated as a performance hint. It is\nsupported for .global state space and generic addresses where the address points to .global state space. Table 29 Cache Eviction Priority Hints for Memory Load and Store Instructions  Cache Eviction Priority Meaning evict_normal Cache data with normal eviction priority. This is the default eviction priority. evict_first Data cached with this priority will be first in the eviction priority order and\nwill likely be evicted when cache eviction is required. This priority is suitable\nfor streaming data. evict_last Data cached with this priority will be last in the eviction priority order and will\nlikely be evicted only after other data with evict_normal or evict_first eviction priotity is already evicted. This priority is suitable for data that\nshould remain persistent in cache. evict_unchanged Do not change eviction priority order as part of this operation. no_allocate Do not allocate data to cache. This priority is suitable for streaming data. 9.7.8.3. Data Movement and Conversion Instructions: mov  mov Set a register variable with the value of a register variable or an immediate value. Take the\nnon-generic address of a variable in global, local, or shared state space. Syntax mov.type  d, a;\nmov.type  d, sreg;\nmov.type  d, avar;       // get address of variable\nmov.type  d, avar+imm;   // get address of variable with offset\nmov.u32   d, fname;      // get address of device function\nmov.u64   d, fname;      // get address of device function\nmov.u32   d, kernel;     // get address of entry function\nmov.u64   d, kernel;     // get address of entry function\n\n.type = { .pred,\n          .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 }; Description Write register d with the value of a . Operand a may be a register, special register, variable with optional offset in an addressable\nmemory space, or function name. For variables declared in .const , .global , .local , and .shared state spaces, mov places the non-generic address of the variable (i.e., the address of the variable in its state\nspace) into the destination register. The generic address of a variable in const , global , local , or shared state space may be generated by first taking the address within the state\nspace with mov and then converting it to a generic address using the cvta instruction;\nalternately, the generic address of a variable declared in const , global , local , or shared state space may be taken directly using the cvta instruction. Note that if the address of a device function parameter is moved to a register, the parameter will\nbe copied onto the stack and the address will be in the local state space. Semantics d = a;\nd = sreg;\nd = &avar;        // address is non-generic; i.e., within the variable's declared state space\nd = &avar+imm; Notes Although only predicate and bit-size types are required, we include the arithmetic types for the\nprogrammer’s convenience: their use enhances program readability and allows additional type\nchecking. When moving address of a kernel or a device function, only .u32 or .u64 instruction types\nare allowed. However, if a signed type is used, it is not treated as a compilation error. The\ncompiler issues a warning in this case. PTX ISA Notes Introduced in PTX ISA version 1.0. Taking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function\naddresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the CUDA\nDynamic Parallelism Programming Guide for details. Target ISA Notes mov.f64 requires sm_13 or higher. Taking the address of kernel entry functions requires sm_35 or higher. Examples mov.f32  d,a;\nmov.u16  u,v;\nmov.f32  k,0.1;\nmov.u32  ptr, A;        // move address of A into ptr\nmov.u32  ptr, A[5];     // move address of A[5] into ptr\nmov.u32  ptr, A+20;     // move address with offset into ptr\nmov.u32  addr, myFunc;  // get address of device function 'myFunc'\nmov.u64  kptr, main;    // get address of entry function 'main' 9.7.8.4. Data Movement and Conversion Instructions: mov  mov Move vector-to-scalar (pack) or scalar-to-vector (unpack). Syntax mov.type  d, a;\n\n.type = { .b16, .b32, .b64, .b128 }; Description Write scalar register d with the packed value of vector register a , or write vector register d with the unpacked values from scalar register a . When destination operand d is a vector register, the sink symbol '_' may be used for one or\nmore elements provided that at least one element is a scalar register. For bit-size types, mov may be used to pack vector elements into a scalar register or unpack\nsub-fields of a scalar register into a vector. Both the overall size of the vector and the size of\nthe scalar must match the size of the instruction type. Semantics // pack two 8-bit elements into .b16\nd = a.x | (a.y << 8)\n// pack four 8-bit elements into .b32\nd = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)\n// pack two 16-bit elements into .b32\nd = a.x | (a.y << 16)\n// pack four 16-bit elements into .b64\nd = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)\n// pack two 32-bit elements into .b64\nd = a.x | (a.y << 32)\n// pack four 32-bit elements into .b128\nd = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)\n// pack two 64-bit elements into .b128\nd = a.x | (a.y << 64)\n\n// unpack 8-bit elements from .b16\n{ d.x, d.y } = { a[0..7], a[8..15] }\n// unpack 8-bit elements from .b32\n{ d.x, d.y, d.z, d.w }\n        { a[0..7], a[8..15], a[16..23], a[24..31] }\n\n// unpack 16-bit elements from .b32\n{ d.x, d.y }  = { a[0..15], a[16..31] }\n// unpack 16-bit elements from .b64\n{ d.x, d.y, d.z, d.w } =\n        { a[0..15], a[16..31], a[32..47], a[48..63] }\n\n// unpack 32-bit elements from .b64\n{ d.x, d.y } = { a[0..31], a[32..63] }\n\n// unpack 32-bit elements from .b128\n{ d.x, d.y, d.z, d.w } =\n        { a[0..31], a[32..63], a[64..95], a[96..127] }\n// unpack 64-bit elements from .b128\n{ d.x, d.y } = { a[0..63], a[64..127] } PTX ISA Notes Introduced in PTX ISA version 1.0. Support for .b128 type introduced in PTX ISA version 8.3. Target ISA Notes Supported on all target architectures. Support for .b128 type requires sm_70 or higher. Examples mov.b32 %r1,{a,b};      // a,b have type .u16\nmov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32\nmov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8\nmov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8\nmov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32\nmov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64\nmov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64 9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated)  shfl (deprecated) Register data shuffle within threads of a warp. Syntax shfl.mode.b32  d[|p], a, b, c;\n\n.mode = { .up, .down, .bfly, .idx }; Deprecation Note The shfl instruction without a .sync qualifier is deprecated in PTX ISA version 6.0. Support for this instruction with .target lower than sm_70 may be removed in a future PTX ISA version. Removal Note Support for shfl instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .target sm_70 or higher. Description Exchange register data between threads of a warp. Each thread in the currently executing warp will compute a source lane index j based on input\noperands b and c and the mode . If the computed source lane index j is in range, the\nthread will copy the input operand a from lane j into its own destination register d ;\notherwise, the thread will simply copy its own input a to destination d . The optional\ndestination predicate p is set to True if the computed source lane is in range, and\notherwise set to False . Note that an out of range value of b may still result in a valid computed source lane index j . In this case, a data transfer occurs and the destination predicate p is True. Note that results are undefined in divergent control flow within a warp, if an active thread sources\na register from an inactive thread. Operand b specifies a source lane or source lane offset, depending on the mode. Operand c contains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index. Semantics lane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);\nminLane = (lane[4:0] & mask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval; PTX ISA Notes Introduced in PTX ISA version 3.0. Deprecated in PTX ISA version 6.0 in favor of shfl.sync . Not supported in PTX ISA version 6.4 for .target sm_70 or higher. Target ISA Notes shfl requires sm_30 or higher. shfl is not supported on sm_70 or higher starting PTX ISA version 6.4. Examples // Warp-level INCLUSIVE PLUS SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;\n@p  add.f32      Rx, Ry, Rx;\n\n\n    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;\n@p  add.f32        Rx, Ry, Rx;\n\n\n    // BUTTERFLY REDUCTION:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    //\n    // All threads now hold sum in Rx 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync  shfl.sync Register data shuffle within threads of a warp. Syntax shfl.sync.mode.b32  d[|p], a, b, c, membermask;\n\n.mode = { .up, .down, .bfly, .idx }; Description Exchange register data between threads of a warp. shfl.sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed shfl.sync with the same qualifiers and same membermask value\nbefore resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s laneid . shfl.sync exchanges register data between threads in membermask . Each thread in the currently executing warp will compute a source lane index j based on input\noperands b and c and the mode . If the computed source lane index j is in range, the\nthread will copy the input operand a from lane j into its own destination register d ;\notherwise, the thread will simply copy its own input a to destination d . The optional\ndestination predicate p is set to True if the computed source lane is in range, and\notherwise set to False . Note that an out of range value of b may still result in a valid computed source lane index j . In this case, a data transfer occurs and the destination predicate p is True. Note that results are undefined if a thread sources a register from an inactive thread or a thread\nthat is not in membermask . Operand b specifies a source lane or source lane offset, depending on the mode. Operand c contains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index. The behavior of shfl.sync is undefined if the executing thread is not in the membermask . Note For .target sm_6x or below, all threads in membermask must execute the same shfl.sync instruction in convergence, and only threads belonging to some membermask can be active when\nthe shfl.sync instruction is executed. Otherwise, the behavior is undefined. Semantics // wait for all threads in membermask to arrive\nwait_for_specified_threads(membermask);\n\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nsegmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);\nminLane = (lane[4:0] & segmask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval; PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples shfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff; 9.7.8.7. Data Movement and Conversion Instructions: prmt  prmt Permute bytes from register pair. Syntax prmt.b32{.mode}  d, a, b, c;\n\n.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 }; Description Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination\nregister. In the generic form (no mode specified), the permute control consists of four 4-bit selection\nvalues. The bytes in the two source registers are numbered from 0 to 7: {b, a} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}} . For each byte in the target register, a 4-bit selection value is defined. The 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the\ntarget position. The msb defines if the byte value should be copied, or if the sign (msb of the\nbyte) should be replicated over all 8 bits of the target position (sign extend of the byte value); msb=0 means copy the literal value; msb=1 means replicate the sign. Note that the sign\nextension is only performed as part of generic form. Thus, the four 4-bit values fully specify an arbitrary byte permute, as a 16b permute code. default mode d.b3 source select d.b2 source select d.b1 source select d.b0 source select index c[15:12] c[11:8] c[7:4] c[3:0] The more specialized form of the permute control uses the two lsb’s of operand c (which is\ntypically an address pointer) to control the byte extraction. mode selector c[1:0] d.b3 source d.b2 source d.b1 source d.b0 source f4e (forward 4 extract) 0 3 2 1 0 1 4 3 2 1 2 5 4 3 2 3 6 5 4 3 b4e (backward 4 extract) 0 5 6 7 0 1 6 7 0 1 2 7 0 1 2 3 0 1 2 3 rc8 (replicate 8) 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 ecl (edge clamp left) 0 3 2 1 0 1 3 2 1 1 2 3 2 2 2 3 3 3 3 3 ecr (edge clamp right) 0 0 0 0 0 1 1 1 1 0 2 2 2 1 0 3 3 2 1 0 rc16 (replicate 16) 0 1 0 1 0 1 3 2 3 2 2 1 0 1 0 3 3 2 3 2 Semantics tmp64 = (b<<32) | a;  // create 8 byte source\n\nif ( ! mode ) {\n   ctl[0] = (c >>  0) & 0xf;\n   ctl[1] = (c >>  4) & 0xf;\n   ctl[2] = (c >>  8) & 0xf;\n   ctl[3] = (c >> 12) & 0xf;\n} else {\n   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;\n}\n\ntmp[07:00] = ReadByte( mode, ctl[0], tmp64 );\ntmp[15:08] = ReadByte( mode, ctl[1], tmp64 );\ntmp[23:16] = ReadByte( mode, ctl[2], tmp64 );\ntmp[31:24] = ReadByte( mode, ctl[3], tmp64 ); PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes prmt requires sm_20 or higher. Examples prmt.b32      r1, r2, r3, r4;\nprmt.b32.f4e  r1, r2, r3, r4; 9.7.8.8. Data Movement and Conversion Instructions: ld  ld Load a register variable from an addressable state space variable. Syntax ld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];\n\nld.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.acquire.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.mmio.relaxed.sys{.global}.type  d, [a];\n\n.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };\n.cop =                      { .ca, .cg, .cs, .lu, .cv };\n.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                              .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 }; Description Load register variable d from the location specified by the source address operand a in\nspecified state space. If no state space is given, perform the load using Generic Addressing . If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands If no sub-qualifier is specified with .param state space, then : ::func is assumed when access is inside a device function. ::entry is assumed when accessing kernel function parameters from entry function. Otherwise, when\naccessing device function parameters or any other .param variables from entry function ::func is assumed by default. For ld.param::entry instruction, operand a must be a kernel parameter address, otherwise behavior\nis undefined. For ld.param::func instruction, operand a must be a device function parameter address,\notherwise behavior is undefined. Instruction ld.param{::func} used for reading value returned from device function call cannot be\npredicated. See Parameter State Space and Function Declarations and Definitions for descriptions\nof the proper use of ld.param . The .relaxed and .acquire qualifiers indicate memory synchronization as described in the Memory Consistency Model . The .scope qualifier\nindicates the set of threads with which an ld.relaxed or ld.acquire instruction can directly\nsynchronize 1 . The .weak qualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means. The semantic details of .mmio qualifier are described in the Memory Consistency Model . Only .sys thread scope is valid for ld.mmio operation. The\nqualifiers .mmio and .relaxed must be specified together. The .weak , .volatile , .relaxed and .acquire qualifiers are mutually exclusive. When\nnone of these is specified, the .weak qualifier is assumed by default. An ld.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location. volatile and non-volatile load operations\nto the same memory location may be reordered. ld.volatile has the same memory synchronization\nsemantics as ld.relaxed.sys . The qualifiers .volatile , .relaxed and .acquire may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio may be used only with .global space and with generic addressing, where the address points to .global space. The optional qualifier .unified must be specified on operand a if a is the address of a\nvariable declared with .unified attribute as described in Variable and Function Attribute\nDirective: .attribute . The qualifier .level::eviction_priority specifies the eviction policy that will be used during\nmemory access. The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively. The qualifier .level::prefetch_size may only be used with .global state space and with\ngeneric addressing where the address points to .global state space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifiers .unified and .level::cache_hint are only supported for .global state\nspace and for generic addressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. 1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model. Semantics d = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See Table 25 for a description of these relaxed type-checking rules. .f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions. .f16x2 data may be loaded using ld.b32 and then used in half precision floating point\ninstructions. PTX ISA Notes ld introduced in PTX ISA version 1.0. ld.volatile introduced in PTX ISA version 1.1. Generic addressing and cache operations introduced in PTX ISA version 2.0. Support for scope qualifier, .relaxed , .acquire , .weak qualifiers introduced in PTX ISA\nversion 6.0. Support for generic addressing of .const space added in PTX ISA version 3.1. Support for .level::eviction_priority , .level::prefetch_size and .level::cache_hint qualifiers introduced in PTX ISA version 7.4. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for .unified qualifier introduced in PTX ISA version 8.0. Support for .mmio qualifier introduced in PTX ISA version 8.2. Support for ::entry and ::func sub-qualifiers on .param space introduced in PTX ISA\nversion 8.3. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes ld.f64 requires sm_13 or higher. Support for scope qualifier, .relaxed , .acquire , .weak qualifiers require sm_70 or\nhigher. Generic addressing requires sm_20 or higher. Cache operations require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::prefetch_size qualifier requires sm_75 or higher. Support for .L2::256B and .L2::cache_hint qualifiers requires sm_80 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .unified qualifier requires sm_90 or higher. Support for .mmio qualifier requires sm_70 or higher. Support for .b128 type requires sm_70 or higher. Examples ld.global.f32    d,[a];\nld.shared.v4.b32 Q,[p];\nld.const.s32     d,[p+4];\nld.local.b32     x,[p+-8]; // negative offset\nld.local.b64     x,[240];  // immediate address\n\nld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg\ncvt.f32.f16      %r,%r;    // up-convert f16 data to f32\n\nld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg\nld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg\nadd.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data\nld.global.relaxed.gpu.u32 %r0, [gbl];\nld.shared.acquire.gpu.u32 %r1, [sh];\nld.global.relaxed.cluster.u32 %r2, [gbl];\nld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];\nld.shared::cluster.u32 %r3, [sh + 8];\nld.global.mmio.relaxed.sys.u32 %r3, [gbl];\n\nld.global.f32    d,[ugbl].unified;\nld.b32           %r0, [%r1].unified;\n\nld.global.L1::evict_last.u32  d, [p];\n\nld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2\nld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2\nld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;\nld.global.L2::cache_hint.b64  x, [p], cache-policy;\nld.param::entry.b32 %rp1, [kparam1];\n\nld.global.b128   %r0, [gbl];   // 128-bit load 9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc  ld.global.nc Load a register variable from global state space via non-coherent cache. Syntax ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};\n\nld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};\nld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};\n\n.cop  =                     { .ca, .cg, .cs };     // cache operation\n.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                              .L1::evict_first, .L1::evict_last, .L1::no_allocate};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.vec  =                     { .v2, .v4 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 }; Description Load register variable d from the location specified by the source address operand a in the\nglobal state space, and optionally cache in non-coherent read-only cache. Note On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than\nthe global memory cache. For applications with sufficient parallelism to cover the longer\nlatency, ld.global.nc should offer better performance than ld.global on such\narchitectures. The address operand a may contain a generic address pointing to the .global state space. Supported addressing modes for operand a and alignment requirements are\ndescribed in Addresses as Operands The qualifier .level::eviction_priority specifies the eviction policy that will be used during\nmemory access. The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. Semantics d = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. .f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt . PTX ISA Notes Introduced in PTX ISA version 3.1. Support for .level::eviction_priority , .level::prefetch_size and .level::cache_hint qualifiers introduced in PTX ISA version 7.4. Support for .b128 type introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_32 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::prefetch_size qualifier requires sm_75 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. Support for .b128 type requires sm_70 or higher. Examples ld.global.nc.f32           d, [a];\nld.gloal.nc.L1::evict_last.u32 d, [a];\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;\nld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;\n\nld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2\nld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2\n\nld.global.nc.b128             d,  [a]; 9.7.8.10. Data Movement and Conversion Instructions: ldu  ldu Load read-only data from an address that is common across threads in the warp. Syntax ldu{.ss}.type      d, [a];       // load from address\nldu{.ss}.vec.type  d, [a];       // vec load from address\n\n.ss   = { .global };             // state space\n.vec  = { .v2, .v4 };\n.type = { .b8, .b16, .b32, .b64, .b128,\n          .u8, .u16, .u32, .u64,\n          .s8, .s16, .s32, .s64,\n                     .f32, .f64 }; Description Load read-only data into register variable d from the location specified by the source address\noperand a in the global state space, where the address is guaranteed to be the same across all\nthreads in the warp. If no state space is given, perform the load using Generic Addressing . Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands Semantics d = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See Table 25 for a description of these relaxed type-checking rules. .f16 data may be loaded using ldu.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions. .f16x2 data may be loaded using ldu.b32 and then used in half precision floating point\ninstructions. PTX ISA Notes Introduced in PTX ISA version 2.0. Support for .b128 type introduced in PTX ISA version 8.3. Target ISA Notes ldu.f64 requires sm_13 or higher. Support for .b128 type requires sm_70 or higher. Examples ldu.global.f32    d,[a];\nldu.global.b32    d,[p+4];\nldu.global.v4.f32 Q,[p];\nldu.global.b128   d,[a]; 9.7.8.11. Data Movement and Conversion Instructions: st  st Store data to an addressable state space variable. Syntax st{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};\nst{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.volatile{.ss}{.vec}.type                           [a], b;\nst.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.release.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.mmio.relaxed.sys{.global}.type         [a], b;\n\n.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };\n.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                              .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level::cache_hint =        { .L2::cache_hint };\n.cop =                      { .wb, .cg, .cs, .wt };\n.sem =                      { .relaxed, .release };\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 }; Description Store the value of operand b in the location specified by the destination address\noperand a in specified state space. If no state space is given, perform the store using Generic\nAddressing . Stores to const memory are illegal. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands If .param is specified without any sub-qualifiers then it defaults to .param::func . Instruction st.param{::func} used for passing arguments to device function cannot be predicated.\nSee Parameter State Space and Function Declarations and\nDefinitions for descriptions of the proper use\nof st.param . The qualifiers .relaxed and .release indicate memory synchronization as described in the Memory Consistency Model . The .scope qualifier\nindicates the set of threads with which an st.relaxed or st.release instruction can directly\nsynchronize 1 . The .weak qualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means. The semantic details of .mmio qualifier are described in the Memory Consistency Model . Only .sys thread scope is valid for st.mmio operation. The\nqualifiers .mmio and .relaxed must be specified together. The .weak , .volatile , .relaxed and .release qualifiers are mutually exclusive. When\nnone of these is specified, the .weak qualifier is assumed by default. An st.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location. st.volatile has the same memory\nsynchronization semantics as st.relaxed.sys . The qualifiers .volatile , .relaxed and .release may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio may be used only with .global space and with generic addressing, where the address points to .global space. The qualifier .level::eviction_priority specifies the eviction policy that will be used during\nmemory access. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. 1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model. Semantics d = a;                // named variable d\n*(&a+immOffset) = b;            // variable-plus-offset\n*a = b;               // register\n*(a+immOffset) = b;   // register-plus-offset\n*(immAddr) = b;       // immediate address Notes Operand b must be in the .reg state space. A source register wider than the specified type may be used. The lower n bits corresponding to\nthe instruction-type width are stored to memory. See Table 24 for a description of these relaxed type-checking rules. .f16 data resulting from a cvt instruction may be stored using st.b16 . .f16x2 data may be stored using st.b32 . PTX ISA Notes st introduced in PTX ISA version 1.0. st.volatile introduced in PTX ISA version 1.1. Generic addressing and cache operations introduced in PTX ISA version 2.0. Support for scope qualifier, .relaxed , .release , .weak qualifiers introduced in PTX ISA\nversion 6.0. Support for .level::eviction_priority and .level::cache_hint qualifiers introduced in PTX\nISA version 7.4. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for .mmio qualifier introduced in PTX ISA version 8.2. Support for ::func sub-qualifier on .param space introduced in PTX ISA version 8.3. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes st.f64 requires sm_13 or higher. Support for scope qualifier, .relaxed , .release , .weak qualifiers require sm_70 or\nhigher. Generic addressing requires sm_20 or higher. Cache operations require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .mmio qualifier requires sm_70 or higher. Support for .b128 type requires sm_70 or higher. Examples st.global.f32    [a],b;\nst.local.b32     [q+4],a;\nst.global.v4.s32 [p],Q;\nst.local.b32     [q+-8],a; // negative offset\nst.local.s32     [100],r7; // immediate address\n\ncvt.f16.f32      %r,%r;    // %r is 32-bit register\nst.b16           [fs],%r;  // store lower\nst.global.relaxed.sys.u32 [gbl], %r0;\nst.shared.release.cta.u32 [sh], %r1;\nst.global.relaxed.cluster.u32 [gbl], %r2;\nst.shared::cta.release.cta.u32 [sh + 4], %r1;\nst.shared::cluster.u32 [sh + 8], %r1;\nst.global.mmio.relaxed.sys.u32 [gbl], %r1;\n\nst.global.L1::no_allocate.f32 [p], a;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nst.global.L2::cache_hint.b32  [a], b, cache-policy;\n\nst.param::func.b64 [param1], %rp1;\n\nst.global.b128  [a], b;  // 128-bit store 9.7.8.12. Data Movement and Conversion Instructions: st.async  st.async Asynchronous store operation on shared memory. Syntax st.async{.weak}{.ss}{.completion_mechanism}{.vec}.type [a], b, [mbar];\n\n.ss   =                 { .shared::cluster };\n.type =                 { .b32, .b64,\n                          .u32, .u64,\n                          .s32, .s64,\n                          .f32, .f64 };\n.vec  =                 { .v2, .v4 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes }; Description st.async is a non-blocking instruction which initiates an asynchronous store operation that\nstores the value specified by source operand b to the destination memory location\nspecified by operand a . The modifier .completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands . The shared memory addresses of destination operand a and the mbarrier object mbar , must\nmeet all of the following conditions: They belong to the same CTA. They are different to the CTA of the executing thread but must be within the same cluster. Otherwise, the behavior is undefined. The state space of the address {.ss} , if specified, is applicable to both operands a and mbar . If not specified, then Generic Addressing is used for\nboth a and mbar . If the generic addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined. The store operation in st.async is treated as a weak memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples st.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr] 9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red  The multimem.* operations operate on multimem addresses and accesses all of the multiple memory\nlocations which the multimem address points to. Multimem addresses can only be accessed only by multimem.* operations. Accessing a multimem address\nwith ld , st or any other memory operations results in undefined behavior. Refer to CUDA programming guide for creation and management of the multimem addresses. multimem.ld_reduce, multimem.st, multimem.red Perform memory operations on the multimem address. Syntax // Integer type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];\nmultimem.st{.stsem}{.scope}{.ss}.type                [a], b;\nmultimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;\n\n.ss =       { .global }\n.ldsem =    { .weak, .relaxed, .acquire }\n.stsem =    { .weak, .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add, .and, .or, .xor }\n.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }\n\n// Floating point type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];\nmultimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;\nmultimem.red{.redsem}{.scope}{.ss}.redop{.vec}.type                 [a], b;\n\n.ss =       { .global }\n.ldsem =    { .weak, .relaxed, .acquire }\n.stsem =    { .weak, .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add }\n.redop  =   { .add }\n.acc_prec = { .acc::f32 }\n.vec =      { .v2, .v4, .v8 }\n.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 } Description Instruction multimem.ld_reduce performs the following operations: load operation on the multimem address a , which involves loading of data from all of the\nmultiple memory locations pointed to by the multimem address a , reduction operation specified by .op on the multiple data loaded from the multimem address a . The result of the reduction operation in returned in register d . Instruction multimem.st performs a store operation of the input operand b to all the memory\nlocations pointed to by the multimem address a . Instruction multimem.red performs a reduction operation on all the memory locations pointed to\nby the multimem address a , with operand b . Instruction multimem.ld_reduce performs reduction on the values loaded from all the memory\nlocations that the multimem address points to. In contrast, the multimem.red perform reduction\non all the memory locations that the multimem address points to. Address operand a must be a multimem address. Otherwise, the behavior is undefined.  Supported\naddressing modes for operand a and alignment requirements are described in Addresses as Operands . If no state space is specified then Generic Addressing is\nused. If the address specified by a does not fall within the address window of .global state\nspace then the behavior is undefined. For floating-point type multi- operations, the size of the specified type along with .vec must\nequal either 32-bits or 64-bits or 128-bits. No other combinations of .vec and type are\nallowed. Type .f64 cannot be used with .vec qualifier. The following table describes the valid combinations of .op and base type: op Base type .add .u32 , .u64 , .s32 .f16 , .f16x2 , .bf16 , .bf16x2 .f32 , .f64 .and , .or , .xor .b32 , .b64 .min , .max .u32 , .s32 , .u64 , .s644 .f16 , .f16x2 , .bf16 , .bf16x2 For multimem.ld_reduce , the default precision of the intermediate accumulation is same as the\nspecified type. Optionally for .f16 , .f16x2 , .bf16 and .bf16x2 types, .acc::f32 can be specified to change the precision of the intermediate accumulation to .f32 . Optional qualifiers .ldsem , .stsem and .redsem specify the memory synchronizing effect\nof the multimem.ld_reduce , multimem.st and multimem.red respectively, as described in Memory Consistency Model . If explicit semantics qualifiers\nare not specified, then multimem.ld_reduce and multimem.st default to .weak and multimem.red defaults to .relaxed . The optional .scope qualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in Memory Consistency Model . If the .scope qualifier is not specified for multimem.red then .sys scope is assumed by default. PTX ISA Notes Introduced in PTX ISA version 8.1. Support for .acc::f32 qualifier introduced in PTX ISA version 8.2. Target ISA Notes Requires sm_90 or higher. Examples multimem.ld_reduce.and.b32                    val1_b32, [addr1];\nmultimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];\n\nmultimem.st.relaxed.gpu.b32                [addr3], val3_b32;\nmultimem.st.release.cta.global.u32         [addr4], val4_u32;\n\nmultimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;\nmultimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};\nmultimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7]; 9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu  prefetch, prefetchu Prefetch line containing a generic address at a specified level of memory hierarchy, in specified\nstate space. Syntax prefetch{.space}.level                    [a];   // prefetch to data cache\nprefetch.global.level::eviction_priority  [a];   // prefetch to data cache\n\nprefetchu.L1  [a];             // prefetch to uniform cache\n\nprefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap\n\n.space =                    { .global, .local };\n.level =                    { .L1, .L2 };\n.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };\n.tensormap_space =          { .const, .param }; Description The prefetch instruction brings the cache line containing the specified address in global or\nlocal memory state space into the specified cache level. If the .tensormap qualifier is specified then the prefetch instruction brings the cache line\ncontaining the specified address in the .const or .param memory state space for subsequent\nuse by the cp.async.bulk.tensor instruction. If no state space is given, the prefetch uses Generic Addressing . Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the\nmodifier .level::eviction_priority . Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands The prefetchu instruction brings the cache line containing the specified generic address into\nthe specified uniform cache level. A prefetch to a shared memory location performs no operation. A prefetch into the uniform cache requires a generic address, and no operation occurs if the\naddress maps to a const , local , or shared memory location. PTX ISA Notes Introduced in PTX ISA version 2.0. Support for .level::eviction_priority qualifier introduced in PTX ISA version 7.4. Support for the .tensormap qualifier is introduced in PTX ISA version 8.0. Target ISA Notes prefetch and prefetchu require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_80 or higher. Support for the .tensormap qualifier requires sm_90 or higher. Examples prefetch.global.L1             [ptr];\nprefetch.global.L2::evict_last [ptr];\nprefetchu.L1  [addr];\nprefetch.global.tensormap      [ptr]; 9.7.8.15. Data Movement and Conversion Instructions: applypriority  applypriority Apply the cache eviction priority to the specified address in the specified cache level. Syntax applypriority{.global}.level::eviction_priority  [a], size;\n\n.level::eviction_priority = { .L2::evict_normal }; Description The applypriority instruction applies the cache eviction priority specified by the .level::eviction_priority qualifier to the address range [a..a+size) in the specified cache\nlevel. If no state space is specified then Generic Addressing is\nused. If the specified address does not fall within the address window of .global state space\nthen the behavior is undefined. The operand size is an integer constant that specifies the amount of data, in bytes, in the\nspecified cache level on which the priority is to be applied. The only supported value for the size operand is 128. Supported addressing modes for operand a are described in Addresses as Operands . a must be aligned to 128 bytes. If the data pointed to by address a is not already present in the specified cache level, then\nthe data will be prefetched before applying the specified priority. PTX ISA Notes Introduced in PTX ISA version 7.4. Target ISA Notes Requires sm_80 or higher. Examples applypriority.global.L2::evict_normal [ptr], 128; 9.7.8.16. Data Movement and Conversion Instructions: discard  discard Invalidate the data in cache at the specified address and cache level. Syntax discard{.global}.level  [a], size;\n\n.level = { .L2 }; Description The discard instruction invalidates the data at the address range [a .. a + (size - 1)] in\nthe cache level specified by the .level qualifier without writing back the data in the cache to\nthe memory. Therefore after the discard operation, the data at the address range [a .. a+ (size - 1)] has undetermined value. The operand size is an integer constant that specifies the amount of data, in bytes, in the\ncache level specified by the .level qualifier to be discarded. The only supported value for the size operand is 128. If no state space is specified then Generic Addressing is\nused. If the specified address does not fall within the address window of .global state space\nthen the behavior is undefined. Supported addressing modes for address operand a are described in Addresses as Operands . a must be aligned to 128 bytes. PTX ISA Notes Introduced in PTX ISA version 7.4. Target ISA Notes Requires sm_80 or higher. Examples discard.global.L2 [ptr], 128; 9.7.8.17. Data Movement and Conversion Instructions: createpolicy  createpolicy Create a cache eviction policy for the specified cache level. Syntax // Range-based policy\ncreatepolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy, [a], primary-size, total-size;\n\n// Fraction-based policy\ncreatepolicy.fractional.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy{, fraction};\n\n// Converting the access property from CUDA APIs\ncreatepolicy.cvt.L2.b64            cache-policy, access-property;\n\n.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,\n                               .L2::evict_first, .L2::evict_unchanged };\n.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged }; Description The createpolicy instruction creates a cache eviction policy for the specified cache level in an\nopaque 64-bit register specified by the destination operand cache-policy . The cache eviction\npolicy specifies how cache eviction priorities are applied to global memory addresses used in memory\noperations with .level::cache_hint qualifier. There are two types of cache eviction policies: Range-based policy The cache eviction policy created using createpolicy.range specifies the cache eviction\nbehaviors for the following three address ranges: [a .. a + (primary-size - 1)] referred to as primary range. [a + primary-size .. a + (total-size - 1)] referred to as trailing secondary range. [a - (total-size - primary-size) .. (a - 1)] referred to as preceding secondary range. When a range-based cache eviction policy is used in a memory operation with .level::cache_hint qualifier, the eviction priorities are applied as follows: If the memory address falls in the primary range, the eviction priority specified by .L2::primary_priority is applied. If the memory address falls in any of the secondary ranges, the eviction priority specified by .L2::secondary_priority is applied. If the memory address does not fall in either of the above ranges, then the applied eviction\npriority is unspecified. The 32-bit operand primary-size specifies the size, in bytes, of the primary range. The\n32-bit operand total-size specifies the combined size, in bytes, of the address range\nincluding primary and secondary ranges. The value of primary-size must be less than or equal\nto the value of total-size . Maximum allowed value of total-size is 4GB. If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged . If no state space is specified then Generic Addressing is\nused. If the specified address does not fall within the address window of .global state space\nthen the behavior is undefined. Fraction-based policy A memory operation with .level::cache_hint qualifier can use the fraction-based cache\neviction policy to request the cache eviction priority specified by .L2:primary_priority to\nbe applied to a fraction of cache accesses specified by the 32-bit floating point operand fraction . The remainder of the cache accesses get the eviction priority specified by .L2::secondary_priority . This implies that in a memory operation that uses a fraction-based\ncache policy, the memory access has a probability specified by the operand fraction of\ngetting the cache eviction priority specified by .L2::primary_priority . The valid range of values for the operand fraction is (0.0,.., 1.0] . If the operand fraction is not specified, it defaults to 1.0. If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged . The access property created using the CUDA APIs can be converted into cache eviction policy by the\ninstruction createpolicy.cvt . The source operand access-property is a 64-bit opaque\nregister. Refer to CUDA programming guide for more details. PTX ISA Notes Introduced in PTX ISA version 7.4. Target ISA Notes Requires sm_80 or higher. Examples createpolicy.fractional.L2::evict_last.b64                      policy, 1.0;\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;\n\ncreatepolicy.range.L2::evict_last.L2::evict_first.b64\n                                            policy, [ptr], 0x100000, 0x200000;\n\n// access-prop is created by CUDA APIs.\ncreatepolicy.cvt.L2.b64 policy, access-prop; 9.7.8.18. Data Movement and Conversion Instructions: isspacep  isspacep Query whether a generic address falls within a specified state space window. Syntax isspacep.space  p, a;    // result is .pred\n\n.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} }; Description Write predicate register p with 1 if generic address a falls within the specified state\nspace window and with 0 otherwise. Destination p has type .pred ; the source address\noperand must be of type .u32 or .u64 . isspacep.param{::entry} returns 1 if the generic address falls within the window of Kernel Function Parameters , otherwise returns 0 . If .param is specified without any sub-qualifiers then it defaults to .param::entry . isspacep.global returns 1 for Kernel Function Parameters as .param window is contained within the .global window. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Note ispacep.shared::cluster will return 1 for every shared memory address that is accessible to\nthe threads in the cluster, whereas ispacep.shared::cta will return 1 only if the address is\nof a variable declared in the executing CTA. PTX ISA Notes Introduced in PTX ISA version 2.0. isspacep.const introduced in PTX ISA version 3.1. isspacep.param introduced in PTX ISA version 7.7. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3. Target ISA Notes isspacep requires sm_20 or higher. isspacep.param{::entry} requires sm_70 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Examples isspacep.const           iscnst, cptr;\nisspacep.global          isglbl, gptr;\nisspacep.local           islcl,  lptr;\nisspacep.shared          isshrd, sptr;\nisspacep.param::entry    isparam, pptr;\nisspacep.shared::cta     isshrdcta, sptr;\nisspacep.shared::cluster ishrdany sptr; 9.7.8.19. Data Movement and Conversion Instructions: cvta  cvta Convert address from .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space to generic, or vice-versa. Take the generic address of a variable declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space. Syntax // convert const, global, local, or shared address to generic address\ncvta.space.size  p, a;        // source address in register a\ncvta.space.size  p, var;      // get generic address of var\ncvta.space.size  p, var+imm;  // generic address of var+offset\n\n// convert generic address to const, global, local, or shared address\ncvta.to.space.size  p, a;\n\n.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\n.size  = { .u32, .u64 }; Description Convert a const , Kernel Function Parameters ( .param ), global , local , or shared address to a generic address, or vice-versa. The\nsource and destination addresses must be the same size. Use cvt.u32.u64 or cvt.u64.u32 to\ntruncate or zero-extend addresses. For variables declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space, the generic address of the variable may be taken using cvta . The source is either a\nregister or a variable defined in const , Kernel Function Parameters ( .param ), global , local , or shared memory\nwith an optional offset. When converting a generic address into a const , Kernel Function Parameters ( .param ), global , local , or shared address, the resulting address is undefined in cases where the generic address does not fall within\nthe address window of the specified state space. A program may use isspacep to guard against\nsuch incorrect behavior. For cvta with .shared state space, the address must belong to the space specified by ::cta or ::cluster sub-qualifier, otherwise the behavior is undefined. If no sub-qualifier\nis specified with .shared state space, then ::cta is assumed by default. If .param is specified without any sub-qualifiers then it defaults to .param::entry . PTX ISA Notes Introduced in PTX ISA version 2.0. cvta.const and cvta.to.const introduced in PTX ISA version 3.1. cvta.param and cvta.to.param introduced in PTX ISA version 7.7. Note: The current implementation does not allow generic pointers to const space variables in\nprograms that contain pointers to constant buffers passed as kernel parameters. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3. Target ISA Notes cvta requires sm_20 or higher. cvta.param{::entry} and cvta.to.param{::entry} requires sm_70 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Examples cvta.const.u32   ptr,cvar;\ncvta.local.u32   ptr,lptr;\ncvta.shared::cta.u32  p,As+4;\ncvta.shared::cluster.u32 ptr, As;\ncvta.to.global.u32  p,gptr;\ncvta.param.u64   ptr,pvar;\ncvta.to.param::entry.u64  epptr, ptr; 9.7.8.20. Data Movement and Conversion Instructions: cvt  cvt Convert a value from one type to another. Syntax cvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding\ncvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding\ncvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;\ncvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;\ncvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;\ncvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;\ncvt.rna{.satfinite}.tf32.f32               d, a;\ncvt.frnd2{.relu}.tf32.f32                  d, a;\ncvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;\ncvt.rn.satfinite{.relu}.f8x2type.f16x2     d, a;\ncvt.rn.{.relu}.f16x2.f8x2type              d, a;\n\n.irnd   = { .rni, .rzi, .rmi, .rpi };\n.frnd   = { .rn,  .rz,  .rm,  .rp  };\n.frnd2  = { .rn,  .rz };\n.dtype = .atype = { .u8,   .u16, .u32, .u64,\n                    .s8,   .s16, .s32, .s64,\n                    .bf16, .f16, .f32, .f64 };\n.f8x2type = { .e4m3x2, .e5m2x2 }; Description Convert between different types and sizes. For .f16x2 and .bf16x2 instruction type, two inputs a and b of .f32 type are\nconverted into .f16 or .bf16 type and the converted values are packed in the destination\nregister d , such that the value converted from input a is stored in the upper half of d and the value converted from input b is stored in the lower half of d For .f16x2 instruction type, destination operand d has .f16x2 or .b32 type. For .bf16 instruction type, operand d has .b16 type. For .bf16x2 instruction type,\noperand d has .b32 type. For .tf32 instruction type, operand d has .b32 type. When converting to .e4m3x2 / .e5m2x2 data formats, the destination operand d has .b16 type. When converting two .f32 inputs to .e4m3x2 / .e5m2x2 , each input is converted to the\nspecified format, and the converted values are packed in the destination operand d such that the\nvalue converted from input a is stored in the upper 8 bits of d and the value converted from\ninput b is stored in the lower 8 bits of d . When converting an .f16x2 input to .e4m3x2 / .e5m2x2 , each .f16 input from operand a is converted to the specified\nformat. The converted values are packed in the destination operand d such that the value\nconverted from the upper 16 bits of input a is stored in the upper 8 bits of d and the value\nconverted from the lower 16 bits of input a is stored in the lower 8 bits of d . When converting from .e4m3x2 / .e5m2x2 to .f16x2 , source operand a has .b16 type. Each 8-bit input value in operand a is converted to .f16 type. The converted values\nare packed in the destination operand d such that the value converted from the upper 8 bits of a is stored in the upper 16 bits of d and the value converted from the lower 8 bits of a is stored in the lower 16 bits of d . Rounding modifier is mandatory in all of the following cases: float-to-float conversions, when destination type is smaller than source type All float-to-int conversions All int-to-float conversions All conversions involving .f16x2 , .e4m3x2, .e5m2x2, .bf16x2 and .tf32 instruction\ntypes. .satfinite modifier is only supported for conversions involving the following types: .e4m3x2 and .e5m2x2 destination types. .satfinite modifier is mandatory for such\nconversions. .f16 , .bf16 , .f16x2 , .bf16x2 as destination types. .tf32 as destination type with rounding mode specified as round to nearest, ties away from\nzero. Semantics if (/* inst type is .f16x2 or .bf16x2 */) {\n    d[31:16] = convert(a);\n    d[15:0]  = convert(b);\n} else {\n    d = convert(a);\n} Integer Notes Integer rounding is required for float-to-integer conversions, and for same-size float-to-float\nconversions where the value is rounded to an integer. Integer rounding is illegal in all other\ninstances. Integer rounding modifiers: .rni round to nearest integer, choosing even integer if source is equidistant between two integers .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity In float-to-integer conversion, NaN inputs are converted to 0. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float\nconversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier .ftz can only be specified when either .dtype or .atype is .f32 and applies only\nto single precision ( .f32 ) inputs and results. sm_1x For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving\nzero. The optional .ftz modifier may be specified in these cases for clarity. Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision\nsubnormal inputs or results to zero if the destination type size was 64-bits. The compiler will\npreserve this behavior for legacy PTX code. Saturation modifier: .sat For integer destination types, .sat limits the result to MININT..MAXINT for the size of\nthe operation. Note that saturation applies to both signed and unsigned integer types. The saturation modifier is allowed only in cases where the destination type’s value range is not\na superset of the source type’s value range; i.e., the .sat modifier is illegal in cases\nwhere saturation is not possible based on the source and destination types. For float-to-integer conversions, the result is clamped to the destination range by default; i.e, .sat is redundant. Floating Point Notes Floating-point rounding is required for float-to-float conversions that result in loss of precision,\nand for integer-to-float conversions. Floating-point rounding is illegal in all other instances. Floating-point rounding modifiers: .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity A floating-point value may be rounded to an integral value using the integer rounding modifiers (see\nInteger Notes). The operands must be of the same size. The result is an integral value, stored in\nfloating-point format. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. Modifier .ftz may be specified to flush\nsingle-precision subnormal inputs and results to sign-preserving zero. Modifier .ftz can only\nbe specified when either .dtype or .atype is .f32 and applies only to single\nprecision ( .f32 ) inputs and results. sm_1x Single-precision subnormal inputs and results are flushed to sign-preserving zero. The optional .ftz modifier may be specified in these cases for clarity. Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush\nsingle-precision subnormal inputs or results to zero if either source or destination type was .f64 . The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX\nISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to\nsign-preserving zero only for cvt.f32.f16 , cvt.f16.f32 , and cvt.f32.f32 instructions. Saturation modifier: .sat : For floating-point destination types, .sat limits the result to the range [0.0, 1.0]. NaN results are flushed to positive zero. Applies to .f16 , .f32 , and .f64 types. .relu : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination types, .relu clamps the result to 0 if negative. NaN results are converted to\ncanonical NaN . .satfinite : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination formats, if the input value is NaN , then the result is NaN in the specified\ndestination format. If the absolute value of input (ignoring sign) is greater than MAX_NORM of\nthe specified destination format, then the result is sign-preserved MAX_NORM of the destination\nformat. Notes A source register wider than the specified type may be used, except when the source operand has .bf16 or .bf16x2 format. The lower n bits corresponding to the instruction-type width\nare used in the conversion. See Operand Size Exceeding Instruction-Type Size for a description of these relaxed\ntype-checking rules. A destination register wider than the specified type may be used, except when the destination\noperand has .bf16 , .bf16x2 or .tf32 format. The result of conversion is sign-extended to\nthe destination register width for signed integers, and is zero-extended to the destination register\nwidth for unsigned, bit-size, and floating-point types. See Operand Size Exceeding Instruction-Type\nSize for a description of these relaxed\ntype-checking rules. For cvt.f32.bf16 , NaN input yields unspecified NaN . PTX ISA Notes Introduced in PTX ISA version 1.0. .relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats\nintroduced in PTX ISA version 7.0. cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16} , cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16 , and cvt.tf32.f32.{relu}.{rn/rz} introduced\nin PTX ISA 7.8. cvt with .e4m3x2 / .e5m2x2 for sm_90 or higher introduced in PTX ISA version 7.8. cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} for sm_90 or higher introduced in PTX ISA version 7.8. cvt with .e4m3x2 / .e5m2x2 for sm_89 introduced in PTX ISA version 8.1. cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} for sm_89 introduced in PTX ISA version 8.1. cvt.satfinite.{f16, bf16, f16x2, bf16x2, tf32}.f32 introduced in PTX ISA version 8.1. Target ISA Notes cvt to or from .f64 requires sm_13 or higher. .relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats require sm_80 or higher. cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16} , cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16 , and cvt.tf32.f32.{relu}.{rn/rz} require sm_90 or higher. cvt with .e4m3x2 / .e5m2x2 requires sm89 or higher. cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} requires sm_89 or higher. Examples cvt.f32.s32 f,i;\ncvt.s32.f64 j,r;     // float-to-int saturates by default\ncvt.rni.f32.f32 x,y; // round to nearest int, result is fp\ncvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets\ncvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode\ncvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs\ncvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output\ncvt.rn.bf16.f32          b, f;        // convert fp32 to bf16\ncvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation\ncvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs\ncvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output\ncvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format\ncvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format\ncvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format\ncvt.bf16.f16.rz          b, f         // convert f16 to bf16 format\ncvt.bf16.u64.rz          b, u         // convert u64 to bf16 format\ncvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format\ncvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format\ncvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output\ncvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu\n                                         // saturation on each output and pack as .e5m2x2\ncvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output 9.7.8.21. Data Movement and Conversion Instructions: cvt.pack  cvt.pack Convert two integer values from one integer type to another and pack the results. Syntax cvt.pack.sat.convertType.abType  d, a, b;\n    .convertType  = { .u16, .s16 }\n    .abType       = { .s32 }\n\ncvt.pack.sat.convertType.abType.cType  d, a, b, c;\n    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }\n    .abType       = { .s32 }\n    .cType        = { .b32 } Description Convert two 32-bit integers a and b into specified type and pack the results into d . Destination d is an unsigned 32-bit integer. Source operands a and b are integers of\ntype .abType and the source operand c is an integer of type .cType . The inputs a and b are converted to values of type specified by .convertType with\nsaturation and the results after conversion are packed into lower bits of d . If operand c is specified then remaining bits of d are copied from lower bits of c . Semantics ta = a < MIN(convertType) ? MIN(convertType) : a;\nta = a > MAX(convertType) ? MAX(convertType) : a;\ntb = b < MIN(convertType) ? MIN(convertType) : b;\ntb = b > MAX(convertType) ? MAX(convertType) : b;\n\nsize = sizeInBits(convertType);\ntd = tb ;\nfor (i = size; i <= 2 * size - 1; i++) {\n    td[i] = ta[i - size];\n}\n\nif (isU16(convertType) || isS16(convertType)) {\n    d = td;\n} else {\n    for (i = 0; i < 2 * size; i++) {\n        d[i] = td[i];\n    }\n    for (i = 2 * size; i <= 31; i++) {\n        d[i] = c[i - 2 * size];\n    }\n} .sat modifier limits the converted values to MIN(convertType) .. MAX(convertedType) (no\noverflow) if the corresponding inputs are not in the range of datatype specified as .convertType . PTX ISA Notes Introduced in PTX ISA version 6.5. Target ISA Notes Requires sm_72 or higher. Sub byte types ( .u4 / .s4 and .u2 / .s2 ) requires sm_75 or higher. Examples cvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion\ncvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion\ncvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }\ncvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion\ncvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion 9.7.8.22. Data Movement and Conversion Instructions: mapa  mapa Map the address of the shared variable in the target CTA. Syntax mapa{.space}.type          d, a, b;\n\n// Maps shared memory address in register a into CTA b.\nmapa.shared::cluster.type  d, a, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh + imm, b;\n\n// Maps generic address in register a into CTA b.\nmapa.type                  d, a, b;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 } Description Get address in the CTA specified by operand b which corresponds to the address specified by\noperand a . Instruction type .type indicates the type of the destination operand d and the source\noperand a . When space is .shared::cluster , source a is either a shared memory variable or a register\ncontaining a valid shared memory address and register d contains a shared memory address. When\nthe optional qualifier .space is not specified, both a and d are registers containing\ngeneric addresses pointing to shared memory. b is a 32-bit integer operand representing the rank of the target CTA. Destination register d will hold an address in CTA b corresponding to operand a . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples mapa.shared::cluster.u64 d1, %reg1, cta;\nmapa.shared::cluster.u32 d2, sh, 3;\nmapa.u64                 d3, %reg2, cta; 9.7.8.23. Data Movement and Conversion Instructions: getctarank  getctarank Generate the CTA rank of the address. Syntax getctarank{.space}.type d, a;\n\n// Get cta rank from source shared memory address in register a.\ngetctarank.shared::cluster.type d, a;\n\n// Get cta rank from shared memory variable.\ngetctarank.shared::cluster.type d, var;\n\n// Get cta rank from shared memory variable+offset.\ngetctarank.shared::cluster.type d, var + imm;\n\n// Get cta rank from generic address of shared memory variable in register a.\ngetctarank.type d, a;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 } Description Write the destination register d with the rank of the CTA which contains the address specified\nin operand a . Instruction type .type indicates the type of source operand a . When space is .shared::cluster , source a is either a shared memory variable or a register\ncontaining a valid shared memory address. When the optional qualifier .space is not specified, a is a register containing a generic addresses pointing to shared memory. Destination d is\nalways a 32-bit register which holds the rank of the CTA. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples getctarank.shared::cluster.u32 d1, addr;\ngetctarank.shared::cluster.u64 d2, sh + 4;\ngetctarank.u64                 d3, src; 9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copy  An asynchronous copy operation performs the underlying operation asynchronously in the background,\nthus allowing the issuing threads to perform subsequent tasks. An asynchronous copy operation can be a bulk operation that operates on a large amount of data, or\na non-bulk operation that operates on smaller sized data. The amount of data handled by a bulk\nasynchronous operation must be a multiple of 16 bytes. 9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operations  A thread must explicitly wait for the completion of an asynchronous copy operation in order to\naccess the result of the operation. Once an asynchronous copy operation is initiated, modifying the\nsource memory location or reading from the destination memory location before the asynchronous\noperation completes, will cause unpredictable results. This section describes two asynchronous copy operation completion mechanisms supported in PTX:\nAsync-group mechanism and mbarrier-based mechanism. Async-group mechanism When using the async-group completion mechanism, the issuing thread specifies a group of\nasynchronous operations, called async-group , using a commit operation and tracks the completion\nof this group using a wait operation. The thread issuing the asynchronous operation must create\nseparate async-groups for bulk and non-bulk asynchronous operations. A commit operation creates a per-thread async-group containing all prior asynchronous operations\ninitiated by the executing thread but none of the asynchronous operations following the commit\noperation. A committed asynchronous operation belongs to a single async-group . When an async-group completes, all the asynchronous operations belonging to that group are\ncomplete and the executing thread that initiated the asynchronous operations can read the result of\nthe asynchronous operations. All async-groups committed by an executing thread always complete in\nthe order in which they were committed. There is no ordering between asynchronous operations within\nan async-group . A typical pattern of using async-group as the completion mechanism is as follows: Initiate the asynchronous operations. Group the asynchronous operations into an async-group using a commit operation. Wait for the completion of the async-group using the wait operation. Once the async-group completes, access the results of all asynchronous operations in that async-group . Mbarrier-based mechanism A thread can track the completion of one or more asynchronous operations using the current phase of\nan mbarrier object . When the current phase of the mbarrier object is complete, it implies that\nall asynchronous operations tracked by this phase are complete, and all threads participating in\nthat mbarrier object can access the result of the asynchronous operations. The mbarrier object to be used for tracking the completion of an asynchronous operation can be\neither specified along with the asynchronous operation as part of its syntax, or as a separate\noperation. For a bulk asynchronous operation, the mbarrier object must be specified in the\nasynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous\noperation. A typical pattern of using mbarrier-based completion mechanism is as follows: Initiate the asynchronous operations. Set up an mbarrier object to track the asynchronous operations in its current phase, either as\npart of the asynchronous operation or as a separate operation. Wait for the mbarrier object to complete its current phase using mbarrier.test_wait or mbarrier.try_wait . Once the mbarrier.test_wait or mbarrier.try_wait operation returns True , access the\nresults of the asynchronous operations tracked by the mbarrier object . 9.7.8.24.2. Async Proxy  The cp{.reduce}.async.bulk operations are performed in the asynchronous proxy (or async\nproxy ). Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async proxy , fence.proxy.async should be used to synchronize memory between generic\nproxy and the async proxy . The completion of a cp{.reduce}.async.bulk operation is followed by an implicit generic-async proxy fence. So the result of the asynchronous operation is made visible to the generic proxy as\nsoon as its completion is observed. Async-group OR mbarrier-based completion mechanism must\nbe used to wait for the completion of the cp{.reduce}.async.bulk instructions. 9.7.8.24.3. Data Movement and Conversion Instructions: cp.async  cp.async Initiates an asynchronous copy operation from one state space to another. Syntax cp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, src-size}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, src-size}{, cache-policy} ;\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, ignore-src}{, cache-policy} ;\n\n.level::cache_hint =     { .L2::cache_hint }\n.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }\ncp-size =                { 4, 8, 16 } Description cp.async is a non-blocking instruction which initiates an asynchronous copy operation of data\nfrom the location specified by source address operand src to the location specified by\ndestination address operand dst . Operand src specifies a location in the global state space\nand dst specifies a location in the shared state space. Operand cp-size is an integer constant which specifies the size of data in bytes to be copied to\nthe destination dst . cp-size can only be 4, 8 and 16. Instruction cp.async allows optionally specifying a 32-bit integer operand src-size . Operand src-size represents the size of the data in bytes to be copied from src to dst and must\nbe less than cp-size . In such case, remaining bytes in destination dst are filled with\nzeros. Specifying src-size larger than cp-size results in undefined behavior. The optional and non-immediate predicate argument ignore-src specifies whether the data from the\nsource location src should be ignored completely. If the source data is ignored then zeros will\nbe copied to destination dst . If the argument ignore-src is not specified then it defaults\nto False . Supported alignment requirements and addressing modes for operand src and dst are described\nin Addresses as Operands . The mandatory .async qualifier indicates that the cp instruction will initiate the memory\ncopy operation asynchronously and control will return to the executing thread before the copy\noperation is complete. The executing thread can then use cp.async.wait_all or cp.async.wait_group or mbarrier instructions to wait for\ncompletion of the asynchronous copy operation. No other synchronization mechanisms described in Memory Consistency Model can be used to guarantee the\ncompletion of the asynchronous copy operations. There is no ordering guarantee between two cp.async operations if they are not explicitly\nsynchronized using cp.async.wait_all or cp.async.wait_group or mbarrier instructions . As described in Cache Operators , the .cg qualifier indicates\ncaching of data only at global level cache L2 and not at L1 whereas .ca qualifier indicates\ncaching of data at all levels including L1 cache. Cache operator are treated as performance hints\nonly. cp.async is treated as a weak memory operation in the Memory Consistency Model . The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively. The qualifier .level::prefetch_size may only be used with .global state space and with\ngeneric addressing where the address points to .global state space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. PTX ISA Notes Introduced in PTX ISA version 7.0. Support for .level::cache_hint and .level::prefetch_size qualifiers introduced in PTX ISA\nversion 7.4. Support for ignore-src operand introduced in PTX ISA version 7.5. Support for sub-qualifier ::cta introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Sub-qualifier ::cta requires sm_30 or higher. Examples cp.async.ca.shared.global  [shrd],    [gbl + 4], 4;\ncp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;\ncp.async.cg.shared.global  [%r2],     [%r3],     16;\n\ncp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;\ncp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;\ncp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;\ncp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;\n\ncp.async.ca.shared.global                   [shrd], [gbl], 4, p;\ncp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy; 9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_group  cp.async.commit_group Commits all prior initiated but uncommitted cp.async instructions into a cp.async-group . Syntax cp.async.commit_group ; Description cp.async.commit_group instruction creates a new cp.async-group per thread and batches all\nprior cp.async instructions initiated by the executing thread but not committed to any cp.async-group into the new cp.async-group . If there are no uncommitted cp.async instructions then cp.async.commit_group results in an empty cp.async-group. An executing thread can wait for the completion of all cp.async operations in a cp.async-group using cp.async.wait_group . There is no memory ordering guarantee provided between any two cp.async operations within the\nsame cp.async-group . So two or more cp.async operations within a cp.async-group copying data\nto the same location results in undefined behavior. PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Examples // Example 1:\ncp.async.ca.shared.global [shrd], [gbl], 4;\ncp.async.commit_group ; // Marks the end of a cp.async group\n\n// Example 2:\ncp.async.ca.shared.global [shrd1],   [gbl1],   8;\ncp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;\ncp.async.commit_group ; // Marks the end of cp.async group 1\n\ncp.async.ca.shared.global [shrd2],    [gbl2],    16;\ncp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;\ncp.async.commit_group ; // Marks the end of cp.async group 2 9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all  cp.async.wait_group/cp.async.wait_all Wait for completion of prior asynchronous copy operations. Syntax cp.async.wait_group N;\ncp.async.wait_all ; Description cp.async.wait_group instruction will cause executing thread to wait till only N or fewer of\nthe most recent cp.async-group s are pending and all the prior cp.async-group s committed by\nthe executing threads are complete. For example, when N is 0, the executing thread waits on all\nthe prior cp.async-group s to complete. Operand N is an integer constant. cp.async.wait_all is equivalent to : cp.async.commit_group;\ncp.async.wait_group 0; An empty cp.async-group is considered to be trivially complete. Writes performed by cp.async operations are made visible to the executing thread only after: The completion of cp.async.wait_all or The completion of cp.async.wait_group on the cp.async-group in which the cp.async belongs to or mbarrier.test_wait returns True on an mbarrier object which is tracking the completion of the cp.async operation. There is no ordering between two cp.async operations that are not synchronized with cp.async.wait_all or cp.async.wait_group or mbarrier objects . cp.async.wait_group and cp.async.wait_all does not provide any ordering and visibility\nguarantees for any other memory operation apart from cp.async . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Examples // Example of .wait_all:\ncp.async.ca.shared.global [shrd1], [gbl1], 4;\ncp.async.cg.shared.global [shrd2], [gbl2], 16;\ncp.async.wait_all;  // waits for all prior cp.async to complete\n\n// Example of .wait_group :\ncp.async.ca.shared.global [shrd3], [gbl3], 8;\ncp.async.commit_group;  // End of group 1\n\ncp.async.cg.shared.global [shrd4], [gbl4], 16;\ncp.async.commit_group;  // End of group 2\n\ncp.async.cg.shared.global [shrd5], [gbl5], 16;\ncp.async.commit_group;  // End of group 3\n\ncp.async.wait_group 1;  // waits for group 1 and group 2 to complete 9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulk  cp.async.bulk Initiates an asynchronous copy operation from one state space to another. Syntax cp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\ncp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n\n\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint} [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint =    { .L2::cache_hint } Description cp.async.bulk is a non-blocking instruction which initiates an asynchronous bulk-copy operation\nfrom the location specified by source address operand srcMem to the location specified by\ndestination address operand dstMem . The direction of bulk-copy is from the state space specified by the .src modifier to the state\nspace specified by the .dst modifiers. The 32-bit operand size specifies the amount of memory to be copied, in terms of number of\nbytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory\nspace and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory\nspace. Otherwise, the behavior is undefined. The addresses dstMem and srcMem must be aligned\nto 16 bytes. When the source of the copy is .shared::cta and the destination is .shared::cluster , the\ndestination has to be in the shared memory of a different CTA within the cluster. The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .shared::cluster .shared::cta .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk variant uses\nmbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.async.bulk variant uses bulk async-group based completion mechanism. The optional modifier .multicast::cluster allows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the\ncluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA. The source data is multicast to the same CTA-relative offset as dstMem in the shared memory of each destination CTA. The mbarrier signal is also multicast to the same\nCTA-relative offset as mbar in the shared memory of the destination CTA. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. The copy operation in cp.async.bulk is treated as a weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Notes .multicast::cluster qualifier is optimized for target architecture sm_90a and may have\nsubstantially reduced performance on other targets and hence .multicast::cluster is advised to\nbe used with .target sm_90a . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. .multicast::cluster qualifier advised to be used with .target sm_90a . Examples // .global -> .shared::cluster:\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                                             [dstMem], [srcMem], size, [mbar], ctaMask;\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n\n// .shared::cta -> .shared::cluster (strictly remote):\ncp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\n// .shared::cta -> .global:\ncp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy; 9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk  cp.reduce.async.bulk Initiates an asynchronous reduction operation. Syntax cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type\n              [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .b32, .u32, .s32, .b64, .u64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type\n               [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global      }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint    = { .L2::cache_hint }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type\n               [dstMem], [srcMem], size{, cache-policy}\n.dst  =                 { .global }\n.src  =                 { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.type =                 { .f16, .bf16 } Description cp.reduce.async.bulk is a non-blocking instruction which initiates an asynchronous reduction\noperation on an array of memory locations specified by the destination address operand dstMem with the source array whose location is specified by the source address operand srcMem . The size\nof the source and the destination array must be the same and is specified by the operand size . Each data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier .redOp . The type of each\ndata element in the source and the destination array is specified by the modifier .type . The source address operand srcMem is located in the state space specified by .src and the\ndestination address operand dstMem is located in the state specified by the .dst . The 32-bit operand size specifies the amount of memory to be copied from the source location and\nused in the reduction operation, in terms of number of bytes. size must be a multiple of 16. If\nthe value is not a multiple of 16, then the behavior is undefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory space. Otherwise, the behavior is\nundefined. The addresses dstMem and srcMem must be aligned to 16 bytes. The operations supported by .redOp are classified as follows: The bit-size operations are .and , .or , and .xor . The integer operations are .add , .inc , .dec , .min , and .max . The .inc and .dec operations return a result in the range [0..x] where x is the value at the source\nstate space. The floating point operation .add rounds to the nearest even. The current implementation of cp.reduce.async.bulk.add.f32 flushes subnormal inputs and results to sign-preserving zero. The cp.reduce.async.bulk.add.f16 and cp.reduce.async.bulk.add.bf16 operations require .noftz qualifier. It preserves input and result subnormals, and does not flush them to zero. The following table describes the valid combinations of .redOp and element type: .dst .redOp Element type .shared::cluster .add .u32 , .s32 , .u64 .min , .max .u32 , .s32 .inc , .dec .u32 .and , .or , .xor .b32 .global .add .u32 , .s32 , .u64 , .f32 , .f64 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .shared::cluster .shared::cta .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.reduce.async.bulk variant\nuses mbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.reduce.async.bulk variant uses bulk\nasync-group based completion mechanism. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. Each reduction operation performed by the cp.reduce.async.bulk has individually .relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk are treated as weak\nmemory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size; 9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch  cp.async.bulk.prefetch Provides a hint to the system to initiate the asynchronous prefetch of data to the cache. Syntax cp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}\n\n.src =                { .global }\n.level::cache_hint =  { .L2::cache_hint } Description cp.async.bulk.prefetch is a non-blocking instruction which may initiate an asynchronous prefetch\nof data from the location specified by source address operand srcMem , in .src statespace, to\nthe L2 cache. The 32-bit operand size specifies the amount of memory to be prefetched in terms of number of\nbytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory\nspace and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory\nspace. Otherwise, the behavior is undefined. The address srcMem must be aligned to 16 bytes. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.async.bulk.prefetch.L2.global                 [srcMem], size;\n\ncp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy; 9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor  cp.async.bulk.tensor Initiates an asynchronous copy operation on the tensor data from one state space to another. Syntax // global -> shared::cluster:\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colOffsets}\n                                   {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.load_mode =            { .tile, .im2col }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> global:\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}\n                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.level::cache_hint =    { .L2::cache_hint } Description cp.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous copy\noperation of tensor data from the location in .src state space to the location in the .dst state space. The operand dstMem specifies the location in the .dst state space into which the tensor data\nhas to be copied and srcMem specifies the location in the .src state space from which the\ntensor data has to be copied. The operand tensorMap is the generic address of the opaque tensor-map object which resides\neither in .param space or .const space or .global space. The operand tensorMap specifies\nthe properties of the tensor copy operation, as described in Tensor-map .\nThe tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating\nthe tensor-map objects on the host side. The dimension of the tensor data is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The number of tensor\ncoordinates in the vector argument tensorCoords should be equal to the dimension specified by\nthe modifier .dim . The individual tensor coordinates in tensorCoords are of type .s32 . The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk.tensor variant\nuses mbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.async.bulk.tensor variant uses bulk\nasync-group based completion mechanism. The qualifier .load_mode specifies how the data in the source location is copied into the\ndestination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column\nat the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least\n3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is\n.im2col. The length of the vector operand im2colOffsets is two less than the number of dimension .dim of the tensor operation. The modifier .im2col_no_offs is the same as .im2col mode\nexcept there is no im2colOffsets vector involved. The optional modifier .multicast::cluster allows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the\ncluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA. The source data is multicast to the same offset as dstMem in the shared\nmemory of each destination CTA. The mbarrier signal is also multicast to the same offset as mbar in the shared memory of the destination CTA. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. The copy operation in cp.async.bulk.tensor is treated as a weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Notes .multicast::cluster qualifier is optimized for target architecture sm_90a and may have\nsubstantially reduced performance on other targets and hence .multicast::cluster is advised to\nbe used with .target sm_90a . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. .multicast::cluster qualifier advised to be used with .target sm_90a . Examples .reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;\n\n@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;\n\n@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3]; 9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor  cp.reduce.async.bulk.tensor Initiates an asynchronous reduction operation on the tensor data. Syntax // shared::cta -> global:\ncp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}\n                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor} Description cp.reduce.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous\nreduction operation of tensor data in the .dst state space with tensor data in the .src state space. The operand srcMem specifies the location of the tensor data in the .src state space using\nwhich the reduction operation has to be performed. The operand tensorMap is the generic address of the opaque tensor-map object which resides\neither in .param space or .const space or .global space. The operand tensorMap specifies\nthe properties of the tensor copy operation, as described in Tensor-map .\nThe tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating\nthe tensor-map objects on the host side. Each element of the tensor data in the .dst state space is reduced inline with the corresponding\nelement from the tensor data in the .src state space. The modifier .redOp specifies the\nreduction operation used for the inline reduction. The type of each tensor data element in the\nsource and the destination tensor is specified in Tensor-map . The dimension of the tensor is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates of the tensor data in the\nglobal memory on which the reduce operation is to be performed. The number of tensor coordinates in\nthe vector argument tensorCoords should be equal to the dimension specified by the modifier .dim . The individual tensor coordinates are of the type .s32 . The following table describes the valid combinations of .redOp and element type: .redOp Element type .add .u32 , .s32 , .u64 , .f32 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the\ninstruction variant. Value .bulk_group of the modifier .completion_mechanism specifies that cp.reduce.async.bulk.tensor instruction uses bulk async-group based completion mechanism. The qualifier .load_mode specifies how the data in the source location is copied into the\ndestination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col_no_offs mode, some dimensions of the source tensors are unrolled in a single dimensional\ncolumn at the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least\n3-dimensional. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. Each reduction operation performed by cp.reduce.async.bulk.tensor has individually .relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk.tensor are treated as weak memory operations and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group\n                                             [tensorMap0, {tc0}], [sMem0];\n\ncp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint\n                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;\n\ncp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group\n                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2] 9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor  cp.async.bulk.prefetch.tensor Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache. Syntax // global -> shared::cluster:\ncp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]\n                                                             {, im2colOffsets } {, cache-policy}\n\n.src =                { .global }\n.dim =                { .1d, .2d, .3d, .4d, .5d }\n.load_mode =          { .tile, .im2col }\n.level::cache_hint =  { .L2::cache_hint } Description cp.async.bulk.prefetch.tensor is a non-blocking instruction which may initiate an asynchronous\nprefetch of tensor data from the location in .src statespace to the L2 cache. The operand tensorMap is the generic address of the opaque tensor-map object which resides\neither in .param space or .const space or .global space. The operand tensorMap specifies\nthe properties of the tensor copy operation, as described in Tensor-map .\nThe tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating\nthe tensor-map objects on the host side. The dimension of the tensor data is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The number of tensor\ncoordinates in the vector argument tensorCoords should be equal to the dimension specified by\nthe modifier .dim . The individual tensor coordinates in tensorCoords are of type .s32 . The qualifier .load_mode specifies how the data in the source location is copied into the\ndestination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column\nat the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least\n3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is\n.im2col. The length of the vector operand im2colOffsets is two less than the number of dimension .dim of the tensor operation. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. cp.async.bulk.prefetch.tensor is treated as a weak memory operation in the Memory Consistency\nModel . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples .reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];\n\n@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];\n\n@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col\n                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint\n                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy; 9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group  cp.async.bulk.commit_group Commits all prior initiated but uncommitted cp.async.bulk instructions into a cp.async.bulk-group . Syntax cp.async.bulk.commit_group; Description cp.async.bulk.commit_group instruction creates a new per-thread bulk async-group and batches\nall prior cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions satisfying the following\nconditions into the new bulk async-group : The prior cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions use bulk_group based\ncompletion mechanism, and They are initiated by the executing thread but not committed to any bulk async-group . If there are no uncommitted cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions then cp.async.bulk.commit_group results in an empty bulk async-group . An executing thread can wait for the completion of all cp{.reduce}.async.bulk.{.prefetch}{.tensor} operations in a bulk async-group using cp.async.wait_group . There is no memory ordering guarantee provided between any two cp{.reduce}.async.bulk.{.prefetch}{.tensor} operations within the same bulk async-group . PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.async.bulk.commit_group; 9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group  cp.async.bulk.wait_group Wait for completion of bulk async-groups . Syntax cp.async.bulk.wait_group{.read} N; Description cp.async.bulk.wait_group instruction will cause the executing thread to wait until only N or\nfewer of the most recent bulk async-groups are pending and all the prior bulk async-groups committed by the executing threads are complete. For example, when N is 0, the executing thread\nwaits on all the prior bulk async-groups to complete. Operand N is an integer constant. By default, cp.async.bulk.wait_group instruction will cause the executing thread to wait till\nall the bulk async operations in the specified bulk async-group have completed all of the\nfollowing: Reading from the source locations. Writing to their respective destination locations. Writes being made visible to the executing thread. The optional .read modifier indicates that the waiting has to be done until all the bulk async\noperations in the specified bulk async-group have completed reading from their source locations. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples cp.async.bulk.wait_group.read   0;\ncp.async.bulk.wait_group        2; 9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace  tensormap.replace Modifies the field of a tensor-map object. Syntax tensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;\ntensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;\ntensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;\n\n.mode    = { .tile }\n.field1  = { .global_address, .rank }\n.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }\n.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .fill_mode }\n.ss      = { .global, .shared::cta }\n.type    = { .b32, .b64 } Description The tensormap.replace instruction replaces the field, specified by .field qualifier,\nof the tensor-map object at the location specified by the address operand addr with a\nnew value. The new value is specified by the argument new_val . Qualifier .mode specifies the mode of the tensor-map object\nlocated at the address operand addr . Instruction type .b1024 indicates the size of the tensor-map object, which is 1024 bits. Operand new_val has the type .type . When .field is specified as .global_address or .global_stride , .type must be .b64 . Otherwise, .type must be .b32 . The immediate integer operand ord specifies the ordinal of the field across the rank of the\ntensor which needs to be replaced in the tensor-map object. For field .rank , the operand new_val must be ones less than the desired tensor rank as\nthis field uses zero-based numbering. When .field3 is specified, the operand new_val must be an immediate and the Table 30 shows the mapping of the operand new_val across various fields. Table 30 Tensormap new_val validity  new_val .field3 .elemtype .interleave_layout .swizzle_mode .fill_mode 0 .u8 No interleave No swizzling Zero fill 1 .u16 16B interleave 32B swizzling OOB-NaN fill 2 .u32 32B interleave 64B swizzling x 3 .s32 x 128B swizzling x 4 .u64 x x x 5 .s64 x x x 6 .f16 x x x 7 .f32 x x x 8 .f32.ftz x x x 9 .f64 x x x 10 .bf16 x x x 11 .tf32 x x x 12 .tf32.ftz x x x If no state space is specified then Generic Addressing is used.\nIf the address specified by addr does not fall within the address window of .global or .shared::cta state space then the behavior is undefined. tensormap.replace is treated as a weak memory operation, on the entire 1024-bit opaque tensor-map object, in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_90a . Examples tensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val; 9.7.9. Texture Instructions  This section describes PTX instructions for accessing textures and samplers. PTX supports the\nfollowing operations on texture and sampler descriptors: Static initialization of texture and sampler descriptors. Module-scope and per-entry scope definitions of texture and sampler descriptors. Ability to query fields within texture and sampler descriptors. 9.7.9.1. Texturing Modes  For working with textures and samplers, PTX has two modes of operation. In the unified mode, texture and sampler information is accessed through a single .texref handle. In the independent\nmode , texture and sampler information each have their own handle, allowing them to be defined\nseparately and combined at the site of usage in the program. The advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior\nto sm_3x ), with the restriction that they correspond 1-to-1 with the 256 possible textures per\nkernel (128 for architectures prior to sm_3x ). The advantage of independent mode is that\ntextures and samplers can be mixed and matched, but the number of samplers is greatly restricted to\n32 per kernel (16 for architectures prior to sm_3x ). Table 31 summarizes the number of textures, samplers and\nsurfaces available in different texturing modes. Table 31 Texture, sampler and surface limits  Texturing mode Resource sm_1x , sm_2x sm_3x+ Unified mode Textures 128 256 Samplers 128 256 Surfaces 8 16 Independent mode Textures 128 256 Samplers 16 32 Surfaces 8 16 The texturing mode is selected using .target options texmode_unified and texmode_independent . A PTX module may declare only one texturing mode. If no texturing mode is\ndeclared, the module is assumed to use unified mode. Example : calculate an element’s power contribution as element’s power/total number of elements. .target texmode_independent\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n...\n.entry compute_power\n  ( .param .texref tex1 )\n{\n  txq.width.b32  r6, [tex1]; // get tex1's width\n  txq.height.b32 r5, [tex1]; // get tex1's height\n  tex.2d.v4.f32.f32  {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}];\n  mul.u32 r5, r5, r6;\n  add.f32 r1, r1, r2;\n  add.f32 r3, r3, r4;\n  add.f32 r1, r1, r3;\n  cvt.f32.u32 r5, r5;\n  div.f32 r1, r1, r5;\n} 9.7.9.2. Mipmaps  A mipmap is a sequence of textures, each of which is a progressively lower resolution\nrepresentation of the same image. The height and width of each image, or level of detail (LOD), in\nthe mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics\napplications to improve rendering speed and reduce aliasing artifacts. For example, a\nhigh-resolution mipmap image is used for objects that are close to the user; lower-resolution images\nare used as the object appears farther away. Mipmap filtering modes are provided when switching\nbetween two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity. Example: If the texture has a basic size of 256 by 256 pixels, then the associated mipmap set\nmay contain a series of eight images, each one-fourth the total area of the previous one: 128×128\npixels, 64×64, 32×32, 16×16, 8×8, 4×4, 2×2, 1×1 (a single pixel). If, for example, a scene is\nrendering this texture in a space of 40×40 pixels, then either a scaled up version of the 32×32\n(without trilinear interpolation) or an interpolation of the 64×64 and the 32×32 mipmaps (with\ntrilinear interpolation) would be used. The total number of LODs in a complete mipmap pyramid is calculated through the following equation: numLODs = 1 + floor(log2(max(w, h, d))) The finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st\nlevel, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller\nmipmap level has half the {width, height, depth} of the previous level, but if this half value is a\nfractional value, it’s rounded down to the next largest integer. Essentially, the size of a mipmap\nlevel can be specified as: max(1, floor(w_b / 2^i)) x\nmax(1, floor(h_b / 2^i)) x\nmax(1, floor(d_b / 2^i)) where i is the ith level beyond the 0th level (the base level). And w_b , h_b and d_b are the\nwidth, height and depth of the base level respectively. PTX support for mipmaps The PTX tex instruction supports three modes for specifying the LOD: base , level , and grad ient. In base mode, the instruction always picks level 0. In level mode, an additional\nargument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector\narguments provide partials (e.g., {ds/dx, dt/dx} and {ds/dy, dt/dy} for a 2d texture),\nwhich the tex instruction uses to compute the LOD. These instructions provide access to texture memory. tex tld4 txq 9.7.9.3. Texture Instructions: tex  tex Perform a texture memory lookup. Syntax tex.geom.v4.dtype.ctype  d, [a, c] {, e} {, f};\ntex.geom.v4.dtype.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\ntex.geom.v2.f16x2.ctype  d[|p], [a, c] {, e} {, f};\ntex.geom.v2.f16x2.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n// mipmaps\ntex.base.geom.v4.dtype.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v4.dtype.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v4.dtype.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\ntex.base.geom.v2.f16x2.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v2.f16x2.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v2.f16x2.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms };\n.dtype = { .u32, .s32, .f16,  .f32 };\n.ctype = {       .s32, .f32 };          // .cube, .acube require .f32\n                                        // .2dms, .a2dms require .s32 Description tex.{1d,2d,3d} Texture lookup using a texture coordinate vector. The instruction loads data from the texture named\nby operand a at coordinates given by operand c into destination d . Operand c is a\nscalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a\nfour-element vector for 3d textures, where the fourth element is ignored. An optional texture\nsampler b may be specified. If no sampler is specified, the sampler behavior is a property of\nthe named texture. The optional destination predicate p is set to True if data from texture\nat specified coordinates is resident in memory, False otherwise. When optional destination\npredicate p is set to False , data loaded will be all zeros. Memory residency of Texture Data\nat specified coordinates is dependent on execution environment setup using Driver API calls, prior\nto kernel launch. Refer to Driver API documentation for more details including any\nsystem/implementation specific behavior. An optional operand e may be specified. Operand e is a vector of .s32 values that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand e is a singleton tuple for 1d textures; is a two\nelement vector 2d textures; and is four-element vector for 3d textures, where the fourth element is\nignored. An optional operand f may be specified for depth textures . Depth textures are special type\nof textures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand f is .f32 scalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in f operand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. When using depth compare operand, the elements in texture coordinate vector c have .f32 type. Depth compare operand is not supported for 3d textures. The instruction returns a two-element vector for destination type .f16x2 . For all other\ndestination types, the instruction returns a four-element vector. Coordinates may be given in either\nsigned 32-bit integer or 32-bit floating point form. A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. tex.{a1d,a2d} Texture array selection, followed by texture lookup. The instruction first selects a texture from\nthe texture array named by operand a using the index given by the first element of the array\ncoordinate vector c . The instruction then loads data from the selected texture at coordinates\ngiven by the remaining elements of operand c into destination d . Operand c is a bit-size\ntype vector or tuple containing an index into the array of textures followed by coordinates within\nthe selected texture, as follows: For 1d texture arrays, operand c has type .v2.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the texture array, and the second element is interpreted as\na 1d texture coordinate of type .ctype . For 2d texture arrays, operand c has type .v4.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the texture array, and the next two elements are\ninterpreted as 2d texture coordinates of type .ctype . The fourth element is ignored. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. An optional operand e may be specified. Operand e is a vector of .s32 values that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand e is a singleton tuple for 1d texture arrays; and is\na two element vector 2d texture arrays. An optional operand f may be specified for depth textures arrays. Operand f is .f32 scalar value that specifies depth compare value for depth textures. When using depth compare\noperand, the coordinates in texture coordinate vector c have .f32 type. The instruction returns a two-element vector for destination type .f16x2 . For all other\ndestination types, the instruction returns a four-element vector. The texture array index is a\n32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point\nvalues. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.cube Cubemap texture lookup. The instruction loads data from the cubemap texture named by operand a at coordinates given by operand c into destination d . Cubemap textures are special\ntwo-dimensional layered textures consisting of six layers that represent the faces of a cube. All\nlayers in a cubemap are of the same size and are square (i.e., width equals height). When accessing a cubemap, the texture coordinate vector c has type .v4.f32 , and comprises\nthree floating-point coordinates ( s , t , r ) and a fourth padding argument which is\nignored. Coordinates ( s , t , r ) are projected onto one of the six cube faces. The ( s , t , r ) coordinates can be thought of as a direction vector emanating from the center of the\ncube. Of the three coordinates ( s , t , r ), the coordinate of the largest magnitude (the\nmajor axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by\nthe absolute value of the major axis to produce a new ( s , t ) coordinate pair to lookup into\nthe selected cube face. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. Offset vector operand e is not supported for cubemap textures. an optional operand f may be specified for cubemap depth textures. operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.acube Cubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture\nfrom the cubemap array named by operand a using the index given by the first element of the\narray coordinate vector c . The instruction then loads data from the selected cubemap texture at\ncoordinates given by the remaining elements of operand c into destination d . Cubemap array textures consist of an array of cubemaps, i.e., the total number of layers is a\nmultiple of six. When accessing a cubemap array texture, the coordinate vector c has type .v4.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the\ncubemap array, and the remaining three elements are interpreted as floating-point cubemap\ncoordinates ( s , t , r ), used to lookup in the selected cubemap as described above. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. Offset vector operand e is not supported for cubemap texture arrays. An optional operand f may be specified for cubemap depth texture arrays. Operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.2dms Multi-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of\nmultiple samples per data element. The instruction loads data from the texture named by operand a from sample number given by first element of the operand c , at coordinates given by\nremaining elements of operand c into destination d . When accessing a multi-sample texture,\ntexture coordinate vector c has type .v4.b32 . The first element in operand c is\ninterpreted as unsigned integer sample number ( .u32 ), and the next two elements are interpreted\nas signed integer ( .s32 ) 2d texture coordinates. The fourth element is ignored. An optional\ntexture sampler b may be specified. If no sampler is specified, the sampler behavior is a\nproperty of the named texture. An optional operand e may be specified. Operand e is a vector of type .v2.s32 that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Depth compare operand f is not supported for multi-sample textures. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. tex.a2dms Multi-sample texture array selection, followed by multi-sample texture lookup. The instruction first\nselects a multi-sample texture from the multi-sample texture array named by operand a using the\nindex given by the first element of the array coordinate vector c . The instruction then loads\ndata from the selected multi-sample texture from sample number given by second element of the\noperand c , at coordinates given by remaining elements of operand c into destination d . When accessing a multi-sample texture array, texture coordinate vector c has type .v4.b32 . The first element in operand c is interpreted as unsigned integer sampler number, the\nsecond element is interpreted as unsigned integer index ( .u32 ) into the multi-sample texture\narray and the next two elements are interpreted as signed integer ( .s32 ) 2d texture\ncoordinates. An optional texture sampler b may be specified. If no sampler is specified, the\nsampler behavior is a property of the named texture. An optional operand e may be specified. Operand e is a vector of type .v2.s32 values\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Depth compare operand f is not supported for multi-sample texture arrays. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. Mipmaps .base (lod zero) Pick level 0 (base level). This is the default if no mipmap mode is specified. No additional arguments. .level (lod explicit) Requires an additional 32-bit scalar argument, lod , which contains the LOD to fetch from. The\ntype of lod follows .ctype (either .s32 or .f32 ). Geometries .2dms and .a2dms are not supported in this mode. .grad (lod gradient) Requires two .f32 vectors, dPdx and dPdy , that specify the partials. The vectors are\nsingletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are\nfour-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d\nand cube geometries. Geometries .2dms and .a2dms are not supported in this mode. For mipmap texture lookup, an optional operand e may be specified. Operand e is a vector of .s32 that specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Offset vector operand is not supported for cube\nand cubemap geometries. An optional operand f may be specified for mipmap textures. Operand f is .f32 scalar\nvalue that specifies depth compare value for depth textures. When using depth compare operand, the\ncoordinates in texture coordinate vector c have .f32 type. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. Depth compare operand is not supported for 3d textures. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture sm_20 or higher. In indirect access, operand a is a .u64 register holding\nthe address of a .texref variable. Notes For compatibility with prior versions of PTX, the square brackets are not required and .v4 coordinate vectors are allowed for any geometry, with the extra elements being ignored. PTX ISA Notes Unified mode texturing introduced in PTX ISA version 1.0. Extension using opaque .texref and .samplerref types and independent mode texturing introduced in PTX ISA version 1.5. Texture arrays tex.{a1d,a2d} introduced in PTX ISA version 2.3. Cubemaps and cubemap arrays introduced in PTX ISA version 3.0. Support for mipmaps introduced in PTX ISA version 3.1. Indirect texture access introduced in PTX ISA version 3.1. Multi-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2. Support for textures returning .f16 and .f16x2 data introduced in PTX ISA version 4.2. Support for tex.grad.{cube, acube} introduced in PTX ISA version 4.3. Offset vector operand introduced in PTX ISA version 4.3. Depth compare operand introduced in PTX ISA version 4.3. Support for optional destination predicate introduced in PTX ISA version 7.1. Target ISA Notes Supported on all target architectures. The cubemap array geometry ( .acube ) requires sm_20 or higher. Mipmaps require sm_20 or higher. Indirect texture access requires sm_20 or higher. Multi-sample textures and multi-sample texture arrays require sm_30 or higher. Texture fetch returning .f16 and .f16x2 data require sm_53 or higher. tex.grad.{cube, acube} requires sm_20 or higher. Offset vector operand requires sm_30 or higher. Depth compare operand requires sm_30 or higher. Support for optional destination predicate requires sm_60 or higher. Examples // Example of unified mode texturing\n // - f4 is required to pad four-element tuple and is ignored\n tex.3d.v4.s32.s32  {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}];\n\n // Example of independent mode texturing\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}];\n\n // Example of 1D texture array, independent texturing mode\n tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}];\n\n // Example of 2D texture array, unified texturing mode\n // - f3 is required to pad four-element tuple and is ignored\n tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}];\n\n // Example of cubemap array, unified textureing mode\n tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}];\n\n // Example of multi-sample texture, unified texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture, independent texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture array, unified texturing mode\n tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}];\n\n // Example of texture returning .f16 data\n tex.1d.v4.f16.f32  {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}];\n\n // Example of texture returning .f16x2 data\n tex.1d.v2.f16x2.f32  {h1,h2}, [tex_a,smpl_x,{f1}];\n\n // Example of 3d texture array access with tex.grad,unified texturing mode\n tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n// Example of cube texture array access with tex.grad,unified texturing mode\n tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n // Example of 1d texture lookup with offset, unified texturing mode\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a, {f1}], {r5};\n\n // Example of 2d texture array lookup with offset, unified texturing mode\n tex.a2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6};\n\n // Example of 2d mipmap texture lookup with offset, unified texturing mode\n tex.level.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}],\n                          flvl, {r7, r8};\n\n // Example of 2d depth texture lookup with compare, unified texturing mode\n tex.1d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a, {f1}], f0;\n\n // Example of depth 2d texture array lookup with offset, compare\n tex.a2d.v4.s32.f32  {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6;\n\n // Example of destination predicate use\n tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}]; 9.7.9.4. Texture Instructions: tld4  tld4 Perform a texture fetch of the 4-texel bilerp footprint. Syntax tld4.comp.2d.v4.dtype.f32    d[|p], [a, c] {, e} {, f};\ntld4.comp.geom.v4.dtype.f32  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n.comp  = { .r, .g, .b, .a };\n.geom  = { .2d, .a2d, .cube, .acube };\n.dtype = { .u32, .s32, .f32 }; Description Texture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction\nloads the bilerp footprint from the texture named by operand a at coordinates given by operand c into vector destination d . The texture component fetched for each texel sample is\nspecified by .comp . The four texel samples are placed into destination vector d in\ncounter-clockwise order starting at lower left. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture. The optional destination predicate p is set to True if data from texture at specified\ncoordinates is resident in memory, False otherwise. When optional destination predicate p is\nset to False , data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior. An optional operand f may be specified for depth textures . Depth textures are special type of\ntextures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand f is .f32 scalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in f operand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. tld4.2d For 2D textures, operand c specifies coordinates as a two-element, 32-bit floating-point vector. An optional operand e may be specified. Operand e is a vector of type .v2.s32 that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7. tld4.a2d Texture array selection, followed by tld4 texture fetch of 2d texture. For 2d texture arrays\noperand c is a four element, 32-bit vector. The first element in operand c is interpreted as an\nunsigned integer index ( .u32 ) into the texture array, and the next two elements are interpreted\nas 32-bit floating point coordinates of 2d texture. The fourth element is ignored. An optional operand e may be specified. Operand e is a vector of type .v2.s32 that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7. tld4.cube For cubemap textures, operand c specifies four-element vector which comprises three\nfloating-point coordinates (s, t, r) and a fourth padding argument which is ignored. Cubemap textures are special two-dimensional layered textures consisting of six layers that\nrepresent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e.,\nwidth equals height). Coordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be\nthought of as a direction vector emanating from the center of the cube. Of the three coordinates (s,\nt, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the\nother two coordinates (the minor axes) are divided by the absolute value of the major axis to\nproduce a new (s, t) coordinate pair to lookup into the selected cube face. Offset vector operand e is not supported for cubemap textures. tld4.acube Cubemap array selection, followed by tld4 texture fetch of cubemap texture. The first element in\noperand c is interpreted as an unsigned integer index ( .u32 ) into the cubemap texture array,\nand the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r),\nused to lookup in the selected cubemap. Offset vector operand e is not supported for cubemap texture arrays. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture sm_20 or higher. In indirect access, operand a is a .u64 register holding\nthe address of a .texref variable. PTX ISA Notes Introduced in PTX ISA version 2.2. Indirect texture access introduced in PTX ISA version 3.1. tld4.{a2d,cube,acube} introduced in PTX ISA version 4.3. Offset vector operand introduced in PTX ISA version 4.3. Depth compare operand introduced in PTX ISA version 4.3. Support for optional destination predicate introduced in PTX ISA version 7.1. Target ISA Notes tld4 requires sm_20 or higher. Indirect texture access requires sm_20 or higher. tld4.{a2d,cube,acube} requires sm_30 or higher. Offset vector operand requires sm_30 or higher. Depth compare operand requires sm_30 or higher. Support for optional destination predicate requires sm_60 or higher. Examples //Example of unified mode texturing\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}];\n\n// Example of independent mode texturing\ntld4.r.2d.v4.u32.f32  {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];\n\n// Example of unified mode texturing using offset\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};\n\n// Example of unified mode texturing using compare\ntld4.r.2d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;\n\n// Example of optional destination predicate\ntld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7; 9.7.9.5. Texture Instructions: txq  txq Query texture and sampler attributes. Syntax txq.tquery.b32         d, [a];       // texture attributes\ntxq.level.tlquery.b32  d, [a], lod;  // texture attributes\ntxq.squery.b32         d, [a];       // sampler attributes\n\n.tquery  = { .width, .height, .depth,\n             .channel_data_type, .channel_order,\n             .normalized_coords, .array_size,\n             .num_mipmap_levels, .num_samples};\n\n.tlquery = { .width, .height, .depth };\n\n.squery  = { .force_unnormalized_coords, .filter_mode,\n             .addr_mode_0, addr_mode_1, addr_mode_2 }; Description Query an attribute of a texture or sampler. Operand a is either a .texref or .samplerref variable, or a .u64 register. Query Returns .width .height .depth value in elements .channel_data_type Unsigned integer corresponding to source language’s channel data type\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both channel_data_type and channel_order queries. .channel_order Unsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both channel_data_type and channel_order queries. .normalized_coords 1 ( True ) or 0 ( False ). .force_unnormalized_coords 1 ( True) or 0 ( False). Defined only for .samplerref variables in independent texture mode. Overrides the normalized_coords field of a .texref variable used with a .samplerref in a tex instruction. .filter_mode Integer from enum { nearest, linear } .addr_mode_0 .addr_mode_1 .addr_mode_2 Integer from enum { wrap, mirror, clamp_ogl, clamp_to_edge, clamp_to_border } .array_size For a texture array, number of textures in array, 0 otherwise. .num_mipmap_levels For a mipmapped texture, number of levels of details (LOD), 0 otherwise. .num_samples For a multi-sample texture, number of samples, 0 otherwise. Texture attributes are queried by supplying a .texref argument to txq . In unified mode,\nsampler attributes are also accessed via a .texref argument, and in independent mode sampler\nattributes are accessed via a separate .samplerref argument. txq.level txq.level requires an additional 32bit integer argument, lod , which specifies LOD and\nqueries requested attribute for the specified LOD. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture sm_20 or higher. In indirect access, operand a is a .u64 register holding\nthe address of a .texref variable. PTX ISA Notes Introduced in PTX ISA version 1.5. Channel data type and channel order queries were added in PTX ISA version 2.1. The .force_unnormalized_coords query was added in PTX ISA version 2.2. Indirect texture access introduced in PTX ISA version 3.1. .array_size , .num_mipmap_levels , .num_samples samples queries were added in PTX ISA\nversion 4.1. txq.level introduced in PTX ISA version 4.3. Target ISA Notes Supported on all target architectures. Indirect texture access requires sm_20 or higher. Querying the number of mipmap levels requires sm_20 or higher. Querying the number of samples requires sm_30 or higher. txq.level requires sm_30 or higher. Examples txq.width.b32       %r1, [tex_A];\ntxq.filter_mode.b32 %r1, [tex_A];   // unified mode\ntxq.addr_mode_0.b32 %r1, [smpl_B];  // independent mode\ntxq.level.width.b32 %r1, [tex_A], %r_lod; 9.7.9.6. Texture Instructions: istypep  istypep Query whether a register points to an opaque variable of a specified type. Syntax istypep.type   p, a;  // result is .pred\n\n.type = { .texref, .samplerref, .surfref }; Description Write predicate register p with 1 if register a points to an opaque variable of the\nspecified type, and with 0 otherwise. Destination p has type .pred ; the source address\noperand must be of type .u64 . PTX ISA Notes Introduced in PTX ISA version 4.0. Target ISA Notes istypep requires sm_30 or higher. Examples istypep.texref istex, tptr;\nistypep.samplerref issampler, sptr;\nistypep.surfref issurface, surfptr; 9.7.10. Surface Instructions  This section describes PTX instructions for accessing surfaces. PTX supports the following\noperations on surface descriptors: Static initialization of surface descriptors. Module-scope and per-entry scope definitions of surface descriptors. Ability to query fields within surface descriptors. These instructions provide access to surface memory. suld sust sured suq 9.7.10.1. Surface Instructions: suld  suld Load from surface memory. Syntax suld.b.geom{.cop}.vec.dtype.clamp  d, [a, b];  // unformatted\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d };\n.cop   = { .ca, .cg, .cs, .cv };               // cache operation\n.vec   = { none, .v2, .v4 };\n.dtype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero }; Description suld.b.{1d,2d,3d} Load from surface memory using a surface coordinate vector. The instruction loads data from the\nsurface named by operand a at coordinates given by operand b into destination d . Operand a is a .surfref variable or .u64 register. Operand b is a scalar or singleton tuple\nfor 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d\nsurfaces, where the fourth element is ignored. Coordinate elements are of type .s32 . suld.b performs an unformatted load of binary data. The lowest dimension coordinate represents a\nbyte offset into the surface and is not scaled, and the size of the data transfer matches the size\nof destination operand d . suld.b.{a1d,a2d} Surface layer selection, followed by a load from the selected surface. The instruction first selects\na surface layer from the surface array named by operand a using the index given by the first\nelement of the array coordinate vector b . The instruction then loads data from the selected\nsurface at coordinates given by the remaining elements of operand b into destination d . Operand a is a .surfref variable or .u64 register. Operand b is a bit-size\ntype vector or tuple containing an index into the array of surfaces followed by coordinates within\nthe selected surface, as follows: For 1d surface arrays, operand b has type .v2.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the second element is interpreted as a\n1d surface coordinate of type .s32 . For 2d surface arrays, operand b has type .v4.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the next two elements are interpreted\nas 2d surface coordinates of type .s32 . The fourth element is ignored. A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp loads data at the nearest surface location (sized appropriately) .zero loads zero for out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes suld.b.trap introduced in PTX ISA version 1.5. Additional clamp modifiers and cache operations introduced in PTX ISA version 2.0. suld.b.3d and suld.b.{a1d,a2d} introduced in PTX ISA version 3.0. Indirect surface access introduced in PTX ISA version 3.1. Target ISA Notes suld.b supported on all target architectures. sm_1x targets support only the .trap clamping modifier. suld.3d and suld.{a1d,a2d} require sm_20 or higher. Indirect surface access requires sm_20 or higher. Cache operations require sm_20 or higher. Examples suld.b.1d.v4.b32.trap  {s1,s2,s3,s4}, [surf_B, {x}];\nsuld.b.3d.v2.b64.trap  {r1,r2}, [surf_A, {x,y,z,w}];\nsuld.b.a1d.v2.b32      {r0,r1}, [surf_C, {idx,x}];\nsuld.b.a2d.b32         r0, [surf_D, {idx,x,y,z}];  // z ignored 9.7.10.2. Surface Instructions: sust  sust Store to surface memory. Syntax sust.b.{1d,2d,3d}{.cop}.vec.ctype.clamp  [a, b], c;  // unformatted\nsust.p.{1d,2d,3d}.vec.b32.clamp          [a, b], c;  // formatted\n\nsust.b.{a1d,a2d}{.cop}.vec.ctype.clamp   [a, b], c;  // unformatted\n\n.cop   = { .wb, .cg, .cs, .wt };                     // cache operation\n.vec   = { none, .v2, .v4 };\n.ctype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero }; Description sust.{1d,2d,3d} Store to surface memory using a surface coordinate vector. The instruction stores data from operand c to the surface named by operand a at coordinates given by operand b . Operand a is\na .surfref variable or .u64 register. Operand b is a scalar or singleton tuple for 1d\nsurfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces,\nwhere the fourth element is ignored. Coordinate elements are of type .s32 . sust.b performs an unformatted store of binary data. The lowest dimension coordinate represents\na byte offset into the surface and is not scaled. The size of the data transfer matches the size of\nsource operand c . sust.p performs a formatted store of a vector of 32-bit data values to a surface sample. The\nsource vector elements are interpreted left-to-right as R , G , B , and A surface\ncomponents. These elements are written to the corresponding surface sample components. Source\nelements that do not occur in the surface sample are ignored. Surface sample components that do not\noccur in the source vector will be written with an unpredictable value. The lowest dimension\ncoordinate represents a sample offset rather than a byte offset. The source data interpretation is based on the surface sample format as follows: If the surface\nformat contains UNORM , SNORM , or FLOAT data, then .f32 is assumed; if the surface\nformat contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. The source data is then converted from this type to the surface\nsample format. sust.b.{a1d,a2d} Surface layer selection, followed by an unformatted store to the selected surface. The instruction\nfirst selects a surface layer from the surface array named by operand a using the index given by\nthe first element of the array coordinate vector b . The instruction then stores the data in\noperand c to the selected surface at coordinates given by the remaining elements of operand b . Operand a is a .surfref variable or .u64 register. Operand b is a bit-size type\nvector or tuple containing an index into the array of surfaces followed by coordinates within the\nselected surface, as follows: For 1d surface arrays, operand b has type .v2.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the second element is interpreted as\na 1d surface coordinate of type .s32 . For 2d surface arrays, operand b has type .v4.b32 . The first element is interpreted as an\nunsigned integer index ( .u32 ) into the surface array, and the next two elements are\ninterpreted as 2d surface coordinates of type .s32 . The fourth element is ignored. A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp stores data at the nearest surface location (sized appropriately) .zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes sust.b.trap introduced in PTX ISA version 1.5. sust.p , additional clamp modifiers, and\ncache operations introduced in PTX ISA version 2.0. sust.b.3d and sust.b.{a1d,a2d} introduced in PTX ISA version 3.0. Indirect surface access introduced in PTX ISA version 3.1. Target ISA Notes sust.b supported on all target architectures. sm_1x targets support only the .trap clamping modifier. sust.3d and sust.{a1d,a2d} require sm_20 or higher. sust.p requires sm_20 or higher. Indirect surface access requires sm_20 or higher. Cache operations require sm_20 or higher. Examples sust.p.1d.v4.b32.trap  [surf_B, {x}], {f1,f2,f3,f4};\nsust.b.3d.v2.b64.trap  [surf_A, {x,y,z,w}], {r1,r2};\nsust.b.a1d.v2.b64      [surf_C, {idx,x}], {r1,r2};\nsust.b.a2d.b32         [surf_D, {idx,x,y,z}], r0;  // z ignored 9.7.10.3. Surface Instructions: sured  sured Reduce surface memory. Syntax sured.b.op.geom.ctype.clamp  [a,b],c; // byte addressing\nsured.p.op.geom.ctype.clamp  [a,b],c; // sample addressing\n\n.op    = { .add, .min, .max, .and, .or };\n.geom  = { .1d, .2d, .3d };\n.ctype = { .u32, .u64, .s32, .b32, .s64 };  // for sured.b\n.ctype = { .b32, .b64 };                    // for sured.p\n.clamp = { .trap, .clamp, .zero }; Description Reduction to surface memory using a surface coordinate vector. The instruction performs a reduction\noperation with data from operand c to the surface named by operand a at coordinates given by\noperand b . Operand a is a .surfref variable or .u64 register. Operand b is a\nscalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a\nfour-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of\ntype .s32 . sured.b performs an unformatted reduction on .u32 , .s32 , .b32 , .u64 , or .s64 data. The lowest dimension coordinate represents a byte offset into the surface and is not\nscaled. Operation add applies to .u32 , .u64 , and .s32 types; min and max apply to .u32 , .s32 , .u64 and .s64 types; operations and and or apply to .b32 type. sured.p performs a reduction on sample-addressed data. The lowest dimension coordinate\nrepresents a sample offset rather than a byte offset. The instruction type .b64 is restricted to min and max operations. For type .b32 , the data is interpreted as .u32 or .s32 based on the surface sample format as follows: if the surface format contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. For\ntype .b64 , if the surface format contains UINT data, then .u64 is assumed; if the\nsurface format contains SINT data, then .s64 is assumed. A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp stores data at the nearest surface location (sized appropriately) .zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes Introduced in PTX ISA version 2.0. Indirect surface access introduced in PTX ISA version 3.1. .u64 / .s64 / .b64 types with .min / .max operations introduced in PTX ISA version\n8.1. Target ISA Notes sured requires sm_20 or higher. Indirect surface access requires sm_20 or higher. .u64 / .s64 / .b64 types with .min / .max operations requires sm_50 or higher. Examples sured.b.add.2d.u32.trap  [surf_A, {x,y}], r1;\nsured.p.min.1d.u32.trap  [surf_B, {x}], r1;\nsured.b.max.1d.u64.trap  [surf_C, {x}], r1;\nsured.p.min.1d.b64.trap  [surf_D, {x}], r1; 9.7.10.4. Surface Instructions: suq  suq Query a surface attribute. Syntax suq.query.b32   d, [a];\n\n.query = { .width, .height, .depth,\n           .channel_data_type, .channel_order,\n           .array_size, .memory_layout }; Description Query an attribute of a surface. Operand a is a .surfref variable or a .u64 register. Query Returns .width .height .depth value in elements .channel_data_type Unsigned integer corresponding to source language’s channel data\ntype enumeration. If the source language combines channel data\ntype and channel order into a single enumeration type, that value\nis returned for both channel_data_type and channel_order queries. .channel_order Unsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and\nchannel order into a single enumeration type, that value is\nreturned for both channel_data_type and channel_order queries. .array_size For a surface array, number of surfaces in array, 0 otherwise. .memory_layout 1 for surface with linear memory layout; 0 otherwise Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of\na .surfref variable. PTX ISA Notes Introduced in PTX ISA version 1.5. Channel data type and channel order queries added in PTX ISA version 2.1. Indirect surface access introduced in PTX ISA version 3.1. The .array_size query was added in PTX ISA version 4.1. The .memory_layout query was added in PTX ISA version 4.2. Target ISA Notes Supported on all target architectures. Indirect surface access requires sm_20 or higher. Examples suq.width.b32       %r1, [surf_A]; 9.7.11. Control Flow Instructions  The following PTX instructions and syntax are for controlling execution in a PTX program: {} @ bra call ret exit 9.7.11.1. Control Flow Instructions: {}  {} Instruction grouping. Syntax { instructionList } Description The curly braces create a group of instructions, used primarily for defining a function body. The\ncurly braces also provide a mechanism for determining the scope of a variable: any variable declared\nwithin a scope is not available outside the scope. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples { add.s32  a,b,c; mov.s32  d,a; } 9.7.11.2. Control Flow Instructions: @  @ Predicated execution. Syntax @{!}p    instruction; Description Execute an instruction or instruction block for threads that have the guard predicate True . Threads with a False guard predicate do nothing. Semantics If {!}p then instruction PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples setp.eq.f32  p,y,0;     // is y zero?\n@!p div.f32      ratio,x,y  // avoid division by zero\n\n@q  bra L23;                // conditional branch 9.7.11.3. Control Flow Instructions: bra  bra Branch to a target and continue execution there. Syntax @p   bra{.uni}  tgt;           // tgt is a label\n     bra{.uni}  tgt;           // unconditional branch Description Continue execution at the target. Conditional branches are specified by using a guard predicate. The\nbranch target must be a label. bra.uni is guaranteed to be non-divergent, i.e. all active threads in a warp that are currently\nexecuting this instruction have identical values for the guard predicate and branch target. Semantics if (p) {\n    pc = tgt;\n} PTX ISA Notes Introduced in PTX ISA version 1.0. Unimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec. Target ISA Notes Supported on all target architectures. Examples bra.uni  L_exit;    // uniform unconditional jump\n@q  bra      L23;   // conditional branch 9.7.11.4. Control Flow Instructions: brx.idx  brx.idx Branch to a label indexed from a list of potential branch targets. Syntax @p    brx.idx{.uni} index, tlist;\n      brx.idx{.uni} index, tlist; Description Index into a list of possible destination labels, and continue execution from the chosen\nlabel. Conditional branches are specified by using a guard predicate. brx.idx.uni guarantees that the branch is non-divergent, i.e. all active threads in a warp that\nare currently executing this instruction have identical values for the guard predicate and the index argument. The index operand is a .u32 register. The tlist operand must be the label of a .branchtargets directive. It is accessed as a zero-based sequence using index . Behaviour is\nundefined if the value of index is greater than or equal to the length of tlist . The .branchtargets directive must be defined in the local function scope before it is used. It\nmust refer to labels within the current function. Semantics if (p) {\n    if (index < length(tlist)) {\n      pc = tlist[index];\n    } else {\n      pc = undefined;\n    }\n} PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples .function foo () {\n    .reg .u32 %r0;\n    ...\n    L1:\n    ...\n    L2:\n    ...\n    L3:\n    ...\n    ts: .branchtargets L1, L2, L3;\n    @p brx.idx %r0, ts;\n    ...\n} 9.7.11.5. Control Flow Instructions: call  call Call a function, recording the return location. Syntax // direct call to named function, func is a symbol\ncall{.uni} (ret-param), func, (param-list);\ncall{.uni} func, (param-list);\ncall{.uni} func;\n\n// indirect call via pointer, with full list of call targets\ncall{.uni} (ret-param), fptr, (param-list), flist;\ncall{.uni} fptr, (param-list), flist;\ncall{.uni} fptr, flist;\n\n// indirect call via pointer, with no knowledge of call targets\ncall{.uni} (ret-param), fptr, (param-list), fproto;\ncall{.uni} fptr, (param-list), fproto;\ncall{.uni} fptr, fproto; Description The call instruction stores the address of the next instruction, so execution can resume at that\npoint after executing a ret instruction. A call is assumed to be divergent unless the .uni suffix is present. The .uni suffix indicates that the call is guaranteed to be\nnon-divergent, i.e. all active threads in a warp that are currently executing this instruction have\nidentical values for the guard predicate and call target. For direct calls, the called location func must be a symbolic function name; for indirect calls,\nthe called location fptr must be an address of a function held in a register. Input arguments\nand return values are optional. Arguments may be registers, immediate constants, or variables in .param space. Arguments are pass-by-value. Indirect calls require an additional operand, flist or fproto , to communicate the list of\npotential call targets or the common function prototype of all call targets,\nrespectively. In the first case, flist gives a complete list of potential call targets and\nthe optimizing backend is free to optimize the calling convention. In the second case, where the\ncomplete list of potential call targets may not be known, the common function prototype is given\nand the call must obey the ABI’s calling convention. The flist operand is either the name of an array (call table) initialized to a list of function\nnames; or a label associated with a .calltargets directive, which declares a list of potential call targets. In both cases the fptr register holds the address of a function listed in the call\ntable or .calltargets list, and the call operands are type-checked against the type\nsignature of the functions indicated by flist . The fproto operand is the name of a label associated with a .callprototype directive. This\noperand is used when a complete list of potential targets is not known. The call operands are\ntype-checked against the prototype, and code generation will follow the ABI calling convention. If a\nfunction that doesn’t match the prototype is called, the behavior is undefined. Call tables may be declared at module scope or local scope, in either the constant or global state\nspace. The .calltargets and .callprototype directives must be declared within a function\nbody. All functions must be declared prior to being referenced in a call table initializer or .calltargets directive. PTX ISA Notes Direct call introduced in PTX ISA version 1.0. Indirect call introduced in PTX ISA version 2.1. Target ISA Notes Direct call supported on all target architectures. Indirect call requires sm_20 or higher. Examples // examples of direct call\n    call     init;    // call function 'init'\n    call.uni g, (a);  // call function 'g' with parameter 'a'\n@p  call     (d), h, (a, b);  // return value into register d\n\n// call-via-pointer using jump table\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n\n.global .u32 jmptbl[5] = { foo, bar, baz };\n      ...\n@p    ld.global.u32  %r0, [jmptbl+4];\n@p    ld.global.u32  %r0, [jmptbl+8];\n      call  (retval), %r0, (x, y), jmptbl;\n\n// call-via-pointer using .calltargets directive\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n      ...\n@p    mov.u32  %r0, foo;\n@q    mov.u32  %r0, baz;\nFtgt: .calltargets foo, bar, baz;\n      call  (retval), %r0, (x, y), Ftgt;\n\n// call-via-pointer using .callprototype directive\n.func dispatch (.reg .u32 fptr, .reg .u32 idx)\n{\n...\nFproto: .callprototype _ (.param .u32 _, .param .u32 _);\n      call  %fptr, (x, y), Fproto;\n... 9.7.11.6. Control Flow Instructions: ret  ret Return from function to instruction after call. Syntax ret{.uni}; Description Return execution to caller’s environment. A divergent return suspends threads until all threads are\nready to return to the caller. This allows multiple divergent ret instructions. A ret is assumed to be divergent unless the .uni suffix is present, indicating that the\nreturn is guaranteed to be non-divergent. Any values returned from a function should be moved into the return parameter variables prior to\nexecuting the ret instruction. A return instruction executed in a top-level entry routine will terminate thread execution. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples ret;\n@p  ret; 9.7.11.7. Control Flow Instructions: exit  exit Terminate a thread. Syntax exit; Description Ends execution of a thread. As threads exit, barriers waiting on all threads are checked to see if the exiting threads are the\nonly threads that have not yet made it to a barrier{.cta} for all threads in the CTA or to a barrier.cluster for all threads in the cluster. If the exiting threads are holding up the\nbarrier, the barrier is released. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples exit;\n@p  exit; 9.7.12. Parallel Synchronization and Communication Instructions  These instructions are: bar{.cta} , barrier{.cta} barrier.cluster bar.warp.sync membar atom red red.async vote match.sync activemask redux.sync griddepcontrol elect.sync mbarrier.init mbarrier.inval mbarrier.arrive mbarrier.arrive_drop mbarrier.test_wait mbarrier.try_wait mbarrier.pending_count cp.async.mbarrier.arrive tensormap.cp_fenceproxy 9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrier  bar{.cta}, barrier{.cta} Barrier synchronization. Syntax barrier{.cta}.sync{.aligned}      a{, b};\nbarrier{.cta}.arrive{.aligned}    a, b;\n\nbarrier{.cta}.red.popc{.aligned}.u32  d, a{, b}, {!}c;\nbarrier{.cta}.red.op{.aligned}.pred   p, a{, b}, {!}c;\n\nbar{.cta}.sync      a{, b};\nbar{.cta}.arrive    a, b;\n\nbar{.cta}.red.popc.u32  d, a{, b}, {!}c;\nbar{.cta}.red.op.pred   p, a{, b}, {!}c;\n\n.op = { .and, .or }; Description Performs barrier synchronization and communication within a CTA. Each CTA instance has sixteen\nbarriers numbered 0..15 . barrier{.cta} instructions can be used by the threads within the CTA for synchronization and\ncommunication. Operands a , b , and d have type .u32 ; operands p and c are predicates. Source\noperand a specifies a logical barrier resource as an immediate constant or register with value 0 through 15 . Operand b specifies the number of threads participating in the barrier. If\nno thread count is specified, all threads in the CTA participate in the barrier. When specifying a\nthread count, the value must be a multiple of the warp size. Note that a non-zero thread count is\nrequired for barrier{.cta}.arrive . Depending on operand b , either specified number of threads (in multiple of warp size) or all\nthreads in the CTA participate in barrier{.cta} instruction. The barrier{.cta} instructions\nsignal the arrival of the executing threads at the named barrier. barrier{.cta} instruction causes executing thread to wait for all non-exited threads from its\nwarp and marks warps’ arrival at barrier. In addition to signaling its arrival at the barrier, the barrier{.cta}.red and barrier{.cta}.sync instructions causes executing thread to wait for\nnon-exited threads of all other warps participating in the barrier to\narrive. barrier{.cta}.arrive does not cause executing thread to wait for threads of other\nparticipating warps. When a barrier completes, the waiting threads are restarted without delay, and the barrier is\nreinitialized so that it can be immediately reused. The barrier{.cta}.sync or barrier{.cta}.red or barrier{.cta}.arrive instruction\nguarantees that when the barrier completes, prior memory accesses requested by this thread are\nperformed relative to all threads participating in the barrier. The barrier{.cta}.sync and barrier{.cta}.red instruction further guarantees that no new memory access is requested by this\nthread before the barrier completes. A memory read (e.g., by ld or atom ) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread participating in the barrier. A\nmemory write (e.g., by st , red or atom ) has been performed when the value written has\nbecome visible to other threads participating in the barrier, that is, when the previous value can\nno longer be read. barrier{.cta}.red performs a reduction operation across threads. The c predicate (or its\ncomplement) from all threads in the CTA are combined using the specified reduction operator. Once\nthe barrier count is reached, the final value is written to the destination register in all threads\nwaiting at the barrier. The reduction operations for barrier{.cta}.red are population-count ( .popc ),\nall-threads-True ( .and ), and any-thread-True ( .or ). The result of .popc is the number of\nthreads with a True predicate, while .and and .or indicate if all the threads had a True predicate or if any of the threads had a True predicate. Instruction barrier{.cta} has optional .aligned modifier. When specified, it indicates that\nall threads in CTA will execute the same barrier{.cta} instruction. In conditionally executed\ncode, an aligned barrier{.cta} instruction should only be used if it is known that all threads\nin CTA evaluate the condition identically, otherwise behavior is undefined. Different warps may execute different forms of the barrier{.cta} instruction using the same\nbarrier name and thread count. One example mixes barrier{.cta}.sync and barrier{.cta}.arrive to implement producer/consumer models. The producer threads execute barrier{.cta}.arrive to\nannounce their arrival at the barrier and continue execution without delay to produce the next\nvalue, while the consumer threads execute the barrier{.cta}.sync to wait for a resource to be\nproduced. The roles are then reversed, using a different barrier, where the producer threads execute\na barrier{.cta}.sync to wait for a resource to consumed, while the consumer threads announce\nthat the resource has been consumed with barrier{.cta}.arrive . Care must be taken to keep a warp\nfrom executing more barrier{.cta} instructions than intended ( barrier{.cta}.arrive followed\nby any other barrier{.cta} instruction to the same barrier) prior to the reset of the\nbarrier. barrier{.cta}.red should not be intermixed with barrier{.cta}.sync or barrier{.cta}.arrive using the same active barrier. Execution in this case is unpredictable. The optional .cta qualifier simply indicates CTA-level applicability of the barrier and it\ndoesn’t change the semantics of the instruction. bar{.cta}.sync is equivalent to barrier{.cta}.sync.aligned . bar{.cta}.arrive is\nequivalent to barrier{.cta}.arrive.aligned . bar{.cta}.red is equivalent to barrier{.cta}.red.aligned . Note For .target sm_6x or below, barrier{.cta} instruction without .aligned modifier is equivalent to .aligned variant and has the same restrictions as of .aligned variant. All threads in warp (except for those have exited) must execute barrier{.cta} instruction\nin convergence. PTX ISA Notes bar.sync without a thread count introduced in PTX ISA version 1.0. Register operands, thread count, and bar.{arrive,red} introduced in PTX ISA version 2.0. barrier instruction introduced in PTX ISA version 6.0. .cta qualifier introduced in PTX ISA version 7.8. Target ISA Notes Register operands, thread count, and bar{.cta}.{arrive,red} require sm_20 or higher. Only bar{.cta}.sync with an immediate barrier number is supported for sm_1x targets. barrier{.cta} instruction requires sm_30 or higher. Examples // Use bar.sync to arrive at a pre-computed barrier number and\n// wait for all threads in CTA to also arrive:\n    st.shared [r0],r1;  // write my result to shared memory\n    bar.cta.sync  1;    // arrive, wait for others to arrive\n    ld.shared r2,[r3];  // use shared results from other threads\n\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for fixed number of cooperating threads to arrive:\n    #define CNT1 (8*12) // Number of cooperating threads\n\n    st.shared [r0],r1;     // write my result to shared memory\n    bar.cta.sync  1, CNT1; // arrive, wait for others to arrive\n    ld.shared r2,[r3];     // use shared results from other threads\n\n// Use bar.red.and to compare results across the entire CTA:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.and.pred r3,1,p; // r3=AND(p) forall threads in CTA\n\n// Use bar.red.popc to compute the size of a group of threads\n// that have a specific condition True:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.popc.u32 r3,1,p; // r3=SUM(p) forall threads in CTA\n\n/* Producer/consumer model. The producer deposits a value in\n * shared memory, signals that it is complete but does not wait\n * using bar.arrive, and begins fetching more data from memory.\n * Once the data returns from memory, the producer must wait\n * until the consumer signals that it has read the value from\n * the shared memory location. In the meantime, a consumer\n * thread waits until the data is stored by the producer, reads\n * it, and then signals that it is done (without waiting).\n */\n    // Producer code places produced value in shared memory.\n    st.shared   [r0],r1;\n    bar.arrive  0,64;\n    ld.global   r1,[r2];\n    bar.sync    1,64;\n    ...\n\n    // Consumer code, reads value from shared memory\n    bar.sync   0,64;\n    ld.shared  r1,[r0];\n    bar.arrive 1,64;\n    ...\n\n    // Examples of barrier.cta.sync\n    st.shared         [r0],r1;\n    barrier.cta.sync  0;\n    ld.shared         r1, [r0]; 9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.sync  bar.warp.sync Barrier synchronization for threads in a warp. Syntax bar.warp.sync      membermask; Description bar.warp.sync will cause executing thread to wait until all threads corresponding to membermask have executed a bar.warp.sync with the same membermask value before resuming\nexecution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s laneid . The behavior of bar.warp.sync is undefined if the executing thread is not in the membermask . bar.warp.sync also guarantee memory ordering among threads participating in barrier. Thus,\nthreads within warp that wish to communicate via memory can store to memory, execute bar.warp.sync , and then safely read values stored by other threads in warp. Note For .target sm_6x or below, all threads in membermask must execute the same bar.warp.sync instruction in convergence, and only threads belonging to some membermask can be active when the bar.warp.sync instruction is executed. Otherwise, the behavior is\nundefined. PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples st.shared.u32 [r0],r1;         // write my result to shared memory\nbar.warp.sync  0xffffffff;     // arrive, wait for others to arrive\nld.shared.u32 r2,[r3];         // read results written by other threads 9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.cluster  barrier.cluster Barrier synchronization within a cluster. Syntax barrier.cluster.arrive{.sem}{.aligned};\nbarrier.cluster.wait{.acquire}{.aligned};\n\n.sem = {.release, .relaxed} Description Performs barrier synchronization and communication within a cluster. barrier.cluster instructions can be used by the threads within the cluster for synchronization\nand communication. barrier.cluster.arrive instruction marks warps’ arrival at barrier without causing executing\nthread to wait for threads of other participating warps. barrier.cluster.wait instruction causes the executing thread to wait for all non-exited threads\nof the cluster to perform barrier.cluster.arrive . In addition, barrier.cluster instructions cause the executing thread to wait for all non-exited\nthreads from its warp. When all non-exited threads that executed barrier.cluster.arrive have executed barrier.cluster.wait , the barrier completes and is reinitialized so it can be reused\nimmediately. Each thread must arrive at the barrier only once before the barrier completes. The barrier.cluster.wait instruction guarantees that when it completes the execution, memory\naccesses (except asynchronous operations) requested, in program order, prior to the preceding barrier.cluster.arrive by all threads in the cluster are complete and visible to the executing\nthread. There is no memory ordering and visibility guarantee for memory accesses requested by the executing\nthread, in program order, after barrier.cluster.arrive and prior to barrier.cluster.wait . The optional .relaxed qualifier on barrier.cluster.arrive specifies that there are no memory\nordering and visibility guarantees provided for the memory accesses performed prior to barrier.cluster.arrive . The optional .sem and .acquire qualifiers on instructions barrier.cluster.arrive and barrier.cluster.wait specify the memory synchronization as described in the Memory Consistency\nModel . If the optional .sem qualifier is absent for barrier.cluster.arrive , .release is assumed by default. If the optional .acquire qualifier is absent for barrier.cluster.wait , .acquire is assumed by default. The optional .aligned qualifier indicates that all threads in the warp must execute the same barrier.cluster instruction. In conditionally executed code, an aligned barrier.cluster instruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 7.8. Support for .acquire , .relaxed , .release qualifiers introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples // use of arrive followed by wait\nld.shared::cluster.u32 r0, [addr];\nbarrier.cluster.arrive.aligned;\n...\nbarrier.cluster.wait.aligned;\nst.shared::cluster.u32 [addr], r1;\n\n// use memory fence prior to arrive for relaxed barrier\n@cta0 ld.shared::cluster.u32 r0, [addr];\nfence.cluster.acq_rel;\nbarrier.cluster.arrive.relaxed.aligned;\n...\nbarrier.cluster.wait.aligned;\n@cta1 st.shared::cluster.u32 [addr], r1; 9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fence  membar/fence Enforce an ordering of memory operations. Syntax // Thread fence :\nfence{.sem}.scope;\n\n// Operation fence :\nfence.op_restrict.release.cluster;\n\n// Proxy fence (bi-directional) :\nfence.proxy.proxykind;\n\n// Proxy fence (uni-directional) :\nfence.proxy.to_proxykind::from_proxykind.release.scope;\nfence.proxy.to_proxykind::from_proxykind.acquire.scope  [addr], size;\n\n// Old style membar :\nmembar.level;\nmembar.proxy.proxykind;\n\n.sem       = { .sc, .acq_rel };\n.scope     = { .cta, .cluster, .gpu, .sys };\n.level     = { .cta, .gl, .sys };\n.proxykind = { .alias, .async, async.global, .async.shared::{cta, cluster} };\n.op_restrict = { .mbarrier_init };\n.to_proxykind::from_proxykind = {.tensormap::generic}; Description The membar instruction guarantees that prior memory accesses requested by this thread ( ld , st , atom and red instructions) are performed at the specified level , before later\nmemory operations requested by this thread following the membar instruction. The level qualifier specifies the set of threads that may observe the ordering effect of this operation. A memory read (e.g., by ld or atom ) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread at the indicated level. A memory\nwrite (e.g., by st , red or atom ) has been performed when the value written has become\nvisible to other threads at the specified level, that is, when the previous value can no longer be\nread. The fence instruction establishes an ordering between memory accesses requested by this thread\n( ld , st , atom and red instructions) as described in the Memory Consistency Model . The scope qualifier specifies the set of threads that may\nobserve the ordering effect of this operation. fence.acq_rel is a light-weight fence that is sufficient for memory synchronization in most\nprograms. Instances of fence.acq_rel synchronize when combined with additional memory operations\nas described in acquire and release patterns in the Memory Consistency Model . If the optional .sem qualifier is absent, .acq_rel is assumed by default. fence.sc is a slower fence that can restore sequential consistency when used in sufficient\nplaces, at the cost of performance. Instances of fence.sc with sufficient scope always\nsynchronize by forming a total order per scope, determined at runtime. This total order can be\nconstrained further by other synchronization in the program. Qualifier .op_restrict restricts the class of prior memory operations for which the fence instruction provides the memory ordering guarantees. When .op_restrict is .mbarrier_init ,\nthe fence only applies to the prior mbarrier.init operations executed by the same thread on mbarrier objects in .shared::cta state space. The address operand addr and the operand size together specifies the memory range [addr, addr+size-1] on which the ordering guarantees on the memory accesses across the proxies is to be\nprovided. The only supported value for the size operand is 128. Generic Addressing is used unconditionally, and the address specified by the operand addr must fall within the .global state space. Otherwise, the behavior is undefined. On sm_70 and higher membar is a synonym for fence.sc 1 , and the membar levels cta , gl and sys are synonymous with the fence scopes cta , gpu and sys respectively. membar.proxy and fence.proxy instructions establish an ordering between memory accesses that\nmay happen through different proxies . A uni-directional proxy ordering from the from-proxykind to the to-proxykind establishes\nordering between a prior memory access performed via the from-proxykind and a subsequent memory access\nperformed via the to-proxykind . A bi-directional proxy ordering between two proxykinds establishes two uni-directional proxy orderings\n: one from the first proxykind to the second proxykind and the other from the second proxykind to the first\nproxykind. The .proxykind qualifier indicates the bi-directional proxy ordering that is established between the memory\naccesses done between the generic proxy and the proxy specified by .proxykind . Value .alias of the .proxykind qualifier refers to memory accesses performed using virtually\naliased addresses to the same memory location. Value .async of the .proxykind qualifier specifies\nthat the memory ordering is established between the async proxy and the generic proxy. The memory\nordering is limited only to the state space specified. If no state space is specified, then the memory\nordering applies on all state spaces. A .release proxy fence can form a release sequence that synchronizes with an acquire\nsequence that contains a .acquire proxy fence. The .to_proxykind and .from_proxykind qualifiers indicate the uni-directional proxy ordering that is established. On sm_70 and higher, membar.proxy is a synonym for fence.proxy . 1 The semantics of fence.sc introduced with sm_70 is a superset of the semantics of membar and the two are compatible; when executing on sm_70 or later architectures, membar acquires the full semantics of fence.sc . PTX ISA Notes membar.{cta,gl} introduced in PTX ISA version 1.4. membar.sys introduced in PTX ISA version 2.0. fence introduced in PTX ISA version 6.0. membar.proxy and fence.proxy introduced in PTX ISA version 7.5. .cluster scope qualifier introduced in PTX ISA version 7.8. .op_restrict qualifier introduced in PTX ISA version 8.0. fence.proxy.async is introduced in PTX ISA version 8.0. .to_proxykind::from_proxykind qualifier introduced in PTX ISA version 8.3. Target ISA Notes membar.{cta,gl} supported on all target architectures. membar.sys requires sm_20 or higher. fence requires sm_70 or higher. membar.proxy requires sm_60 or higher. fence.proxy requires sm_70 or higher. .cluster scope qualifier requires sm_90 or higher. .op_restrict qualifier requires sm_90 or higher. fence.proxy.async requires sm_90 or higher. .to_proxykind::from_proxykind qualifier requires sm_90 or higher. Examples membar.gl;\nmembar.cta;\nmembar.sys;\nfence.sc;\nfence.sc.cluster;\nfence.proxy.alias;\nmembar.proxy.alias;\nfence.mbarrier_init.release.cluster;\nfence.proxy.async;\nfence.proxy.async.shared::cta;\nfence.proxy.async.shared::cluster;\nfence.proxy.async.global;\n\ntensormap.replace.tile.global_address.global.b1024.b64   [gbl], new_addr;\nfence.proxy.tensormap::generic.release.gpu;\nfence.proxy.tensormap::generic.acquire.gpu [tmap], 128;\ncvta.global.u64  tmap, gbl;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [tmap, {tc0}], [mbar0]; 9.7.12.5. Parallel Synchronization and Communication Instructions: atom  atom Atomic reduction operations for thread-to-thread communication. Syntax Atomic operation with scalar type: atom{.sem}{.scope}{.space}.op{.level::cache_hint}.type d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.op.type d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b16 d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b128 d, [a], b, c {, cache-policy};\natom{.sem}{.scope}{.space}.exch{.level::cache_hint}.b128 d, [a], b {, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16     d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2   d, [a], b{, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16    d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2  d, [a], b{, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                { .relaxed, .acquire, .release, .acq_rel };\n.scope =              { .cta, .cluster, .gpu, .sys };\n\n.op =                 { .and, .or, .xor,\n                        .cas, .exch,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; Atomic operation with vector type: atom{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32                  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_16_bit.half_word_type  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type     d, [a], b{, cache-policy};\n\n.sem =               { .relaxed, .acquire, .release, .acq_rel };\n.scope =             { .cta, .cluster, .gpu, .sys };\n.op =                { .add, .min, .max };\n.half_word_type =    { .f16, .bf16 };\n.packed_type =       { .f16x2, .bf16x2 };\n.vec_16_bit =        { .v2, .v4, .v8 }\n.vec_32_bit =        { .v2, .v4 };\n.level::cache_hint = { .L2::cache_hint } Description Atomically loads the original value at location a into destination register d , performs a\nreduction operation with operand b and the value in location a , and stores the result of the\nspecified operation at location a , overwriting the original value. Operand a specifies a\nlocation in the specified state space. If no state space is given, perform the memory accesses using Generic Addressing . atom with scalar type may be used only\nwith .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. atom with vector type may be used only with .global space\nand with generic addressing where the address points to .global space. For atom with vector type, operands d and b are brace-enclosed vector expressions, size\nof which is equal to the size of vector qualifier. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory\nConsistency Model . If the .sem qualifier is absent, .relaxed is assumed by default. The optional .scope qualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is absent, .gpu scope is\nassumed by default. For atom with vector type, the supported combinations of vector qualifier and types, and atomic\noperations supported on these combinations are depicted in the following table: Vector qualifier Types .f16 / bf16 .f16x2 / bf16x2 .f32 .v2 .add , .min , .max .add , .min , .max .add .v4 .add , .min , .max .add , .min , .max .add .v8 .add , .min , .max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write. atom instruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch cases, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire atom is not guaranteed to be atomic as a single access. For sm_6x and earlier architectures, atom operations on .shared state space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory atomic\ninstructions, e.g., by inserting barriers between normal stores and atomic operations to a common\naddress, or by using atom.exch to store to locations accessed by other atomic operations. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands The bit-size operations are .and , .or , .xor , .cas (compare-and-swap), and .exch (exchange). The integer operations are .add , .inc , .dec , .min , .max . The .inc and .dec operations return a result in the range [0..b] . The floating-point operation .add operation rounds to nearest even. Current implementation of atom.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas atom.add.f32 on shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero. atom.add.f16 , atom.add.f16x2 , atom.add.bf16 and atom.add.bf16x2 operation requires\nthe .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to\nzero. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. Semantics atomic {\n    d = *a;\n    *a = (operation == cas) ? operation(*a, b, c)\n                            : operation(*a, b);\n}\nwhere\n    inc(r, s)  = (r >= s) ? 0 : r+1;\n    dec(r, s)  = (r==0 || r > s)  ? s : r-1;\n    exch(r, s) =  s;\n    cas(r,s,t) = (r == s) ? t : r; Notes Simple reductions may be specified by using the bit bucket destination operand _ . PTX ISA Notes 32-bit atom.global introduced in PTX ISA version 1.1. atom.shared and 64-bit atom.global.{add,cas,exch} introduced in PTX ISA 1.2. atom.add.f32 and 64-bit atom.shared.{add,cas,exch} introduced in PTX ISA 2.0. 64-bit atom.{and,or,xor,min,max} introduced in PTX ISA 3.1. atom.add.f64 introduced in PTX ISA 5.0. .scope qualifier introduced in PTX ISA 5.0. .sem qualifier introduced in PTX ISA version 6.0. atom.add.noftz.f16x2 introduced in PTX ISA 6.2. atom.add.noftz.f16 and atom.cas.b16 introduced in PTX ISA 6.3. Per-element atomicity of atom.f16x2 clarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2. Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4. atom.add.noftz.bf16 and atom.add.noftz.bf16x2 introduced in PTX ISA 7.8. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for vector types introduced in PTX ISA version 8.1. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes atom.global requires sm_11 or higher. atom.shared requires sm_12 or higher. 64-bit atom.global.{add,cas,exch} require sm_12 or higher. 64-bit atom.shared.{add,cas,exch} require sm_20 or higher. 64-bit atom.{and,or,xor,min,max} require sm_32 or higher. atom.add.f32 requires sm_20 or higher. atom.add.f64 requires sm_60 or higher. .scope qualifier requires sm_60 or higher. .sem qualifier requires sm_70 or higher. Use of generic addressing requires sm_20 or higher. atom.add.noftz.f16x2 requires sm_60 or higher. atom.add.noftz.f16 and atom.cas.b16 requires sm_70 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. atom.add.noftz.bf16 and atom.add.noftz.bf16x2 require sm_90 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for vector types requires sm_90 or higher. Support for .b128 type requires sm_90 or higher. Examples atom.global.add.s32  d,[a],1;\natom.shared::cta.max.u32  d,[x+4],0;\n@p  atom.global.cas.b32  d,[p],my_val,my_new_val;\natom.global.sys.add.u32 d, [a], 1;\natom.global.acquire.sys.inc.u32 ans, [gbl], %r0;\natom.add.noftz.f16x2 d, [a], b;\natom.add.noftz.f16   hd, [ha], hb;\natom.global.cas.b16  hd, [ha], hb, hc;\natom.add.noftz.bf16   hd, [a], hb;\natom.add.noftz.bf16x2 bd, [b], bb;\natom.add.shared::cluster.noftz.f16   hd, [ha], hb;\natom.shared.b128.cas d, a, b, c; // 128-bit atom\natom.global.b128.exch d, a, b;   // 128-bit atom\n\natom.global.cluster.relaxed.add.u32 d, [a], 1;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\natom.global.add.L2::cache_hint.s32  d, [a], 1, cache-policy;\n\natom.global.v8.f16.max.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v8.bf16.add.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v2.f16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v2.bf16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v4.b16x2.min.noftz  {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3};\natom.global.v4.f32.add  {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3};\natom.global.v2.f16x2.min.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.bf16x2.max.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1}; 9.7.12.6. Parallel Synchronization and Communication Instructions: red  red Reduction operations on global and shared memory. Syntax Reduction operation with scalar type: red{.sem}{.scope}{.space}.op{.level::cache_hint}.type          [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16    [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2  [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16\n                                                      [a], b {, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2\n                                                      [a], b {, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                {.relaxed, .release};\n.scope =              {.cta, .cluster, .gpu, .sys};\n\n.op =                 { .and, .or, .xor,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; Reduction operation with vector type: red{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy};\n\n.sem =                { .relaxed, .release };\n.scope =              { .cta, .cluster, .gpu, .sys };\n.op =                 { .add, .min, .max };\n.half_word_type =     { .f16, .bf16 };\n.packed_type =        { .f16x2,.bf16x2 };\n.vec_16_bit =         { .v2, .v4, .v8 }\n.vec_32_bit =         { .v2, .v4 };\n.level::cache_hint =  { .L2::cache_hint } Description Performs a reduction operation with operand b and the value in location a , and stores the\nresult of the specified operation at location a , overwriting the original value. Operand a specifies a location in the specified state space. If no state space is given, perform the memory\naccesses using Generic Addressing . red with scalar type may\nbe used only with .global and .shared spaces and with generic addressing, where the address\npoints to .global or .shared space. red with vector type may be used only with .global space and with generic addressing where the address points to .global space. For red with vector type, operand b is brace-enclosed vector expressions, size of which is\nequal to the size of vector qualifier. If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory\nConsistency Model . If the .sem qualifier is absent, .relaxed is assumed by default. The optional .scope qualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is absent, .gpu scope is\nassumed by default. For red with vector type, the supported combinations of vector qualifier, types and reduction\noperations supported on these combinations are depicted in following table: Vector qualifier Types .f16 / bf16 .f16x2 / bf16x2 .f32 .v2 .add , .min , .max .add , .min , .max .add .v4 .add , .min , .max .add , .min , .max .add .v8 .add , .min , .max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write. red instruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch case, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire red is not guaranteed to be atomic as a single access. For sm_6x and earlier architectures, red operations on .shared state space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory reduction\ninstructions, e.g., by inserting barriers between normal stores and reduction operations to a common\naddress, or by using atom.exch to store to locations accessed by other reduction operations. Supported addressing modes for operand a and alignment requirements are described in Addresses\nas Operands The bit-size operations are .and , .or , and .xor . The integer operations are .add , .inc , .dec , .min , .max . The .inc and .dec operations return a result in the range [0..b] . The floating-point operation .add operation rounds to nearest even. Current implementation of red.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas red.add.f32 on shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero. red.add.f16 , red.add.f16x2 , red.add.bf16 and red.add.bf16x2 operation requires the .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to zero. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is\nrequired. The 64-bit operand cache-policy specifies the cache eviction policy that may be used\nduring the memory access. The qualifier .level::cache_hint is only supported for .global state space and for generic\naddressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. Semantics *a = operation(*a, b);\n\nwhere\n    inc(r, s) = (r >= s) ? 0 : r+1;\n    dec(r, s) = (r==0 || r > s)  ? s : r-1; PTX ISA Notes Introduced in PTX ISA version 1.2. red.add.f32 and red.shared.add.u64 introduced in PTX ISA 2.0. 64-bit red.{and,or,xor,min,max} introduced in PTX ISA 3.1. red.add.f64 introduced in PTX ISA 5.0. .scope qualifier introduced in PTX ISA 5.0. .sem qualifier introduced in PTX ISA version 6.0. red.add.noftz.f16x2 introduced in PTX ISA 6.2. red.add.noftz.f16 introduced in PTX ISA 6.3. Per-element atomicity of red.f16x2 clarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2 Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4. red.add.noftz.bf16 and red.add.noftz.bf16x2 introduced in PTX ISA 7.8. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for vector types introduced in PTX ISA version 8.1. Target ISA Notes red.global requires sm_11 or higher red.shared requires sm_12 or higher. red.global.add.u64 requires sm_12 or higher. red.shared.add.u64 requires sm_20 or higher. 64-bit red.{and,or,xor,min,max} require sm_32 or higher. red.add.f32 requires sm_20 or higher. red.add.f64 requires sm_60 or higher. .scope qualifier requires sm_60 or higher. .sem qualifier requires sm_70 or higher. Use of generic addressing requires sm_20 or higher. red.add.noftz.f16x2 requires sm_60 or higher. red.add.noftz.f16 requires sm_70 or higher. Support for .level::cache_hint qualifier requires sm_80 or higher. red.add.noftz.bf16 and red.add.noftz.bf16x2 require sm_90 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for vector types requires sm_90 or higher. Examples red.global.add.s32  [a],1;\nred.shared::cluster.max.u32  [x+4],0;\n@p  red.global.and.b32  [p],my_val;\nred.global.sys.add.u32 [a], 1;\nred.global.acquire.sys.add.u32 [gbl], 1;\nred.add.noftz.f16x2 [a], b;\nred.add.noftz.bf16   [a], hb;\nred.add.noftz.bf16x2 [b], bb;\nred.global.cluster.relaxed.add.u32 [a], 1;\nred.shared::cta.min.u32  [x+4],0;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nred.global.and.L2::cache_hint.b32 [a], 1, cache-policy;\n\nred.global.v8.f16.add.noftz  [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v2.f16.add.noftz [gbl], {%h0, %h1};\nred.global.v2.bf16.add.noftz [gbl], {%h0, %h1};\nred.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3};\nred.global.v4.f32.add  [gbl], {%f0, %f1, %f2, %f3};\nred.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1}; 9.7.12.7. Parallel Synchronization and Communication Instructions: red.async  red.async Asynchronous reduction operation on shared memory. Syntax // Increment and Decrement reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.ss   =                 { .shared::cluster };\n.op   =                 { .inc, .dec };\n.type =                 { .u32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n\n// MIN and MAX reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.ss   = { .shared::cluster };\n.op   = { .min, .max };\n.type = { .u32, .s32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// Bitwise AND, OR and XOR reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.ss   = { .shared::cluster };\n.op   = { .and, .or, .xor };\n.type = { .b32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// ADD reductions\nred.async.relaxed.cluster{.ss}.completion_mechanism.add.type [a], b, [mbar];\n\n.ss   = { .shared::cluster };\n.type = { .u32, .s32, .u64 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes }; Description red.async is a non-blocking instruction which initiates an asynchronous reduction operation\nspecified by .op , with the operand b and the value at destination shared memory location\nspecified by operand a . The .inc and .dec operations return a result in the range [0..b] . The modifier .completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be\nperformed on the mbarrier object specified by the operand mbar . Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands . The shared memory addresses of destination operand a and the mbarrier object mbar , must\nmeet all of the following conditions: They Belong to the same CTA. They are different to the CTA of the executing thread but must be within the same cluster. Otherwise, the behavior is undefined. The state space of the address {.ss} , if specified, is applicable to both operands a and mbar . If not specified, then Generic Addressing is used for\nboth a and mbar . With .shared::cluster , if the addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined. The reduce operation in red.async is treated as a relaxed memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples red.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr]; 9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated)  vote (deprecated) Vote across thread group. Syntax vote.mode.pred  d, {!}a;\nvote.ballot.b32 d, {!}a;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni }; Deprecation Note The vote instruction without a .sync qualifier is deprecated in PTX ISA version 6.0. Support for this instruction with .target lower than sm_70 may be removed in a future PTX\nISA version. Removal Note Support for vote instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .target sm_70 or higher. Description Performs a reduction of the source predicate across all active threads in a warp. The destination\npredicate value is the same across all threads in the warp. The reduction modes are: .all True if source predicate is True for all active threads in warp. Negate the source\npredicate to compute .none . .any True if source predicate is True for some active thread in warp. Negate the source\npredicate to compute .not_all . .uni True if source predicate has the same value in all active threads in warp. Negating the\nsource predicate also computes .uni . In the ballot form, vote.ballot.b32 simply copies the predicate from each thread in a warp\ninto the corresponding bit position of destination register d , where the bit position\ncorresponds to the thread’s lane id. An inactive thread in warp will contribute a 0 for its entry when participating in vote.ballot.b32 . PTX ISA Notes Introduced in PTX ISA version 1.2. Deprecated in PTX ISA version 6.0 in favor of vote.sync . Not supported in PTX ISA version 6.4 for .target sm_70 or higher. Target ISA Notes vote requires sm_12 or higher. vote.ballot.b32 requires sm_20 or higher. vote is not supported on sm_70 or higher starting PTX ISA version 6.4. Release Notes Note that vote applies to threads in a single warp, not across an entire CTA. Examples vote.all.pred    p,q;\nvote.uni.pred    p,q;\nvote.ballot.b32  r1,p;  // get 'ballot' across warp 9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync  vote.sync Vote across thread group. Syntax vote.sync.mode.pred  d, {!}a, membermask;\nvote.sync.ballot.b32 d, {!}a, membermask;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni }; Description vote.sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed vote.sync with the same qualifiers and same membermask value\nbefore resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid . Operand a is a\npredicate register. In the mode form, vote.sync performs a reduction of the source predicate across all non-exited\nthreads in membermask . The destination operand d is a predicate register and its value is\nthe same across all threads in membermask . The reduction modes are: .all True if source predicate is True for all non-exited threads in membermask . Negate the\nsource predicate to compute .none . .any True if source predicate is True for some thread in membermask . Negate the source\npredicate to compute .not_all . .uni True if source predicate has the same value in all non-exited threads in membermask . Negating the source predicate also computes .uni . In the ballot form, the destination operand d is a .b32 register. In this form, vote.sync.ballot.b32 simply copies the predicate from each thread in membermask into the\ncorresponding bit position of destination register d , where the bit position corresponds to the\nthread’s lane id. A thread not specified in membermask will contribute a 0 for its entry in vote.sync.ballot.b32 . The behavior of vote.sync is undefined if the executing thread is not in the membermask . Note For .target sm_6x or below, all threads in membermask must execute the same vote.sync instruction in convergence, and only threads belonging to some membermask can be active when\nthe vote.sync instruction is executed. Otherwise, the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples vote.sync.all.pred    p,q,0xffffffff;\nvote.sync.ballot.b32  r1,p,0xffffffff;  // get 'ballot' across warp 9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync  match.sync Broadcast and compare a value across threads in warp. Syntax match.any.sync.type  d, a, membermask;\nmatch.all.sync.type  d[|p], a, membermask;\n\n.type = { .b32, .b64 }; Description match.sync will cause executing thread to wait until all non-exited threads from membermask have executed match.sync with the same qualifiers and same membermask value before resuming\nexecution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid. match.sync performs broadcast and compare of operand a across all non-exited threads in membermask and sets destination d and optional predicate p based on mode. Operand a has instruction type and d has .b32 type. Destination d is a 32-bit mask where bit position in mask corresponds to thread’s laneid. The matching operation modes are: .all d is set to mask corresponding to non-exited threads in membermask if all non-exited\nthreads in membermask have same value of operand a ; otherwise d is set\nto 0. Optionally predicate p is set to true if all non-exited threads in membermask have\nsame value of operand a ; otherwise p is set to false. The sink symbol ‘_’ may be used in\nplace of any one of the destination operands. .any d is set to mask of non-exited threads in membermask that have same value of operand a . The behavior of match.sync is undefined if the executing thread is not in the membermask . PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_70 or higher. Release Notes Note that match.sync applies to threads in a single warp, not across an entire CTA. Examples match.any.sync.b32    d, a, 0xffffffff;\nmatch.all.sync.b64    d|p, a, mask; 9.7.12.11. Parallel Synchronization and Communication Instructions: activemask  activemask Queries the active threads within a warp. Syntax activemask.b32 d; Description activemask queries predicated-on active threads from the executing warp and sets the destination d with 32-bit integer mask where bit position in the mask corresponds to the thread’s laneid . Destination d is a 32-bit destination register. An active thread will contribute 1 for its entry in the result and exited or inactive or\npredicated-off thread will contribute 0 for its entry in the result. PTX ISA Notes Introduced in PTX ISA version 6.2. Target ISA Notes Requires sm_30 or higher. Examples activemask.b32  %r1; 9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync  redux.sync Perform reduction operation on the data from each predicated active thread in the thread group. Syntax redux.sync.op.type dst, src, membermask;\n.op   = {.add, .min, .max}\n.type = {.u32, .s32}\n\nredux.sync.op.b32 dst, src, membermask;\n.op   = {.and, .or, .xor} Description redux.sync will cause the executing thread to wait until all non-exited threads corresponding to membermask have executed redux.sync with the same qualifiers and same membermask value\nbefore resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid . redux.sync performs a reduction operation .op of the 32 bit source register src across\nall non-exited threads in the membermask . The result of the reduction operation is written to\nthe 32 bit destination register dst . Reduction operation can be one of the bitwise operation in .and , .or , .xor or arithmetic\noperation in .add , .min , .max . For the .add operation result is truncated to 32 bits. The behavior of redux.sync is undefined if the executing thread is not in the membermask . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Release Notes Note that redux.sync applies to threads in a single warp, not across an entire CTA. Examples .reg .b32 dst, src, init, mask;\nredux.sync.add.s32 dst, src, 0xff;\nredux.sync.xor.b32 dst, src, mask; 9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol  griddepcontrol Control execution of dependent grids. Syntax griddepcontrol.action;\n\n.action   = { .launch_dependents, .wait } Description The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by\nthe runtime, to control execution in the following way: .launch_dependents modifier signals that specific dependents the runtime system designated to\nreact to this instruction can be scheduled as soon as all other CTAs in the grid issue the same\ninstruction or have completed. The dependent may launch before the completion of the current\ngrid. There is no guarantee that the dependent will launch before the completion of the current\ngrid. Repeated invocations of this instruction by threads in the current CTA will have no additional\nside effects past that of the first invocation. .wait modifier causes the executing thread to wait until all prerequisite grids in flight have\ncompleted and all the memory operations from the prerequisite grids are performed and made visible\nto the current grid. Note If the prerequisite grid is using griddepcontrol.launch_dependents , then the dependent grid\nmust use griddepcontrol.wait to ensure correct functional execution. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples griddepcontrol.launch_dependents;\ngriddepcontrol.wait; 9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync  elect.sync Elect a leader thread from a set of threads. Syntax elect.sync d|p, membermask; Description elect.sync elects one predicated active leader thread from among a set of threads specified by membermask . laneid of the elected thread is returned in the 32-bit destination operand d . The sink symbol ‘_’ can be used for destination operand d . The predicate destination p is set to True for the leader thread, and False for all other threads. Operand membermask specifies a 32-bit integer indicating the set of threads from which a leader\nis to be elected. The behavior is undefined if the executing thread is not in membermask . Election of a leader thread happens deterministically, i.e. the same leader thread is elected for\nthe same membermask every time. The mandatory .sync qualifier indicates that elect causes the executing thread to wait until\nall threads in the membermask execute the elect instruction before resuming execution. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples elect.sync    %r0|%p0, 0xffffffff; 9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier  mbarrier is a barrier created in shared memory that supports : Synchronizing any subset of threads within a CTA One-way synchronization of threads across CTAs of a cluster. As noted in mbarrier support with\nshared memory , threads can\nperform only arrive operations but not *_wait on an mbarrier located in shared::cluster space. Waiting for completion of asynchronous memory operations initiated by a thread and making them\nvisible to other threads. An mbarrier object is an opaque object in memory which can be initialized and invalidated using : mbarrier.init mbarrier.inval Operations supported on mbarrier object s are : mbarrier.expect_tx mbarrier.complete_tx mbarrier.arrive mbarrier.arrive_drop mbarrier.test_wait mbarrier.try_wait mbarrier.pending_count cp.async.mbarrier.arrive Performing any mbarrier operation except mbarrier.init on an uninitialized mbarrier object results in undefined behavior. Unlike bar{.cta} / barrier{.cta} instructions which can access a limited number of barriers\nper CTA, mbarrier objects are used defined and are only limited by the total shared memory size\navailable. mbarrier operations enable threads to perform useful work after the arrival at the mbarrier and\nbefore waiting for the mbarrier to complete. 9.7.12.15.1. Size and alignment of mbarrier object  An mbarrier object is an opaque object with the following type and alignment requirements : Type Alignment (bytes) Memory space .b64 8 .shared 9.7.12.15.2. Contents of the mbarrier object  An opaque mbarrier object keeps track of the following information : Current phase of the mbarrier object Count of pending arrivals for the current phase of the mbarrier object Count of expected arrivals for the next phase of the mbarrier object Count of pending asynchronous memory operations (or transactions) tracked by the current phase of\nthe mbarrier object . This is also referred to as tx-count . An mbarrier object progresses through a sequence of phases where each phase is defined by threads\nperforming an expected number of arrive-on operations. The valid range of each of the counts is as shown below: Count name Minimum value Maximum value Expected arrival count 1 2 20 - 1 Pending arrival count 0 2 20 - 1 tx-count -(2 20 - 1) 2 20 - 1 9.7.12.15.3. Lifecycle of the mbarrier object  The mbarrier object must be initialized prior to use. An mbarrier object is used to synchronize threads and asynchronous memory operations. An mbarrier object may be used to perform a sequence of such synchronizations. An mbarrier object must be invalidated to repurpose its memory. 9.7.12.15.4. Phase of the mbarrier object  The phase of an mbarrier object is the number of times the mbarrier object has been used to\nsynchronize threads and cp.async operations. In each phase {0, 1, 2, …}, threads perform in program order : arrive-on operations to complete the current phase and test_wait / try_wait operations to check for the completion of the current phase. An mbarrier object is automatically reinitialized upon completion of the current phase for\nimmediate use in the next phase. The current phase is incomplete and all prior phases are complete. For each phase of the mbarrier object, at least one test_wait or try_wait operation must be\nperformed which returns True for waitComplete before an arrive-on operation\nin the subsequent phase. 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object  Starting with the Hopper architecture ( sm_9x ), mbarrier object supports a new count, called tx-count , which is used for tracking the completion of asynchronous memory operations or\ntransactions. tx-count tracks the number of asynchronous transactions, in units specified by the\nasynchronous memory operation, that are outstanding and yet to be complete. The tx-count of an mbarrier object must be set to the total amount of asynchronous memory\noperations, in units as specified by the asynchronous operations, to be tracked by the current\nphase. Upon completion of each of the asynchronous operations, the complete-tx operation will be performed on the mbarrier object and thus progress the mbarrier towards the\ncompletion of the current phase. 9.7.12.15.5.1. expect-tx operation  The expect-tx operation, with an expectCount argument, increases the tx-count of an mbarrier object by the value specified by expectCount . This makes the current phase of the mbarrier object to expect and track the completion of additional asynchronous transactions. 9.7.12.15.5.2. complete-tx operation  The complete-tx operation, with an completeCount argument, on an mbarrier object consists of the following: mbarrier signaling Signals the completion of asynchronous transactions that were tracked by the current phase. As a\nresult of this, tx-count is decremented by completeCount . mbarrier potentially completing the current phase If the current phase has been completed then the mbarrier transitions to the next phase. Refer to Phase Completion of the mbarrier object for details on phase completion requirements and phase transition process. 9.7.12.15.6. Phase Completion of the mbarrier object  The requirements for completion of the current phase are described below. Upon completion of the\ncurrent phase, the phase transitions to the subsequent phase as described below. Current phase completion requirements An mbarrier object completes the current phase when all of the following conditions are met: The count of the pending arrivals has reached zero. The tx-count has reached zero. Phase transition When an mbarrier object completes the current phase, the following actions are performed\natomically: The mbarrier object transitions to the next phase. The pending arrival count is reinitialized to the expected arrival count. 9.7.12.15.7. Arrive-on operation on mbarrier object  An arrive-on operation, with an optional count argument, on an mbarrier object consists of the\nfollowing 2 steps : mbarrier signalling: Signals the arrival of the executing thread OR completion of the cp.async instruction which\nsignals the arrive-on operation initiated by the executing thread on the mbarrier object . As a\nresult of this, the pending arrival count is decremented by count . If the count argument is\nnot specified, then it defaults to 1. mbarrier potentially completing the current phase: If the current phase has been completed then the mbarrier transitions to the next phase. Refer to Phase Completion of the mbarrier object for details on phase completion requirements and phase transition process. 9.7.12.15.8. mbarrier support with shared memory  The following table summarizes the support of various mbarrier operations on mbarrier objects located at different shared memory locations: mbarrier operations .shared::cta .shared::cluster mbarrier.arrive Supported Supported, cannot return result mbarrier.expect_tx Supported Supported mbarrier.complete_tx Supported Supported Other mbarrier operations Supported Not supported 9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init  mbarrier.init Initialize the mbarrier object . Syntax mbarrier.init{.shared{::cta}}.b64 [addr], count; Description mbarrier.init initializes the mbarrier object at the location specified by the address operand addr with the unsigned 32-bit integer count . The value of operand count must be in the range\nas specified in Contents of the mbarrier object . Initialization of the mbarrier object involves : Initializing the current phase to 0. Initializing the expected arrival count to count . Initializing the pending arrival count to count . Initializing the tx-count to 0. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Examples .shared .b64 shMem, shMem2;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n\ncvta.shared.u64          addr, shMem2;\nmbarrier.init.b64        [addr],   %r1;\nbar.cta.sync             0;\n// ... other mbarrier operations on addr\n\nmbarrier.init.shared::cta.b64 [shMem], 12;\nbar.sync                 0;\n// ... other mbarrier operations on shMem 9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval  mbarrier.inval Invalidates the mbarrier object . Syntax mbarrier.inval{.shared{::cta}}.b64 [addr]; Description mbarrier.inval invalidates the mbarrier object at the location specified by the address\noperand addr . An mbarrier object must be invalidated before using its memory location for any other purpose. Performing any mbarrier operation except mbarrier.init on an invalidated mbarrier object\nresults in undefined behaviour. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Examples .shared .b64 shmem;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n.reg    .pred t0;\n\n// Example 1 :\nbar.sync                      0;\n@t0 mbarrier.init.b64     [addr], %r1;\n// ... other mbarrier operations on addr\nbar.sync                      0;\n@t0 mbarrier.inval.b64    [addr];\n\n\n// Example 2 :\nbar.cta.sync                  0;\nmbarrier.init.shared.b64           [shmem], 12;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared.b64      [shmem];\n\n// shmem can be reused here for unrelated use :\nbar.cta.sync                  0;\nst.shared.b64                      [shmem], ...;\n\n// shmem can be re-initialized as mbarrier object :\nbar.cta.sync                  0;\n@t0 mbarrier.init.shared.b64       [shmem], 24;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared::cta.b64 [shmem]; 9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx  mbarrier.expect_tx Perfoms expect-tx operation on the mbarrier object . Syntax mbarrier.expect_tx{.sem}{.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.expect_tx performs an expect-tx operation on the mbarrier object at the location specified by the address operand addr . The\n32-bit unsigned integer operand txCount specifies the expectCount argument to the expect-tx operation. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr are as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . This operation does not provide any memory ordering semantics and thus is a relaxed operation. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples mbarrier.expect_tx.b64                       [addr], 32;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj1], 512;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj2], 512; 9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx  mbarrier.complete_tx Perfoms complete-tx operation on the mbarrier object . Syntax mbarrier.complete_tx{.sem}{.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.complete_tx performs a complete-tx operation on the mbarrier object at the location specified by the address operand addr . The\n32-bit unsigned integer operand txCount specifies the completeCount argument to the complete-tx operation. mbarrier.complete_tx does not involve any asynchronous memory operations and only simulates the\ncompletion of an asynchronous memory operation and its side effect of signaling to the mbarrier\nobject . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr are as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . This operation does not provide any memory ordering semantics and thus is a relaxed operation. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90 or higher. Examples mbarrier.complete_tx.b64             [addr],     32;\nmbarrier.complete_tx.shared.b64      [mbarObj1], 512;\nmbarrier.complete_tx.relaxed.cta.b64 [addr2],    32; 9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive  mbarrier.arrive Performs arrive-on operation on the mbarrier object . Syntax mbarrier.arrive{.sem}{.scope}{.shared{::cta}}.b64           state, [addr]{, count};\nmbarrier.arrive{.sem}{.scope}{.shared::cluster}.b64         _, [addr] {,count}\nmbarrier.arrive.expect_tx{.sem}{.scope}{.shared{::cta}}.b64 state, [addr], txCount;\nmbarrier.arrive.expect_tx{.sem}{.scope}{.shared::cluster}.b64   _, [addr], txCount;\nmbarrier.arrive.noComplete{.sem}{.cta}{.shared{::cta}}.b64  state, [addr], count;\n\n.sem   = { .release }\n.scope = { .cta, .cluster } Description A thread executing mbarrier.arrive performs an arrive-on operation\non the mbarrier object at the location specified by the address operand addr . The 32-bit\nunsigned integer operand count specifies the count argument to the arrive-on operation. If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . The optional qualifier .expect_tx specifies that an expect-tx operation is performed prior to the arrive-on operation. The 32-bit unsigned integer operand txCount specifies the expectCount argument to\nthe expect-tx operation. When both qualifiers .arrive and .expect_tx are specified, then\nthe count argument of the arrive-on operation is assumed to be 1. A mbarrier.arrive operation with .noComplete qualifier must not cause the mbarrier to\ncomplete its current phase, otherwise the behavior is undefined. The value of the operand count must be in the range as specified in Contents of the mbarrier\nobject . Note: for sm_8x , when the argument count is specified, the modifier .noComplete is\nrequired. mbarrier.arrive operation on an mbarrier object located in .shared::cta returns an opaque\n64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the\ndestination operand state. Contents of the state operand are implementation\nspecific. Optionally, sink symbol '_' can be used for the state argument. mbarrier.arrive operation on an mbarrier object located in .shared::cluster but not in .shared::cta cannot return a value. Sink symbol ‘_’ is mandatory for the destination operand for\nsuch cases. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory\nConsistency Model . If the .sem qualifier is absent, .release is assumed by default. The optional .scope qualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is not specified then it\ndefaults to .cta . In contrast, the .shared::<scope> indicates the state space where the\nmbarrier resides. PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Support for count argument without the modifier .noComplete introduced in PTX ISA version\n7.8. Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0. Support for qualifier .expect_tx is introduced in PTX ISA version 8.0. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes Requires sm_80 or higher. Support for count argument without the modifier .noComplete requires sm_90 or higher. Qualifier .expect_tx requires sm_90 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples .reg .b32 cnt, remoteAddr32, remoteCTAId, addr32;\n.reg .b64 %r<3>, addr, remoteAddr64;\n.shared .b64 shMem, shMem2;\n\ncvta.shared.u64            addr, shMem2;\nmov.b32                    addr32, shMem2;\nmapa.shared::cluster.u32   remoteAddr32, addr32, remoteCTAId;\nmapa.u64                   remoteAddr64, addr,   remoteCTAId;\n\ncvta.shared.u64          addr, shMem2;\n\nmbarrier.arrive.shared.b64                       %r0, [shMem];\nmbarrier.arrive.shared::cta.b64                  %r0, [shMem2];\nmbarrier.arrive.release.cta.shared::cluster.b64  _, [remoteAddr32];\nmbarrier.arrive.release.cluster.b64              _, [remoteAddr64], cnt;\nmbarrier.arrive.expect_tx.release.cluster.b64    _, [remoteAddr64], tx_count;\nmbarrier.arrive.noComplete.b64                   %r1, [addr], 2;\nmbarrier.arrive.b64                              %r2, [addr], cnt; 9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop  mbarrier.arrive_drop Decrements the expected count of the mbarrier object and performs arrive-on operation . Syntax mbarrier.arrive_drop{.sem}{.scope}{.shared{::cta}}.b64 state,           [addr]{, count};\nmbarrier.arrive_drop{.sem}{.scope}{.shared::cluster}.b64           _,   [addr] {,count};\nmbarrier.arrive_drop.expect_tx{.shared{::cta}}{.sem}{.scope}.b64 state, [addr], tx_count;\nmbarrier.arrive_drop.expect_tx{.shared::cluster}{.sem}{.scope}.b64   _, [addr], tx_count;\nmbarrier.arrive_drop.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state,  [addr], count;\n\n.sem   = { .release }\n.scope = { .cta, .cluster } Description A thread executing mbarrier.arrive_drop on the mbarrier object at the location specified by\nthe address operand addr performs the following steps: Decrements the expected arrival count of the mbarrier object by the value specified by the\n32-bit integer operand count . If count operand is not specified, it defaults to 1. Performs an arrive-on operation on the mbarrier object . The operand count specifies the count argument to the arrive-on\noperation . The decrement done in the expected arrivals count of the mbarrier object will be for all the\nsubsequent phases of the mbarrier object . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . The optional qualifier .expect_tx specifies that an expect-tx operation is performed prior to the arrive-on operation. The 32-bit unsigned integer operand txCount specifies the expectCount argument to\nthe expect-tx operation. When both qualifiers .arrive and .expect_tx are specified, then\nthe count argument of the arrive-on operation is assumed to be 1. mbarrier.arrive_drop operation forms the release pattern as described in the Memory\nConsistency Model and synchronizes with the acquire patterns. The optional .scope qualifier indicates the set of threads that an mbarrier.arrive_drop instruction can directly synchronize. If the .scope qualifier is not specified then it defaults\nto .cta . In contrast, the .shared::<scope> indicates the state space where the mbarrier\nresides. A mbarrier.arrive_drop with .noComplete qualifier must not complete the mbarrier, otherwise the behavior is undefined. The value of the operand count must be in the range as specified in Contents of the mbarrier\nobject . Note: for sm_8x , when the argument count is specified, the modifier .noComplete is\nrequired. A thread that wants to either exit or opt out of participating in the arrive-on operation can use mbarrier.arrive_drop to drop itself from the mbarrier . mbarrier.arrive_drop operation on an mbarrier object located in .shared::cta returns an\nopaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on\noperation in the destination operand state . Contents of the returned state are implementation\nspecific. Optionally, sink symbol '_' can be used for the state argument. mbarrier.arrive_drop operation on an mbarrier object located in .shared::cluster but not\nin .shared::cta cannot return a value. Sink symbol ‘_’ is mandatory for the destination operand\nfor such cases. PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Support for count argument without the modifier .noComplete introduced in PTX ISA version\n7.8. Support for qualifier .expect_tx is introduced in PTX ISA version 8.0. Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes Requires sm_80 or higher. Support for count argument without the modifier .noComplete requires sm_90 or higher. Qualifier .expect_tx requires sm_90 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples .reg .b32 cnt;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\n// Example 1\n@p mbarrier.arrive_drop.shared.b64 _, [shMem];\n@p exit;\n@p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a;\n@p2 exit;\n..\n@!p mbarrier.arrive.shared.b64   %r1, [shMem];\n@!p mbarrier.test_wait.shared.b64  q, [shMem], %r1;\n\n// Example 2\nmbarrier.arrive_drop.shared::cluster.b64 _, [addr];\nmbarrier.arrive_drop.shared::cta.release.cluster.b64     _, [addr], cnt;\n\n// Example 3\nmbarrier.arrive_drop.expect_tx.shared::cta.release.cta.b64 state, [addr], tx_count; 9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive  cp.async.mbarrier.arrive Makes the mbarrier object track all prior cp.async operations initiated by the\nexecuting thread. Syntax cp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr]; Description Causes an arrive-on operation to be\ntriggered by the system on the mbarrier object upon the completion of all prior cp.async operations initiated by the\nexecuting thread. The mbarrier object is at the location specified by the operand addr . The arrive-on operation is\nasynchronous to execution of cp.async.mbarrier.arrive . When .noinc modifier is not specified, the pending count of the mbarrier object is incremented\nby 1 prior to the asynchronous arrive-on operation . This\nresults in a zero-net change for the pending count from the asynchronous arrive-on operation\nduring the current phase. The pending count of the mbarrier object after the increment should not\nexceed the limit as mentioned in Contents of the mbarrier object . Otherwise,\nthe behavior is undefined. When the .noinc modifier is specified, the increment to the pending count of the mbarrier\nobject is not performed. Hence the decrement of the pending count done by the asynchronous arrive-on operation must be\naccounted for in the initialization of the mbarrier object . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . PTX ISA Notes Introduced in PTX ISA version 7.0. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_80 or higher. Examples // Example 1: no .noinc\nmbarrier.init.shared.b64 [shMem], threadCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n....\n// Absence of .noinc accounts for arrive-on from completion of prior cp.async operations.\n// So mbarrier.init must only account for arrive-on from mbarrier.arrive.\ncp.async.mbarrier.arrive.shared.b64 [shMem];\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n\n\n\n// Example 2: with .noinc\n\n// Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.\n\n// All threads participating in the mbarrier perform cp.async\nmov.b32 copyOperationCnt, threadCount;\n\n// 3 arrive-on operations will be triggered per-thread\nmul.lo.u32 copyArrivalCnt, copyOperationCnt, 3;\n\nadd.u32 totalCount, threadCount, copyArrivalCnt;\n\nmbarrier.init.shared.b64 [shMem], totalCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n...\n// Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance\n....\ncp.async.ca.shared.global [shard3], [gbl3], 4;\ncp.async.ca.shared.global [shard4], [gbl4], 16;\ncp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance\n....\ncp.async.ca.shared.global [shard5], [gbl5], 4;\ncp.async.cg.shared.global [shard6], [gbl6], 16;\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop; 9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait  mbarrier.test_wait/mbarrier.try_wait Checks whether the mbarrier object has completed the phase. Syntax mbarrier.test_wait{.sem}{.scope}{.shared{::cta}}.b64        waitComplete, [addr], state;\nmbarrier.test_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity;\n\nmbarrier.try_wait{.sem}{.scope}{.shared{::cta}}.b64         waitComplete, [addr], state\n                                                               {, suspendTimeHint};\n\nmbarrier.try_wait.parity{.sem}{.scope}{.shared{::cta}}.b64  waitComplete, [addr], phaseParity\n                                                               {, suspendTimeHint};\n\n.sem   = { .acquire }\n.scope = { .cta, .cluster } Description The test_wait and try_wait operations test for the completion of the current or the immediately\npreceding phase of an mbarrier object at the location specified by the operand addr . mbarrier.test_wait is a non-blocking instruction which tests for the completion of the phase. mbarrier.try_wait is a potentially blocking instruction which tests for the completion of the\nphase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes\nexecution when the specified phase completes OR before the phase completes following a\nsystem-dependent time limit. The optional 32-bit unsigned integer operand suspendTimeHint specifies the time limit, in nanoseconds, that may be used for the time limit instead of the\nsystem-dependent limit. mbarrier.test_wait and mbarrier.try_wait test for completion of the phase : Specified by the operand state , which was returned by an mbarrier.arrive instruction on\nthe same mbarrier object during the current or the immediately preceding phase. Or Indicated by the operand phaseParity , which is the integer parity of either the current phase\nor the immediately preceding phase of the mbarrier object . The .parity variant of the instructions test for the completion of the phase indicated by the\noperand phaseParity , which is the integer parity of either the current phase or the immediately\npreceding phase of the mbarrier object . An even phase has integer parity 0 and an odd phase has\ninteger parity of 1. So the valid values of phaseParity operand are 0 and 1. Note: the use of the .parity variants of the instructions requires tracking the phase of an mbarrier object throughout its lifetime. The test_wait and try_wait operations are valid only for : the current incomplete phase, for which waitComplete returns False . the immediately preceding phase, for which waitComplete returns True . If no state space is specified then Generic Addressing is\nused. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size\nand alignment of mbarrier object . When mbarrier.test_wait and mbarrier.try_wait operations return True , they form the acquire pattern as described in the Memory Consistency Model . The optional .scope qualifier indicates the set of threads that the mbarrier.test_wait and mbarrier.try_wait instructions can directly synchronize. If the .scope qualifier is not\nspecified then it defaults to .cta . In contrast, the .shared::<scope> indicates the state\nspace where the mbarrier resides. The following ordering of memory operations hold for the executing thread when mbarrier.test_wait or mbarrier.try_wait returns True : All memory accesses (except async operations ) requested prior, in program\norder, to mbarrier.arrive during the completed phase by the participating threads of the CTA\nare performed and are visible to the executing thread. All cp.async operations\nrequested prior, in program order, to cp.async.mbarrier.arrive during the completed phase by\nthe participating threads of the CTA are performed and made visible to the executing thread. All cp.async.bulk asynchronous operations using the same mbarrier object requested prior,\nin program order, to mbarrier.arrive during the completed phase by the participating threads\nof the CTA are performed and made visible to the executing thread. All memory accesses requested after the mbarrier.test_wait or mbarrier.try_wait , in\nprogram order, are not performed and not visible to memory accesses performed prior to mbarrier.arrive , in program order, by other threads participating in the mbarrier . There is no ordering and visibility guarantee for memory accesses requested by the thread after mbarrier.arrive and prior to mbarrier.test_wait , in program order. PTX ISA Notes mbarrier.test_wait introduced in PTX ISA version 7.0. Modifier .parity is introduced in PTX ISA version 7.1. mbarrier.try_wait introduced in PTX ISA version 7.8. Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes mbarrier.test_wait requires sm_80 or higher. mbarrier.try_wait requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples // Example 1a, thread synchronization with test_wait:\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.test_wait.shared.b64    complete, [shMem], %r1;\n@!complete nanosleep.u32 20;\n@!complete bra waitLoop;\n\n// Example 1b, thread synchronization with try_wait :\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.try_wait.shared.b64    complete, [shMem], %r1;\n@!complete bra waitLoop;\n\n\n// Example 2, thread synchronization using phase parity :\n\n.reg .b32 i, parArg;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmov.b32 i, 0;\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nloopStart :                           // One phase per loop iteration\n    ...\n    mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads\n    ...\n    and.b32 parArg, i, 1;\n    waitLoop:\n    mbarrier.test_wait.parity.shared.b64  complete, [shMem], parArg;\n    @!complete nanosleep.u32 20;\n    @!complete bra waitLoop;\n    ...\n    add.u32 i, i, 1;\n    setp.lt.u32 p, i, IterMax;\n@p bra loopStart;\n\n\n// Example 3, Asynchronous copy completion waiting :\n\n.reg .b64 state;\n.shared .b64 shMem2;\n.shared .b64 shard1, shard2;\n.global .b64 gbl1, gbl2;\n\nmbarrier.init.shared.b64 [shMem2], threadCount;\n...\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n\n// Absence of .noinc accounts for arrive-on from prior cp.async operation\ncp.async.mbarrier.arrive.shared.b64 [shMem2];\n...\nmbarrier.arrive.shared.b64 state, [shMem2];\n\nwaitLoop:\nmbarrier.test_wait.shared::cta.b64 p, [shMem2], state;\n@!p bra waitLoop;\n\n// Example 4, Synchronizing the CTA0 threads with cluster threads\n.reg .b64 %r1, addr, remAddr;\n.shared .b64 shMem;\n\ncvta.shared.u64          addr, shMem;\nmapa.u64                 remAddr, addr, 0;     // CTA0’s shMem instance\n\n// One thread from CTA0 executing the below initialization operation\n@p0 mbarrier.init.shared::cta.b64 [shMem], N;  // N = no of cluster threads\n\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Entire cluster executing the below arrive operation\nmbarrier.arrive.release.cluster.b64              _, [remAddr];\n\n// computation not requiring mbarrier synchronization ...\n\n// Only CTA0 threads executing the below wait operation\nwaitLoop:\nmbarrier.try_wait.parity.acquire.cluser.shared::cta.b64  complete, [shMem], 0;\n@!complete bra waitLoop; 9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count  mbarrier.pending_count Query the pending arrival count from the opaque mbarrier state. Syntax mbarrier.pending_count.b64 count, state; Description The pending count can be queried from the opaque mbarrier state using mbarrier.pending_count . The state operand is a 64-bit register that must be the result of a prior mbarrier.arrive.noComplete or mbarrier.arrive_drop.noComplete instruction. Otherwise, the\nbehavior is undefined. The destination register count is a 32-bit unsigned integer representing the pending count of\nthe mbarrier object prior to the arrive-on operation from\nwhich the state register was obtained. PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_80 or higher. Examples .reg .b32 %r1;\n.reg .b64 state;\n.shared .b64 shMem;\n\nmbarrier.arrive.noComplete.b64 state, [shMem], 1;\nmbarrier.pending_count.b64 %r1, state; 9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy  tensormap.cp_fenceproxy A fused copy and fence operation. Syntax tensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned  [dst], [src], size;\n\n.cp_qualifiers    = { .global.shared::cta }\n.fence_qualifiers = { .to_proxy::from_proxy.release.scope }\n.to_proxy::from_proxy  = { .tensormap::generic }\n.scope            = { .cta, .cluster, .gpu , .sys } Description The tensormap.cp_fence instructions perform the following operations in order : Copies data of size specified by the size argument, in bytes, from the location specified\nby the address operand src in shared memory to the location specified by the address operand dst in the global memory, in the generic proxy. Establishes a uni-directional proxy release pattern on the ordering from the copy operation\nto the subsequent access performed in the tensormap proxy on the address dst . The valid value of size operand is 128. The operands src and dst specify non-generic addresses in shared::cta and global state space respectively. The optional .scope qualifier specifies the set of threads that can directly observe the proxy\nsynchronizing effect of this operation, as described in Memory Consistency Model . The mandatory .sync qualifier indicates that tensormap.cp_fenceproxy causes the executing\nthread to wait until all threads in the warp execute the same tensormap.cp_fenceproxy instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tensormap.cp_fenceproxy instruction. In conditionally executed code, an aligned tensormap.cp_fenceproxy instruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_90 or higher. Examples // Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor\n\n.reg .b64 new_addr;\n.global .align 128 .b8 gbl[128];\n.shared .align 128 .b8 sMem[128];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar];\n...\ntry_wait_loop:\nmbarrier.try_wait.shared.b64 p, [mbar], state;\n@!p bra try_wait loop;\n\ntensormap.replace.tile.global_address.shared.b1024.b64   [sMem], new_addr;\ntensormap.cp_fenceproxy.global.shared::cta.proxy.tensormap::generic.release.gpu\n                                    .sync.aligned        [gbl], [sMem], 128;\nfence.proxy.tensormap::generic.acquire.gpu [gbl], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [gbl, {tc0}], [mbar0]; 9.7.13. Warp Level Matrix Multiply-Accumulate Instructions  The matrix multiply and accumulate operation has the following form: D = A * B + C where D and C are called accumulators and may refer to the same matrix. PTX provides two ways to perform matrix multiply-and-accumulate computation: Using wmma instructions: This warp-level computation is performed collectively by all threads in the warp as follows: Load matrices A, B and C from memory into registers using the wmma.load operation. When\nthe operation completes, the destination registers in each thread hold a fragment of the\nloaded matrix. Perform the matrix multiply and accumulate operation using the wmma.mma operation on the\nloaded matrices. When the operation completes, the destination registers in each thread hold\na fragment of the result matrix returned by the wmma.mma operation. Store result Matrix D back to memory using the wmma.store operation. Alternately, result\nmatrix D can also be used as argument C for a subsequent wmma.mma operation. The wmma.load and wmma.store instructions implicitly handle the organization of matrix\nelements when loading the input matrices from memory for the wmma.mma operation and when\nstoring the result back to memory. Using mma instruction: Similar to wmma , mma also requires computation to be performed collectively by all\nthreads in the warp however distribution of matrix elements across different threads in warp\nneeds to be done explicitly before invoking the mma operation. The mma instruction\nsupports both dense as well as sparse matrix A. The sparse variant can be used when A is a\nstructured sparse matrix as described in Sparse matrix storage . 9.7.13.1. Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices. The following matrix shapes are supported for the specified types: Instruction Sparsity Multiplicand Data-type Shape PTX ISA version wmma Dense Floating-point - .f16 .m16n16k16 , .m8n32k16 ,\nand .m32n8k16 PTX ISA version 6.0 wmma Dense Alternate floating-point format - .bf16 .m16n16k16 , .m8n32k16 ,\nand .m32n8k16 PTX ISA version 7.0 wmma Dense Alternate floating-point format - .tf32 .m16n16k8 PTX ISA version 7.0 wmma Dense Integer - .u8 / .s8 .m16n16k16 , .m8n32k16 ,\nand .m32n8k16 PTX ISA version 6.3 wmma Dense Sub-byte integer - .u4 / .s4 .m8n8k32 PTX ISA version 6.3\n(preview feature) wmma Dense Single-bit - .b1 .m8n8k128 PTX ISA version 6.3\n(preview feature) mma Dense Floating-point - .f64 .m8n8k4 PTX ISA version 7.0 .m16n8k4 , .m16n8k8 ,\nand .m16n8k16 PTX ISA version 7.8 mma Dense Floating-point - .f16 .m8n8k4 PTX ISA version 6.4 .m16n8k8 PTX ISA version 6.5 .m16n8k16 PTX ISA version 7.0 mma Dense Alternate floating-point format - .bf16 .m16n8k8 and .m16n8k16 PTX ISA version 7.0 mma Dense Alternate floating-point format - .tf32 .m16n8k4 and .m16n8k8 PTX ISA version 7.0 mma Dense Integer - .u8 / .s8 .m8n8k16 PTX ISA version 6.5 .m16n8k16 and .m16n8k32 PTX ISA version 7.0 mma Dense Sub-byte integer - .u4 / .s4 .m8n8k32 PTX ISA version 6.5 .m16n8k32 and .m16n8k64 PTX ISA version 7.0 mma Dense Single-bit - .b1 .m8n8k128 , .m16n8k128 ,\nand .m16n8k256 PTX ISA version 7.0 mma Dense Alternate floating-point format - .e4m3 / .e5m2 .m16n8k32 PTX ISA version 8.4 mma Sparse Floating-point - .f16 .m16n8k16 and .m16n8k32 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .bf16 .m16n8k16 and .m16n8k32 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .tf32 .m16n8k8 and .m16n8k16 PTX ISA version 7.1 mma Sparse Integer - .u8 / .s8 .m16n8k32 and .m16n8k64 PTX ISA version 7.1 mma Sparse Sub-byte integer - .u4 / .s4 .m16n8k64 and .m16n8k128 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .e4m3 / .e5m2 .m16n8k64 PTX ISA version 8.4 mma Sparse\nwith\nordered\nmetadata Floating-point - .f16 .m16n8k16 and .m16n8k32 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Alternate floating-point format - .bf16 .m16n8k16 and .m16n8k32 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Alternate floating-point format - .tf32 .m16n8k8 and .m16n8k16 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Integer - .u8 / .s8 .m16n8k32 and .m16n8k64 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Sub-byte integer - .u4 / .s4 .m16n8k64 and .m16n8k128 PTX ISA version 8.5 mma Sparse\nwith\nordered\nmetadata Alternate floating-point format - .e4m3 / .e5m2 .m16n8k64 PTX ISA version 8.5 9.7.13.2. Matrix Data-types  The matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point. For floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later. Data-type Multiplicands (A or B) Accumulators (C or D) Integer .u8 , .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 or .e5m2 .f32 Floating Point .f64 .f64 Sub-byte integer both .u4 or both .s4 .s32 Single-bit integer .b1 .s32 9.7.13.3. Matrix multiply-accumulate operation using wmma instructions  This section describes warp level wmma.load, wmma.mma and wmma.store instructions and the\norganization of various matrices invovled in these instruction. 9.7.13.3.1. Matrix Fragments for WMMA  Each thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the\nthreads in a warp is unspecified and is target architecture dependent, and hence the identity of the\nfragment within the matrix is also unspecified and is target architecture dependent. The fragment\nreturned by a wmma operation can be used as an operand for another wmma operation if the\nshape, layout and element type of the underlying matrix matches. Since fragment layout is\narchitecture dependent, using the fragment returned by a wmma operation in one function as an\noperand for a wmma operation in a different function may not work as expected if the two\nfunctions are linked together but were compiled for different link-compatible SM architectures. Note\npassing wmma fragment to a function having .weak linkage is unsafe since at link time\nreferences to such function may get resolved to a function in different compilation module. Each fragment is a vector expression whose contents are determined as follows. The identity of\nindividual matrix elements in the fragment is unspecified. Integer fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .u8 or .s8 .m16n16k16 A A vector expression of two .b32 registers, with each\nregister containing four elements from the matrix. B A vector expression of two .b32 registers, with each\nregister containing four elements from the matrix. .m8n32k16 A A vector expression containing a single .b32 register\ncontaining four elements from the matrix. B A vector expression of four .b32 registers, with each\nregister containing four elements from the matrix. .m32n8k16 A A vector expression of four .b32 registers, with each\nregister containing four elements from the matrix. B A vector expression containing single .b32 register,\nwith each containing four elements from the matrix. Accumulators (C or D): Data-type Shape Fragment .s32 .m16n16k16 A vector expression of eight .s32 registers. .m8n32k16 .m32n8k16 Floating point fragments Data-type Matrix Fragment .f16 A or B A vector expression of eight .f16x2 registers. .f16 C or D A vector expression of four .f16x2 registers. .f32 A vector expression of eight .f32 registers. Floating point fragments for .bf16 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .bf16 .m16n16k16 A A vector expression of four .b32 registers, with each\nregister containing two elements from the matrix. B .m8n32k16 A A vector expression containing a two .b32 registers,\nwith containing two elements from the matrix. B A vector expression of eight .b32 registers, with\neach register containing two elements from the matrix. .m32n8k16 A A vector expression of eight .b32 registers, with\neach register containing two elements from the matrix. B A vector expression containing two .b32 registers,\nwith each containing two elements from the matrix. Accumulators (C or D): Data-type Matrix Fragment .f32 C or D A vector expression containing eight .f32 registers. Floating point fragments for .tf32 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .tf32 .m16n16k8 A A vector expression of four .b32 registers. B A vector expression of four .b32 registers. Accumulators (C or D): Data-type Shape Matrix Fragment .f32 .m16n16k8 C or D A vector expression containing eight .f32 registers. Double precision floating point fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .f64 .m8n8k4 A or B A vector expression of single .f64 register. Accumulators (C or D): Data-type Shape Matrix Fragment .f64 .m8n8k4 C or D A vector expression containing single .f64 register. Sub-byte integer and single-bit fragments Multiplicands (A or B): Data-type Shape Fragment .u4 or .s4 .m8n8k32 A vector expression containing a single .b32 register, containing eight elements from the matrix. .b1 .m8n8k128 A vector expression containing a single .b32 register, containing 32 elements from the matrix. Accumulators (C or D): Data-type Shape Fragment .s32 .m8n8k32 A vector expression of two .s32 registers. .m8n8k128 A vector expression of two .s32 registers. Manipulating fragment contents The contents of a matrix fragment can be manipulated by reading and writing to individual\nregisters in the fragment, provided the following conditions are satisfied: All matrix element in the fragment are operated on uniformly across threads, using the same\nparameters. The order of the matrix elements is not changed. For example, if each register corresponding to a given matrix is multiplied by a uniform constant\nvalue, then the resulting matrix is simply the scaled version of the original matrix. Note that type conversion between .f16 and .f32 accumulator fragments is not supported in\neither direction. The result is undefined even if the order of elements in the fragment remains\nunchanged. 9.7.13.3.2. Matrix Storage for WMMA  Each matrix can be stored in memory with a row-major or column-major layout. In a row-major format, consecutive elements of each row are stored in contiguous memory locations, and the row is\ncalled the leading dimension of the matrix. In a column-major format, consecutive elements of\neach column are stored in contiguous memory locations and the column is called the leading\ndimension of the matrix. Consecutive instances of the leading dimension (rows or columns) need not be stored contiguously\nin memory. The wmma.load and wmma.store operations accept an optional argument stride that specifies the offset from the beginning of each row (or column) to the next, in terms of matrix\nelements (and not bytes). For example, the matrix being accessed by a wmma operation may be a\nsubmatrix from a larger matrix stored in memory. This allows the programmer to compose a\nmultiply-and-accumulate operation on matrices that are larger than the shapes supported by the wmma operation. Address Alignment: The starting address of each instance of the leading dimension (row or column) must be aligned\nwith the size of the corresponding fragment in bytes. Note that the starting address is\ndetermined by the base pointer and the optional stride . Consider the following instruction as an example: wmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s; Fragment size in bytes = 32 (eight elements of type .f16x2 ) Actual stride in bytes = 2 * s (since stride is specified in terms of .f16 elements, not bytes) For each row of this matrix to be aligned at fragment size the following must be true: p is a multiple of 32. 2*s is a multiple of 32. Default value for stride: The default value of the stride is the size of the leading dimension of the matrix. For\nexample, for an MxK matrix, the stride is K for a row-major layout and M for a column-major layout. In particular, the default strides for the supported matrix shapes are as\nfollows: Shape A (row) A (column) B (row) B (column) Accumulator (row) Accumulator (column) 16x16x16 16 16 16 16 16 16 8x32x16 16 8 32 16 32 8 32x8x16 16 32 8 16 8 32 8x8x32 32 8 8 32 8 8 8x8x128 128 8 8 128 8 8 16x16x8 8 16 16 8 16 16 8x8x4 4 8 8 4 8 8 9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load  wmma.load Collectively load a matrix from memory for WMMA Syntax Floating point format .f16 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride};\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride};\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f16, .s8, .u8};\n.btype  = {.f16, .s8, .u8};\n.ctype  = {.f16, .f32, .s32}; Alternate floating point format .bf16 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.bf16 };\n.btype  = {.bf16 };\n.ctype  = {.f32 }; Alternate floating point format .tf32 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.tf32 };\n.btype  = {.tf32 };\n.ctype  = {.f32 }; Double precision Floating point .f64 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f64 };\n.btype  = {.f64 };\n.ctype  = {.f64 }; Sub-byte loads: wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\n.ctype  = {.s32}; Single-bit loads: wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.b1};\n.btype  = {.b1};\n.ctype  = {.s32}; Description Collectively load a matrix across all threads in a warp from the location indicated by address\noperand p in the specified state space into destination register r . If no state space is given, perform the memory accesses using Generic Addressing . wmma.load operation may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. The mutually exclusive qualifiers .a , .b and .c indicate whether matrix A, B or C is\nbeing loaded respectively for the wmma computation. The destination operand r is a brace-enclosed vector expression that can hold the fragment\nreturned by the load operation, as described in Matrix Fragments for WMMA . The .shape qualifier indicates the dimensions of all the matrix arguments involved in the\nintended wmma computation. The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or column-major format. stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the leading dimension (rows or columns). The default\nvalue of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior. The required alignment for address p and stride is described in the Matrix Storage for WMMA . The mandatory .sync qualifier indicates that wmma.load causes the executing thread to wait\nuntil all threads in the warp execute the same wmma.load instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.load instruction. In conditionally executed code, a wmma.load instruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined. The behavior of wmma.load is undefined if all threads do not use the same qualifiers and the\nsame values of p and stride , or if any thread in the warp has exited. wmma.load is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.0. .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. .m8n8k4 and .m16n16k8 on wmma introduced in PTX ISA version 7.0. Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. All\ndetails are subject to change with no guarantees of backward compatibility on future PTX ISA\nversions or SM architectures. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher. Double precision and alternate floating point precision wmma requires sm_80 or higher. Examples // Load elements from f16 row-major matrix B\n.reg .b32 x<8>;\n\nwmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from f32 column-major matrix C and scale the values:\n.reg .b32 x<8>;\n\nwmma.load.c.sync.aligned.m16n16k16.col.f32\n                 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr];\n\nmul.f32 x0, x0, 0.1;\n// repeat for all registers x<8>;\n...\nmul.f32 x7, x7, 0.1;\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from integer matrix A:\n.reg .b32 x<4>\n// destination registers x<4> contain four packed .u8 values each\nwmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];\n\n// Load elements from sub-byte integer matrix A:\n.reg .b32 x0;\n// destination register x0 contains eight packed .s4 values\nwmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];\n\n// Load elements from .bf16 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k16.row.bf16\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .tf32 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k8.row.tf32\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .f64 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m8n8k4.row.f64\n                {x0}, [ptr]; 9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store  wmma.store Collectively store a matrix into memory for WMMA Syntax wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f16, .f32, .s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32, .m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.type   = {.s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.type   = {.f64}; Description Collectively store a matrix across all threads in a warp at the location indicated by address\noperand p in the specified state space from source register r . If no state space is given, perform the memory accesses using Generic Addressing . wmma.load operation may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. The source operand r is a brace-enclosed vector expression that matches the shape of the\nfragment expected by the store operation, as described in Matrix Fragments for WMMA . The .shape qualifier indicates the dimensions of all the matrix arguments involved in the\nintended wmma computation. It must match the .shape qualifier specified on the wmma.mma instruction that produced the D matrix being stored. The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or column-major format. stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the leading dimension (rows or columns). The default\nvalue of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior. The required alignment for address p and stride is described in the Matrix Storage for WMMA . The mandatory .sync qualifier indicates that wmma.store causes the executing thread to wait\nuntil all threads in the warp execute the same wmma.store instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.store instruction. In conditionally executed code, a wmma.store instruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined. The behavior of wmma.store is undefined if all threads do not use the same qualifiers and the\nsame values of p and stride , or if any thread in the warp has exited. wmma.store is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.0. .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. .m16n16k8 introduced in PTX ISA version 7.0. Double precision wmma introduced in PTX ISA version 7.0. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. All\ndetails are subject to change with no guarantees of backward compatibility on future PTX ISA\nversions or SM architectures. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher. Double precision wmma and shape .m16n16k8 requires sm_80 or higher. Examples // Storing f32 elements computed by a wmma.mma\n.reg .b32 x<8>;\n\nwmma.mma.sync.m16n16k16.row.col.f32.f32\n              {d0, d1, d2, d3, d4, d5, d6, d7}, ...;\nwmma.store.d.sync.m16n16k16.row.f32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m16n16k16 shape:\n.reg .b32 d<8>;\nwmma.store.d.sync.aligned.m16n16k16.row.s32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m8n8k128 shape:\n.reg .b32 d<2>\nwmma.store.d.sync.aligned.m8n8k128.row.s32\n[ptr], {d0, d1};\n\n// Store f64 accumulator for m8n8k4 shape:\n.reg .f64 d<2>;\nwmma.store.d.sync.aligned.m8n8k4.row.f64\n              [ptr], {d0, d1}; 9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma  wmma.mma Perform a single matrix multiply-and-accumulate operation across a warp Syntax // Floating point (.f16 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c;\n\n// Integer (.u8/.s8 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape  =  {.m16n16k16, .m8n32k16, .m32n8k16};\n.dtype   = {.f16, .f32};\n.atype   = {.s8, .u8};\n.btype   = {.s8, .u8};\n.ctype   = {.f16, .f32}; Floating point format .bf16 wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k16, .m8n32k16, .m32n8k16};\n.atype   = {.bf16 };\n.btype   = {.bf16}; Floating point format .tf32 wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k8 };\n.atype   = {.tf32 };\n.btype   = {.tf32}; Floating point Double precision wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m8n8k4 };\n.rnd = { .rn, .rz, .rm, .rp }; Sub-byte ( .u4 / .s4 multiplicands) wmma.mma : wmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n.shape  = {.m8n8k32};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4}; Single-bit ( .b1 multiplicands) wmma.mma : wmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c;\n.shape  = {.m8n8k128};\n.atype  = {.b1};\n.btype  = {.b1};\n.op     = {.xor, .and} Description Perform a warp-level matrix multiply-and-accumulate computation D = A * B + C using matrices A,\nB and C loaded in registers a , b and c respectively, and store the result matrix in\nregister d . The register arguments a , b , c and d hold unspecified fragments of\nthe corresponding matrices as described in Matrix Fragments for WMMA The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the\nelements in the matrices D, A, B and C respectively. For wmma.mma without explicit .atype and .btype : .atype and .btype are\nimplicitly set to .f16 . For integer wmma , .ctype and .dtype must be specified as .s32 . Also, the values for .atype and .btype must be the same, i.e., either both are .s8 or both are .u8 . For sub-byte single-bit wmma , .ctype and .dtype must be specified as .s32 . Also, the\nvalues for .atype and .btype must be the same; i.e., either both are .s4 , both are .u4 , or both are .b1 . For single-bit wmma , multiplication is replaced by a sequence of logical operations;\nspecifically, wmma.xor.popc and wmma.and.popc computes the XOR, AND respectively of a\n128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result\n( popc ). This result is added to the corresponding element of C and written into D. The qualifiers .alayout and .blayout must match the layout specified on the wmma.load instructions that produce the contents of operands a and b respectively. Similarly, the\nqualifiers .atype , .btype and .ctype must match the corresponding qualifiers on the wmma.load instructions that produce the contents of operands a , b and c respectively. The .shape qualifier must match the .shape qualifier used on the wmma.load instructions\nthat produce the contents of all three input operands a , b and c respectively. The destination operand d is a brace-enclosed vector expression that matches the .shape of\nthe fragment computed by the wmma.mma instruction. Saturation at the output: The optional qualifier .satfinite indicates that the final values in the destination register\nare saturated as follows: The output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the\naccumulation would overflow, the value wraps. Precision and rounding for .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .ctype or .dtype is .f32 , accumulation of the intermediate values is performed with\nat least single precision. When both .ctype and .dtype are specified as .f16 , the\naccumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs is unspecified. Precision and rounding for .bf16 , .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision. The accumulation order, rounding and handling of subnormal inputs is unspecified. Rounding modifiers on double precision wmma.mma (default is .rn ): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The mandatory .sync qualifier indicates that wmma.mma causes the executing thread to wait\nuntil all threads in the warp execute the same wmma.mma instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.mma instruction. In conditionally executed code, a wmma.mma instruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined. The behavior of wmma.mma is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited. PTX ISA Notes Introduced in PTX ISA version 6.0. .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0. Support for .and operation in single-bit wmma introduced in PTX ISA version 7.1. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3. Support for .satfinite on floating point wmma.mma is deprecated in PTX ISA version 6.4 and\nis removed from PTX ISA version 6.5. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA. All details are\nsubject to change with no guarantees of backward compatibility on future PTX ISA versions or SM\narchitectures. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher. Double precision, alternate floating point precision wmma require sm_80 or higher. .and operation in single-bit wmma requires sm_80 or higher. Examples .global .align 32 .f16 A[256], B[256];\n.global .align 32 .f32 C[256], D[256];\n.reg .b32 a<8> b<8> c<8> d<8>;\n\nwmma.load.a.sync.aligned.m16n16k16.global.row.f16\n        {a0, a1, a2, a3, a4, a5, a6, a7}, [A];\nwmma.load.b.sync.aligned.m16n16k16.global.col.f16\n        {b0, b1, b2, b3, b4, b5, b6, b7}, [B];\n\nwmma.load.c.sync.aligned.m16n16k16.global.row.f32\n        {c0, c1, c2, c3, c4, c5, c6, c7}, [C];\n\nwmma.mma.sync.aligned.m16n16k16.row.col.f32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3, a4, a5, a6, a7},\n        {b0, b1, b2, b3, b4, b5, b6, b7},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\nwmma.store.d.sync.aligned.m16n16k16.global.col.f32\n        [D], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Compute an integer WMMA:\n.reg .b32  a, b<4>;\n.reg .b32 c<8>, d<8>;\nwmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a}, {b0, b1, b2,  b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\n// Compute sub-byte WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute single-bit type WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute double precision wmma\n.reg .f64 a, b, c<2>, d<2>;\nwmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute alternate floating point precision wmma\n.reg .b32 a<2>, b<2>, c<8>, d<8>;\nwmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3}, {b0, b1, b2, b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7}; 9.7.13.4. Matrix multiply-accumulate operation using mma instruction  This section describes warp-level mma , ldmatrix , stmatrix , and movmatrix instructions and the organization of various matrices involved in these instructions. 9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type  A warp executing mma.m8n8k4 with .f16 floating point type will compute 4 MMA operations of shape .m8n8k4 . Elements of 4 matrices need to be distributed across the threads in a warp. The following table\nshows distribution of matrices for MMA operations. MMA Computation Threads participating in MMA computation MMA computation 1 Threads with %laneid 0-3 (low group) and 16-19 (high group) MMA computation 2 Threads with %laneid 4-7 (low group) and 20-23 (high group) MMA computation 3 Threads with %laneid 8-11 (low group) and 24-27 (high group) MMA computation 4 Threads with %laneid 12-15 (low group) and 28-31 (high group) For each of the individual MMA computation shown above, each of the required thread holds a fragment\nof the matrix for performing mma operation as follows: Multiplicand A: .atype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers,\nwith each register containing two .f16 elements from\nthe matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix A is shown in Figure 21 . Figure 21 MMA .m8n8k4 fragment layout for row-major matrix A with .f16 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid < 16 ( % laneid % 4 ) + 4 otherwise col = i for ai where i = { 0 ,.., 3 } Fragment layout for Column Major matrix A is shown in Figure 22 . The layout of the fragments held by different threads is shown below: Figure 22 MMA .m8n8k4 fragment layout for column-major matrix A with .f16 type  The row and column of a matrix fragment can be computed as: row = i % 4 for ai where i = { 0 ,.., 3 } if % laneid < 16 ( i % 4 ) + 4 for ai where i = { 0 ,.., 3 } otherwise col = % laneid % 4 Multiplicand B: .btype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix B is shown in Figure 23 . Figure 23 MMA .m8n8k4 fragment layout for row-major matrix B with .f16 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = i for bi where i = { 0 ,.., 3 } if % laneid < 16 i + 4 for bi where i = { 0 ,.., 3 } otherwise Fragment layout for Column Major matrix B is shown in Figure 24 . Figure 24 MMA .m8n8k4 fragment layout for column-major matrix B with .f16 type  The row and column of a matrix fragment can be computed as: row = i for bi where i = { 0 ,.., 3 } col = % laneid % 4 if % laneid < 16 ( % laneid % 4 ) + 4 otherwise Accumulators C (or D): .ctype / .dtype Fragment Elements (low to high) .f16 A vector expression containing four .f16x2 registers, with\neach register containing two .f16 elements from the matrix\nC (or D). c0, c1, c2, c3, c4, c5, c6, c7 .f32 A vector expression of eight .f32 registers. The layout of the fragments held by different threads is shown below: Fragment layout for accumulator matrix when .ctype is .f16 is shown in Figure 25 . Figure 25 MMA .m8n8k4 fragment layout for matrix C/D with .ctype = .f16  The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid < 16 ( % laneid % 4 ) + 4 otherwise col = i for ci where i = { 0 ,.., 7 } Fragment layout for accumulator matrix when .ctype is .f32 is shown in Figure 26 and Figure 27 . Figure 26 MMA .m8n8k4 computation 1 and 2 fragment layout for matrix C/D with .ctype = .f32  Figure 27 MMA .m8n8k4 computation 3 and 4 fragment layout for matrix C/D with .ctype = .f32  The row and column of a matrix fragment can be computed as: row = X if % laneid < 16 X + 4 otherwise where X = ( % laneid & 0b1 ) + ( i & 0b10 ) for ci where i = { 0 ,.., 7 } col = ( i & 0b100 ) + ( % laneid & 0b10 ) + ( i & 0b1 ) for ci where i = { 0 ,.., 7 } 9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point type  A warp executing mma.m8n8k4 with .f64 floating point type will compute an MMA operation of\nshape .m8n8k4 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .f64 A vector expression containing a single .f64 register, containing\nsingle .f64 element from the matrix A. a0 The layout of the fragments held by different threads is shown in Figure 28 . Figure 28 MMA .m8n8k4 fragment layout for matrix A  with .f64 type  The row and column of a matrix fragment can be computed as: row = % laneid >> 2 col = % laneid % 4 Multiplicand B: .btype Fragment Elements (low to high) .f64 A vector expression containing a single .f64 register, containing a single .f64 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 29 . Figure 29 MMA .m8n8k4 fragment layout for matrix B  with .f64 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = % laneid >> 2 Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing of two .f64 registers containing two .f64 elements from the matrix C. c0, c1 The layout of the fragments held by different threads is shown in Figure 30 . Figure 30 MMA .m8n8k4 fragment layout for accumulator matrix C/D  with .f64 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 } 9.7.13.4.3. Matrix Fragments for mma.m8n8k16  A warp executing mma.m8n8k16 will compute an MMA operation of shape .m8n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing\nfour .s8 or .u8 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 31 . Figure 31 MMA .m8n8k16 fragment layout for matrix A with .u8 / .s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 4 ) + i for ai where i = { 0 ,.., 3 } Multiplicand B: .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing\nfour .s8 or .u8 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 32 . Figure 32 MMA .m8n8k16 fragment layout for matrix B with .u8 / .s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing of two .s32 registers. c0, c1 The layout of the fragments held by different threads is shown in Figure 33 . Figure 33 MMA .m8n8k16 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.4. Matrix Fragments for mma.m8n8k32  A warp executing mma.m8n8k32 will compute an MMA operation of shape .m8n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing\neight .s4 or .u4 elements from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 34 . Figure 34 MMA .m8n8k32 fragment layout for matrix A with .u4 / .s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 8 ) + i for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing\neight .s4 or .u4 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 35 . Figure 35 MMA .m8n8k32 fragment layout for matrix B with .u4 / .s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + i for bi where i = { 0 ,.., 7 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression of two .s32 registers. c0, c1 The layout of the fragments held by different threads is shown in Figure 36 : Figure 36 MMA .m8n8k32 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.5. Matrix Fragments for mma.m8n8k128  A warp executing mma.m8n8k128 will compute an MMA operation of shape .m8n8k128 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty\ntwo .b1 elements from the matrix A. a0, a1, … a30, a31 The layout of the fragments held by different threads is shown in Figure 37 . Figure 37 MMA .m8n8k128 fragment layout for matrix A with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 32 ) + i for ai where i = { 0 ,.., 31 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty\ntwo .b1 elements from the matrix B. b0, b1, …, b30, b31 The layout of the fragments held by different threads is shown in Figure 38 . Figure 38 MMA .m8n8k128 fragment layout for matrix B with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,.., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing two .s32 registers, containing two .s32 elements from the matrix C (or D). c0, c1 The layout of the fragments held by different threads is shown in Figure 39 . Figure 39 MMA .m8n8k128 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.6. Matrix Fragments for mma.m16n8k4  A warp executing mma.m16n8k4 will compute an MMA operation of shape .m16n8k4 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix A. a0, a1 The layout of the fragments held by different threads is shown in Figure 40 . Figure 40 MMA .m16n8k4 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix A. a0, a1 The layout of the fragments held by different threads is shown in Figure 41 . Figure 41 MMA .m16n8k4 fragment layout for matrix A with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group Multiplicand B: .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression of a single .b32 register, containing a single .tf32 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 42 . Figure 42 MMA .m16n8k4 fragment layout for matrix B with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression of a single .f64 register, containing a single .f64 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 43 . Figure 43 MMA .m16n8k4 fragment layout for matrix B with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID Accumulators (C or D): .tf32 : .ctype / .dtype Fragment Elements (low to high) .f32 A vector expression containing four .f32 registers, containing four .f32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 44 . Figure 44 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 45 . Figure 45 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.7. Matrix Fragments for mma.m16n8k8  A warp executing mma.m16n8k8 will compute an MMA operation of shape .m16n8k8 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 / .bf16 elements from the\nmatrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 46 . Figure 46 MMA .m16n8k8 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = threadID_in_group * 2 + ( i & 0x1 ) for ai where i = { 0 ,.., 3 } .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, containing four .tf32 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 47 . Figure 47 MMA .m16n8k8 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 48 . Figure 48 MMA .m16n8k8 fragment layout for matrix A with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing a single .f16x2 register, containing\ntwo .f16 / .bf16 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 49 . Figure 49 MMA .m16n8k8 fragment layout for matrix B with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + i for bi where i = { 0 , 1 } col = groupID .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 50 . Figure 50 MMA .m16n8k8 fragment layout for matrix B with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 51 . Figure 51 MMA .m16n8k8 fragment layout for matrix B with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID Accumulators (C or D): .f16 , .bf16 and .tf32 : .ctype / .dtype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression of four .f32 registers. The layout of the fragments held by different threads is shown in Figure 52 . Figure 52 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f16x2 / .f32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression of four .f64 registers containing four .f64 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 53 . Figure 53 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type  A warp executing mma.m16n8k16 floating point types will compute an MMA operation of shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with\neach register containing two .f16 / .bf16 elements\nfrom the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 54 . Figure 54 MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 2 || 4 <= i < 6 groupID + 8 Otherwise col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ai where i < 4 ( threadID_in_group * 2 ) + ( i & 0x1 ) + 8 for ai where i >= 4 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing eight .f64 registers, with each\nregister containing one .f64 element from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 55 . Figure 55 MMA .m16n8k16 fragment layout for matrix A with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i % 2 = 0 groupID + 8 Otherwise col = ( i * 2 ) + threadID_in_group for ai where i % 2 = 0 ( i * 2 ) - 2 + ( threadID_in_group Otherwise Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with\neach register containing two .f16 / .bf16 elements\nfrom the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 56 . Figure 56 MMA .m16n8k16 fragment layout for matrix B with .f16 / .bf16 type.  where the row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + ( i & 0x1 ) for bi where i < 2 ( threadID_in_group * 2 ) + ( i & 0x1 ) + 8 for bi where i >= 2 col = groupID .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, with each\nregister containing one .f64 element from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 57 . Figure 57 MMA .m16n8k16 fragment layout for matrix B with .f64 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group + ( i * 4 ) for bi where i < 4 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers containing .f64 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression containing four .f32 registers containing\nfour .f32 elements from the matrix C (or D). .f16 A vector expression containing two .f16x2 registers, with each\nregister containing two .f16 elements from the matrix C (or D). The layout of the fragments held by different threads is shown in Figure 58 . Figure 58 MMA .m16n8k16 fragment layout for accumulator matrix matrix C/D.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type  A warp executing mma.m16n8k16 with .u8 or .s8 integer type will compute an MMA operation\nof shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .u8 / .s8 A vector expression containing two .b32 registers, with each\nregister containing four .u8 / .s8 elements from the\nmatrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 59 . Figure 59 MMA .m16n8k16 fragment layout for matrix A with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i < 4 groupID + 8 for ai where i >= 4 col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing a single .b32 register, containing\nfour .u8 / .s8 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 60 . Figure 60 MMA .m16n8k16 fragment layout for matrix B with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 61 . Figure 61 MMA .m16n8k16 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.10. Matrix Fragments for mma.m16n8k32  A warp executing mma.m16n8k32 will compute an MMA operation of shape .m16n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .s4 or .u4 : .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each\nregister containing eight .u4 / .s4 elements from the\nmatrix A. a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 62 . Figure 62 MMA .m16n8k32 fragment layout for matrix A with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i < 8 groupID + 8 for ai where i >= 8 col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i = { 0 ,.., 15 } .s8 or .u8 or .e4m3 or .e5m2 : .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each\nregister containing four .s8 / .u8 elements from the\nmatrix A. a0, a1, .., a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each\nregister containing four .e4m3 / .e5m2 elements from\nthe matrix A. a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 63 . Figure 63 MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 4 || 8 <= i < 12 groupID + 8 otherwise col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i < 8 ( threadID_in_group * 4 ) + ( i & 0x3 ) + 16 for ai where i >= 8 Multiplicand B: .s4 or .u4 : .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register,\ncontaining eight .s4 / .u4 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 64 . Figure 64 MMA .m16n8k32 fragment layout for matrix B with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = { 0 ,.., 7 } col = groupID .s8 or .u8 or .e4m3 or .e5m2 : .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing two .b32 registers, with each\nregister containing four .s8 / .u8 elements from the\nmatrix B. b0, b1, b2, b3, b4, b5, b6, b7 .e4m3 / .e5m2 A vector expression containing two .b32 registers, with each\nregister containing four .e4m3 / .e5m2 elements from the\nmatrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 65 and Figure 66 . Figure 65 MMA .m16n8k32 fragment layout for rows 0–15 of matrix B with .u8 / .s8 type.  Figure 66 MMA .m16n8k32 fragment layout for rows 16–31 of matrix B with .u8 / .s8 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + ( i & 0x3 ) for bi where i < 4 ( threadID_in_group * 4 ) + ( i & 0x3 ) + 16 for bi where i >= 4 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression containing four .f32 registers, containing\nfour .f32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 67 . Figure 67 MMA .m16n8k32 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.11. Matrix Fragments for mma.m16n8k64  A warp executing mma.m16n8k64 will compute an MMA operation of shape .m16n8k64 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing four .b32 registers, with each\nregister containing eight .s4 / .u4 elements from the\nmatrix A. a0, a1, …, a30, a31 The layout of the fragments held by different threads is shown in Figure 68 . Figure 68 MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 8 || 16 <= i < 24 groupID + 8 otherwise col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i < 16 ( threadID_in_group * 8 ) + ( i & 0x7 ) + 32 for ai where i >= 16 Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each\nregister containing eight .s4 / .u4 elements from the\nmatrix B. b0, b1, …, b14, b15 The layout of the fragments held by different threads is shown in Figure 69 and Figure 70 . Figure 69 MMA .m16n8k64 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type.  Figure 70 MMA .m16n8k64 fragment layout for rows 32–63 of matrix B with .u4 / .s4 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i < 8 ( threadID_in_group * 8 ) + ( i & 0x7 ) + 32 for bi where i >= 8 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 71 . Figure 71 MMA .m16n8k64 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.12. Matrix Fragments for mma.m16n8k128  A warp executing mma.m16n8k128 will compute an MMA operation of shape .m16n8k128 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register containing\nthirty two .b1 elements from the matrix A. a0, a1, …, a62, a63 The layout of the fragments held by different threads is shown in Figure 72 . Figure 72 MMA .m16n8k128 fragment layout for matrix A with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i < 32 groupID + 8 for ai where i >= 32 col = ( threadID_in_group * 32 ) + ( i & 0x1F ) for ai where i = { 0 , ..., 63 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register containing thirty\ntwo .b1 elements from the matrix B. b0, b1, … , b30, b31 The layout of the fragments held by different threads is shown in Figure 73 . Figure 73 MMA .m16n8k128 fragment layout for matrix B with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,..., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 74 . Figure 74 MMA .m16n8k128 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.13. Matrix Fragments for mma.m16n8k256  A warp executing mma.m16n8k256 will compute an MMA operation of shape .m16n8k256 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each register\ncontaining thirty two .b1 elements from the matrix A. a0, a1, …, a126, a127 The layout of the fragments held by different threads is shown in Figure 75 . Figure 75 MMA .m16n8k256 fragment layout for matrix A with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 32 || 64 <= i < 96 groupID + 8 otherwise col = ( threadID_in_group * 32 ) + i for ai where i < 64 ( threadID_in_group * 32 ) + ( i & 0x1F ) + 128 for ai where i >= 64 Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register\ncontaining thirty two .b1 elements from the matrix B. b0, b1, …, b62, b63 The layout of the fragments held by different threads is shown in Figure 76 and Figure 77 . Figure 76 MMA .m16n8k256 fragment layout for rows 0–127 of matrix B with .b1 type.  Figure 77 MMA .m16n8k256 fragment layout for rows 128–255 of matrix B with .b1 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + ( i & 0x1F ) for bi where i < 32 ( threadID_in_group * 32 ) + ( i & 0x1F ) + 128 for bi where i >= 32 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing\nfour .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 78 . Figure 78 MMA .m16n8k256 fragment layout for accumulator matrix C/D with .s32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i < 2 groupID + 8 for ci where i >= 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.14. Multiply-and-Accumulate Instruction: mma  mma Perform matrix multiply-and-accumulate operation Syntax Half precision floating point type: mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.ctype   = {.f16, .f32};\n.dtype   = {.f16, .f32}; Alternate floating point type : mma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32        d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32      d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32       d, a, b, c;\nmma.sync.aligned.m16n8k32.row.col.f32.f8type.f8type.f32   d, a, b, c;\n\n.atype  = {.bf16, .tf32};\n.btype  = {.bf16, .tf32};\n.f8type = {.e4m3, .e5m2}; Double precision floating point type: mma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c;\n\n.shape   = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16}; Integer type: mma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k16, .m16n8k16, .m16n8k32}\n.atype   = {.u8, .s8};\n.btype   = {.u8, .s8};\n\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k32, .m16n8k32, .m16n8k64}\n.atype   = {.u4, .s4};\n.btype   = {.u4, .s4}; Single bit: mma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c;\n\n.bitOp = {.xor, .and}\n.shape = {.m8n8k128, .m16n8k128, .m16n8k256} Description Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C , where the A matrix is MxK , the B matrix is KxN , and the C and D matrices are MxN . A warp executing mma.sync.m8n8k4 instruction computes 4 matrix multiply and accumulate\noperations. Rest of the mma.sync operations compute a single matrix mutliply and accumulate\noperation per warp. For single-bit mma.sync , multiplication is replaced by a sequence of logical operations;\nspecifically, mma.xor.popc and mma.and.popc computes the XOR, AND respectively of a k-bit\nrow of A with a k-bit column of B, then counts the number of set bits in the result ( popc ). This\nresult is added to the corresponding element of C and written into D. Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp. The registers in each thread hold a fragment of matrix as described in Matrix multiply-accumulate\noperation using mma instruction . The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the\nelements in the matrices D, A, B and C respectively. Specific shapes have type restrictions : .m8n8k4 : When .ctype is .f32 , .dtype must also be .f32 . .m16n8k8 : .dtype must be the same as .ctype . .atype must be the same as .btype . The qualifiers .alayout and .blayout indicate the row-major or column-major layouts of\nmatrices A and B respectively. Precision and rounding : .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single\nprecision. When .ctype or .dtype is .f32 , accumulation of the intermediate values\nis performed with at least single precision. When both .ctype and .dtype are specified\nas .f16 , the accumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .e4m3 and .e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .f64 floating point operations : Precision of the element-wise multiplication and addition operation is identical to that of .f64 precision fused multiply-add. Supported rounding modifiers are : .rn : mantissa LSB rounds to nearest even. This is the default. .rz : mantissa LSB rounds towards zero. .rm : mantissa LSB rounds towards negative infinity. .rp : mantissa LSB rounds towards positive infinity. Integer operations : The integer mma operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that mma instruction causes the executing thread to\nwait until all threads in the warp execute the same mma instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma instruction. In conditionally executed code, a mma instruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined. The behavior of mma instruction is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited. Notes Programs using double precision floating point mma instruction with shapes .m16n8k4 , .m16n8k8 , and .m16n8k16 require at least 64 registers for compilation. PTX ISA Notes Introduced in PTX ISA version 6.4. .f16 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version\n6.4. .f16 floating point type mma operation with .m16n8k8 shape introduced in PTX ISA version\n6.5. .u8/.s8 integer type mma operation with .m8n8k16 shape introduced in PTX ISA version\n6.5. .u4/.s4 integer type mma operation with .m8n8k32 shape introduced in PTX ISA version\n6.5. .f64 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version\n7.0. .f16 floating point type mma operation with .m16n8k16 shape introduced in PTX ISA\nversion 7.0. .bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes\nintroduced in PTX ISA version 7.0. .tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes\nintroduced in PTX ISA version 7.0. .u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes introduced in\nPTX ISA version 7.0. .u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes introduced in\nPTX ISA version 7.0. .b1 single-bit integer type mma operation with .m8n8k128 , .m16n8k128 and .m16n8k256 shapes introduced in PTX ISA version 7.0. Support for .and operation in single-bit mma introduced in PTX ISA version 7.1. .f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes introduced in PTX ISA version 7.8. Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in\nPTX ISA version 8.4. Target ISA Notes Requires sm_70 or higher. .f16 floating point type mma operation with .m8n8k4 shape requires sm_70 or higher. Note mma.sync.m8n8k4 is optimized for target architecture sm_70 and may have substantially\nreduced performance on other target architectures. .f16 floating point type mma operation with .m16n8k8 shape requires sm_75 or higher. .u8/.s8 integer type mma operation with .m8n8k16 shape requires sm_75 or higher. .u4/.s4 integer type mma operation with .m8n8k32 shape sm_75 or higher. .b1 single-bit integer type mma operation with .m8n8k128 shape sm_75 or higher. .f64 floating point type mma operation with .m8n8k4 shape requires sm_80 or higher. .f16 floating point type mma operation with .m16n8k16 shape requires sm_80 or\nhigher. .bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes\nrequires sm_80 or higher. .tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes\nrequires sm_80 or higher. .u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes requires sm_80 or higher. .u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes requires sm_80 or higher. .b1 single-bit integer type mma operation with .m16n8k128 and .m16n8k256 shapes\nrequires sm_80 or higher. .and operation in single-bit mma requires sm_80 or higher. .f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes require sm_90 or higher. .e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher. Examples of half precision floating point type // f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4> %Rd<4>\nmma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n\n// f16 elements in C and f32 elements in D\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4>\n.reg .f32 %Rd<8>\nmma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n // f32 elements in C and D\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<4>, %Rb<2>, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .f16 %Ra<4>, %Rb<2>;\n.reg .f32 %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of alternate floating point type .reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Rb2, %Rb3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of integer type .reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\n\n// s8 elements in A and u8 elements in B\nmma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// u4 elements in A and B matrix\nmma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// s8 elements in A and u8 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u4 elements in A and s4 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// s8 elements in A and s8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u8 elements in A and u8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1 },\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of single bit type // b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of .f64 floating point type .reg .f64 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .f64 %Ra<8>, %Rb<4>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}; 9.7.13.4.15. Warp-level matrix load instruction: ldmatrix  ldmatrix Collectively load one or more matrices from shared memory for mma instruction Syntax ldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];\n\n.shape  = {.m8n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16}; Description Collectively load one or more matrices across all threads in a warp from the location indicated by\nthe address operand p , from .shared state space into destination register r . If no state\nspace is provided, generic addressing is used, such that the address in p points into .shared space. If the generic address doesn’t fall in .shared state space, then the behavior\nis undefined. The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit data as indicated by the .type qualifier. The values .x1 , .x2 and .x4 for .num indicate one, two or four matrices\nrespectively. The mandatory .sync qualifier indicates that ldmatrix causes the executing thread to wait\nuntil all threads in the warp execute the same ldmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same ldmatrix instruction. In conditionally executed code, an ldmatrix instruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined. The behavior of ldmatrix is undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited. The destination operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of .num . Each component of the vector expression holds a fragment\nfrom the corresponding matrix. Supported addressing modes for p are described in Addresses as Operands . Consecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of .num as shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on. .num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 Note For .target sm_75 or below, all threads must contain valid addresses. Otherwise, the behavior\nis undefined. For .num = .x1 and .num = .x2 , addresses contained in lower threads can be\ncopied to higher threads to achieve the expected behavior. When reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses\nmust be naturally aligned accordingly. Each thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its\nregister r , and so on. A group of four threads loads an entire row of the matrix as shown in Figure 79 . Figure 79 ldmatrix fragment layout  When .num = .x2 , the elements of the second matrix are loaded in the next destination\nregister in each thread as per the layout in above table. Similarly, when .num = .x4 ,\nelements of the third and fourth matrices are loaded in the subsequent destination registers in each\nthread. Optional qualifier .trans indicates that the matrix is loaded in column-major format. The ldmatrix instruction is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.5. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. Examples // Load a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 d;\nldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];\n\n// Load two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];\n\n// Load four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr]; 9.7.13.4.16. Warp-level matrix store instruction: stmatrix  stmatrix Collectively store one or more matrices to shared memory. Syntax stmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r;\n\n.shape  = {.m8n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16}; Description Collectively store one or more matrices across all threads in a warp to the location indicated by\nthe address operand p , in .shared state space. If no state space is provided, generic\naddressing is used, such that the address in p points into .shared space. If the generic\naddress doesn’t fall in .shared state space, then the behavior is undefined. The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit data as indicated by the .type qualifier. The values .x1 , .x2 and .x4 for .num indicate one, two or four matrices\nrespectively. The mandatory .sync qualifier indicates that stmatrix causes the executing thread to wait\nuntil all threads in the warp execute the same stmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same stmatrix instruction. In conditionally executed code, an stmatrix instruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined. The behavior of stmatrix is undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited. The source operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of .num . Each component of the vector expression holds a fragment\nfrom the corresponding matrix. Supported addressing modes for p are described in Addresses as Operands . Consecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of .num as shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on. .num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 When storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. The matrix addresses\nmust be naturally aligned accordingly. Each thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its\nregister r , and so on. A group of four threads stores an entire row of the matrix as shown in Figure 80 . Figure 80 stmatrix fragment layout  When .num = .x2 , the elements of the second matrix are storedd from the next source register\nin each thread as per the layout in above table. Similarly, when .num = .x4 , elements of the\nthird and fourth matrices are stored from the subsequent source registers in each thread. Optional qualifier .trans indicates that the matrix is stored in column-major format. The stmatrix instruction is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples // Store a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};\n\n// Store two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};\n\n// Store four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3}; 9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix  movmatrix Transpose a matrix in registers across the warp. Syntax movmatrix.sync.aligned.shape.trans.type d, a;\n\n.shape  = {.m8n8};\n.type   = {.b16}; Description Move a row-major matrix across all threads in a warp, reading elements from source a , and\nwriting the transposed elements to destination d . The .shape qualifier indicates the dimensions of the matrix being transposed. Each matrix\nelement holds 16-bit data as indicated by the .type qualifier. The mandatory .sync qualifier indicates that movmatrix causes the executing thread to wait\nuntil all threads in the warp execute the same movmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same movmatrix instruction. In conditionally executed code, a movmatrix instruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nthe behavior is undefined. Operands a and d are 32-bit registers containing fragments of the input matrix and the\nresulting matrix respectively. The mandatory qualifier .trans indicates that the resulting\nmatrix in d is a transpose of the input matrix specified by a . Each thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first\nfragment in register a , and so on. A group of four threads holds an entire row of the input\nmatrix as shown in Figure 81 . Figure 81 movmatrix source matrix fragment layout  Each thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the\nfirst fragment in register d , and so on. A group of four threads holds an entire column of the\nresult matrix as shown in Figure 82 . Figure 82 movmatrix result matrix fragment layout  PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_75 or higher. Examples .reg .b32 d, a;\nmovmatrix.sync.aligned.m8n8.trans.b16 d, a; 9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A  This section describes warp-level mma.sp{::ordered_metadata} instruction with sparse matrix A.\nThis variant of the mma operation can be used when A is a structured sparse matrix with 50%\nzeros in each row distributed in a shape-specific granularity. For an MxNxK sparse mma.sp{::ordered_metadata} operation, the MxK matrix A is packed into MxK/2 elements.\nFor each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements\nare packed in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata. 9.7.13.5.1. Sparse matrix storage  Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a 16x16 matrix A, sparsity is expected to be at 2:4\ngranularity, i.e. each 4-element vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row\ncontains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata\noperand. Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and\nwill result in undefined behavior. In a group of four consecutive threads, one or more threads store\nthe metadata for the whole group depending upon the matrix shape. These threads are specified using\nan additional sparsity selector operand. Figure 83 shows an example of a 16x16 matrix A represented in sparse format and sparsity\nselector indicating which thread in a group of four consecutive threads stores the metadata. Figure 83 Sparse MMA storage example  Granularities for different matrix shapes and data types are described below. Sparse mma.sp{::ordered_metadata} with half-precision and .bf16 type For the .m16n8k16 and .m16n8k32 mma.sp{::ordered_metadata} operations, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements\nin a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are\nstored in the operand representing matrix A and their positions in the four-wide chunk in matrix\nA are indicated by two 2-bit indices in the metadata operand. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values\nof indices; any other values result in an undefined behavior. Figure 84 Sparse MMA metadata example for .f16 / .bf16 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k16 : One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}. m16n8k32 : A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior. Sparse mma.sp{::ordered_metadata} with .tf32 type When matrix A has .tf32 elements, matrix A is structured sparse at a granularity of 1:2. In\nother words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero\nelement. Only the non-zero elements are stored in the operand for matrix A and their positions in a\ntwo-wide chunk in matrix A are indicated by the 4-bit index in the metadata. 0b1110 and 0b0100 are the only meaningful index values; any other values result in an undefined behavior. Figure 85 Sparse MMA metadata example for .tf32 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k8 : One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}. m16n8k16 : A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior. Sparse mma.sp{::ordered_metadata} with integer type When matrices A and B have .u8 / .s8 elements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes\nand two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their\npositions in the four-wide chunk are indicated by two 2-bit indices in the metadata. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior. Figure 86 Sparse MMA metadata example for .u8 / .s8 type.  when matrices A and B have .u4 / .s4 elements, matrix A is pair-wise structured sparse at a\ngranularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has\nfour zeroes and four non-zero values. Further, the zero and non-zero values are clustered in\nsub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within\nthe eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored\nin sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the\neight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior. Figure 87 Sparse MMA metadata example for .u4 / .s4 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k32 with .u8 / .s8 type and m16n8k64 with .u4 / .s4 type: A thread-pair\nwithin a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity\nselector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an\nundefined behavior. m16n8k64 with .u8 / .s8 type and m16n8k128 with .u4 / .s4 type: All threads\nwithin a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity\nselector in this case must be 0. Any other value of sparsity selector results in an undefined\nbehavior. Sparse mma.sp{::ordered_metadata} with .e4m3 / .e5m2 type When matrices A and B have .e4m3 / .e5m2 elements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes and\ntwo non-zero elements. Only the two non-zero elements are stored in sparse matrix and their positions\nin the four-wide chunk are indicated by two 2-bit indices in the metadata. 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values\nresult in an undefined behavior. Figure 88 Sparse MMA metadata example for .e4m3 / .e5m2 type.  The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k64 : All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior. 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of\nvarious matrices and the sparsity metadata. The following conventions are used throughout this\nsection: For matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data. For matrix B, when the combination of matrix dimension and the supported data type is not already\ncovered in Matrix multiply-accumulate operation using mma instruction , a pictorial representation of matrix\nfragments is provided. For matrices C and D, since the matrix dimension - data type combination is the same for all\nsupported shapes, and is already covered in Matrix multiply-accumulate operation using mma\ninstruction , the pictorial representations\nof matrix fragments are not included in this section. For the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present\nin cell [x][y..z] indicates that bits m through n (with m being higher) in the\nmetadata operand of thread with %laneid=k contains the indices of the non-zero elements from\nthe chunk [x][y]..[x][z] of matrix A. 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types  A warp executing sparse mma.m16n8k16 with .f16 / .bf16 floating point type will compute\nan MMA operation of shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing two .b32 registers,\nwith each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from\nmatrix A. Mapping of the non-zero elements is as\ndescribed in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 89 . Figure 89 Sparse MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 lastcol = firstcol + 3 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k16 with floating point type for .f16 / .b16 formats. Metadata: A .b32 register containing 16 2-bit vectors each storing the index of a non-zero\nelement of a 4-wide chunk of matrix A as shown in Figure 90 . Figure 90 Sparse MMA .m16n8k16 metadata layout for .f16 / .bf16 type.  9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types  A warp executing sparse mma.m16n8k32 with .f16 / .bf16 floating point type will compute\nan MMA operation of shape .m16n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing four .b32 registers,\nwith each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from\nmatrix A. Mapping of the non-zero elements is as\ndescribed in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 91 . Figure 91 Sparse MMA .m16n8k32 fragment layout for matrix A with .f16 / .bf16 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 2 || 4 <= i < 6 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 For ai where i < 4 ( threadID_in_group * 4 ) + 16 for ai where i >= 4 lastcol = firstcol + 3 Multiplicand B: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .b32 registers, each\ncontaining two .f16 / .bf16 elements from matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 92 . Figure 92 Sparse MMA .m16n8k32 fragment layout for matrix B with .f16 / .bf16 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k16 with floating point type for .f16 / .b16 formats. Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero element from a 4-wide chunk of matrix A as shown in Figure 93 . Figure 93 Sparse MMA .m16n8k32 metadata layout for .f16 / .bf16 type.  9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type  A warp executing sparse mma.m16n8k16 with .tf32 floating point type will compute an MMA\noperation of shape .m16n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing four .b32 registers, with each\nregister containing one non-zero .tf32 element out of 2\nconsecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 94 . Figure 94 Sparse MMA .m16n8k16 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 for a0 and a1 ( threadID_in_group * 2 ) + 8 for a2 and a3 lastcol = firstcol + 1 Multiplicand B: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, each\ncontaining four .tf32 elements from matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 95 . Figure 95 Sparse MMA .m16n8k16 fragment layout for matrix B with .tf32 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k16 with floating point type . Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in Figure 96 . Figure 96 Sparse MMA .m16n8k16 metadata layout for .tf32 type.  9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type  A warp executing sparse mma.m16n8k8 with .tf32 floating point type will compute an MMA\noperation of shape .m16n8k8 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing two .b32 registers, each\ncontaining one non-zero .tf32 element out of 2\nconsecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 97 . Figure 97 Sparse MMA .m16n8k8 fragment layout for matrix A with .tf32 type.  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 lastcol = firstcol + 1 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k8 for .tf32 format. Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in Figure 98 . Figure 98 Sparse MMA .m16n8k8 metadata layout for .tf32 type.  9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type  A warp executing sparse mma.m16n8k32 with .u8 / .s8 integer type will compute an MMA\noperation of shape .m16n8k32 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u8 / .s8 A vector expression containing two .b32 registers, with each\nregister containing four non-zero .u8 / .s8 elements out\nof 8 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 99 . Figure 99 Sparse MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 4 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 8 lastcol = firstcol + 7 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k32 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 100 . Figure 100 Sparse MMA .m16n8k32 metadata layout for .u8 / .s8 type.  9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 type  A warp executing sparse mma.m16n8k64 with .u8 / .s8 / .e4m3 / .e5m2 type will compute an MMA\noperation of shape .m16n8k64 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u8 / .s8 A vector expression containing four .b32 registers, with each\nregister containing four non-zero .u8 / .s8 elements out\nof 8 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each\nregister containing four non-zero .e4m3 / .e5m2 elements\nout of 8 consecutive elements from matrix A. The layout of the fragments held by different threads is shown in Figure 101 and Figure 102 . Figure 101 Sparse MMA .m16n8k64 fragment layout for columns 0–31 of matrix A with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 102 Sparse MMA .m16n8k64 fragment layout for columns 32–63 of matrix A with .u8 / .s8 / .e4m3 / .e5m2 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 4 || 8 <= i < 12 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 8 For ai where i < 8 ( threadID_in_group * 8 ) + 32 For ai where i >= 8 lastcol = firstcol + 7 Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing four .b32 registers,\neach containing four .u8 / .s8 elements from\nmatrix B. b0, b1, b2, b3, …, b15 .e4m3 / .e5m2 A vector expression containing four .b32 registers,\neach containing four .e4m3 / .e5m2 elements from\nmatrix B. The layout of the fragments held by different threads is shown in Figure 103 , Figure 104 , Figure 105 and Figure 106 . Figure 103 Sparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 104 Sparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 105 Sparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 106 Sparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k16 with integer type . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 107 and Figure 108 . Figure 107 Sparse MMA .m16n8k64 metadata layout for columns 0–31 for .u8 / .s8 / .e4m3 / .e5m2 type.  Figure 108 Sparse MMA .m16n8k64 metadata layout for columns 32–63 for .u8 / .s8 / .e4m3 / .e5m2 type.  9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type  A warp executing sparse mma.m16n8k64 with .u4 / .s4 integer type will compute an MMA\noperation of shape .m16n8k64 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u4 / .s4 A vector expression containing two .b32 registers, with each\nregister containing eight non-zero .u4 / .s4 elements\nout of 16 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 109 . Figure 109 Sparse MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 8 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 16 lastcol = firstcol + 15 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix\nFragments for mma.m16n8k64 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 110 . Figure 110 Sparse MMA .m16n8k64 metadata layout for .u4 / .s4 type.  9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer type  A warp executing sparse mma.m16n8k128 with .u4 / .s4 integer type will compute an MMA\noperation of shape .m16n8k128 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix. Multiplicand A: .atype Fragment Elements .u4 / .s4 A vector expression containing four .b32 registers, with each\nregister containing eight non-zero .u4 / .s4 elements out\nof 16 consecutive elements from matrix A. Mapping of the non-zero elements is\nas described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 111 and Figure 112 . Figure 111 Sparse MMA .m16n8k128 fragment layout for columns 0–63 of matrix A with .u4 / .s4 type.  Figure 112 Sparse MMA .m16n8k128 fragment layout for columns 64–127 of matrix A with .u4 / .s4 type.  groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 <= i < 8 || 16 <= i < 24 groupID + 8 Otherwise col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 16 For ai where i < 16 ( threadID_in_group * 16 ) + 64 For ai where i >= 16 lastcol = firstcol + 15 Multiplicand B: .atype Fragment Elements (low to high) .u4 / .s4 A vector expression containing four .b32 registers, each containing\neight .u4 / .s4 elements from matrix B. b0, b1, b2, b3, …, b31 The layout of the fragments held by different threads is shown in Figure 113 , Figure 114 , Figure 115 , Figure 116 . Figure 113 Sparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type.  Figure 114 Sparse MMA .m16n8k128 fragment layout for rows 31–63 of matrix B with .u4 / .s4 type.  Figure 115 Sparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with .u4 / .s4 type.  Figure 116 Sparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with .u4 / .s4 type.  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for\nmma.m16n8k64 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 117 and Figure 118 . Figure 117 Sparse MMA .m16n8k128 metadata layout for  columns 0–63 for .u4 / .s4 type.  Figure 118 Sparse MMA .m16n8k128 metadata layout for  columns 64–127 for .u4 / .s4 type.  9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata  mma.sp/mma.sp::ordered_metadata Perform matrix multiply-and-accumulate operation with sparse matrix A Syntax Half precision floating point type: mma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\n\n.ctype     = {.f16, .f32};\n.dtype     = {.f16, .f32};\n.spvariant = {.sp, .sp::ordered_metadata}; Alternate floating point type : mma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32      d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f;\n\n.f8type    = {.e4m3, .e5m2};\n.spvariant = {.sp, .sp::ordered_metadata}; Integer type: mma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k32, .m16n8k64}\n.atype     = {.u8, .s8};\n.btype     = {.u8, .s8};\n.spvariant = {.sp, .sp::ordered_metadata};\n\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k64, .m16n8k128}\n.atype     = {.u4, .s4};\n.btype     = {.u4, .s4};\n.spvariant = {.sp, .sp::ordered_metadata}; Description Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C , where the A matrix is MxK , the B matrix is KxN , and the C and D matrices are MxN . A warp executing mma.sp.sync/mma.sp::ordered_metadata.sync instruction compute a single matrix\nmutliply and accumulate operation. Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp. Matrix A\nis structured sparse as described in Sparse matrix storage Operands e and f represent sparsity\nmetadata and sparsity selector respectively. Operand e is a 32-bit integer and operand f is\na 32-bit integer constant with values in the range 0..3 Instruction mma.sp::ordered_metadata requires the indices in the sparsity metadata to be sorted\nin an increasing order starting from LSB, otherwise behavior is undefined. The registers in each thread hold a fragment of matrix as described in Matrix fragments for\nmultiply-accumulate operation with sparse matrix A . The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the\nelements in the matrices D, A, B and C respectively. In case of shapes .m16n8k16 and .m16n8k32 , .dtype must be the same as .ctype Precision and rounding : .f16 floating point operations : Element-wise multiplication of matrix A and B is performed with at least single\nprecision. When .ctype or .dtype is .f32 , accumulation of the intermediate values\nis performed with at least single precision. When both .ctype and .dtype are specified\nas .f16 , the accumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .e4m3 and .e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. Integer operations : The integer mma.sp/mma.sp::ordered_metadata operation is performed with .s32 accumulators.\nThe .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that mma.sp/mma.sp::ordered_metadata instruction causes\nthe executing thread to wait until all threads in the warp execute the same mma.sp/mma.sp::ordered_metadata instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma.sp/mma.sp::ordered_metadata instruction. In conditionally executed code, a mma.sp/mma.sp::ordered_metadata instruction should only be used if it is known that all threads in the warp evaluate the condition identically,\notherwise behavior is undefined. The behavior of mma.sp/mma.sp::ordered_metadata instruction is undefined if all threads in the same warp\ndo not use the same qualifiers, or if any thread in the warp has exited. Notes mma.sp instruction may have substantially reduced performance on some target architectures.\nHence, it is advised to use mma.sp::ordered_metadata instruction. PTX ISA Notes Introduced in PTX ISA version 7.1. Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in\nPTX ISA version 8.4. mma.sp::ordered_metadata introduced in PTX ISA version 8.5. Target ISA Notes Requires sm_80 or higher. .e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher. mma.sp::ordered_metadata requires sm_80 or higher. Examples of half precision floating point type // f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\n\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\n\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1; Examples of alternate floating point type .reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; Examples of integer type .reg .b32 %Ra<4>, %Rb<4>, %Rc<4>, %Rd<4>;\n.reg .u32 %Re;\n\n// u8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// s8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// s8 elements in A and B matrix with ordered metadata\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0; 9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions  The warpgroup level matrix multiply and accumulate operation has either of the following forms,\nwhere matrix D is called accumulator: D = A * B + D D = A * B , where the input from accumulator D is disabled. The wgmma instructions perform warpgroup level matrix multiply-and-accumulate operation by\nhaving all threads in a warpgroup collectively perform the following actions: Load matrices A, B and D into registers or into shared memory. Perform the following fence operations: wgmma.fence operations to indicate that the register/shared-memory across the warpgroup\nhave been written into. fence.proxy.async operation to make the generic proxy operations visible to the async\nproxy. Issue the asynchronous matrix multiply and accumulate operations using the wgmma.mma_async operation on the input matrices. The wgmma.mma_async operation is performed in the async\nproxy. Create a wgmma-group and commit all the prior outstanding wgmma.mma_async operations into the\ngroup, by using wgmma.commit_group operation. Wait for the completion of the required wgmma-group. Once the wgmma-group completes, all the wgmma.mma_async operations have been performed and\ncompleted. 9.7.14.1. Warpgroup  A warpgroup is a set of four contiguous warps such that the warp-rank of the first warp is a\nmultiple of 4. warp-rank of a warp is defined as: (%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32 9.7.14.2. Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while D is a MxN matrix. The following matrix shapes are supported for the specified types for the wgmma.mma_async operation: Multiplicand Data type Sparsity Shape Floating-point - .f16 Dense .m64n8k16 , .m64n16k16 , .m64n24k16 , .m64n32k16 , .m64n40k16 , .m64n48k16 , .m64n56k16 , .m64n64k16 , .m64n72k16 , .m64n80k16 , .m64n88k16 , .m64n96k16 , .m64n104k16 , .m64n112k16 , .m64n120k16 , .m64n128k16 , .m64n136k16 , .m64n144k16 , .m64n152k16 , .m64n160k16 , .m64n168k16 , .m64n176k16 , .m64n184k16 , .m64n192k16 , .m64n200k16 , .m64n208k16 , .m64n216k16 , .m64n224k16 , .m64n232k16 , .m64n240k16 , .m64n248k16 , .m64n256k16 Alternate floating-point\nformat - .bf16 Alternate floating-point\nformat - .tf32 Sparse Alternate floating-point\nformat - .tf32 Dense .m64n8k8 , .m64n16k8 , .m64n24k8 , .m64n32k8 , .m64n40k8 , .m64n48k8 , .m64n56k8 , .m64n64k8 , .m64n72k8 , .m64n80k8 , .m64n88k8 , .m64n96k8 , .m64n104k8 , .m64n112k8 , .m64n120k8 , .m64n128k8 , .m64n136k8 , .m64n144k8 , .m64n152k8 , .m64n160k8 , .m64n168k8 , .m64n176k8 , .m64n184k8 , .m64n192k8 , .m64n200k8 , .m64n208k8 , .m64n216k8 , .m64n224k8 , .m64n232k8 , .m64n240k8 , .m64n248k8 , .m64n256k8 Alternate floating-point\nformat - .e4m3 / .e5m2 Dense .m64n8k32 , .m64n16k32 , .m64n24k32 , .m64n32k32 , .m64n40k32 , .m64n48k32 , .m64n56k32 , .m64n64k32 , .m64n72k32 , .m64n80k32 , .m64n88k32 , .m64n96k32 , .m64n104k32 , .m64n112k32 , .m64n120k32 , .m64n128k32 , .m64n136k32 , .m64n144k32 , .m64n152k32 , .m64n160k32 , .m64n168k32 , .m64n176k32 , .m64n184k32 , .m64n192k32 , .m64n200k32 , .m64n208k32 , .m64n216k32 , .m64n224k32 , .m64n232k32 , .m64n240k32 , .m64n248k32 , .m64n256k32 Floating point - .f16 Sparse Altername floating-point\nformat - .bf16 Integer - .u8 / .s8 Dense .m64n8k32 , .m64n16k32 , .m64n24k32 , .m64n32k32 , .m64n48k32 , .m64n64k32 , .m64n80k32 , .m64n96k32 , .m64n112k32 , .m64n128k32 , .m64n144k32 , .m64n160k32 , .m64n176k32 , .m64n192k32 , .m64n208k32 , .m64n224k32 , .m64n240k32 , .m64n256k32 Alternate floating-point\nformat - .e4m3 / .e5m2 Sparse .m64n8k64 , .m64n16k64 , .m64n24k64 , .m64n32k64 , .m64n40k64 , .m64n48k64 , .m64n56k64 , .m64n64k64 , .m64n72k64 , .m64n80k64 , .m64n88k64 , .m64n96k64 , .m64n104k64 , .m64n112k64 , .m64n120k64 , .m64n128k64 , .m64n136k64 , .m64n144k64 , .m64n152k64 , .m64n160k64 , .m64n168k64 , .m64n176k64 , .m64n184k64 , .m64n192k64 , .m64n200k64 , .m64n208k64 , .m64n216k64 , .m64n224k64 , .m64n232k64 , .m64n240k64 , .m64n248k64 , .m64n256k64 Integer - .u8 / .s8 Sparse .m64n8k64 , .m64n16k64 , .m64n24k64 , .m64n32k64 , .m64n48k64 , .m64n64k64 , .m64n80k64 , .m64n96k64 , .m64n112k64 , .m64n128k64 , .m64n144k64 , .m64n160k64 , .m64n176k64 , .m64n192k64 , .m64n208k64 , .m64n224k64 , .m64n240k64 , .m64n256k64 Single-bit - .b1 Dense .m64n8k256 , .m64n16k256 , .m64n24k256 , .m64n32k256 , .m64n48k256 , .m64n64k256 , .m64n80k256 , .m64n96k256 , .m64n112k256 , .m64n128k256 , .m64n144k256 , .m64n160k256 , .m64n176k256 , .m64n192k256 , .m64n208k256 , .m64n224k256 , .m64n240k256 , .m64n256k256 9.7.14.3. Matrix Data-types  The matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point. For floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later. For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have\nelements of the same data-type, e.g. both signed integer or both unsigned integer. Data-type Multiplicands (A or B) Accumulator (D) Integer both .u8 or both .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 , .e5m2 .f16 , .f32 Single-bit integer .b1 .s32 9.7.14.4. Async Proxy  The wgmma.mma_async operations are performed in the asynchronous proxy (or async proxy). Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async\nproxy, fence.proxy.async should be used to synchronize memory between generic proxy and the\nasync proxy. The completion of a wgmma.mma_async operation is followed by an implicit generic-async proxy\nfence. So the result of the asynchronous operation is made visible to the generic proxy as soon as\nits completion is observed. wgmma.commit_group and wgmma.wait_group operations must be used\nto wait for the completion of the wgmma.mma_async instructions. 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction  This section describes warpgroup level wgmma.mma_async instruction and the organization of\nvarious matrices involved in this instruction. 9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts  The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared\nmemory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This\nsection describes the layouts of register fragments and shared memory expected by the warpgroup MMA\ninstructions. When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes. 9.7.14.5.1.1. Register Fragments  This section describes the organization of various matrices located in register operands of the wgmma.mma_async instruction. 9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16  A warpgroup executing wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with each\nregister containing two .f16 / .bf16 elements from matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 119 . Figure 119 WGMMA .m64nNk16 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) .f16 A vector expression containing N/4 number of .f16x2 registers, with each register containing two .f16 elements from matrix D. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ... , 32} .f32 A vector expression containing N/2 number of .f32 registers. The layout of the fragments held by different threads is shown in Figure 120 . Figure 120 WGMMA .m64nNk16 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8  A warpgroup executing wgmma.mma_async.m64nNk8 will compute an MMA operation of shape .m64nNk8 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers containing\nfour .tf32 elements from matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 121 . Figure 121 WGMMA .m64nNk8 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) .f32 A vector expression containing N/2 number of .f32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ... , 32} The layout of the fragments held by different threads is shown in Figure 122 . Figure 122 WGMMA .m64nNk8 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32  A warpgroup executing wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each\nregister containing four .u8 / .s8 elements from matrix A. a0, a1, a2, a3, … , a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each\nregister containing four .e4m3 / .e5m2 elements from\nmatrix A. The layout of the fragments held by different threads is shown in Figure 123 . Figure 123 WGMMA .m64nNk32 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) Miscellaneous Information .s32 A vector expression containing\nN/2 number of .s32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N depends on .dtype, as\ndescribed in the next column. N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} .f32 A vector expression containing\nN/2 number of .f32 registers. N = 8*i where i = {1, 2, ... , 32} .f16 A vector expression containing\nN/4 number of .f16x2 registers, with each register\ncontaining two .f16 elements from matrix D. The layout of the fragments held by different threads is shown in Figure 124 . Figure 124 WGMMA .m64nNk32 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256  A warpgroup executing wgmma.mma_async.m64nNk256 will compute an MMA operation of shape .m64nNk256 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each\nregister containing thirty two .b1 element from matrix A. a0, a1, a2, …, a127 The layout of the fragments held by different threads is shown in Figure 125 . Figure 125 WGMMA .m64nNk256 register fragment layout for matrix A.  Accumulator D: .dtype Fragment Elements (low to high) .s32 A vector expression containing N/2 number of .s32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} The layout of the fragments held by different threads is shown in Figure 126 . Figure 126 WGMMA .m64nNk256 register fragment layout for accumulator matrix D.  9.7.14.5.1.2. Shared Memory Matrix Layout  Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each\ncore matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy\ncontiguous space in shared memory. Matrix A is made up of 8x2 core matrices and Matrix B is made up of 2x(N/8) core matrices. This\nsection describes the layout of the core matrices for each shape. 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of eight .f16 / .bf16 elements. 8x8 B Each column is made up of eight .f16 / .bf16 elements. 8x8 Matrices A and B consist of core matrices as shown in Figure 127 .\nEach colored cell represents a core matrix. Figure 127 WGMMA .m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 128 . Each numbered\ncell represents an individual element of the core matrix. Figure 128 WGMMA .m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 129 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 129 WGMMA .m64nNk16 core matrix layout for B  9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of four .tf32 elements. 8x4 B Each row is made up of four .tf32 elements. 4x8 Matrices A and B consist of core matrices as shown in Figure 130 . Each\ncolored cell represents a core matrix. Figure 130 WGMMA .m64nNk8 core matrices for A and B  Layout of core matrices of A is shown in Figure 131 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 131 WGMMA .m64nNk8 core matrix layout for A  Layout of core matrices of B is shown in Figure 132 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 132 WGMMA .m64nNk8 core matrix layout for B  9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32  Core matrices of A and B are as follows: .atype/ .btype Core matrix Matrix description Matrix size .s8 / .u8 A Each row is made up of sixteen .s8 / .u8 elements. 8x4 .e4m3 / .e5m2 Each row is made up of sixteen .e4m3 / .e5m2 elements. .s8 / .u8 B Each column is made up of sixteen .s8 / .u8 elements. 4x8 .e4m3 / .e5m2 Each column is made up of sixteen .e4m3 / .e5m2 elements. Matrices A and B consist of core matrices as shown in Figure 133 . Each\ncolored cell represents a core matrix. Figure 133 WGMMA .m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 134 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 134 WGMMA .m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 135 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 135 WGMMA .m64nNk32 core matrix layout for B  9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of 256 .b1 elements. 8x128 B Each column is made up of 256 .b1 elements. 128x8 Matrices A and B consist of core matrices as shown in Figure 136 . Each\ncolored cell represents a core matrix. Figure 136 WGMMA .m64nNk256 core matrices for A and B  Layout of core matrices of A is shown in Figure 137 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 137 WGMMA .m64nNk256 core matrix layout for A  Layout of core matrices of B is shown in Figure 138 . Each numbered cell\nrepresents an individual element of the core matrix. Figure 138 WGMMA .m64nNk256 core matrix layout for B  9.7.14.5.1.2.5. Strides  Leading dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent\ncore matrices in the K dimension. Stride dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core\nmatrices in the M or N dimension. Figure 139 and Figure 140 show the leading dimension byte offset and the stride dimension byte offsets for A and B matrices. Matrix A: Figure 139 WGMMA stride and leading dimension byte offset for matrix A  Matrix B: Figure 140 WGMMA stride and leading dimension byte offset for matrix B  Leading dimension byte offset and stride dimension byte offset must be specified in the matrix\ndescriptor as described in Matrix Descriptor Format . 9.7.14.5.1.2.6. Swizzling Modes  The core matrices can be swizzled in the shared memory by specifying one of the following swizzling\nmodes: No swizzling: All the elements of the entire core matrix are adjacent to each other and there is\nno swizzling. Figure 141 illustrates this: Figure 141 WGMMA core matrices with no swizzling  32-Byte swizzling: A group of two adjacent core matrices are swizzled as shown in Figure 142 . The\nswizzling pattern repeats for the remaining core matrices. Figure 142 WGMMA core matrices with 32-byte swizzling  64-Byte swizzling: A group of four adjacent core matrices are swizzled as shown in Figure 143 . The\nswizzling pattern repeats for the remaining core matrices. Figure 143 WGMMA core matrices with 64-byte swizzling  128-Byte swizzling: A group of eight adjacent core matrices are swizzled as shown in Figure 144 . The\nswizzling pattern repeats for the remaining core matrices. Figure 144 WGMMA core matrices with 128-byte swizzling  9.7.14.5.1.2.7. Matrix Descriptor Format  Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in\nthe matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the\nfollowing layout: Bit-field Size in bits Description 13–0 14 matrix-descriptor-encode(Matrix start address) 29–16 14 matrix-descriptor-encode(Leading dimension byte offset) 45–32 14 matrix-descriptor-encode(Stride dimension byte offset) 51–49 3 Matrix base offset. This is valid for all swizzling modes except the no-swizzle mode. 63–62 2 Specifies the swizzling mode to be used: 0: No swizzle 1: 128-Byte swizzle 2: 64-Byte swizzle 3: 32-Byte swizzle where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 0x4 The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as\nper the below table: Swizzling mode Starting address of the repeating pattern 128-Byte swizzle 1024-Byte boundary 64-Byte swizzle 512-Byte boundary 32-Byte swizzle 256-Byte boundary Otherwise, the base offset must be a non-zero value, computed using the following formula: base offset = (pattern start addr >> 0x7) & 0x7 9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async  wgmma.mma_async Perform matrix multiply-and-accumulate operation across warpgroup Syntax Half precision floating point type: wgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype   = {.f16, .f32}; Alternate floating point type : .bf16 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\n.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,\n            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,\n            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,\n            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,\n            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,\n            .m64n168k8, .m648176k8, .m64n184k8, .m64n192k8,\n            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,\n            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32}; Integer type: wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,\n            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,\n            .m648176k32, .m64n192k32, .m64n208k32, .m64n224k32};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8}; Single bit: wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,\n            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,\n            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,\n            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,\n            .m64n240k256, .m64n256k256};\n.op  = {.and}; Description Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D = A*B+D , where the A matrix is MxK , the B matrix is KxN , and the D matrix is MxN . The operation of the form D = A*B is issued when the input predicate argument scale-d is\nfalse. wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async instruction from their prior accesses. Otherwise, the behavior is undefined. wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed. Register operand d represents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand a represents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the warps\nin the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format . Matrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0\nand 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16 / .bf16 types on matrices accessed from shared memory using matrix descriptors. For the floating point variants of the wgmma.mma_async operation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of imm-scale-a and imm-scale-b are -1 and 1. The qualifiers .dtype , .atype and .btype indicate the data type of the elements in\nmatrices D, A and B respectively. .atype and .btype must be the same for all floating point wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the wgmma.mma_async operation are as follows: Matrices A and B have 8-bit data elements when .atype / .btype is .e4m3 / .e5m2 . Matrices A and B have 16-bit data elements when .atype / .btype is .bf16 . Matrices A and B have 32-bit data elements when .atype / .btype is .tf32 . Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .dtype is .f32 , accumulation of the intermediate values is performed with at least single\nprecision. When .dtype is .f16 , the accumulation is performed with at least half\nprecision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified\nprecision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. Integer operations: The integer wgmma.mma_async operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the\nrange MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.mma_async instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4. Target ISA Notes Requires sm_90a . Examples of half precision floating point type .reg .f16x2 f16a<40>, f16d<40>;\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n.reg .pred  scaleD;\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16\n  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,\n   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},\n  descA,\n  descB,\n  scaleD, -1, 1, 1, 0; Examples of alternate floating point type .reg .f32   f32d<40>;\n.reg .b32   bf16a<40>\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,\n   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,\n   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,\n   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,\n   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,\n   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},\n  {bf16a0, bf16a1, bf16a2, bf16a3},\n  descB,\n  scaleD, -1, -1, 0;\n\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},\n  descA,\n  descB,\n  0, -1, -1;\n\n.reg .b32 f16d<8>, f16a<8>;\n.reg .f32 f32d<8>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2\n  {f16d0, f16d1},\n  descA,\n  descB,\n  scaleD, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1; Examples of integer type .reg .s32 s32d<8>, s32a<8>;\n.reg .u32 u32a<8>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD; Examples of single bit type .reg .s32 s32d<4>;\n.reg .b32 b32a<4>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\n\nwgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc\n  {s32d0, s32d1, s32d2, s32d3},\n  {b32a0, b32a1, b32a2, b32a3},\n  descB,\n  scaleD; 9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction  This section describes warp-level wgmma.mma_async.sp instruction with sparse matrix A. This\nvariant of the wgmma.mma_async operation can be used when A is a structured sparse matrix with\n50% zeros in each row distributed in a shape-specific granularity. For an MxNxK sparse wgmma.mma_async.sp operation, the MxK matrix A is packed into MxK/2 elements. For each\nK-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements are\npacked in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata. 9.7.14.6.1. Sparse matrix storage  Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a 64x32 matrix A used in floating point wgmma.mma_async operations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element\nvector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each\nnon-zero element in a sub-chunk is stored in the metadata operand. Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and will result in undefined behavior. In a\ngroup of four consecutive threads, one or more threads store the metadata for the whole group\ndepending upon the matrix shape. These threads are specified using an additional sparsity selector operand. Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in Figure 83 , with an appropriate matrix size. Granularities for different matrix shapes and data types are described below. Sparse wgmma.mma_async.sp with half-precision and .bf16 type For .f16 and .bf16 types, for all supported 64xNx32 shapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand. Figure 145 Sparse WGMMA metadata example for .f16 / .bf16 type.  The sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior. Sparse wgmma.mma_async.sp with .tf32 type For .tf32 type, for all supported 64xNx16 shapes, matrix A is structured sparse at a\ngranularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have\none zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and\nthe 4-bit index in the metadata indicates the position of the non-zero element in the two-wide\nchunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in\nan undefined behavior. Figure 146 Sparse WGMMA metadata example for .tf32 type.  The sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior. Sparse wgmma.mma_async.sp with .e4m3 and .e5m2 floating point type For .e4m3 and .e5m2 types, for all supported 64xNx64 shapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand. Figure 147 Sparse WGMMA metadata example for .e4m3 / .e5m2 type.  All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior. Sparse wgmma.mma_async.sp with integer type For the integer type, for all supported 64xNx64 shapes, matrix A is structured sparse at a\ngranularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have\ntwo zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two\n2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide\nchunk. Figure 148 Sparse WGMMA metadata example for .u8 / .s8 type.  All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior. 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of A\nmatrix and the sparsity metadata. Each warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following\ntable shows the assignment of warps to rows of matrix A: Warp Sparsity information for rows of matrix A %warpid % 4 = 3 48-63 %warpid % 4 = 2 32-47 %warpid % 4 = 1 16-31 %warpid % 4 = 0 0-15 The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data. For matrix D, since the matrix dimension - data type combination is the same for all supported\nshapes, and is already covered in Matrix multiply-accumulate operation using wgmma instruction , the pictorial\nrepresentations of matrix fragments are not included in this section. For the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present\nin cell [x][y..z] indicates that bits m through n (with m being higher) in the\nmetadata operand of thread with %laneid=k contains the indices of the non-zero elements from\nthe chunk [x][y]..[x][z] of matrix A. 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32  A warpgroup executing sparse wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A, from shared memory is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk32 . Multiplicand A, from registers: .atype Fragments Elements .f16 / .bf16 A vector expression containing four .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A. Non-zero elements: a0, a1, a2, a3, a4, a5, a6, a7 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 149 . Figure 149 Sparse WGMMA .m64nNk32 fragment layout for matrix A with .f16 / .bf16 type.  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32\nwith floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk32 . Metadata operand is a .b32 register containing 16 2-bit vectors each storing the index of a\nnon-zero element of a 4-wide chunk of matrix A. Figure 150 shows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable i represents the value of the sparsity\nselector operand. Figure 150 Sparse WGMMA .m64nNk32 metadata layout for .f16 / .bf16 type.  9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16  A warpgroup executing sparse wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A, from shared memory is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk16 . Multiplicand A, from registers: .atype Fragments Elements .tf32 A vector expression containing four .b32 registers, containing four non-zero .tf32 elements out of eight consecutive elements from matrix A. Non-zero elements: a0, a1, a2, a3 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 151 . Figure 151 Sparse WGMMA .m64nNk16 fragment layout for matrix A with .tf32 type.  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk8\nwith floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk16 . Metadata operand is a .b32 register containing eight 4-bit vectors each storing the index of a\nnon-zero element of a 2-wide chunk of matrix A. Figure 152 shows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable i represents the value of the sparsity\nselector operand. Figure 152 Sparse WGMMA .m64nNk16 metadata layout for .tf32 type.  9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64  A warpgroup executing sparse wgmma.mma_async.m64nNk64 will compute an MMA operation of shape .m64nNk64 where N is a valid n dimension as listed in Matrix shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix. Multiplicand A, from shared memory is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk64 . Multiplicand A, from registers: .atype Fragments Elements .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four non-zero .e4m3 / .e5m2 elements out of eight consecutive elements from matrix A. Non-zero elements: a0, a1, a2, … , a15 Mapping of the non-zero elements is as described in Sparse matrix storage .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four non-zero .s8 / .u8 elements out of eight consecutive elements from matrix A. The layout of the fragments held by different threads is shown in Figure 153 . Figure 153 Sparse WGMMA .m64nNk64 fragment layout for matrix A with .e4m3 / .e5m2 / .s8 / .u8 type.  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32\nwith floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for\nwgmma.mma_async.m64nNk64 . Metadata operand is a .b32 register containing 16 4-bit vectors each storing the indices of\ntwo non-zero elements of a 4-wide chunk of matrix A. Figure 154 shows the mapping of the metadata\nbits to the elements of columns 0–31 of matrix A. Figure 154 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 0–31  Figure 155 shows the mapping of the metadata\nbits to the elements of columns 32–63 of matrix A. Figure 155 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 32–63  9.7.14.6.3. Shared Memory Matrix Layout  Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each\ncore matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy\ncontiguous space in shared memory. Matrix A is made up of 8x2 packed core matrices and Matrix B is made up of 4x (N/8) core\nmatrices. This section describes the layout of the core matrices for each shape. 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of sixteen .f16 / .bf16 elements,\nwith two non-zero elements out of four consecutive elements. 8x16 B Each column is made up of eight .f16 / .bf16 elements. 8x8 Matrices A and B consist of core matrices as shown in Figure 156 .\nEach colored cell represents a core matrix. Figure 156 Sparse WGMMA .m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 157 . Figure 157 Sparse WGMMA .m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 158 . Figure 158 Sparse WGMMA .m64nNk32 core matrix layout for B  9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of eight .tf32 elements\nwith a non-zero element out of two consecutive elements. 8x8 B Each column is made up of four .tf32 elements. 4x8 Matrices A and B consist of core matrices as shown in Figure 159 .\nEach colored cell represents a core matrix. Figure 159 Sparse WGMMA .m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 160 . Figure 160 Sparse WGMMA .m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 161 . Figure 161 Sparse WGMMA .m64nNk16 core matrix layout for B  9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of thirty-two .e4m3 / .e5m2 elements,\nwith two non-zero elements out of four consecutive elements. 8x32 B Each column is made up of eight .f16 / .bf16 elements. 16x8 Matrices A and B consist of core matrices as shown in Figure 162 .\nEach colored cell represents a core matrix. Figure 162 Sparse WGMMA .m64nNk64 core matrices for A and B  Layout of core matrices of A is shown in Figure 163 . Figure 163 Sparse WGMMA .m64nNk64 core matrix layout for A  Layout of core matrices of B is shown in Figure 164 . Figure 164 Sparse WGMMA .m64nNk64 core matrix layout for B  9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp  wgmma.mma_async.sp Perform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup Syntax Half precision floating point type: wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype   = {.f16, .f32}; Alternate floating point type : .bf16 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,\n            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,\n            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,\n            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,\n            .m64n168k64, .m648176k64, .m64n184k64, .m64n192k64,\n            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,\n            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32}; Integer type: wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;\n\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,\n            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,\n            .m648176k64, .m64n192k64, .m64n208k64, .m64n224k64,\n            .m64n240k64, .m64n256k64};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8}; Description Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D = A*B+D , where the A matrix is MxK , the B matrix is KxN , and the D matrix is MxN . The matrix A is stored in the packed format Mx(K/2) as described in Matrix multiply-accumulate\noperation using wgmma.mma_async.sp instruction with sparse matrix A . The operation of the form D = A*B is issued when the input predicate argument scale-d is\nfalse. wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async instruction from their prior accesses. Otherwise, the behavior is undefined. wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed. Register operand d represents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand a represents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the\nwarps in the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format . Matrix A is\nstructured sparse as described in Sparse matrix storage .  Operands sp-meta and sp-sel represent sparsity metadata and sparsity selector respectively. Operand sp-meta is a 32-bit\ninteger and operand sp-sel is a 32-bit integer constant with values in the range 0..3. The valid values of sp-meta and sp-sel for each shape is specified in Matrix\nmultiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A and are summarized here : Matrix shape .atype Valid values of sp-meta Valid values of sp-sel .m64nNk16 .tf32 0b1110 , 0b0100 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk32 .f16 / .bf16 0b00, 0b01, 0b10, 0b11 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk64 .e4m3 / .e5m2 / .s8 / .u8 0b00, 0b01, 0b10, 0b11 0 (all threads contribute) Matrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0\nand 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16 / .bf16 types on matrices accessed from shared memory using matrix descriptors. For the floating point variants of the wgmma.mma_async operation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of imm-scale-a and imm-scale-b are -1 and 1. The qualifiers .dtype , .atype and .btype indicate the data type of the elements in\nmatrices D, A and B respectively. .atype and .btype must be the same for all floating point wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the wgmma.mma_async operation are as follows: Matrices A and B have 8-bit data elements when .atype / .btype is .e4m3 / .e5m2 . Matrices A and B have 16-bit data elements when .atype / .btype is .bf16 . Matrices A and B have 32-bit data elements when .atype / .btype is .tf32 . Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .dtype is .f32 , accumulation of the intermediate values is performed with at least single\nprecision. When .dtype is .f16 , the accumulation is performed with at least half\nprecision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified\nprecision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. Integer operations: The integer wgmma.mma_async operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the\nrange MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.mma_async instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.2. Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4. Target ISA Notes Requires sm_90a . Examples of integer type wgmma.fence.sync.aligned;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0; 9.7.14.7. Asynchronous wgmma Proxy Operations  This section describes warpgroup level wgmma.fence , wgmma.commit_group and wgmma.wait_group instructions. 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence  wgmma.fence Enforce an ordering of register accesses between wgmma.mma_async and other operations. Syntax wgmma.fence.sync.aligned; Description wgmma.fence instruction establishes an ordering between prior accesses to any warpgroup\nregisters and subsequent accesses to the same registers by a wgmma.mma_async instruction. Only\nthe accumulator register and the input registers containing the fragments of matrix A require this\nordering. The wgmma.fence instruction must be issued by all warps of the warpgroup at the following\nlocations: Before the first wgmma.mma_async operation in a warpgroup. Between a register access by a thread in the warpgroup and any wgmma.mma_async instruction\nthat accesses the same registers, either as accumulator or input register containing fragments of\nmatrix A, except when these are accumulator register accesses across multiple wgmma.mma_async instructions of the same shape. In the latter case, an ordering guarantee is provided by default. Otherwise, the behavior is undefined. An async proxy fence must be used to establish an ordering between prior writes to shared memory\nmatrices and subsequent reads of the same matrices in a wgmma.mma_async instruction. The mandatory .sync qualifier indicates that wgmma.fence instruction causes the executing\nthread to wait until all threads in the warp execute the same wgmma.fence instruction before\nresuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.fence instruction. In conditionally executed code, an wgmma.fence instruction\nshould only be used if it is known that all threads in the warpgroup evaluate the condition\nidentically, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples // Example 1, first use example:\nwgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n\n// Example 2, use-case with the input value updated in between:\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\n...\nmov.b32 s32d0, new_val;\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},\n                                                 {s32d0, s32d1, s32d2, s32d3},\n                                                  descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0; 9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group  wgmma.commit_group Commits all prior uncommitted wgmma.mma_async operations into a wgmma-group . Syntax wgmma.commit_group.sync.aligned; Description wgmma.commit_group instruction creates a new wgmma-group per warpgroup and batches all prior wgmma.mma_async instructions initiated by the executing warp but not committed to any\nwgmma-group into the new wgmma-group. If there are no uncommitted wgmma.mma_async instructions\nthen wgmma.commit_group results in an empty wgmma-group. An executing thread can wait for the completion of all wgmma.mma_async operations in a\nwgmma-group by using wgmma.wait_group . The mandatory .sync qualifier indicates that wgmma.commit_group instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.commit_group instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.commit_group instruction. In conditionally executed code, an wgmma.commit_group instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples wgmma.commit_group.sync.aligned; 9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group  wgmma.wait_group Signal the completion of a preceding warpgroup operation. Syntax wgmma.wait_group.sync.aligned N; Description wgmma.wait_group instruction will cause the executing thread to wait until only N or fewer of\nthe most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing\nthreads are complete. For example, when N is 0, the executing thread waits on all the prior\nwgmma-groups to complete. Operand N is an integer constant. Accessing the accumulator register or the input register containing the fragments of matrix A of a wgmma.mma_async instruction without first performing a wgmma.wait_group instruction that\nwaits on a wgmma-group including that wgmma.mma_async instruction is undefined behavior. The mandatory .sync qualifier indicates that wgmma.wait_group instruction causes the\nexecuting thread to wait until all threads in the warp execute the same wgmma.wait_group instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame wgmma.wait_group instruction. In conditionally executed code, an wgmma.wait_group instruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples wgmma.fence.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},\n                                                  {f16a0, f16a1, f16a2, f16a3},\n                                                   descB, 1, -1, -1, 1;\nwgmma.commit_group.sync.aligned;\n\nwgmma.wait_group.sync.aligned 0; 9.7.15. Stack Manipulation Instructions  The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the\nstack frame of the current function. The stack manipulation instrucitons are: stacksave stackrestore alloca 9.7.15.1. Stack Manipulation Instructions: stacksave  stacksave Save the value of stack pointer into a register. Syntax stacksave.type  d;\n\n.type = { .u32, .u64 }; Description Copies the current value of stack pointer into the destination register d . Pointer returned by stacksave can be used in a subsequent stackrestore instruction to restore the stack\npointer. If d is modified prior to use in stackrestore instruction, it may corrupt data in\nthe stack. Destination operand d has the same type as the instruction type. Semantics d = stackptr; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: stacksave is a preview feature in PTX ISA version 7.3. All details are subject to change with\nno guarantees of backward compatibility on future PTX ISA versions or SM architectures. Target ISA Notes stacksave requires sm_52 or higher. Examples .reg .u32 rd;\nstacksave.u32 rd;\n\n.reg .u64 rd1;\nstacksave.u64 rd1; 9.7.15.2. Stack Manipulation Instructions: stackrestore  stackrestore Update the stack pointer with a new value. Syntax stackrestore.type  a;\n\n.type = { .u32, .u64 }; Description Sets the current stack pointer to source register a . When stackrestore is used with operand a written by a prior stacksave instruction, it\nwill effectively restore the state of stack as it was before stacksave was executed. Note that\nif stackrestore is used with an arbitrary value of a , it may cause corruption of stack\npointer. This implies that the correct use of this feature requires that stackrestore.type a is\nused after stacksave.type a without redefining the value of a between them. Operand a has the same type as the instruction type. Semantics stackptr = a; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: stackrestore is a preview feature in PTX ISA version 7.3. All details are subject to change\nwith no guarantees of backward compatibility on future PTX ISA versions or SM architectures. Target ISA Notes stackrestore requires sm_52 or higher. Examples .reg .u32 ra;\nstacksave.u32 ra;\n// Code that may modify stack pointer\n...\nstackrestore.u32 ra; 9.7.15.3. Stack Manipulation Instructions: alloca  alloca Dynamically allocate memory on stack. Syntax alloca.type  ptr, size{, immAlign};\n\n.type = { .u32, .u64 }; Description The alloca instruction dynamically allocates memory on the stack frame of the current function\nand updates the stack pointer accordingly. The returned pointer ptr points to local memory and\ncan be used in the address operand of ld.local and st.local instructions. If sufficient memory is unavailable for allocation on the stack, then execution of alloca may\nresult in stack overflow. In such cases, attempting to access the allocated memory with ptr will\nresult in undefined program behavior. The memory allocated by alloca is deallocated in the following ways: It is automatically deallocated when the function exits. It can be explicitly deallocated using stacksave and stackrestore instructions: stacksave can be used to save the value of stack pointer before executing alloca , and stackrestore can be used after alloca to restore stack pointer to the original value which\nwas previously saved with stacksave . Note that accessing deallocated memory after executing stackrestore results in undefined behavior. size is an unsigned value which specifies the amount of memory in number of bytes to be\nallocated on stack. size = 0 may not lead to a valid memory allocation. Both ptr and size have the same type as the instruction type. immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the\nmemory allocated by alloca . It is an integer constant, must be a power of 2 and must not exceed\n2^23. immAlign is an optional argument with default value being 8 which is the minimum\nguaranteed alignment. Semantics alloca.type ptr, size, immAlign:\n\na = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment\n\n// Allocate size bytes of stack memory with alignment a and update the stack pointer.\n// Since the stack grows down, the updated stack pointer contains a lower address.\nstackptr = alloc_stack_mem(size, a);\n\n// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory\n// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).\nstacksave ptr; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: alloca is a preview feature in PTX ISA version 7.3. All details are subject to change with no\nguarantees of backward compatibility on future PTX ISA versions or SM architectures. Target ISA Notes alloca requires sm_52 or higher. Examples .reg .u32 ra, stackptr, ptr, size;\n\nstacksave.u32 stackptr;     // Save the current stack pointer\nalloca ptr, size, 8;        // Allocate stack memory\nst.local.u32 [ptr], ra;     // Use the allocated stack memory\nstackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer 9.7.16. Video Instructions  All video instructions operate on 32-bit register operands. However, the video instructions may be\nclassified as either scalar or SIMD based on whether their core operation applies to one or multiple\nvalues. The video instructions are: vadd , vadd2 , vadd4 vsub , vsub2 , vsub4 vmad vavrg2 , vavrg4 vabsdiff , vabsdiff2 , vabsdiff4 vmin , vmin2 , vmin4 vmax , vmax2 , vmax4 vshl vshr vset , vset2 , vset4 9.7.16.1. Scalar Video Instructions  All scalar video instructions operate on 32-bit register operands. The scalar video instructions\nare: vadd vsub vabsdiff vmin vmax vshl vshr vmad vset The scalar video instructions execute the following stages: Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to\nproduce signed 33-bit input values. Perform a scalar arithmetic operation to produce a signed 34-bit result. Optionally clamp the result to the range of the destination type. Optionally perform one of the following: apply a second operation to the intermediate result and a third operand, or truncate the intermediate result to a byte or half-word value and merge into a specified\nposition in the third operand to produce the final result. The general format of scalar video instructions is as follows: // 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;\n\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.secop = { .add, .min, .max }; The source and destination operands are all 32-bit registers. The type of each operand ( .u32 or .s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are\nextracted and sign- or zero-extended internally to .s33 values. The primary operation is then\nperformed to produce an .s34 intermediate result. The sign of the intermediate result depends on\ndtype. The intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging. .s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {\n    if ( !sat )  return tmp;\n\n    switch ( dsel ) {\n        case .b0, .b1, .b2, .b3:\n            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );\n            else         return CLAMP( tmp, U8_MAX, U8_MIN );\n        case .h0, .h1:\n            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );\n            else         return CLAMP( tmp, U16_MAX, U16_MIN );\n        default:\n            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );\n            else         return CLAMP( tmp, U32_MAX, U32_MIN );\n    }\n} This intermediate result is then optionally combined with the third source operand using a secondary\narithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the\nthird operand is based on dtype . .s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {\n    switch ( secop ) {\n        .add:     return tmp + c;\n        .min:     return MIN(tmp, c);\n        .max      return MAX(tmp, c);\n        default:  return tmp;\n    }\n} .s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {\n    switch ( dsel ) {\n        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);\n        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);\n        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);\n        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);\n        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);\n        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);\n        default:   return tmp;\n    }\n} The lower 32-bits are then written to the destination operand. 9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax  vadd, vsub Integer byte/half-word/word addition/subtraction. vabsdiff Integer byte/half-word/word absolute value of difference. vmin, vmax Integer byte/half-word/word minimum/maximum. Syntax // 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vadd, vsub, vabsdiff, vmin, vmax };\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max }; Description Perform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\n\nswitch ( vop ) {\n    case vadd:     tmp = ta + tb;\n    case vsub:     tmp = ta - tb;\n    case vabsdiff: tmp = | ta - tb |;\n    case vmin:     tmp = MIN( ta, tb );\n    case vmax:     tmp = MAX( ta, tb );\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vadd , vsub , vabsdiff , vmin , vmax require sm_20 or higher. Examples vadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;\nvsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;\nvabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;\nvmin.s32.s32.s32.sat.add  r1, r2, r3, c; 9.7.16.1.2. Scalar Video Instructions: vshl, vshr  vshl, vshr Integer byte/half-word/word left/right shift. Syntax // 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};\nvop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vshl, vshr };\n.dtype = .atype = { .u32, .s32 };\n.mode  = { .clamp, .wrap };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max }; Description vshl Shift a left by unsigned amount in b with optional saturate, and optional secondary\narithmetic operation or subword data merge. Left shift fills with zero. vshr Shift a right by unsigned amount in b with optional saturate, and optional secondary\narithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift\nfills with zero. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a,atype, asel );\ntb = partSelectSignExtend( b, .u32, bsel );\nif ( mode == .clamp  && tb > 32 )  tb = 32;\nif ( mode == .wrap )                       tb = tb & 0x1f;\nswitch ( vop ){\n   case vshl:  tmp = ta << tb;\n   case vshr:  tmp = ta >> tb;\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vshl , vshr require sm_20 or higher. Examples vshl.s32.u32.u32.clamp  r1, r2, r3;\nvshr.u32.u32.u32.wrap   r1, r2, r3.h1; 9.7.16.1.3. Scalar Video Instructions: vmad  vmad Integer byte/half-word/word multiply-accumulate. Syntax // 32-bit scalar operation\nvmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},\n                                         {-}c;\nvmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.scale = { .shr7, .shr15 }; Description Calculate (a*b) + c , with optional operand negates, plus one mode, and scaling. The source operands support optional negation with some restrictions. Although PTX syntax allows\nseparate negation of the a and b operands, internally this is represented as negation of the\nproduct (a*b) . That is, (a*b) is negated if and only if exactly one of a or b is\nnegated. PTX allows negation of either (a*b) or c . The plus one mode ( .po ) computes (a*b) + c + 1 , which is used in computing averages. Source\noperands may not be negated in .po mode. The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product (a*b) is not negated; otherwise, the intermediate result is signed. Input c has the same\nsign as the intermediate result. The final result is unsigned if the intermediate result is unsigned and c is not negated. Depending on the sign of the a and b operands, and the operand negates, the following\ncombinations of operands are supported for VMAD: (u32 * u32) + u32  // intermediate unsigned; final unsigned\n-(u32 * u32) + s32  // intermediate   signed; final   signed\n (u32 * u32) - u32  // intermediate unsigned; final   signed\n (u32 * s32) + s32  // intermediate   signed; final   signed\n-(u32 * s32) + s32  // intermediate   signed; final   signed\n (u32 * s32) - s32  // intermediate   signed; final   signed\n (s32 * u32) + s32  // intermediate   signed; final   signed\n-(s32 * u32) + s32  // intermediate   signed; final   signed\n (s32 * u32) - s32  // intermediate   signed; final   signed\n (s32 * s32) + s32  // intermediate   signed; final   signed\n-(s32 * s32) + s32  // intermediate   signed; final   signed\n (s32 * s32) - s32  // intermediate   signed; final   signed The intermediate result is optionally scaled via right-shift; this result is sign-extended if the\nfinal result is signed, and zero-extended otherwise. The final result is optionally saturated to the appropriate 32-bit range based on the type (signed\nor unsigned) of the final result. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\nsignedFinal = isSigned(atype) || isSigned(btype) ||\n                                 (a.negate ^ b.negate) || c.negate;\ntmp[127:0] = ta * tb;\n\nlsb = 0;\nif ( .po )                  {              lsb = 1; } else\nif ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else\nif ( c.negate )             { c   = ~c;    lsb = 1; }\n\nc128[127:0] = (signedFinal) sext32( c ) : zext ( c );\ntmp = tmp + c128 + lsb;\nswitch( scale ) {\n   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;\n   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;\n}\nif ( .sat ) {\n     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);\n     else             result = CLAMP(result, U32_MAX, U32_MIN);\n} PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vmad requires sm_20 or higher. Examples vmad.s32.s32.u32.sat    r0, r1, r2, -r3;\nvmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3; 9.7.16.1.4. Scalar Video Instructions: vset  vset Integer byte/half-word/word comparison. Syntax // 32-bit scalar operation, with optional secondary operation\nvset.atype.btype.cmp       d, a{.asel}, b{.bsel};\nvset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max }; Description Compare input values using specified comparison, with optional secondary arithmetic operation or\nsubword data merge. The intermediate result of the comparison is always unsigned, and therefore destination d and\noperand c are also unsigned. Semantics // extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\ntmp = compare( ta, tb, cmp ) ? 1 : 0;\nd = optSecondaryOp( op2, tmp, c );    // optional secondary operation\nd = optMerge( dsel, tmp, c );         // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vset requires sm_20 or higher. Examples vset.s32.u32.lt    r1, r2, r3;\nvset.u32.u32.ne    r1, r2, r3.h1; 9.7.16.2. SIMD Video Instructions  The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values. The SIMD video instructions are: vadd2 , vadd4 vsub2 , vsub4 vavrg2 , vavrg4 vabsdiff2 , vabsdiff4 vmin2 , vmin4 vmax2 , vmax4 vset2 , vset4 PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit\nvalues. The SIMD video instructions execute the following stages: Form input vectors by extracting and sign- or zero-extending byte or half-word values from the\nsource operands, to form pairs of signed 17-bit values. Perform a SIMD arithmetic operation on the input pairs. Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the\ndestination type. Optionally perform one of the following: perform a second SIMD merge operation, or apply a scalar accumulate operation to reduce the intermediate SIMD results to a single\nscalar. The general format of dual half-word SIMD video instructions is as follows: // 2-way SIMD operation, with second SIMD merge or accumulate\nvop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };\n.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } }; The general format of quad byte SIMD video instructions is as follows: // 4-way SIMD operation, with second SIMD merge or accumulate\nvop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 }; The source and destination operands are all 32-bit registers. The type of each operand ( .u32 or .s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are\nextracted and sign- or zero-extended internally to .s33 values. The primary operation is then\nperformed to produce an .s34 intermediate result. The sign of the intermediate result depends on dtype . The intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging. 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2  vadd2, vsub2 Integer dual half-word SIMD addition/subtraction. vavrg2 Integer dual half-word SIMD average. vabsdiff2 Integer dual half-word SIMD absolute value of difference. vmin2, vmax2 Integer dual half-word SIMD minimum/maximum. Syntax // SIMD instruction with secondary SIMD merge operation\nvop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32 Description Two-way SIMD parallel arithmetic operation with secondary operation. Elements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands a and b using the asel and bsel modifiers. The selected half-words are then operated on in parallel. The results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation. For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into\ndestination d . For all other positions, the corresponding half-word from source operand c is copied to d . For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to operand c , producing a result in d . Semantics // extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\n\nfor (i=0; i<2; i++) {\n    switch ( vop2 ) {\n       case vadd2:             t[i] = Va[i] + Vb[i];\n       case vsub2:             t[i] = Va[i] - Vb[i];\n       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;\n       case vmin2:             t[i] = MIN( Va[i], Vb[i] );\n       case vmax2:             t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );\n        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vadd2 , vsub2 , varvg2 , vabsdiff2 , vmin2 , vmax2 require sm_30 or higher. Examples vadd2.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;\nvmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1; 9.7.16.2.2. SIMD Video Instructions: vset2  vset2 Integer dual half-word SIMD comparison. Syntax // SIMD instruction with secondary SIMD merge operation\nvset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32 Description Two-way SIMD parallel comparison with secondary operation. Elements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands a and b using the asel and bsel modifiers. The selected half-words are then compared in parallel. The intermediate result of the comparison is always unsigned, and therefore the half-words of\ndestination d and operand c are also unsigned. For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into\ndestination d . For all other positions, the corresponding half-word from source operand b is copied to d . For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to operand c , producing a result in d . Semantics // extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\nfor (i=0; i<2; i++) {\n    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vset2 requires sm_30 or higher. Examples vset2.s32.u32.lt      r1, r2, r3, r0;\nvset2.u32.u32.ne.add  r1, r2, r3, r0; 9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4  vadd4, vsub4 Integer quad byte SIMD addition/subtraction. vavrg4 Integer quad byte SIMD average. vabsdiff4 Integer quad byte SIMD absolute value of difference. vmin4, vmax4 Integer quad byte SIMD minimum/maximum. Syntax // SIMD instruction with secondary SIMD merge operation\nvop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\nvop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654 Description Four-way SIMD parallel arithmetic operation with secondary operation. Elements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands a and b using the asel and bsel modifiers. The selected bytes are then operated on in parallel. The results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation. For instructions with a secondary SIMD merge operation: For byte positions indicated in mask, the selected byte results are copied into destination d . For all other positions, the corresponding byte from source operand c is copied to d . For instructions with a secondary accumulate operation: For byte positions indicated in mask, the selected byte results are added to operand c ,\nproducing a result in d . Semantics // extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    switch ( vop4 ) {\n        case vadd4:            t[i] = Va[i] + Vb[i];\n        case vsub4:            t[i] = Va[i] - Vb[i];\n        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;\n        case vmin4:            t[i] = MIN( Va[i], Vb[i] );\n        case vmax4:            t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );\n        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vadd4 , vsub4 , varvg4 , vabsdiff4 , vmin4 , vmax4 require sm_30 or higher. Examples vadd4.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;\nvmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1; 9.7.16.2.4. SIMD Video Instructions: vset4  vset4 Integer quad byte SIMD comparison. Syntax // SIMD instruction with secondary SIMD merge operation\nvset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654 Description Four-way SIMD parallel comparison with secondary operation. Elements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands a and b using the asel and bsel modifiers. The selected bytes are then compared in parallel. The intermediate result of the comparison is always unsigned, and therefore the bytes of destination d and operand c are also unsigned. For instructions with a secondary SIMD merge operation: For byte positions indicated in mask, the selected byte results are copied into destination d . For all other positions, the corresponding byte from source operand b is copied to d . For instructions with a secondary accumulate operation: For byte positions indicated in mask, the selected byte results are added to operand c ,\nproducing a result in d . Semantics // extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n} PTX ISA Notes Introduced in PTX ISA version 3.0. Target ISA Notes vset4 requires sm_30 or higher. Examples vset4.s32.u32.lt      r1, r2, r3, r0;\nvset4.u32.u32.ne.max  r1, r2, r3, r0; 9.7.17. Miscellaneous Instructions  The Miscellaneous instructions are: brkpt nanosleep pmevent trap setmaxnreg 9.7.17.1. Miscellaneous Instructions: brkpt  brkpt Breakpoint. Syntax brkpt; Description Suspends execution. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes brkpt requires sm_11 or higher. Examples brkpt;\n@p  brkpt; 9.7.17.2. Miscellaneous Instructions: nanosleep  nanosleep Suspend the thread for an approximate delay given in nanoseconds. Syntax nanosleep.u32 t; Description Suspends the thread for a sleep duration approximately close to the delay t , specified in\nnanoseconds. t may be a register or an immediate value. The sleep duration is approximated, but guaranteed to be in the interval [0, 2*t] . The maximum\nsleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual\nthreads within a warp such that all sleeping threads in the warp wake up together. PTX ISA Notes nanosleep introduced in PTX ISA 6.3. Target ISA Notes nanosleep requires sm_70 or higher. Examples .reg .b32 r;\n.reg .pred p;\n\nnanosleep.u32 r;\nnanosleep.u32 42;\n@p nanosleep.u32 r; 9.7.17.3. Miscellaneous Instructions: pmevent  pmevent Trigger one or more Performance Monitor events. Syntax pmevent       a;    // trigger a single performance monitor event\npmevent.mask  a;    // trigger one or more performance monitor events Description Triggers one or more of a fixed number of performance monitor events, with event index or mask\nspecified by immediate operand a . pmevent (without modifier .mask ) triggers a single performance monitor event indexed by\nimmediate operand a , in the range 0..15 . pmevent.mask triggers one or more of the performance monitor events. Each bit in the 16-bit\nimmediate operand a controls an event. Programmatic performance moniter events may be combined with other hardware events using Boolean\nfunctions to increment one of the four performance counters. The relationship between events and\ncounters is programmed via API calls from the host. Notes Currently, there are sixteen performance monitor events, numbered 0 through 15. PTX ISA Notes pmevent introduced in PTX ISA version 1.4. pmevent.mask introduced in PTX ISA version 3.0. Target ISA Notes pmevent supported on all target architectures. pmevent.mask requires sm_20 or higher. Examples pmevent      1;\n@p  pmevent      7;\n@q  pmevent.mask 0xff; 9.7.17.4. Miscellaneous Instructions: trap  trap Perform trap operation. Syntax trap; Description Abort execution and generate an interrupt to the host CPU. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples trap;\n@p  trap; 9.7.17.5. Miscellaneous Instructions: setmaxnreg  setmaxnreg Hint to change the number of registers owned by the warp. Syntax setmaxnreg.action.sync.aligned.u32 imm-reg-count;\n\n.action = { .inc, .dec }; Description setmaxnreg provides a hint to the system to update the maximum number of per-thread registers\nowned by the executing warp to the value specified by the imm-reg-count operand. Qualifier .dec is used to release extra registers such that the absolute per-thread maximum\nregister count is reduced from its current value to imm-reg-count . Qualifier .inc is used to\nrequest additional registers such that the absolute per-thread maximum register count is increased\nfrom its current value to imm-reg-count . A pool of available registers is maintained per-CTA. Register adjustments requested by the setmaxnreg instructions are handled by supplying extra registers from this pool to the\nrequesting warp or by releasing extra registers from the requesting warp to this pool, depending\nupon the value of the .action qualifier. The setmaxnreg.inc instruction blocks the execution until enough registers are available in the\nCTA’s register pool. After the instruction setmaxnreg.inc obtains new registers from the CTA\npool, the initial contents of the new registers are undefined. The new registers must be initialized\nbefore they are used. The same setmaxnreg instruction must be executed by all warps in a warpgroup . After executing a setmaxnreg instruction, all warps in the warpgroup must synchronize explicitly before\nexecuting subsequent setmaxnreg instructions. If a setmaxnreg instruction is not executed by all\nwarps in the warpgroup , then the behavior is undefined. Operand imm-reg-count is an integer constant. The value of imm-reg-count must be in the\nrange 24 to 256 (both inclusive) and must be a multiple of 8. Changes to the register file of the warp always happen at the tail-end of the register file. The setmaxnreg instruction requires that the kernel has been launched with a valid value of\nmaximum number of per-thread registers specified via the appropriate compilation via the appropriate\ncompile-time option or the appropriate performance tuning directive. Otherwise, the setmaxnreg instruction may have no effect. When qualifier .dec is specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of setmaxnreg instruction should be greater than or equal to the imm-reg-count . Otherwise, the behaviour is undefined. When qualifier .inc is specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of setmaxnreg instruction should be less than or equal to the imm-reg-count . Otherwise, the behaviour is undefined. The mandatory .sync qualifier indicates that setmaxnreg instruction causes the executing\nthread to wait until all threads in the warp execute the same setmaxnreg instruction before\nresuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the\nsame setmaxnreg instruction. In conditionally executed code, setmaxnreg instruction should\nonly be used if it is known that all threads in warpgroup evaluate the condition identically,\notherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_90a . Examples setmaxnreg.dec.sync.aligned.u32 64;\nsetmaxnreg.inc.sync.aligned.u32 192; 10. Special Registers  PTX includes a number of predefined, read-only variables, which are\nvisible as special registers and accessed through mov or cvt instructions. The special registers are: %tid %ntid %laneid %warpid %nwarpid %ctaid %nctaid %smid %nsmid %gridid %is_explicit_cluster %clusterid %nclusterid %cluster_ctaid %cluster_nctaid %cluster_ctarank %cluster_nctarank %lanemask_eq , %lanemask_le , %lanemask_lt , %lanemask_ge , %lanemask_gt %clock , %clock_hi , %clock64 %pm0, ..., %pm7 %pm0_64, ..., %pm7_64 %envreg0, ..., %envreg31 %globaltimer , %globaltimer_lo , %globaltimer_hi %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset<2> %total_smem_size %aggr_smem_size %dynamic_smem_size %current_graph_exec 10.1. Special Registers: %tid  %tid Thread identifier within a CTA. Syntax (predefined) .sreg .v4 .u32 %tid;                  // thread id vector\n.sreg .u32 %tid.x, %tid.y, %tid.z;    // thread id components Description A predefined, read-only, per-thread special register initialized with the thread identifier within\nthe CTA. The %tid special register contains a 1D, 2D, or 3D vector to match the CTA shape; the %tid value in unused dimensions is 0 . The fourth element is unused and always returns\nzero. The number of threads in each dimension are specified by the predefined special register %ntid . Every thread in the CTA has a unique %tid . %tid component values range from 0 through %ntid-1 in each CTA dimension. %tid.y == %tid.z == 0 in 1D CTAs. %tid.z == 0 in 2D CTAs. It is guaranteed that: 0  <=  %tid.x <  %ntid.x\n0  <=  %tid.y <  %ntid.y\n0  <=  %tid.z <  %ntid.z PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %tid . Target ISA Notes Supported on all target architectures. Examples mov.u32      %r1,%tid.x;  // move tid.x to %rh\n\n// legacy code accessing 16-bit components of %tid\nmov.u16      %rh,%tid.x;\ncvt.u32.u16  %r2,%tid.z;  // zero-extend tid.z to %r2 10.2. Special Registers: %ntid  %ntid Number of thread IDs per CTA. Syntax (predefined) .sreg .v4 .u32 %ntid;                   // CTA shape vector\n.sreg .u32 %ntid.x, %ntid.y, %ntid.z;   // CTA dimensions Description A predefined, read-only special register initialized with the number of thread ids in each CTA\ndimension. The %ntid special register contains a 3D CTA shape vector that holds the CTA\ndimensions. CTA dimensions are non-zero; the fourth element is unused and always returns zero. The\ntotal number of threads in a CTA is (%ntid.x * %ntid.y * %ntid.z) . %ntid.y == %ntid.z == 1 in 1D CTAs.\n%ntid.z ==1 in 2D CTAs. Maximum values of %ntid.{x,y,z} are as follows: .target architecture %ntid.x %ntid.y %ntid.z sm_1x 512 512 64 sm_20 , sm_3x , sm_5x , sm_6x , sm_7x , sm_8x , sm_9x 1024 1024 64 PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %ntid . Target ISA Notes Supported on all target architectures. Examples // compute unified thread id for 2D CTA\nmov.u32  %r0,%tid.x;\nmov.u32  %h1,%tid.y;\nmov.u32  %h2,%ntid.x;\nmad.u32  %r0,%h1,%h2,%r0;\n\nmov.u16  %rh,%ntid.x;      // legacy code 10.3. Special Registers: %laneid  %laneid Lane Identifier. Syntax (predefined) .sreg .u32 %laneid; Description A predefined, read-only special register that returns the thread’s lane within the warp. The lane\nidentifier ranges from zero to WARP_SZ-1 . PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples mov.u32  %r, %laneid; 10.4. Special Registers: %warpid  %warpid Warp identifier. Syntax (predefined) .sreg .u32 %warpid; Description A predefined, read-only special register that returns the thread’s warp identifier. The warp\nidentifier provides a unique warp number within a CTA but not across CTAs within a grid. The warp\nidentifier will be the same for all threads within a single warp. Note that %warpid is volatile and returns the location of a thread at the moment when read, but\nits value may change during execution, e.g., due to rescheduling of threads following\npreemption. For this reason, %ctaid and %tid should be used to compute a virtual warp index\nif such a value is needed in kernel code; %warpid is intended mainly to enable profiling and\ndiagnostic code to sample and log information such as work place mapping and load distribution. PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples mov.u32  %r, %warpid; 10.5. Special Registers: %nwarpid  %nwarpid Number of warp identifiers. Syntax (predefined) .sreg .u32 %nwarpid; Description A predefined, read-only special register that returns the maximum number of warp identifiers. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %nwarpid requires sm_20 or higher. Examples mov.u32  %r, %nwarpid; 10.6. Special Registers: %ctaid  %ctaid CTA identifier within a grid. Syntax (predefined) .sreg .v4 .u32 %ctaid;                      // CTA id vector\n.sreg .u32 %ctaid.x, %ctaid.y, %ctaid.z;    // CTA id components Description A predefined, read-only special register initialized with the CTA identifier within the CTA\ngrid. The %ctaid special register contains a 1D, 2D, or 3D vector, depending on the shape and\nrank of the CTA grid. The fourth element is unused and always returns zero. It is guaranteed that: 0  <=  %ctaid.x <  %nctaid.x\n0  <=  %ctaid.y <  %nctaid.y\n0  <=  %ctaid.z <  %nctaid.z PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %ctaid . Target ISA Notes Supported on all target architectures. Examples mov.u32  %r0,%ctaid.x;\nmov.u16  %rh,%ctaid.y;   // legacy code 10.7. Special Registers: %nctaid  %nctaid Number of CTA ids per grid. Syntax (predefined) .sreg .v4 .u32 %nctaid                      // Grid shape vector\n.sreg .u32 %nctaid.x,%nctaid.y,%nctaid.z;   // Grid dimensions Description A predefined, read-only special register initialized with the number of CTAs in each grid\ndimension. The %nctaid special register contains a 3D grid shape vector, with each element\nhaving a value of at least 1 . The fourth element is unused and always returns zero. Maximum values of %nctaid.{x,y,z} are as follows: .target architecture %nctaid.x %nctaid.y %nctaid.z sm_1x , sm_20 65535 65535 65535 sm_3x , sm_5x , sm_6x , sm_7x , sm_8x , sm_9x 2 31 -1 65535 65535 PTX ISA Notes Introduced in PTX ISA version 1.0 with type .v4.u16 . Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit mov and cvt instructions may be used to read the lower 16-bits of each component of %nctaid . Target ISA Notes Supported on all target architectures. Examples mov.u32  %r0,%nctaid.x;\nmov.u16  %rh,%nctaid.x;     // legacy code 10.8. Special Registers: %smid  %smid SM identifier. Syntax (predefined) .sreg .u32 %smid; Description A predefined, read-only special register that returns the processor (SM) identifier on which a\nparticular thread is executing. The SM identifier ranges from 0 to %nsmid-1 . The SM\nidentifier numbering is not guaranteed to be contiguous. Notes Note that %smid is volatile and returns the location of a thread at the moment when read, but\nits value may change during execution, e.g. due to rescheduling of threads following\npreemption. %smid is intended mainly to enable profiling and diagnostic code to sample and log\ninformation such as work place mapping and load distribution. PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples mov.u32  %r, %smid; 10.9. Special Registers: %nsmid  %nsmid Number of SM identifiers. Syntax (predefined) .sreg .u32 %nsmid; Description A predefined, read-only special register that returns the maximum number of SM identifiers. The SM\nidentifier numbering is not guaranteed to be contiguous, so %nsmid may be larger than the\nphysical number of SMs in the device. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %nsmid requires sm_20 or higher. Examples mov.u32  %r, %nsmid; 10.10. Special Registers: %gridid  %gridid Grid identifier. Syntax (predefined) .sreg .u64 %gridid; Description A predefined, read-only special register initialized with the per-grid temporal grid identifier. The %gridid is used by debuggers to distinguish CTAs and clusters within concurrent (small) grids. During execution, repeated launches of programs may occur, where each launch starts a\ngrid-of-CTAs. This variable provides the temporal grid launch number for this context. For sm_1x targets, %gridid is limited to the range [0..2 16 -1]. For sm_20 , %gridid is limited to the range [0..2 32 -1]. sm_30 supports the entire 64-bit range. PTX ISA Notes Introduced in PTX ISA version 1.0 as type .u16 . Redefined as type .u32 in PTX ISA version 1.3. Redefined as type .u64 in PTX ISA version 3.0. For compatibility with legacy PTX code, 16-bit and 32-bit mov and cvt instructions may be\nused to read the lower 16-bits or 32-bits of each component of %gridid . Target ISA Notes Supported on all target architectures. Examples mov.u64  %s, %gridid;  // 64-bit read of %gridid\nmov.u32  %r, %gridid;  // legacy code with 32-bit %gridid 10.11. Special Registers: %is_explicit_cluster  %is_explicit_cluster Checks if user has explicitly specified cluster launch. Syntax (predefined) .sreg .pred %is_explicit_cluster; Description A predefined, read-only special register initialized with the predicate value of whether the cluster\nlaunch is explicitly specified by user. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .pred p;\n\nmov.pred  p, %is_explicit_cluster; 10.12. Special Registers: %clusterid  %clusterid Cluster identifier within a grid. Syntax (predefined) .sreg .v4 .u32 %clusterid;\n.sreg .u32 %clusterid.x, %clusterid.y, %clusterid.z; Description A predefined, read-only special register initialized with the cluster identifier in a grid in each\ndimension. Each cluster in a grid has a unique identifier. The %clusterid special register contains a 1D, 2D, or 3D vector, depending upon the shape and\nrank of the cluster. The fourth element is unused and always returns zero. It is guaranteed that: 0  <=  %clusterid.x <  %nclusterid.x\n0  <=  %clusterid.y <  %nclusterid.y\n0  <=  %clusterid.z <  %nclusterid.z PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %clusterid.x;\nmov.u32     %r1, %clusterid.z;\nmov.v4.u32  %rx, %clusterid; 10.13. Special Registers: %nclusterid  %nclusterid Number of cluster identifiers per grid. Syntax (predefined) .sreg .v4 .u32 %nclusterid;\n.sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z; Description A predefined, read-only special register initialized with the number of clusters in each grid\ndimension. The %nclusterid special register contains a 3D grid shape vector that holds the grid dimensions\nin terms of clusters. The fourth element is unused and always returns zero. Refer to the Cuda Programming Guide for details on the maximum values of %nclusterid.{x,y,z} . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %nclusterid.x;\nmov.u32     %r1, %nclusterid.z;\nmov.v4.u32  %rx, %nclusterid; 10.14. Special Registers: %cluster_ctaid  %cluster_ctaid CTA identifier within a cluster. Syntax (predefined) .sreg .v4 .u32 %cluster_ctaid;\n.sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z; Description A predefined, read-only special register initialized with the CTA identifier in a cluster in each\ndimension. Each CTA in a cluster has a unique CTA identifier. The %cluster_ctaid special register contains a 1D, 2D, or 3D vector, depending upon the shape of\nthe cluster. The fourth element is unused and always returns zero. It is guaranteed that: 0  <=  %cluster_ctaid.x <  %cluster_nctaid.x\n0  <=  %cluster_ctaid.y <  %cluster_nctaid.y\n0  <=  %cluster_ctaid.z <  %cluster_nctaid.z PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_ctaid.x;\nmov.u32     %r1, %cluster_ctaid.z;\nmov.v4.u32  %rx, %cluster_ctaid; 10.15. Special Registers: %cluster_nctaid  %cluster_nctaid Number of CTA identifiers per cluster. Syntax (predefined) .sreg .v4 .u32 %cluster_nctaid;\n.sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z; Description A predefined, read-only special register initialized with the number of CTAs in a cluster in each\ndimension. The %cluster_nctaid special register contains a 3D grid shape vector that holds the cluster\ndimensions in terms of CTAs. The fourth element is unused and always returns zero. Refer to the Cuda Programming Guide for details on the maximum values of %cluster_nctaid.{x,y,z} . PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_nctaid.x;\nmov.u32     %r1, %cluster_nctaid.z;\nmov.v4.u32  %rx, %cluster_nctaid; 10.16. Special Registers: %cluster_ctarank  %cluster_ctarank CTA identifier in a cluster across all dimensions. Syntax (predefined) .sreg .u32 %cluster_ctarank; Description A predefined, read-only special register initialized with the CTA rank within a cluster across all\ndimensions. It is guaranteed that: 0  <=  %cluster_ctarank <  %cluster_nctarank PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r;\n\nmov.u32  %r, %cluster_ctarank; 10.17. Special Registers: %cluster_nctarank  %cluster_nctarank Number of CTA identifiers in a cluster across all dimensions. Syntax (predefined) .sreg .u32 %cluster_nctarank; Description A predefined, read-only special register initialized with the nunber of CTAs within a cluster across\nall dimensions. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .reg .b32 %r;\n\nmov.u32  %r, %cluster_nctarank; 10.18. Special Registers: %lanemask_eq  %lanemask_eq 32-bit mask with bit set in position equal to the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_eq; Description A predefined, read-only special register initialized with a 32-bit mask with a bit set in the\nposition equal to the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_eq requires sm_20 or higher. Examples mov.u32     %r, %lanemask_eq; 10.19. Special Registers: %lanemask_le  %lanemask_le 32-bit mask with bits set in positions less than or equal to the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_le; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than or equal to the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_le requires sm_20 or higher. Examples mov.u32     %r, %lanemask_le 10.20. Special Registers: %lanemask_lt  %lanemask_lt 32-bit mask with bits set in positions less than the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_lt; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_lt requires sm_20 or higher. Examples mov.u32     %r, %lanemask_lt; 10.21. Special Registers: %lanemask_ge  %lanemask_ge 32-bit mask with bits set in positions greater than or equal to the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_ge; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than or equal to the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_ge requires sm_20 or higher. Examples mov.u32     %r, %lanemask_ge; 10.22. Special Registers: %lanemask_gt  %lanemask_gt 32-bit mask with bits set in positions greater than the thread’s lane number in the warp. Syntax (predefined) .sreg .u32 %lanemask_gt; Description A predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than the thread’s lane number in the warp. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %lanemask_gt requires sm_20 or higher. Examples mov.u32     %r, %lanemask_gt; 10.23. Special Registers: %clock, %clock_hi  %clock, %clock_hi %clock A predefined, read-only 32-bit unsigned cycle counter. %clock_hi The upper 32-bits of %clock64 special register. Syntax (predefined) .sreg .u32 %clock;\n.sreg .u32 %clock_hi; Description Special register %clock and %clock_hi are unsigned 32-bit read-only cycle counters that wrap\nsilently. PTX ISA Notes %clock introduced in PTX ISA version 1.0. %clock_hi introduced in PTX ISA version 5.0. Target ISA Notes %clock supported on all target architectures. %clock_hi requires sm_20 or higher. Examples mov.u32 r1,%clock;\nmov.u32 r2, %clock_hi; 10.24. Special Registers: %clock64  %clock64 A predefined, read-only 64-bit unsigned cycle counter. Syntax (predefined) .sreg .u64 %clock64; Description Special register %clock64 is an unsigned 64-bit read-only cycle counter that wraps silently. Notes The lower 32-bits of %clock64 are identical to %clock . The upper 32-bits of %clock64 are identical to %clock_hi . PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes %clock64 requires sm_20 or higher. Examples mov.u64  r1,%clock64; 10.25. Special Registers: %pm0..%pm7  %pm0..%pm7 Performance monitoring counters. Syntax (predefined) .sreg .u32 %pm<8>; Description Special registers %pm0..%pm7 are unsigned 32-bit read-only performance monitor counters. Their\nbehavior is currently undefined. PTX ISA Notes %pm0..%pm3 introduced in PTX ISA version 1.3. %pm4..%pm7 introduced in PTX ISA version 3.0. Target ISA Notes %pm0..%pm3 supported on all target architectures. %pm4..%pm7 require sm_20 or higher. Examples mov.u32  r1,%pm0;\nmov.u32  r1,%pm7; 10.26. Special Registers: %pm0_64..%pm7_64  %pm0_64..%pm7_64 64 bit Performance monitoring counters. Syntax (predefined) .sreg .u64 %pm0_64;\n.sreg .u64 %pm1_64;\n.sreg .u64 %pm2_64;\n.sreg .u64 %pm3_64;\n.sreg .u64 %pm4_64;\n.sreg .u64 %pm5_64;\n.sreg .u64 %pm6_64;\n.sreg .u64 %pm7_64; Description Special registers %pm0_64..%pm7_64 are unsigned 64-bit read-only performance monitor\ncounters. Their behavior is currently undefined. Notes The lower 32bits of %pm0_64..%pm7_64 are identical to %pm0..%pm7 . PTX ISA Notes %pm0_64..%pm7_64 introduced in PTX ISA version 4.0. Target ISA Notes %pm0_64..%pm7_64 require sm_50 or higher. Examples mov.u32  r1,%pm0_64;\nmov.u32  r1,%pm7_64; 10.27. Special Registers: %envreg<32>  %envreg<32> Driver-defined read-only registers. Syntax (predefined) .sreg .b32 %envreg<32>; Description A set of 32 pre-defined read-only registers used to capture execution environment of PTX program\noutside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch\nand can contain cta-wide or grid-wide values. Precise semantics of these registers is defined in the driver documentation. PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Supported on all target architectures. Examples mov.b32      %r1,%envreg0;  // move envreg0 to %r1 10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi  %globaltimer, %globaltimer_lo, %globaltimer_hi %globaltimer A predefined, 64-bit global nanosecond timer. %globaltimer_lo The lower 32-bits of %globaltimer. %globaltimer_hi The upper 32-bits of %globaltimer. Syntax (predefined) .sreg .u64 %globaltimer;\n.sreg .u32 %globaltimer_lo, %globaltimer_hi; Description Special registers intended for use by NVIDIA tools. The behavior is target-specific and may change\nor be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is\nunspecified. PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Requires target sm_30 or higher. Examples mov.u64  r1,%globaltimer; 10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2>  %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2> %reserved_smem_offset_begin Start of the reserved shared memory region. %reserved_smem_offset_end End of the reserved shared memory region. %reserved_smem_offset_cap Total size of the reserved shared memory region. %reserved_smem_offset_<2> Offsets in the reserved shared memory region. Syntax (predefined) .sreg .b32 %reserved_smem_offset_begin;\n.sreg .b32 %reserved_smem_offset_end;\n.sreg .b32 %reserved_smem_offset_cap;\n.sreg .b32 %reserved_smem_offset_<2>; Description These are predefined, read-only special registers containing information about the shared memory\nregion which is reserved for the NVIDIA system software use. This region of shared memory is not\navailable to users, and accessing this region from user code results in undefined behavior. Refer to CUDA Programming Guide for details. PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes Require sm_80 or higher. Examples .reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1;\n\nmov.b32 %reg_begin,   %reserved_smem_offset_begin;\nmov.b32 %reg_end,     %reserved_smem_offset_end;\nmov.b32 %reg_cap,     %reserved_smem_offset_cap;\nmov.b32 %reg_offset0, %reserved_smem_offset_0;\nmov.b32 %reg_offset1, %reserved_smem_offset_1; 10.30. Special Registers: %total_smem_size  %total_smem_size Total size of shared memory used by a CTA of a kernel. Syntax (predefined) .sreg .u32 %total_smem_size; Description A predefined, read-only special register initialized with total size of shared memory allocated\n(statically and dynamically, excluding the shared memory reserved for the NVIDIA system software\nuse) for the CTA of a kernel at launch time. Size is returned in multiples of shared memory allocation unit size supported by target\narchitecture. Allocation unit values are as follows: Target architecture Shared memory allocation unit size sm_2x 128 bytes sm_3x , sm_5x , sm_6x , sm_7x 256 bytes sm_8x , sm_9x 128 bytes PTX ISA Notes Introduced in PTX ISA version 4.1. Target ISA Notes Requires sm_20 or higher. Examples mov.u32  %r, %total_smem_size; 10.31. Special Registers: %aggr_smem_size  %aggr_smem_size Total size of shared memory used by a CTA of a kernel. Syntax (predefined) .sreg .u32 %aggr_smem_size; Description A predefined, read-only special register initialized with total aggregated size of shared memory\nconsisting of the size of user shared memory allocated (statically and dynamically) at launch time\nand the size of shared memory region which is reserved for the NVIDIA system software use. PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples mov.u32  %r, %aggr_smem_size; 10.32. Special Registers: %dynamic_smem_size  %dynamic_smem_size Size of shared memory allocated dynamically at kernel launch. Syntax (predefined) .sreg .u32 %dynamic_smem_size; Description Size of shared memory allocated dynamically at kernel launch. A predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time. PTX ISA Notes Introduced in PTX ISA version 4.1. Target ISA Notes Requires sm_20 or higher. Examples mov.u32  %r, %dynamic_smem_size; 10.33. Special Registers: %current_graph_exec  %current_graph_exec An Identifier for currently executing CUDA device graph. Syntax (predefined) .sreg .u64 %current_graph_exec; Description A predefined, read-only special register initialized with the identifier referring to the CUDA\ndevice graph being currently executed. This register is 0 if the executing kernel is not part of a\nCUDA device graph. Refer to the CUDA Programming Guide for more details on CUDA device graphs. PTX ISA Notes Introduced in PTX ISA version 8.0. Target ISA Notes Requires sm_50 or higher. Examples mov.u64  r1, %current_graph_exec; 11. Directives  11.1. PTX Module Directives  The following directives declare the PTX ISA version of the code in the module, the target\narchitecture for which the code was generated, and the size of addresses within the PTX module. .version .target .address_size 11.1.1. PTX Module Directives: .version  .version PTX ISA version number. Syntax .version  major.minor    // major, minor are integers Description Specifies the PTX language version number. The major number is incremented when there are incompatible changes to the PTX language, such as\nchanges to the syntax or semantics. The version major number is used by the PTX compiler to ensure\ncorrect execution of legacy PTX code. The minor number is incremented when new features are added to PTX. Semantics Indicates that this module must be compiled with tools that support an equal or greater version\nnumber. Each PTX module must begin with a .version directive, and no other .version directive is\nallowed anywhere else within the module. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples .version 3.1\n.version 3.0\n.version 2.3 11.1.2. PTX Module Directives: .target  .target Architecture and Platform target. Syntax .target stringlist         // comma separated list of target specifiers\nstring = { sm_90a, sm_90,               // sm_9x target architectures\n           sm_80, sm_86, sm_87, sm_89,  // sm_8x target architectures\n           sm_70, sm_72, sm_75,         // sm_7x target architectures\n           sm_60, sm_61, sm_62,         // sm_6x target architectures\n           sm_50, sm_52, sm_53,         // sm_5x target architectures\n           sm_30, sm_32, sm_35, sm_37,  // sm_3x target architectures\n           sm_20,                       // sm_2x target architectures\n           sm_10, sm_11, sm_12, sm_13,  // sm_1x target architectures\n           texmode_unified, texmode_independent,   // texturing mode\n           debug,                                  // platform option\n           map_f64_to_f32 };                       // platform option Description Specifies the set of features in the target architecture for which the current PTX code was\ngenerated. In general, generations of SM architectures follow an onion layer model, where each\ngeneration adds new features and retains all features of previous generations. The onion layer model\nallows the PTX code generated for a given target to be run on later generation devices. Target architectures with suffix “ a ”, such as sm_90a , include architecture-accelerated\nfeatures that are supported on the specified architecture only, hence such targets do not follow the\nonion layer model. Therefore, PTX code generated for such targets cannot be run on later generation\ndevices. Architecture-accelerated features can only be used with targets that support these\nfeatures. Semantics Each PTX module must begin with a .version directive, immediately followed by a .target directive containing a target architecture and optional platform options. A .target directive\nspecifies a single target architecture, but subsequent .target directives can be used to change\nthe set of target features allowed during parsing. A program with multiple .target directives\nwill compile and run only on devices that support all features of the highest-numbered architecture\nlisted in the program. PTX features are checked against the specified target architecture, and an error is generated if an\nunsupported feature is used.  The following table summarizes the features in PTX that vary according\nto target architecture. Target Description sm_90 Baseline feature set for sm_90 architecture. sm_90a Adds support for sm_90a accelerated wgmma and setmaxnreg instructions. Target Description sm_80 Baseline feature set for sm_80 architecture. sm_86 Adds support for .xorsign modifier on min and max instructions. sm_87 Baseline feature set for sm_86 architecture. sm_89 Baseline feature set for sm_86 architecture. Target Description sm_70 Baseline feature set for sm_70 architecture. sm_72 Adds support for integer multiplicand and accumulator matrices in wmma instructions. Adds support for cvt.pack instruction. sm_75 Adds support for sub-byte integer and single-bit multiplicant matrices in wmma instructions. Adds support for ldmatrix instruction. Adds support for movmatrix instruction. Adds support for tanh instruction. Target Description sm_60 Baseline feature set for sm_60 architecture. sm_61 Adds support for dp2a and dp4a instructions. sm_62 Baseline feature set for sm_61 architecture. Target Description sm_50 Baseline feature set for sm_50 architecture. sm_52 Baseline feature set for sm_50 architecture. sm_53 Adds support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types. Target Description sm_30 Baseline feature set for sm_30 architecture. sm_32 Adds 64-bit {atom,red}.{and,or,xor,min,max} instructions. Adds shf instruction. Adds ld.global.nc instruction. sm_35 Adds support for CUDA Dynamic Parallelism. sm_37 Baseline feature set for sm_35 architecture. Target Description sm_20 Baseline feature set for sm_20 architecture. Target Description sm_10 Baseline feature set for sm_10 architecture. Requires map_f64_to_f32 if any .f64 instructions used. sm_11 Adds 64-bit {atom,red}.{and,or,xor,min,max} instructions. Requires map_f64_to_f32 if any .f64 instructions used. sm_12 Adds {atom,red}.shared , 64-bit {atom,red}.global , vote instructions. Requires map_f64_to_f32 if any .f64 instructions used. sm_13 Adds double-precision support, including expanded rounding modifiers. Disallows use of map_f64_to_f32 . The texturing mode is specified for an entire module and cannot be changed within the module. The .target debug option declares that the PTX file contains DWARF debug information, and\nsubsequent compilation of PTX will retain information needed for source-level debugging. If the\ndebug option is declared, an error message is generated if no DWARF information is found in the\nfile. The debug option requires PTX ISA version 3.0 or later. map_f64_to_f32 indicates that all double-precision instructions map to single-precision\nregardless of the target architecture. This enables high-level language compilers to compile\nprograms containing type double to target device that do not support double-precision\noperations. Note that .f64 storage remains as 64-bits, with only half being used by instructions\nconverted from .f64 to .f32 . Notes Targets of the form compute_xx are also accepted as synonyms for sm_xx targets. PTX ISA Notes Introduced in PTX ISA version 1.0. Target strings sm_10 and sm_11 introduced in PTX ISA version 1.0. Target strings sm_12 and sm_13 introduced in PTX ISA version 1.2. Texturing mode introduced in PTX ISA version 1.5. Target string sm_20 introduced in PTX ISA version 2.0. Target string sm_30 introduced in PTX ISA version 3.0. Platform option debug introduced in PTX ISA version 3.0. Target string sm_35 introduced in PTX ISA version 3.1. Target strings sm_32 and sm_50 introduced in PTX ISA version 4.0. Target strings sm_37 and sm_52 introduced in PTX ISA version 4.1. Target string sm_53 introduced in PTX ISA version 4.2. Target string sm_60 , sm_61 , sm_62 introduced in PTX ISA version 5.0. Target string sm_70 introduced in PTX ISA version 6.0. Target string sm_72 introduced in PTX ISA version 6.1. Target string sm_75 introduced in PTX ISA version 6.3. Target string sm_80 introduced in PTX ISA version 7.0. Target string sm_86 introduced in PTX ISA version 7.1. Target string sm_87 introduced in PTX ISA version 7.4. Target string sm_89 introduced in PTX ISA version 7.8. Target string sm_90 introduced in PTX ISA version 7.8. Target string sm_90a introduced in PTX ISA version 8.0. Target ISA Notes The .target directive is supported on all target architectures. Examples .target sm_10       // baseline target architecture\n.target sm_13       // supports double-precision\n.target sm_20, texmode_independent\n.target sm_90       // baseline target architecture\n.target sm_90a      // PTX using arch accelerated features 11.1.3. PTX Module Directives: .address_size  .address_size Address size used throughout PTX module. Syntax .address_size  address-size\naddress-size = { 32, 64 }; Description Specifies the address size assumed throughout the module by the PTX code and the binary DWARF\ninformation in PTX. Redefinition of this directive within a module is not allowed. In the presence of separate\ncompilation all modules must specify (or default to) the same address size. The .address_size directive is optional, but it must immediately follow the .target directive if present within a module. Semantics If the .address_size directive is omitted, the address size defaults to 32. PTX ISA Notes Introduced in PTX ISA version 2.3. Target ISA Notes Supported on all target architectures. Examples // example directives\n   .address_size 32       // addresses are 32 bit\n   .address_size 64       // addresses are 64 bit\n\n// example of directive placement within a module\n   .version 2.3\n   .target sm_20\n   .address_size 64\n...\n.entry foo () {\n...\n} 11.2. Specifying Kernel Entry Points and Functions  The following directives specify kernel entry points and functions. .entry .func 11.2.1. Kernel and Function Directives: .entry  .entry Kernel entry point and body, with optional parameters. Syntax .entry kernel-name ( param-list )  kernel-body\n.entry kernel-name  kernel-body Description Defines a kernel entry point name, parameters, and body for the kernel function. Parameters are passed via .param space memory and are listed within an optional parenthesized\nparameter list. Parameters may be referenced by name within the kernel body and loaded into\nregisters using ld.param{::entry} instructions. In addition to normal parameters, opaque .texref , .samplerref , and .surfref variables\nmay be passed as parameters. These parameters can only be referenced by name within texture and\nsurface load, store, and query instructions and cannot be accessed via ld.param instructions. The shape and size of the CTA executing the kernel are available in special registers. Semantics Specify the entry point for a kernel program. At kernel launch, the kernel dimensions and properties are established and made available via\nspecial registers, e.g., %ntid , %nctaid , etc. PTX ISA Notes For PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter\nlist. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body. The maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764\nbytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table\nshows the allowed parameter size for a PTX ISA version: PTX ISA Version Maximum parameter size (In bytes) PTX ISA version 8.1 and above 32764 PTX ISA version 1.5 and above 4352 PTX ISA version 1.4 and above 256 The CUDA and OpenCL drivers support the following limits for parameter memory: Driver Parameter memory size CUDA 256 bytes for sm_1x , 4096 bytes for sm_2x and higher ,\n32764 bytes fo sm_70 and higher OpenCL 32764 bytes for sm_70 and higher, 4352 bytes on sm_6x and lower Target ISA Notes Supported on all target architectures. Examples .entry cta_fft\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r<99>;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n\n.entry prefix_sum ( .param .align 4 .s32 pitch[8000] )\n{\n    .reg .s32 %t;\n    ld.param::entry.s32  %t, [pitch];\n    ...\n} 11.2.2. Kernel and Function Directives: .func  .func Function definition. Syntax .func {.attribute(attr-list)} fname {.noreturn} function-body\n.func {.attribute(attr-list)} fname (param-list) {.noreturn} function-body\n.func {.attribute(attr-list)} (ret-param) fname (param-list) function-body Description Defines a function, including input and return parameters and optional function body. An optional .noreturn directive indicates that the function does not return to the caller\nfunction. .noreturn directive cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in Performance-Tuning Directives: .noreturn . An optional .attribute directive specifies additional information associated with the\nfunction. See the description of Variable and Function Attribute Directive: .attribute for allowed attributes. A .func definition with no body provides a function prototype. The parameter lists define locally-scoped variables in the function body. Parameters must be base\ntypes in either the register or parameter state space. Parameters in register state space may be\nreferenced directly within instructions in the function body. Parameters in .param space are\naccessed using ld.param{::func} and st.param{::func} instructions in the body. Parameter\npassing is call-by-value. The last parameter in the parameter list may be a .param array of type .b8 with no size\nspecified. It is used to pass an arbitrary number of parameters to the function packed into a single\narray object. When calling a function with such an unsized last argument, the last argument may be omitted from\nthe call instruction if no parameter is passed through it. Accesses to this array parameter must\nbe within the bounds of the array. The result of an access is undefined if no array was passed, or\nif the access was outside the bounds of the actual array being passed. Semantics The PTX syntax hides all details of the underlying calling convention and ABI. The implementation of parameter passing is left to the optimizing translator, which may use a\ncombination of registers and stack locations to pass parameters. Release Notes For PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and\nrecursion is illegal. PTX ISA versions 2.0 and later with target sm_20 or higher allow parameters in the .param state space, implements an ABI with stack, and supports recursion. PTX ISA versions 2.0 and later with target sm_20 or higher support at most one return value. PTX ISA Notes Introduced in PTX ISA version 1.0. Support for unsized array parameter introduced in PTX ISA version 6.0. Support for .noreturn directive introduced in PTX ISA version 6.4. Support for .attribute directive introduced in PTX ISA version 8.0. Target ISA Notes Functions without unsized array parameter supported on all target architectures. Unsized array parameter requires sm_30 or higher. .noreturn directive requires sm_30 or higher. .attribute directive requires sm_90 or higher. Examples .func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl)\n{\n.reg .b32 localVar;\n\n... use N, dbl;\nother code;\n\nmov.b32 rval,result;\nret;\n}\n\n...\ncall (fooval), foo, (val0, val1);  // return value in fooval\n...\n\n.func foo (.reg .b32 N, .reg .f64 dbl) .noreturn\n{\n.reg .b32 localVar;\n... use N, dbl;\nother code;\nmov.b32 rval, result;\nret;\n}\n...\ncall foo, (val0, val1);\n...\n\n.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[])\n{\n    .reg .b32 input0, input1;\n    ld.param.b32   input0, [numbers + 0];\n    ld.param.b32   input1, [numbers + 4];\n    ...\n    other code;\n    ret;\n}\n...\n\n.param .u32 N;\n.param .align 4 .b8 numbers[8];\nst.param.u32    [N], 2;\nst.param.b32    [numbers + 0], 5;\nst.param.b32    [numbers + 4], 10;\ncall (rval), bar, (N, numbers);\n... 11.2.3. Kernel and Function Directives: .alias  .alias Define an alias to existing function symbol. Syntax .alias fAlias, fAliasee; Description .alias is a module scope directive that defines identifier fAlias to be an alias to function\nspecified by fAliasee . Both fAlias and fAliasee are non-entry function symbols. Identifier fAlias is a function declaration without body. Identifier fAliasee is a function symbol which must be defined in the same module as .alias declaration. Function fAliasee cannot have .weak linkage. Prototype of fAlias and fAliasee must match. Program can use either fAlias or fAlisee identifiers to reference function defined with fAliasee . PTX ISA Notes .alias directive introduced in PTX ISA 6.3. Target ISA Notes .alias directive requires sm_30 or higher. Examples .visible .func foo(.param .u32 p) {\n   ...\n}\n.visible .func bar(.param .u32 p);\n.alias bar, foo;\n.entry test()\n{\n      .param .u32 p;\n      ...\n      call foo, (p);       // call foo directly\n       ...\n       .param .u32 p;\n       call bar, (p);        // call foo through alias\n}\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r1, %r2, %r3;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n} 11.3. Control Flow Directives  PTX provides directives for specifying potential targets for brx.idx and call instructions. See the descriptions of brx.idx and call for more information. .branchtargets .calltargets .callprototype 11.3.1. Control Flow Directives: .branchtargets  .branchtargets Declare a list of potential branch targets. Syntax Label:   .branchtargets  list-of-labels ; Description Declares a list of potential branch targets for a subsequent brx.idx , and associates the list\nwith the label at the start of the line. All control flow labels in the list must occur within the same function as the declaration. The list of labels may use the compact, shorthand syntax for enumerating a range of labels having a\ncommon prefix, similar to the syntax described in Parameterized Variable Names . PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Requires sm_20 or higher. Examples .function foo () {\n      .reg .u32 %r0;\n      ...\n      L1:\n      ...\n      L2:\n      ...\n      L3:\n      ...\n      ts: .branchtargets L1, L2, L3;\n      @p brx.idx %r0, ts;\n      ...\n\n.function bar() {\n      .reg .u32 %r0;\n      ...\n      N0:\n      ...\n      N1:\n      ...\n      N2:\n      ...\n      N3:\n      ...\n      N4:\n      ...\n      ts: .branchtargets N<5>;\n      @p brx.idx %r0, ts;\n      ... 11.3.2. Control Flow Directives: .calltargets  .calltargets Declare a list of potential call targets. Syntax Label:   .calltargets  list-of-functions ; Description Declares a list of potential call targets for a subsequent indirect call, and associates the list\nwith the label at the start of the line. All functions named in the list must be declared prior to the .calltargets directive, and all\nfunctions must have the same type signature. PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Requires sm_20 or higher. Examples calltgt:  .calltargets  fastsin, fastcos;\n...\n@p   call  (%f1), %r0, (%x), calltgt;\n... 11.3.3. Control Flow Directives: .callprototype  .callprototype Declare a prototype for use in an indirect call. Syntax // no input or return parameters\nlabel: .callprototype _ .noreturn;\n// input params, no return params\nlabel: .callprototype _ (param-list) .noreturn;\n// no input params, // return params\nlabel: .callprototype (ret-param) _ ;\n// input, return parameters\nlabel: .callprototype (ret-param) _ (param-list); Description Defines a prototype with no specific function name, and associates the prototype with a label. The\nprototype may then be used in indirect call instructions where there is incomplete knowledge of the\npossible call targets. Parameters may have either base types in the register or parameter state spaces, or array types in\nparameter state space. The sink symbol '_' may be used to avoid dummy parameter names. An optional .noreturn directive indicates that the function does not return to the caller\nfunction. .noreturn directive cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in Performance-Tuning Directives: .noreturn . PTX ISA Notes Introduced in PTX ISA version 2.1. Support for .noreturn directive introduced in PTX ISA version 6.4. Target ISA Notes Requires sm_20 or higher. .noreturn directive requires sm_30 or higher. Examples Fproto1: .callprototype  _ ;\nFproto2: .callprototype  _ (.param .f32 _);\nFproto3: .callprototype  (.param .u32 _) _ ;\nFproto4: .callprototype  (.param .u32 _) _ (.param .f32 _);\n...\n@p   call  (%val), %r0, (%f1), Fproto4;\n...\n\n// example of array parameter\nFproto5: .callprototype _ (.param .b8 _[12]);\n\nFproto6: .callprototype  _ (.param .f32 _) .noreturn;\n...\n@p   call  %r0, (%f1), Fproto6;\n... 11.4. Performance-Tuning Directives  To provide a mechanism for low-level performance tuning, PTX supports the following directives,\nwhich pass information to the backend optimizing compiler. .maxnreg .maxntid .reqntid .minnctapersm .maxnctapersm (deprecated) .pragma The .maxnreg directive specifies the maximum number of registers to be allocated to a single\nthread; the .maxntid directive specifies the maximum number of threads in a thread block (CTA);\nthe .reqntid directive specifies the required number of threads in a thread block (CTA); and the .minnctapersm directive specifies a minimum number of thread blocks to be scheduled on a single\nmultiprocessor (SM). These can be used, for example, to throttle the resource requirements (e.g.,\nregisters) to increase total thread count and provide a greater opportunity to hide memory\nlatency. The .minnctapersm directive can be used together with either the .maxntid or .reqntid directive to trade-off registers-per-thread against multiprocessor utilization without\nneeded to directly specify a maximum number of registers. This may achieve better performance when\ncompiling PTX for multiple devices having different numbers of registers per SM. Currently, the .maxnreg , .maxntid , .reqntid , and .minnctapersm directives may be\napplied per-entry and must appear between an .entry directive and its body. The directives take\nprecedence over any module-level constraints passed to the optimizing backend. A warning message is\ngenerated if the directives’ constraints are inconsistent or cannot be met for the specified target\ndevice. A general .pragma directive is supported for passing information to the PTX backend. The\ndirective passes a list of strings to the backend, and the strings have no semantics within the PTX\nvirtual machine model. The interpretation of .pragma values is determined by the backend\nimplementation and is beyond the scope of the PTX ISA. Note that .pragma directives may appear\nat module (file) scope, at entry-scope, or as statements within a kernel or device function body. 11.4.1. Performance-Tuning Directives: .maxnreg  .maxnreg Maximum number of registers that can be allocated per thread. Syntax .maxnreg n Description Declare the maximum number of registers per thread in a CTA. Semantics The compiler guarantees that this limit will not be exceeded. The actual number of registers used\nmay be less; for example, the backend may be able to compile to fewer registers, or the maximum\nnumber of registers may be further constrained by .maxntid and .maxctapersm . PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples .entry foo .maxnreg 16 { ... }  // max regs per thread = 16 11.4.2. Performance-Tuning Directives: .maxntid  .maxntid Maximum number of threads in the thread block (CTA). Syntax .maxntid nx\n.maxntid nx, ny\n.maxntid nx, ny, nz Description Declare the maximum number of threads in the thread block (CTA). This maximum is specified by giving\nthe maximum extent of each dimension of the 1D, 2D, or 3D CTA.  The maximum number of threads is the\nproduct of the maximum extent in each dimension. Semantics The maximum number of threads in the thread block, computed as the product of the maximum extent\nspecified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in\nwhich this directive appears. Exceeding the maximum number of threads results in a runtime error or\nkernel launch failure. Note that this directive guarantees that the total number of threads does not exceed the maximum,\nbut does not guarantee that the limit in any particular dimension is not exceeded. PTX ISA Notes Introduced in PTX ISA version 1.3. Target ISA Notes Supported on all target architectures. Examples .entry foo .maxntid 256       { ... }  // max threads = 256\n.entry bar .maxntid 16,16,4   { ... }  // max threads = 1024 11.4.3. Performance-Tuning Directives: .reqntid  .reqntid Number of threads in the thread block (CTA). Syntax .reqntid nx\n.reqntid nx, ny\n.reqntid nx, ny, nz Description Declare the number of threads in the thread block (CTA) by specifying the extent of each dimension\nof the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in\neach dimension. Semantics The size of each CTA dimension specified in any invocation of the kernel is required to be equal to\nthat specified in this directive. Specifying a different CTA dimension at launch will result in a\nruntime error or kernel launch failure. Notes The .reqntid directive cannot be used in conjunction with the .maxntid directive. PTX ISA Notes Introduced in PTX ISA version 2.1. Target ISA Notes Supported on all target architectures. Examples .entry foo .reqntid 256       { ... }  // num threads = 256\n.entry bar .reqntid 16,16,4   { ... }  // num threads = 1024 11.4.4. Performance-Tuning Directives: .minnctapersm  .minnctapersm Minimum number of CTAs per SM. Syntax .minnctapersm ncta Description Declare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor\n(SM). Notes Optimizations based on .minnctapersm need either .maxntid or .reqntid to be specified as\nwell. If the total number of threads on a single SM resulting from .minnctapersm and .maxntid / .reqntid exceed maximum number of threads supported by an SM then directive .minnctapersm will be ignored. In PTX ISA version 2.1 or higher, a warning is generated if .minnctapersm is specified without\nspecifying either .maxntid or .reqntid . PTX ISA Notes Introduced in PTX ISA version 2.0 as a replacement for .maxnctapersm . Target ISA Notes Supported on all target architectures. Examples .entry foo .maxntid 256 .minnctapersm 4 { ... } 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated)  .maxnctapersm Maximum number of CTAs per SM. Syntax .maxnctapersm ncta Description Declare the maximum number of CTAs from the kernel’s grid that may be mapped to a single\nmultiprocessor (SM). Notes Optimizations based on .maxnctapersm generally need .maxntid to be specified as well. The\noptimizing backend compiler uses .maxntid and .maxnctapersm to compute an upper-bound on\nper-thread register usage so that the specified number of CTAs can be mapped to a single\nmultiprocessor. However, if the number of registers used by the backend is sufficiently lower than\nthis bound, additional CTAs may be mapped to a single multiprocessor. For this reason, .maxnctapersm has been renamed to .minnctapersm in PTX ISA version 2.0. PTX ISA Notes Introduced in PTX ISA version 1.3. Deprecated in PTX ISA version 2.0. Target ISA Notes Supported on all target architectures. Examples .entry foo .maxntid 256 .maxnctapersm 4 { ... } 11.4.6. Performance-Tuning Directives: .noreturn  .noreturn Indicate that the function does not return to its caller function. Syntax .noreturn Description Indicate that the function does not return to its caller function. Semantics An optional .noreturn directive indicates that the function does not return to caller\nfunction. .noreturn directive can only be specified on device functions and must appear between\na .func directive and its body. The directive cannot be specified on functions which have return parameters. If a function with .noreturn directive returns to the caller function at runtime, then the\nbehavior is undefined. PTX ISA Notes Introduced in PTX ISA version 6.4. Target ISA Notes Requires sm_30 or higher. Examples .func foo .noreturn { ... } 11.4.7. Performance-Tuning Directives: .pragma  .pragma Pass directives to PTX backend compiler. Syntax .pragma list-of-strings ; Description Pass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler. The .pragma directive may occur at module-scope, at entry-scope, or at statement-level. Semantics The interpretation of .pragma directive strings is implementation-specific and has no impact on\nPTX semantics. See Descriptions of .pragma Strings for\ndescriptions of the pragma strings defined in ptxas . PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Supported on all target architectures. Examples .pragma \"nounroll\";    // disable unrolling in backend\n\n// disable unrolling for current kernel\n.entry foo .pragma \"nounroll\"; { ... } 11.5. Debugging Directives  DWARF-format debug information is passed through PTX modules using the following directives: @@DWARF .section .file .loc The .section directive was introduced in PTX ISA version 2.0 and replaces the @@DWARF syntax. The @@DWARF syntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX\nISA version 1.x code. Beginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the .target debug platform option. This forward declaration directs PTX compilation to retain\nmappings for source-level debugging. 11.5.1. Debugging Directives: @@dwarf  @@dwarf DWARF-format information. Syntax @@DWARF dwarf-string\n\ndwarf-string may have one of the\n.byte   byte-list   // comma-separated hexadecimal byte values\n.4byte  int32-list  // comma-separated hexadecimal integers in range [0..2^32-1]\n.quad   int64-list  // comma-separated hexadecimal integers in range [0..2^64-1]\n.4byte  label\n.quad   label PTX ISA Notes Introduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by .section directive. Target ISA Notes Supported on all target architectures. Examples @@DWARF .section .debug_pubnames, \"\", @progbits\n@@DWARF .byte   0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n@@DWARF .4byte  .debug_info\n@@DWARF .4byte  0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n@@DWARF .4byte  0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n@@DWARF .byte   0x00, 0x00, 0x00, 0x00, 0x00 11.5.2. Debugging Directives: .section  .section PTX section definition. Syntax .section section_name { dwarf-lines }\n\ndwarf-lines have the following formats:\n  .b8    byte-list       // comma-separated list of integers\n                         // in range [-128..255]\n  .b16   int16-list      // comma-separated list of integers\n                         // in range [-2^15..2^16-1]\n  .b32   int32-list      // comma-separated list of integers\n                         // in range [-2^31..2^32-1]\n  label:                 // Define label inside the debug section\n  .b64   int64-list      // comma-separated list of integers\n                         // in range [-2^63..2^64-1]\n  .b32   label\n  .b64   label\n  .b32   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 32bit)\n  .b64   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 64bit)\n  .b32   label1-label2   // a difference in label addresses between labels in\n                         // the same dwarf section (32bit)\n  .b64   label3-label4   // a difference in label addresses between labels in\n                         // the same dwarf section (64bit) PTX ISA Notes Introduced in PTX ISA version 2.0, replaces @@DWARF syntax. label+imm expression introduced in PTX ISA version 3.2. Support for .b16 integers in dwarf-lines introduced in PTX ISA version 6.0. Support for defining label inside the DWARF section is introduced in PTX ISA version 7.2. label1-label2 expression introduced in PTX ISA version 7.5. Negative numbers in dwarf lines introduced in PTX ISA version 7.5. Target ISA Notes Supported on all target architectures. Examples .section .debug_pubnames\n{\n    .b32    LpubNames_end0-LpubNames_begin0\n  LpubNames_begin0:\n    .b8     0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n    .b32    .debug_info\n  info_label1:\n    .b32    0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n    .b32    0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n    .b8     0x00, 0x00, 0x00, 0x00, 0x00\n  LpubNames_end0:\n}\n\n.section .debug_info\n{\n    .b32 11430\n    .b8 2, 0\n    .b32 .debug_abbrev\n    .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49\n    .b8 0\n    .b32 3, 37, 176, -99\n    .b32 info_label1\n    .b32 .debug_loc+0x4\n    .b8 -11, 11, 112, 97\n    .b32 info_label1+12\n    .b64 -1\n    .b16 -5, -65535\n} 11.5.3. Debugging Directives: .file  .file Source file name. Syntax .file file_index \"filename\" {, timestamp, file_size} Description Associates a source filename with an integer index. .loc directives reference source files by\nindex. .file directive allows optionally specifying an unsigned number representing time of last\nmodification and an unsigned integer representing size in bytes of source file. timestamp and file_size value can be 0 to indicate this information is not available. timestamp value is in format of C and C++ data type time_t . file_size is an unsigned 64-bit integer. The .file directive is allowed only in the outermost scope, i.e., at the same level as kernel\nand device function declarations. Semantics If timestamp and file size are not specified, they default to 0. PTX ISA Notes Introduced in PTX ISA version 1.0. Timestamp and file size introduced in PTX ISA version 3.2. Target ISA Notes Supported on all target architectures. Examples .file 1 \"example.cu\"\n.file 2 \"kernel.cu\"\n.file 1 “kernel.cu”, 1339013327, 64118 11.5.4. Debugging Directives: .loc  .loc Source file location. Syntax .loc file_index line_number column_position\n.loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2 Description Declares the source file location (source file, line number, and column position) to be associated\nwith lexically subsequent PTX instructions. .loc refers to file_index which is defined by a .file directive. To indicate PTX instructions that are generated from a function that got inlined, additional\nattribute .inlined_at can be specified as part of the .loc directive. .inlined_at attribute specifies source location at which the specified function is inlined. file_index2 , line_number2 , and column_position2 specify the location at which function is inlined. Source\nlocation specified as part of .inlined_at directive must lexically precede as source location in .loc directive. The function_name attribute specifies an offset in the DWARF section named .debug_str . Offset is specified as label expression or label + immediate expression\nwhere label is defined in .debug_str section. DWARF section .debug_str contains ASCII\nnull-terminated strings that specify the name of the function that is inlined. Note that a PTX instruction may have a single associated source location, determined by the nearest\nlexically preceding .loc directive, or no associated source location if there is no preceding .loc\ndirective. Labels in PTX inherit the location of the closest lexically following instruction. A\nlabel with no following PTX instruction has no associated source location. PTX ISA Notes Introduced in PTX ISA version 1.0. function_name and inlined_at attributes are introduced in PTX ISA version 7.2. Target ISA Notes Supported on all target architectures. Examples .loc 2 4237 0\nL1:                        // line 4237, col 0 of file #2,\n                           // inherited from mov\n    mov.u32  %r1,%r2;      // line 4237, col 0 of file #2\n    add.u32  %r2,%r1,%r3;  // line 4237, col 0 of file #2\n...\nL2:                        // line 4239, col 5 of file #2,\n                           // inherited from sub\n    .loc 2 4239 5\n    sub.u32  %r2,%r1,%r3;  // line 4239, col 5 of file #2\n    .loc 1 21 3\n    .loc 1 9 3, function_name info_string0, inlined_at 1 21 3\n    ld.global.u32   %r1, [gg]; // Function at line 9\n    setp.lt.s32 %p1, %r1, 8;   // inlined at line 21\n    .loc 1 27 3\n    .loc 1 10 5, function_name info_string1, inlined_at 1 27 3\n    .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5\n    setp.ne.s32 %p2, %r1, 18;\n    @%p2 bra    BB2_3;\n\n    .section .debug_str {\n    info_string0:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 102 // f\n     .b8 111 // o\n     .b8 111 // o\n     .b8 118 // v\n     .b8 0\n\n    info_string1:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 98  // b\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 99  // c\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n    } 11.6. Linking Directives  .extern .visible .weak 11.6.1. Linking Directives: .extern  .extern External symbol declaration. Syntax .extern identifier Description Declares identifier to be defined external to the current module. The module defining such\nidentifier must define it as .weak or .visible only once in a single object file. Extern\ndeclaration of symbol may appear multiple times and references to that get resolved against the\nsingle definition of that symbol. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples .extern .global .b32 foo;  // foo is defined in another module 11.6.2. Linking Directives: .visible  .visible Visible (externally) symbol declaration. Syntax .visible identifier Description Declares identifier to be globally visible. Unlike C, where identifiers are globally visible unless\ndeclared static, PTX identifiers are visible only within the current module unless declared .visible outside the current. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples .visible .global .b32 foo;  // foo will be externally visible 11.6.3. Linking Directives: .weak  .weak Visible (externally) symbol declaration. Syntax .weak identifier Description Declares identifier to be globally visible but weak . Weak symbols are similar to globally visible\nsymbols, except during linking, weak symbols are only chosen after globally visible symbols during\nsymbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak\nsymbol, and references to a symbol get resolved against a weak symbol only if no global symbols have\nthe same name. PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Supported on all target architectures. Examples .weak .func (.reg .b32 val) foo;  // foo will be externally visible 11.6.4. Linking Directives: .common  .common Visible (externally) symbol declaration. Syntax .common identifier Description Declares identifier to be globally visible but “common”. Common symbols are similar to globally visible symbols. However multiple object files may declare\nthe same common symbol and they may have different types and sizes and references to a symbol get\nresolved against a common symbol with the largest size. Only one object file can initialize a common symbol and that must have the largest size among all\nother definitions of that common symbol from different object files. .common linking directive can be used only on variables with .global storage. It cannot be\nused on function symbols or on symbols with opaque type. PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes .common directive requires sm_20 or higher. Examples .common .global .u32 gbl; 11.7. Cluster Dimension Directives  The following directives specify information about clusters: .reqnctapercluster .explicitcluster .maxclusterrank The .reqnctapercluster directive specifies the number of CTAs in the cluster. The .explicitcluster directive specifies that the kernel should be launched with explicit cluster\ndetails. The .maxclusterrank directive specifies the maximum number of CTAs in the cluster. The cluster dimension directives can be applied only on kernel functions. 11.7.1. Cluster Dimension Directives: .reqnctapercluster  .reqnctapercluster Declare the number of CTAs in the cluster. Syntax .reqnctapercluster nx\n.reqnctapercluster nx, ny\n.reqnctapercluster nx, ny, nz Description Set the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of\nthe 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each\ndimension. For kernels with .reqnctapercluster directive specified, runtime will use the\nspecified values for configuring the launch if the same are not specified at launch time. Semantics If cluster dimension is explicitly specified at launch time, it should be equal to the values\nspecified in this directive. Specifying a different cluster dimension at launch will result in a\nruntime error or kernel launch failure. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .entry foo .reqnctapercluster 2         { . . . }\n.entry bar .reqnctapercluster 2, 2, 1   { . . . }\n.entry ker .reqnctapercluster 3, 2      { . . . } 11.7.2. Cluster Dimension Directives: .explicitcluster  .explicitcluster Declare that Kernel must be launched with cluster dimensions explicitly specified. Syntax .explicitcluster Description Declares that this Kernel should be launched with cluster dimension explicitly specified. Semantics Kernels with .explicitcluster directive must be launched with cluster dimension explicitly\nspecified (either at launch time or via .reqnctapercluster ), otherwise program will fail with\nruntime error or kernel launch failure. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .entry foo .explicitcluster         { . . . } 11.7.3. Cluster Dimension Directives: .maxclusterrank  .maxclusterrank Declare the maximum number of CTAs that can be part of the cluster. Syntax .maxclusterrank n Description Declare the maximum number of thread blocks (CTAs) allowed to be part of the cluster. Semantics Product of the number of CTAs in each cluster dimension specified in any invocation of the kernel is\nrequired to be less or equal to that specified in this directive. Otherwise invocation will result\nin a runtime error or kernel launch failure. The .maxclusterrank directive cannot be used in conjunction with the .reqnctapercluster directive. PTX ISA Notes Introduced in PTX ISA version 7.8. Target ISA Notes Requires sm_90 or higher. Examples .entry foo ..maxclusterrank 8         { . . . } 12. Release Notes  This section describes the history of change in the PTX ISA and implementation. The first section\ndescribes ISA and implementation changes in the current release of PTX ISA version 8.5, and the\nremaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX\nISA version 2.0. Table 32 shows the PTX release history. Table 32 PTX Release History  PTX ISA Version CUDA Release Supported Targets PTX ISA 1.0 CUDA 1.0 sm_{10,11} PTX ISA 1.1 CUDA 1.1 sm_{10,11} PTX ISA 1.2 CUDA 2.0 sm_{10,11,12,13} PTX ISA 1.3 CUDA 2.1 sm_{10,11,12,13} PTX ISA 1.4 CUDA 2.2 sm_{10,11,12,13} PTX ISA 1.5 driver r190 sm_{10,11,12,13} PTX ISA 2.0 CUDA 3.0, driver r195 sm_{10,11,12,13} , sm_20 PTX ISA 2.1 CUDA 3.1, driver r256 sm_{10,11,12,13} , sm_20 PTX ISA 2.2 CUDA 3.2, driver r260 sm_{10,11,12,13} , sm_20 PTX ISA 2.3 CUDA 4.0, driver r270 sm_{10,11,12,13} , sm_20 PTX ISA 3.0 CUDA 4.1, driver r285 sm_{10,11,12,13} , sm_20 CUDA 4.2, driver r295 sm_{10,11,12,13} , sm_20 , sm_30 PTX ISA 3.1 CUDA 5.0, driver r302 sm_{10,11,12,13} , sm_20 , sm_{30,35} PTX ISA 3.2 CUDA 5.5, driver r319 sm_{10,11,12,13} , sm_20 , sm_{30,35} PTX ISA 4.0 CUDA 6.0, driver r331 sm_{10,11,12,13} , sm_20 , sm_{30,32,35} , sm_50 PTX ISA 4.1 CUDA 6.5, driver r340 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52} PTX ISA 4.2 CUDA 7.0, driver r346 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} PTX ISA 4.3 CUDA 7.5, driver r352 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} PTX ISA 5.0 CUDA 8.0, driver r361 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} PTX ISA 6.0 CUDA 9.0, driver r384 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 PTX ISA 6.1 CUDA 9.1, driver r387 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 PTX ISA 6.2 CUDA 9.2, driver r396 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 PTX ISA 6.3 CUDA 10.0, driver r400 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 6.4 CUDA 10.1, driver r418 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 6.5 CUDA 10.2, driver r440 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 7.0 CUDA 11.0, driver r445 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_80 PTX ISA 7.1 CUDA 11.1, driver r455 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.2 CUDA 11.2, driver r460 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.3 CUDA 11.3, driver r465 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.4 CUDA 11.4, driver r470 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.5 CUDA 11.5, driver r495 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.6 CUDA 11.6, driver r510 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.7 CUDA 11.7, driver r515 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.8 CUDA 11.8, driver r520 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_90 PTX ISA 8.0 CUDA 12.0, driver r525 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.1 CUDA 12.1, driver r530 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.2 CUDA 12.2, driver r535 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.3 CUDA 12.3, driver r545 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.4 CUDA 12.4, driver r550 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.5 CUDA 12.5, driver r555 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} 12.1. Changes in PTX ISA Version 8.5  New Features PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction. Semantic Changes and Clarifications Values 0b0000 , 0b0101 , 0b1010 , 0b1111 for sparsity metadata (operand e )\nof instruction mma.sp are invalid and their usage results in undefined behavior. 12.2. Changes in PTX ISA Version 8.4  New Features PTX ISA version 8.4 introduces the following new features: Extends ld , st and atom instructions with .b128 type to support .sys scope. Extends integer wgmma.mma_async instruction to support .u8.s8 and .s8.u8 as .atype and .btype respectively. Extends mma , mma.sp instructions to support FP8 types .e4m3 and .e5m2 . Semantic Changes and Clarifications None. 12.3. Changes in PTX ISA Version 8.3  New Features PTX ISA version 8.3 introduces the following new features: Adds support for pragma used_bytes_mask that is used to specify mask for used bytes for a load operation. Extends isspacep , cvta.to , ld and st instructions to accept ::entry and ::func sub-qualifiers with .param state space qualifier. Adds support for .b128 type on instructions ld , ld.global.nc , ldu , st , mov and atom . Add support for instructions tensormap.replace , tensormap.cp_fenceproxy and support for qualifier .to_proxykind::from_proxykind on instruction fence.proxy to support modifying tensor-map . Semantic Changes and Clarifications None. 12.4. Changes in PTX ISA Version 8.2  New Features PTX ISA version 8.2 introduces the following new features: Adds support for .mmio qualifier on ld and st instructions. Extends lop3 instruction to allow predicate destination. Extends multimem.ld_reduce instruction to support .acc::f32 qualifer to allow .f32 precision of the intermediate accumulation. Extends the asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma.mma_async to support .sp modifier that allows matrix multiply-accumulate operation\nwhen input matrix A is sparse. Semantic Changes and Clarifications The .multicast::cluster qualifier on cp.async.bulk and cp.async.bulk.tensor instructions\nis optimized for target architecture sm_90a and may have substantially reduced performance on\nother targets and hence .multicast::cluster is advised to be used with sm_90a . 12.5. Changes in PTX ISA Version 8.1  New Features PTX ISA version 8.1 introduces the following new features: Adds support for st.async and red.async instructions for asynchronous store and\nasynchronous reduction operations respectively on shared memory. Adds support for .oob modifier on half-precision fma instruction. Adds support for .satfinite saturation modifer on cvt instruction for .f16 , .bf16 and .tf32 formats. Extends support for cvt with .e4m3 / .e5m2 to sm_89 . Extends atom and red instructions to support vector types. Adds support for special register %aggr_smem_size . Extends sured instruction with 64-bit min / max operations. Adds support for increased kernel parameter size of 32764 bytes. Adds support for multimem addresses in memory consistency model. Adds support for multimem.ld_reduce , multimem.st and multimem.red instructions to\nperform memory operations on multimem addresses. Semantic Changes and Clarifications None. 12.6. Changes in PTX ISA Version 8.0  New Features PTX ISA version 8.0 introduces the following new features: Adds support for target sm_90a that supports specialized accelerated features. Adds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma . Extends the asynchronous copy operations with bulk operations that operate on large data,\nincluding tensor data. Introduces packed integer types .u16x2 and .s16x2 . Extends integer arithmetic instruction add to allow packed integer types .u16x2 and .s16x2 . Extends integer arithmetic instructions min and max to allow packed integer types .u16x2 and .s16x2 , as well as saturation modifier .relu on .s16x2 and .s32 types. Adds support for special register %current_graph_exec that identifies the currently executing\nCUDA device graph. Adds support for elect.sync instruction. Adds support for .unified attribute on functions and variables. Adds support for setmaxnreg instruction. Adds support for .sem qualifier on barrier.cluster instruction. Extends the fence instruction to allow opcode-specific synchronizaion using op_restrict qualifier. Adds support for .cluster scope on mbarrier.arrive , mbarrier.arrive_drop , mbarrier.test_wait and mbarrier.try_wait operations. Adds support for transaction count operations on mbarrier objects, specified with .expect_tx and .complete_tx qualifiers. Semantic Changes and Clarifications None. 12.7. Changes in PTX ISA Version 7.8  New Features PTX ISA version 7.8 introduces the following new features: Adds support for sm_89 target architecture. Adds support for sm_90 target architecture. Extends bar and barrier instructions to accept optional scope qualifier .cta . Extends .shared state space qualifier with optional sub-qualifier ::cta . Adds support for movmatrix instruction which transposes a matrix in registers across a warp. Adds support for stmatrix instruction which stores one or more matrices to shared memory. Extends the .f64 floating point type mma operation with shapes .m16n8k4 , .m16n8k8 ,\nand .m16n8k16 . Extends add , sub , mul , set , setp , cvt , tanh , ex2 , atom and red instructions with bf16 alternate floating point data format. Adds support for new alternate floating-point data formats .e4m3 and .e5m2 . Extends cvt instruction to convert .e4m3 and .e5m2 alternate floating point data formats. Adds support for griddepcontrol instruction as a communication mechanism to control the\nexecution of dependent grids. Extends mbarrier instruction to allow a new phase completion check operation try_wait . Adds support for new thread scope .cluster which is a set of Cooperative Thread Arrays (CTAs). Extends fence / membar , ld , st , atom , and red instructions to accept .cluster scope. Adds support for extended visibility of shared state space to all threads within a cluster. Extends .shared state space qualifier with ::cluster sub-qualifier for cluster-level\nvisibility of shared memory. Extends isspacep , cvta , ld , st , atom , and red instructions to accept ::cluster sub-qualifier with .shared state space qualifier. Adds support for mapa instruction to map a shared memory address to the corresponding address\nin a different CTA within the cluster. Adds support for getctarank instruction to query the rank of the CTA that contains a given\naddress. Adds support for new barrier synchronization instruction barrier.cluster . Extends the memory consistency model to include the new cluster scope. Adds support for special registers related to cluster information: %is_explicit_cluster , %clusterid , %nclusterid , %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank . Adds support for cluster dimension directives .reqnctapercluster , .explicitcluster , and .maxclusterrank . Semantic Changes and Clarifications None. 12.8. Changes in PTX ISA Version 7.7  New Features PTX ISA version 7.7 introduces the following new features: Extends isspacep and cvta instructions to include the .param state space for kernel\nfunction parameters. Semantic Changes and Clarifications None. 12.9. Changes in PTX ISA Version 7.6  New Features PTX ISA version 7.6 introduces the following new features: Support for szext instruction which performs sign-extension or zero-extension on a specified\nvalue. Support for bmsk instruction which creates a bitmask of the specified width starting at the\nspecified bit position. Support for special registers %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset<2> . Semantic Changes and Clarifications None. 12.10. Changes in PTX ISA Version 7.5  New Features PTX ISA version 7.5 introduces the following new features: Debug information enhancements to support label difference and negative values in the .section debugging directive. Support for ignore-src operand on cp.async instruction. Extensions to the memory consistency model to introduce the following new concepts: A memory proxy as an abstract label for different methods of memory access. Virtual aliases as distinct memory addresses accessing the same physical memory location. Support for new fence.proxy and membar.proxy instructions to allow synchronization of\nmemory accesses performed via virtual aliases. Semantic Changes and Clarifications None. 12.11. Changes in PTX ISA Version 7.4  New Features PTX ISA version 7.4 introduces the following new features: Support for sm_87 target architecture. Support for .level::eviction_priority qualifier which allows specifying cache eviction\npriority hints on ld , ld.global.nc , st , and prefetch instructions. Support for .level::prefetch_size qualifier which allows specifying data prefetch hints on ld and cp.async instructions. Support for createpolicy instruction which allows construction of different types of cache\neviction policies. Support for .level::cache_hint qualifier which allows the use of cache eviction policies with ld , ld.global.nc , st , atom , red and cp.async instructions. Support for applypriority and discard operations on cached data. Semantic Changes and Clarifications None. 12.12. Changes in PTX ISA Version 7.3  New Features PTX ISA version 7.3 introduces the following new features: Extends mask() operator used in initializers to also support integer constant expression. Adds support for stack manpulation instructions that allow manipulating stack using stacksave and stackrestore instructions and allocation of per-thread stack using alloca instruction. Semantic Changes and Clarifications The unimplemented version of alloca from the older PTX ISA specification has been replaced with\nnew stack manipulation instructions in PTX ISA version 7.3. 12.13. Changes in PTX ISA Version 7.2  New Features PTX ISA version 7.2 introduces the following new features: Enhances .loc directive to represent inline function information. Adds support to define labels inside the debug sections. Extends min and max instructions to support .xorsign and .abs modifiers. Semantic Changes and Clarifications None. 12.14. Changes in PTX ISA Version 7.1  New Features PTX ISA version 7.1 introduces the following new features: Support for sm_86 target architecture. Adds a new operator, mask() , to extract a specific byte from variable’s address used in\ninitializers. Extends tex and tld4 instructions to return an optional predicate that indicates if data\nat specified coordinates is resident in memory. Extends single-bit wmma and mma instructions to support .and operation. Extends mma instruction to support .sp modifier that allows matrix multiply-accumulate\noperation when input matrix A is sparse. Extends mbarrier.test_wait instruction to test the completion of specific phase parity. Semantic Changes and Clarifications None. 12.15. Changes in PTX ISA Version 7.0  New Features PTX ISA version 7.0 introduces the following new features: Support for sm_80 target architecture. Adds support for asynchronous copy instructions that allow copying of data asynchronously from one\nstate space to another. Adds support for mbarrier instructions that allow creation of mbarrier objects in memory and\nuse of these objects to synchronize threads and asynchronous copy operations initiated by threads. Adds support for redux.sync instruction which allows reduction operation across threads in a\nwarp. Adds support for new alternate floating-point data formats .bf16 and .tf32 . Extends wmma instruction to support .f64 type with shape .m8n8k4 . Extends wmma instruction to support .bf16 data format. Extends wmma instruction to support .tf32 data format with shape .m16n16k8 . Extends mma instruction to support .f64 type with shape .m8n8k4 . Extends mma instruction to support .bf16 and .tf32 data formats with shape .m16n8k8 . Extends mma instruction to support new shapes .m8n8k128 , .m16n8k4 , .m16n8k16 , .m16n8k32 , .m16n8k64 , .m16n8k128 and .m16n8k256 . Extends abs and neg instructions to support .bf16 and .bf16x2 data formats. Extends min and max instructions to support .NaN modifier and .f16 , .f16x2 , .bf16 and .bf16x2 data formats. Extends fma instruction to support .relu saturation mode and .bf16 and .bf16x2 data formats. Extends cvt instruction to support .relu saturation mode and .f16 , .f16x2 , .bf16 , .bf16x2 and .tf32 destination formats. Adds support for tanh instruction that computes hyperbolic-tangent. Extends ex2 instruction to support .f16 and .f16x2 types. Semantic Changes and Clarifications None. 12.16. Changes in PTX ISA Version 6.5  New Features PTX ISA version 6.5 introduces the following new features: Adds support for integer destination types for half precision comparison instruction set . Extends abs instruction to support .f16 and .f16x2 types. Adds support for cvt.pack instruction which allows converting two integer values and packing\nthe results together. Adds new shapes .m16n8k8 , .m8n8k16 and .m8n8k32 on the mma instruction. Adds support for ldmatrix instruction which loads one or more matrices from shared memory for mma instruction. Removed Features PTX ISA version 6.5 removes the following features: Support for .satfinite qualifier on floating point wmma.mma instruction has been\nremoved. This support was deprecated since PTX ISA version 6.4. Semantic Changes and Clarifications None. 12.17. Changes in PTX ISA Version 6.4  New Features PTX ISA version 6.4 introduces the following new features: Adds support for .noreturn directive which can be used to indicate a function does not return\nto it’s caller function. Adds support for mma instruction which allows performing matrix multiply-and-accumulate\noperation. Deprecated Features PTX ISA version 6.4 deprecates the following features: Support for .satfinite qualifier on floating point wmma.mma instruction. Removed Features PTX ISA version 6.4 removes the following features: Support for shfl and vote instructions without the .sync qualifier has been removed\nfor .target sm_70 and higher. This support was deprecated since PTX ISA version 6.0 as\ndocumented in PTX ISA version 6.2. Semantic Changes and Clarifications Clarified that resolving references of a .weak symbol considers only .weak or .visible symbols with the same name and does not consider local symbols with the same name. Clarified that in cvt instruction, modifier .ftz can only be specified when either .atype or .dtype is .f32 . 12.18. Changes in PTX ISA Version 6.3  New Features PTX ISA version 6.3 introduces the following new features: Support for sm_75 target architecture. Adds support for a new instruction nanosleep that suspends a thread for a specified duration. Adds support for .alias directive which allows definining alias to function symbol. Extends atom instruction to perform .f16 addition operation and .cas.b16 operation. Extends red instruction to perform .f16 addition operation. The wmma instructions are extended to support multiplicand matrices of type .s8 , .u8 , .s4 , .u4 , .b1 and accumulator matrices of type .s32 . Semantic Changes and Clarifications Introduced the mandatory .aligned qualifier for all wmma instructions. Specified the alignment required for the base address and stride parameters passed to wmma.load and wmma.store . Clarified that layout of fragment returned by wmma operation is architecture dependent and\npassing wmma fragments around functions compiled for different link compatible SM\narchitectures may not work as expected. Clarified that atomicity for {atom/red}.f16x2} operations is guranteed separately for each of\nthe two .f16 elements but not guranteed to be atomic as single 32-bit access. 12.19. Changes in PTX ISA Version 6.2  New Features PTX ISA version 6.2 introduces the following new features: A new instruction activemask for querying active threads in a warp. Extends atomic and reduction instructions to perform .f16x2 addition operation with mandatory .noftz qualifier. Deprecated Features PTX ISA version 6.2 deprecates the following features: The use of shfl and vote instructions without the .sync is deprecated retrospectively\nfrom PTX ISA version 6.0, which introduced the sm_70 architecture that implements Independent Thread Scheduling . Semantic Changes and Clarifications Clarified that wmma instructions can be used in conditionally executed code only if it is\nknown that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined. In the memory consistency model, the definition of morally strong operations was updated to\nexclude fences from the requirement of complete overlap since fences do not access memory. 12.20. Changes in PTX ISA Version 6.1  New Features PTX ISA version 6.1 introduces the following new features: Support for sm_72 target architecture. Support for new matrix shapes 32x8x16 and 8x32x16 in wmma instruction. Semantic Changes and Clarifications None. 12.21. Changes in PTX ISA Version 6.0  New Features PTX ISA version 6.0 introduces the following new features: Support for sm_70 target architecture. Specifies the memory consistency model for programs running on sm_70 and later architectures. Various extensions to memory instructions to specify memory synchronization semantics and scopes\nat which such synchronization can be observed. New instruction wmma for matrix operations which allows loading matrices from memory,\nperforming multiply-and-accumulate on them and storing result in memory. Support for new barrier instruction. Extends neg instruction to support .f16 and .f16x2 types. A new instruction fns which allows finding n-th set bit in integer. A new instruction bar.warp.sync which allows synchronizing threads in warp. Extends vote and shfl instructions with .sync modifier which waits for specified\nthreads before executing the vote and shfl operation respectively. A new instruction match.sync which allows broadcasting and comparing a value across threads in\nwarp. A new instruction brx.idx which allows branching to a label indexed from list of potential\ntargets. Support for unsized array parameter for .func which can be used to implement variadic\nfunctions. Support for .b16 integer type in dwarf-lines. Support for taking address of device function return parameters using mov instruction. Semantic Changes and Clarifications Semantics of bar instruction were updated to indicate that executing thread waits for other\nnon-exited threads from it’s warp. Support for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from\nthe spec. Support for taking address of labels, using labels in initializers which was unimplemented has\nbeen removed from the spec. Support for variadic functions which was unimplemented has been removed from the spec. 12.22. Changes in PTX ISA Version 5.0  New Features PTX ISA version 5.0 introduces the following new features: Support for sm_60 , sm_61 , sm_62 target architecture. Extends atomic and reduction instructions to perform double-precision add operation. Extends atomic and reduction instructions to specify scope modifier. A new .common directive to permit linking multiple object files containing declarations of the\nsame symbol with different size. A new dp4a instruction which allows 4-way dot product with accumulate operation. A new dp2a instruction which allows 2-way dot product with accumulate operation. Support for special register %clock_hi . Semantic Changes and Clarifications Semantics of cache modifiers on ld and st instructions were clarified to reflect cache\noperations are treated as performance hint only and do not change memory consistency behavior of the\nprogram. Semantics of volatile operations on ld and st instructions were clarified to reflect how volatile operations are handled by optimizing compiler. 12.23. Changes in PTX ISA Version 4.3  New Features PTX ISA version 4.3 introduces the following new features: A new lop3 instruction which allows arbitrary logical operation on 3 inputs. Adds support for 64-bit computations in extended precision arithmetic instructions. Extends tex.grad instruction to support cube and acube geometries. Extends tld4 instruction to support a2d , cube and acube geometries. Extends tex and tld4 instructions to support optional operands for offset vector and depth\ncompare. Extends txq instruction to support querying texture fields from specific LOD. Semantic Changes and Clarifications None. 12.24. Changes in PTX ISA Version 4.2  New Features PTX ISA version 4.2 introduces the following new features: Support for sm_53 target architecture. Support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types. Support for memory_layout field for surfaces and suq instruction support for querying this\nfield. Semantic Changes and Clarifications Semantics for parameter passing under ABI were updated to indicate ld.param and st.param instructions used for argument passing cannot be predicated. Semantics of {atom/red}.add.f32 were updated to indicate subnormal inputs and results are\nflushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on\nshared memory preserve subnormal inputs and results and don’t flush them to zero. 12.25. Changes in PTX ISA Version 4.1  New Features PTX ISA version 4.1 introduces the following new features: Support for sm_37 and sm_52 target architectures. Support for new fields array_size , num_mipmap_levels and num_samples for Textures, and\nthe txq instruction support for querying these fields. Support for new field array_size for Surfaces, and the suq instruction support for\nquerying this field. Support for special registers %total_smem_size and %dynamic_smem_size . Semantic Changes and Clarifications None. 12.26. Changes in PTX ISA Version 4.0  New Features PTX ISA version 4.0 introduces the following new features: Support for sm_32 and sm_50 target architectures. Support for 64bit performance counter special registers %pm0_64,..,%pm7_64 . A new istypep instruction. A new instruction, rsqrt.approx.ftz.f64 has been added to compute a fast approximation of the\nsquare root reciprocal of a value. Support for a new directive .attribute for specifying special attributes of a variable. Support for .managed variable attribute. Semantic Changes and Clarifications The vote instruction semantics were updated to clearly indicate that an inactive thread in a\nwarp contributes a 0 for its entry when participating in vote.ballot.b32 . 12.27. Changes in PTX ISA Version 3.2  New Features PTX ISA version 3.2 introduces the following new features: The texture instruction supports reads from multi-sample and multisample array textures. Extends .section debugging directive to include label + immediate expressions. Extends .file directive to include timestamp and file size information. Semantic Changes and Clarifications The vavrg2 and vavrg4 instruction semantics were updated to indicate that instruction adds 1\nonly if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than\nbeing divided by 2). 12.28. Changes in PTX ISA Version 3.1  New Features PTX ISA version 3.1 introduces the following new features: Support for sm_35 target architecture. Support for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work. ld.global.nc for loading read-only global data though the non-coherent texture cache. A new funnel shift instruction, shf . Extends atomic and reduction instructions to perform 64-bit {and, or, xor} operations, and\n64-bit integer {min, max} operations. Adds support for mipmaps . Adds support for indirect access to textures and surfaces. Extends support for generic addressing to include the .const state space, and adds a new\noperator, generic() , to form a generic address for .global or .const variables used in\ninitializers. A new .weak directive to permit linking multiple object files containing declarations of the\nsame symbol. Semantic Changes and Clarifications PTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer. PTX 3.1 code should either\ninclude explicit generic() operators in initializers, use cvta.global to form generic\naddresses at runtime, or load from the non-generic address using ld.global . Instruction mad.f32 requires a rounding modifier for sm_20 and higher targets. However for\nPTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently\ndefaults to mad.rn.f32 . For PTX ISA version 3.1, ptxas generates a warning and defaults to mad.rn.f32 , and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later. 12.29. Changes in PTX ISA Version 3.0  New Features PTX ISA version 3.0 introduces the following new features: Support for sm_30 target architectures. SIMD video instructions. A new warp shuffle instruction. Instructions mad.cc and madc for efficient, extended-precision integer multiplication. Surface instructions with 3D and array geometries. The texture instruction supports reads from cubemap and cubemap array textures. Platform option .target debug to declare that a PTX module contains DWARF debug information. pmevent.mask , for triggering multiple performance monitor events. Performance monitor counter special registers %pm4..%pm7 . Semantic Changes and Clarifications Special register %gridid has been extended from 32-bits to 64-bits. PTX ISA version 3.0 deprecates module-scoped .reg and .local variables when compiling to the\nApplication Binary Interface (ABI). When compiling without use of the ABI, module-scoped .reg and .local variables are supported as before. When compiling legacy PTX code (ISA versions prior\nto 3.0) containing module-scoped .reg or .local variables, the compiler silently disables\nuse of the ABI. The shfl instruction semantics were updated to clearly indicate that value of source operand a is unpredictable for inactive and predicated-off threads within the warp. PTX modules no longer allow duplicate .version directives. This feature was unimplemented, so\nthere is no semantic change. Unimplemented instructions suld.p and sust.p.{u32,s32,f32} have been removed. 12.30. Changes in PTX ISA Version 2.3  New Features PTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D\nor 2D textures, where an integer indexes into the array of textures, and then one or two\nsingle-precision floating point coordinates are used to address within the selected 1D or 2D\ntexture. PTX 2.3 adds a new directive, .address_size , for specifying the size of addresses. Variables in .const and .global state spaces are initialized to zero by default. Semantic Changes and Clarifications The semantics of the .maxntid directive have been updated to match the current\nimplementation. Specifically, .maxntid only guarantees that the total number of threads in a\nthread block does not exceed the maximum. Previously, the semantics indicated that the maximum was\nenforced separately in each dimension, which is not the case. Bit field extract and insert instructions BFE and BFI now indicate that the len and pos operands are restricted to the value range 0..255 . Unimplemented instructions {atom,red}.{min,max}.f32 have been removed. 12.31. Changes in PTX ISA Version 2.2  New Features PTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a\nnew directives for specifying that a kernel parameter is a pointer, for specifying to which state\nspace the parameter points, and for optionally specifying the alignment of the memory to which the\nparameter points. PTX 2.2 adds a new field named force_unnormalized_coords to the .samplerref opaque\ntype. This field is used in the independent texturing mode to override the normalized_coords field in the texture header. This field is needed to support languages such as OpenCL, which\nrepresent the property of normalized/unnormalized coordinates in the sampler header rather than in\nthe texture header. PTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the .const state space. Legacy PTX that uses explicit constant banks is still supported. PTX 2.2 adds a new tld4 instruction for loading a component ( r , g , b , or a ) from\nthe four texels compising the bilinear interpolation footprint of a given texture location. This\ninstruction may be used to compute higher-precision bilerp results in software, or for performing\nhigher-bandwidth texture loads. Semantic Changes and Clarifications None. 12.32. Changes in PTX ISA Version 2.1  New Features The underlying, stack-based ABI is supported in PTX ISA version 2.1 for sm_2x targets. Support for indirect calls has been implemented for sm_2x targets. New directives, .branchtargets and .calltargets , have been added for specifying potential\ntargets for indirect branches and indirect function calls. A .callprototype directive has been\nadded for declaring the type signatures for indirect function calls. The names of .global and .const variables can now be specified in variable initializers to\nrepresent their addresses. A set of thirty-two driver-specific execution environment special registers has been added. These\nare named %envreg0..%envreg31 . Textures and surfaces have new fields for channel data type and channel order, and the txq and suq instructions support queries for these fields. Directive .minnctapersm has replaced the .maxnctapersm directive. Directive .reqntid has been added to allow specification of exact CTA dimensions. A new instruction, rcp.approx.ftz.f64 , has been added to compute a fast, gross approximate\nreciprocal. Semantic Changes and Clarifications A warning is emitted if .minnctapersm is specified without also specifying .maxntid . 12.33. Changes in PTX ISA Version 2.0  New Features Floating Point Extensions This section describes the floating-point changes in PTX ISA version 2.0 for sm_20 targets. The\ngoal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility\nwith legacy PTX ISA version 1.x code and sm_1x targets. The changes from PTX ISA version 1.x are as follows: Single-precision instructions support subnormal numbers by default for sm_20 targets. The .ftz modifier may be used to enforce backward compatibility with sm_1x . Single-precision add , sub , and mul now support .rm and .rp rounding modifiers\nfor sm_20 targets. A single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754\ncompliant rounding modifiers and support for subnormal numbers. The fma.f32 instruction also\nsupports .ftz and .sat modifiers. fma.f32 requires sm_20 . The mad.f32 instruction has been extended with rounding modifiers so that it’s synonymous with fma.f32 for sm_20 targets. Both fma.f32 and mad.f32 require a rounding modifier for sm_20 targets. The mad.f32 instruction without rounding is retained so that compilers can generate code for sm_1x targets. When code compiled for sm_1x is executed on sm_20 devices, mad.f32 maps to fma.rn.f32 . Single- and double-precision div , rcp , and sqrt with IEEE 754 compliant rounding have\nbeen added. These are indicated by the use of a rounding modifier and require sm_20 . Instructions testp and copysign have been added. New Instructions A load uniform instruction, ldu , has been added. Surface instructions support additional .clamp modifiers, .clamp and .zero . Instruction sust now supports formatted surface stores. A count leading zeros instruction, clz , has been added. A find leading non-sign bit instruction , bfind , has been added. A bit reversal instruction, brev , has been added. Bit field extract and insert instructions, bfe and bfi , have been added. A population count instruction, popc , has been added. A vote ballot instruction, vote.ballot.b32 , has been added. Instructions {atom,red}.add.f32 have been implemented. Instructions {atom,red} .shared have been extended to handle 64-bit data types for sm_20 targets. A system-level membar instruction, membar.sys , has been added. The bar instruction has been extended as follows: A bar.arrive instruction has been added. Instructions bar.red.popc.u32 and bar.red.{and,or}.pred have been added. bar now supports optional thread count and register operands. Scalar video instructions (includes prmt ) have been added. Instruction isspacep for querying whether a generic address falls within a specified state space\nwindow has been added. Instruction cvta for converting global, local, and shared addresses to generic address and\nvice-versa has been added. Other New Features Instructions ld , ldu , st , prefetch , prefetchu , isspacep , cvta , atom ,\nand red now support generic addressing. New special registers %nwarpid , %nsmid , %clock64 , %lanemask_{eq,le,lt,ge,gt} have\nbeen added. Cache operations have been added to instructions ld , st , suld , and sust , e.g., for prefetching to specified level of memory hierarchy. Instructions prefetch and prefetchu have also been added. The .maxnctapersm directive was deprecated and replaced with .minnctapersm to better match\nits behavior and usage. A new directive, .section , has been added to replace the @@DWARF syntax for passing\nDWARF-format debugging information through PTX. A new directive, .pragma nounroll , has been added to allow users to disable loop unrolling. Semantic Changes and Clarifications The errata in cvt.ftz for PTX ISA versions 1.4 and earlier, where single-precision subnormal\ninputs and results were not flushed to zero if either source or destination type size was 64-bits,\nhas been fixed. In PTX ISA version 1.5 and later, cvt.ftz (and cvt for .target sm_1x ,\nwhere .ftz is implied) instructions flush single-precision subnormal inputs and results to\nsign-preserving zero for all combinations of floating-point instruction types. To maintain\ncompatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs\nand results are flushed to sign-preserving zero only when neither source nor destination type size\nis 64-bits. Components of special registers %tid , %ntid , %ctaid , and %nctaid have been extended\nfrom 16-bits to 32-bits. These registers now have type .v4.u32 . The number of samplers available in independent texturing mode was incorrectly listed as thirty-two\nin PTX ISA version 1.5; the correct number is sixteen. 14. Descriptions of .pragma Strings  This section describes the .pragma strings defined by ptxas. 14.1. Pragma Strings: “nounroll”  “nounroll” Disable loop unrolling in optimizing the backend compiler. Syntax .pragma \"nounroll\"; Description The \"nounroll\" pragma is a directive to disable loop unrolling in the optimizing backend\ncompiler. The \"nounroll\" pragma is allowed at module, entry-function, and statement levels, with the\nfollowing meanings: module scope disables unrolling for all loops in module, including loops preceding the .pragma . entry-function scope disables unrolling for all loops in the entry function body. statement-level pragma disables unrolling of the loop for which the current block is the loop header. Note that in order to have the desired effect at statement level, the \"nounroll\" directive must\nappear before any instruction statements in the loop header basic block for the desired loop. The\nloop header block is defined as the block that dominates all blocks in the loop body and is the\ntarget of the loop backedge. Statement-level \"nounroll\" directives appearing outside of loop\nheader blocks are silently ignored. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Ignored for sm_1x targets. Examples .entry foo (...)\n.pragma \"nounroll\";  // do not unroll any loop in this function\n{\n...\n}\n\n.func bar (...)\n{\n...\nL1_head:\n     .pragma \"nounroll\";  // do not unroll this loop\n     ...\n@p   bra L1_end;\nL1_body:\n     ...\nL1_continue:\n     bra L1_head;\nL1_end:\n     ...\n} 14.2. Pragma Strings: “used_bytes_mask”  “used_bytes_mask” Mask for indicating used bytes in data of ld operation. Syntax .pragma \"used_bytes_mask mask\"; Description The \"used_bytes_mask\" pragma is a directive that specifies used bytes in a load\noperation based on the mask provided. \"used_bytes_mask\" pragma needs to be specified prior to a load instruction for which\ninformation about bytes used from the load operation is needed.\nPragma is ignored if instruction following it is not a load instruction. For a load instruction without this pragma, all bytes from the load operation are assumed\nto be used. Operand mask is a 32-bit integer with set bits indicating the used bytes in data of\nload operation. Semantics Each bit in mask operand corresponds to a byte data where each set bit represents the used byte.\nMost-significant bit corresponds to most-significant byte of data.\n\n// For 4 bytes load with only lower 3 bytes used\n.pragma \"used_bytes_mask 0x7\";\nld.global.u32 %r0, [gbl];     // Higher 1 byte from %r0 is unused\n\n// For vector load of 16 bytes with lower 12 bytes used\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];  // %r3 unused PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_50 or higher. Examples .pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used 15. Notices  15.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 15.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 15.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html", "content_type": "text/html", "text": "CUDA Installation Guide for Linux 1. Introduction 1.1. System Requirements 1.2. OS Support Policy 1.3. Host Compiler Support Policy 1.3.1. Supported C++ Dialects 1.4. About This Document 2. Pre-installation Actions 2.1. Verify You Have a CUDA-Capable GPU 2.2. Verify You Have a Supported Version of Linux 2.3. Verify the System Has gcc Installed 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed 2.5. Install GPUDirect Storage 2.6. Choose an Installation Method 2.7. Download the NVIDIA CUDA Toolkit 2.8. Address Custom xorg.conf, If Applicable 2.9. Handle Conflicting Installation Methods 3. Package Manager Installation 3.1. Overview 3.2. RHEL 8 / Rocky 8 3.2.1. Prepare RHEL 8 / Rocky 8 3.2.2. Local Repo Installation for RHEL 8 / Rocky 8 3.2.3. Network Repo Installation for RHEL 8 / Rocky 8 3.2.4. Common Instructions for RHEL 8 / Rocky 8 3.3. RHEL 9 / Rocky 9 3.3.1. Prepare RHEL 9 / Rocky 9 3.3.2. Local Repo Installation for RHEL 9 / Rocky 9 3.3.3. Network Repo Installation for RHEL 9 / Rocky 9 3.3.4. Common Instructions for RHEL 9 / Rocky 9 3.4. KylinOS 10 3.4.1. Prepare KylinOS 10 3.4.2. Local Repo Installation for KylinOS 3.4.3. Network Repo Installation for KylinOS 3.4.4. Common Instructions for KylinOS 10 3.5. Fedora 3.5.1. Prepare Fedora 3.5.2. Local Repo Installation for Fedora 3.5.3. Network Repo Installation for Fedora 3.5.4. Common Installation Instructions for Fedora 3.6. SLES 3.6.1. Prepare SLES 3.6.2. Local Repo Installation for SLES 3.6.3. Network Repo Installation for SLES 3.6.4. Common Installation Instructions for SLES 3.7. OpenSUSE 3.7.1. Prepare OpenSUSE 3.7.2. Local Repo Installation for OpenSUSE 3.7.3. Network Repo Installation for OpenSUSE 3.7.4. Common Installation Instructions for OpenSUSE 3.8. WSL 3.8.1. Prepare WSL 3.8.2. Local Repo Installation for WSL 3.8.3. Network Repo Installation for WSL 3.8.4. Common Installation Instructions for WSL 3.9. Ubuntu 3.9.1. Prepare Ubuntu 3.9.2. Local Repo Installation for Ubuntu 3.9.3. Network Repo Installation for Ubuntu 3.9.4. Common Installation Instructions for Ubuntu 3.10. Debian 3.10.1. Prepare Debian 3.10.2. Local Repo Installation for Debian 3.10.3. Network Repo Installation for Debian 3.10.4. Common Installation Instructions for Debian 3.11. Amazon Linux 2023 3.11.1. Prepare Amazon Linux 2023 3.11.2. Local Repo Installation for Amazon Linux 3.11.3. Network Repo Installation for Amazon Linux 3.11.4. Common Installation Instructions for Amazon Linux 3.12. Additional Package Manager Capabilities 3.12.1. Available Packages 3.12.2. Meta Packages 3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpm 3.12.4. Package Upgrades 4. Driver Installation 5. NVIDIA Open GPU Kernel Modules 5.1. CUDA Runfile 5.2. Debian 5.3. Fedora 5.4. KylinOS 10 5.5. RHEL 9 and Rocky 9 5.6. RHEL 8 and Rocky 8 5.7. OpenSUSE and SLES 5.8. Ubuntu 6. Precompiled Streams 6.1. Precompiled Streams Support Matrix 6.2. Modularity Profiles 7. Kickstart Installation 7.1. RHEL 8 / Rocky Linux 8 7.2. RHEL 9 / Rocky Linux 9 8. Runfile Installation 8.1. Runfile Overview 8.2. Installation 8.3. Disabling Nouveau 8.3.1. Fedora 8.3.2. RHEL / Rocky and KylinOS 8.3.3. OpenSUSE 8.3.4. SLES 8.3.5. WSL 8.3.6. Ubuntu 8.3.7. Debian 8.4. Device Node Verification 8.5. Advanced Options 8.6. Uninstallation 9. Conda Installation 9.1. Conda Overview 9.2. Installing CUDA Using Conda 9.3. Uninstalling CUDA Using Conda 9.4. Installing Previous CUDA Releases 9.5. Upgrading from cudatoolkit Package 10. Pip Wheels 11. Tarball and Zip Archive Deliverables 11.1. Parsing Redistrib JSON 11.2. Importing Tarballs into CMake 11.3. Importing Tarballs into Bazel 12. CUDA Cross-Platform Environment 12.1. CUDA Cross-Platform Installation 12.2. CUDA Cross-Platform Samples 13. Post-installation Actions 13.1. Mandatory Actions 13.1.1. Environment Setup 13.2. Recommended Actions 13.2.1. Install Persistence Daemon 13.2.2. Install Writable Samples 13.2.3. Verify the Installation 13.2.3.1. Verify the Driver Version 13.2.3.2. Running the Binaries 13.2.4. Install Nsight Eclipse Plugins 13.2.5. Local Repo Removal 13.3. Optional Actions 13.3.1. Install Third-party Libraries 13.3.2. Install the Source Code for cuda-gdb 13.3.3. Select the Active Version of CUDA 14. Advanced Setup 15. Frequently Asked Questions 15.1. How do I install the Toolkit in a different location? 15.2. Why do I see “nvcc: No such file or directory” when I try to build a CUDA application? 15.3. Why do I see “error while loading shared libraries: <lib name>: cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library? 15.4. Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu? 15.5. How can I tell X to ignore a GPU for compute-only use? 15.6. Why doesn’t the cuda-repo package install the CUDA Toolkit and Drivers? 15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04? 15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update? 15.9. How do I install a CUDA driver with a version less than 367 using a network repo? 15.10. How do I install an older CUDA version using a network repo? 15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency? 15.12. How do I handle “Errors were encountered while processing: glx-diversions”? 16. Additional Considerations 17. Switching between Driver Module Flavors 18. Removing CUDA Toolkit and Driver 19. Notices 19.1. Notice 19.2. OpenCL 19.3. Trademarks 20. Copyright Installation Guide for Linux » 1. Introduction v12.5 | PDF | Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux. 1. Introduction  CUDA ® is a parallel computing platform and programming model invented by NVIDIA ® . It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU). CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C﻿+﻿+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources. CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements  To use NVIDIA CUDA on your system, you will need the following installed: CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release. The following table lists the supported Linux distributions. Please review the footnotes associated with the table. Table 1 Native Linux Distribution Support in CUDA 12.5 Update 1  Distribution Kernel 1 Default GCC GLIBC x86_64 RHEL 9.y (y <= 4) 5.14.0-427 11.4.1 2.34 RHEL 8.y (y <= 10) 4.18.0-553 8.5.0 2.28 OpenSUSE Leap 15.y (y <= 5) 5.14.21-150500 7.5.0 2.31 Rocky Linux 8.y (y<=10) 4.18.0-553 8.5.0 2.28 Rocky Linux 9.y (y<=4) 5.14.0-427 11.4.1 2.34 SUSE SLES 15.y (y <= 5) 5.14.21-150500 7.5.0 2.31 Ubuntu 24.04 LTS 6.8.0-31 13.2.0 2.39 Ubuntu 22.04.z (z <= 4) LTS 6.5.0-27 12.3.0 2.35 Ubuntu 20.04.z (z <= 6) LTS 5.15.0-67 9.4.0 2.31 Debian 12.x (x<=5) 6.1.76-1 12.2.0 2.36 Debian 11.y (y<=9) 5.10.209-2 10.2.1 2.31 Debian 10.z (z<=13) 4.19.0-21 8.3.0 2.28 Fedora 39 6.5.6-300 13.2.1 2.38 KylinOS V10 SP2 4.19.90-25.14.v2101.ky10 7.3.0 2.28 Amazon Linux 2023 6.1.82-99.168 11.4.1 2.34 Arm64 sbsa RHEL 9.y (y <= 4) 5.14.0-427 11.4.1 2.34 RHEL 8.y (y <= 10) 4.18.0-553 8.5.0 2.28 SUSE SLES 15.y (y <= 5) 5.14.21-150500 7.5.0 2.32 Ubuntu 24.04 LTS 6.8.0-31 13.2.0 2.39 Ubuntu 22.04 LTS (z <= 5) LTS 5.15.0-102 11.4.0 2.35 Ubuntu 20.04.z (z <= 5) LTS 5.4.0-174 9.4.0 2.31 Arm64 sbsa Jetson (dGPU) 20.04.06 LTS Rel35 JP 5.x 5.10.192-tegra 9.4.0 2.31 22.04.4 LTS Rel36 - JP6.x 5.15.136-tegra 11.4.0 2.35 Aarch64 Jetson (iGPU) L4T Ubuntu 22.04 Rel36 - JP6.x 6.1.80-tegra 11.4.0 2.3.5 The following notes apply to the kernel versions supported by CUDA: For specific kernel versions supported on Red Hat Enterprise Linux (RHEL), visit https://access.redhat.com/articles/3078 . A list of kernel versions including the release dates for SUSE Linux Enterprise Server (SLES) is available at https://www.suse.com/support/kb/doc/?id=000019587 . L4T provides a Linux kernel and a sample root filesystem derived from Ubuntu 20.04. For more details, visit https://developer.nvidia.com/embedded/jetson-linux . 1.2. OS Support Policy  CUDA support for Ubuntu 20.04.x, Ubuntu 22.04.x, RHEL 8.x, RHEL 9.x, Rocky Linux 8.x, Rocky Linux 9.x, SUSE SLES 15.x and OpenSUSE Leap 15.x will be until the standard EOSS as defined for each OS. Please refer to the support lifecycle for these OSes to know their support timelines. CUDA supports the latest Fedora release version. For Fedora release timelines, visit https://docs.fedoraproject.org/en-US/releases/ . CUDA supports a single KylinOS release version. For details, visit https://www.kylinos.cn/ . Refer to the support lifecycle for these supported OSes to know their support timelines and plan to move to newer releases accordingly. 1.3. Host Compiler Support Policy  In order to compile the CPU “Host” code in the CUDA source, the CUDA compiler NVCC requires a compatible host compiler to be installed on the system. The version of the host compiler supported on Linux platforms is tabulated as below. NVCC performs a version check on the host compiler’s major version and so newer minor versions of the compilers listed below will be supported, but major versions falling outside the range will not be supported. Table 2 Supported Compilers  Distribution GCC Clang NVHPC XLC ArmC/C++ ICC x86_64 6.x - 13.2 7.x - 17.0 23.x No No 2021.7 Arm64 sbsa 6.x - 13.2 7.x - 17.0 22.x No 23.04.1 No For GCC and Clang, the preceding table indicates the minimum version and the latest version supported. If you are on a Linux distribution that may use an older version of GCC toolchain as default than what is listed above, it is recommended to upgrade to a newer toolchain CUDA 11.0 or later toolkit. Newer GCC toolchains are available with the Red Hat Developer Toolset for example. For platforms that ship a compiler version older than GCC 6 by default, linking to static or dynamic libraries that are shipped with the CUDA Toolkit is not supported. We only support libstdc++ (GCC’s implementation) for all the supported host compilers for the platforms listed above. 1.3.1. Supported C++ Dialects  NVCC and NVRTC (CUDA Runtime Compiler) support the following C++ dialect: C++11, C++14, C++17, C++20 on supported host compilers. The default C++ dialect of NVCC  is determined by the default dialect of the host compiler used for compilation. Refer to host compiler documentation and the CUDA Programming Guide for more details on language support. C++20 is supported with the following flavors of host compiler in both host and device code. Distribution GCC Clang NVHPC Arm C/C++ x86_64 >=10.x >=11.x >=22.x 22.x 1.4. About This Document  This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line. You do not need previous experience with CUDA or experience with parallel computation. Note: This guide covers installation only on systems with X Windows installed. Note Many commands in this document might require superuser privileges. On most distributions of Linux, this will require you to log in as root. For systems that have enabled the sudo package, use the sudo prefix for all necessary commands. 2. Pre-installation Actions  Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux: Verify the system has a CUDA-capable GPU. Verify the system is running a supported version of Linux. Verify the system has gcc installed. Verify the system has the correct kernel headers and development packages installed. Download the NVIDIA CUDA Toolkit. Handle conflicting installation methods. Note You can override the install-time prerequisite checks by running the installer with the -override flag. Remember that the prerequisites will still be required to use the NVIDIA CUDA Toolkit. 2.1. Verify You Have a CUDA-Capable GPU  To verify that your GPU is CUDA-capable, go to your distribution’s equivalent of System Properties, or, from the command line, enter: lspci | grep -i nvidia If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin ) at the command line and rerun the previous lspci command. If your graphics card is from NVIDIA and it is listed in https://developer.nvidia.com/cuda-gpus , your GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products. 2.2. Verify You Have a Supported Version of Linux  The CUDA Development Tools are only supported on some specific distributions of Linux. These are listed in the CUDA Toolkit release notes. To determine which distribution and release number you’re running, type the following at the command line: uname -m && cat /etc/*release You should see output similar to the following, modified for your particular system: x86_64\nRed Hat Enterprise Linux Workstation release 6.0 (Santiago) The x86_64 line indicates you are running on a 64-bit system. The remainder gives information about your distribution. 2.3. Verify the System Has gcc Installed  The gcc compiler is required for development using the CUDA Toolkit. It is not required for running CUDA applications. It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly. To verify the version of gcc installed on your system, type the following on the command line: gcc --version If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web. 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed  The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. For example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed. While the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed. However, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using. Therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version. The version of the kernel your system is running can be found by running the following command: uname -r This is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers. This command will be used multiple times below to specify the version of the packages to install. Note that below are the common-case scenarios for kernel usage. More advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running. Note If you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed. Otherwise, the CUDA Driver will fail to work with the new kernel. 2.5. Install GPUDirect Storage  If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package. GDS packages can be installed using the CUDA packaging guide. Follow the instructions in MLNX_OFED Requirements and Installation . GDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode. Installation instructions for them differ slightly. Compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations. Full GDS support is restricted to the following Linux distros: Ubuntu 20.04, Ubuntu 22.04 RHEL 8.3, RHEL 8.4, RHEL 9.0 Starting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver. Follow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages. 2.6. Choose an Installation Method  The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages). The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution’s native package management system. The distribution-specific packages interface with the distribution’s native package management system. It is recommended to use the distribution-specific packages, where possible. Note For both native as well as cross development, the toolkit must be installed using the distribution-specific installer. See the CUDA Cross-Platform Installation section for more details. 2.7. Download the NVIDIA CUDA Toolkit  The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads . Choose the platform you are using and download the NVIDIA CUDA Toolkit. The CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources. Download Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. To calculate the MD5 checksum of the downloaded file, run the following: md5sum <file> 2.8. Address Custom xorg.conf, If Applicable  The driver relies on an automatically generated xorg.conf file at /etc/X11/xorg.conf . If a custom-built xorg.conf file is present, this functionality will be disabled and the driver may not work. You can try removing the existing xorg.conf file, or adding the contents of /etc/X11/xorg.conf.d/00-nvidia.conf to the xorg.conf file. The xorg.conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration. 2.9. Handle Conflicting Installation Methods  Before installing CUDA, any previous installations that could conflict should be uninstalled. This will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs. Runfile). See the following charts for specifics. Table 3 CUDA Toolkit Installation Compatibility Matrix  Installed Toolkit Version == X.Y Installed Toolkit Version != X.Y RPM/Deb run RPM/Deb run Installing Toolkit Version X.Y RPM/Deb No Action Uninstall Run No Action No Action run Uninstall RPM/Deb Uninstall Run No Action No Action Table 4 NVIDIA Driver Installation Compatibility Matrix  Installed Driver Version == X.Y Installed Driver Version != X.Y RPM/Deb run RPM/Deb run Installing Driver Version X.Y RPM/Deb No Action Uninstall Run No Action Uninstall Run run Uninstall RPM/Deb No Action Uninstall RPM/Deb No Action Use the following command to uninstall a Toolkit runfile installation: sudo /usr/local/cuda-X.Y/bin/cuda-uninstaller Use the following command to uninstall a Driver runfile installation: sudo /usr/bin/nvidia-uninstall Use the following commands to uninstall an RPM/Deb installation: sudo dnf remove <package_name>                      # RHEL 8 / Rocky Linux 8 sudo dnf remove <package_name>                      # Fedora sudo zypper remove <package_name>                   # OpenSUSE / SLES sudo apt-get --purge remove <package_name>          # Ubuntu 3. Package Manager Installation  Basic instructions can be found in the Quick Start Guide . Read on for more detailed instructions. 3.1. Overview  Installation using RPM or Debian packages interfaces with your system’s package management system. When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in /var/. Such a package only informs the package manager where to find the actual installation packages, but will not install them. If the online network repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper. Distribution-specific instructions detail how to install CUDA: RHEL 8 / Rocky Linux 8 RHEL 9 / Rocky Linux 9 KylinOS 10 Fedora SLES OpenSUSE WSL Ubuntu Debian Amazon Linux 2023 Finally, some helpful package manager capabilities are detailed. These instructions are for native development only. For cross-platform development, see the CUDA Cross-Platform Environment section. Note Optional components such as nvidia-fs , libnvidia_nscq , and fabricmanager are not installed by default and will have to be installed separately as needed. 3.2. RHEL 8 / Rocky 8  3.2.1. Prepare RHEL 8 / Rocky 8  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) If matching kernel-headers and kernel-devel packages are not available for the currently running kernel version, you may need to use the previously shipped version of these packages. See https://bugzilla.redhat.com/show_bug.cgi?id=1986132 for more information. Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau . Those packages are only available on third-party repositories, such as EPEL . Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding. To enable EPEL: sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Enable optional repos: On RHEL 8 Linux only, execute the following steps to enable optional repositories. On x86_64 systems: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms\nsubscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms\nsubscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.2.2. Local Repo Installation for RHEL 8 / Rocky 8  Install local repository on file system: sudo rpm --install cuda-repo-<distro>-X-Y-local-<version>*.<arch>.rpm 3.2.3. Network Repo Installation for RHEL 8 / Rocky 8  Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: rhel8/cross-linux-sbsa rhel8/sbsa rhel8/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. For upgrades, you must also also fetch an updated .repo entry: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Clean Yum repository cache: sudo dnf clean expire-cache 3.2.4. Common Instructions for RHEL 8 / Rocky 8  These instructions apply to both local and network installation. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms\nsudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions. 3.3. RHEL 9 / Rocky 9  3.3.1. Prepare RHEL 9 / Rocky 9  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau . Those packages are only available on third-party repositories, such as EPEL . Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding. To enable EPEL: sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm Enable optional repos: On RHEL 9 Linux only, execute the following steps to enable optional repositories. On x86_64 systems: subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms\nsubscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms\nsubscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.3.2. Local Repo Installation for RHEL 9 / Rocky 9  Install local repository on file system: sudo rpm --install cuda-repo-<distro>-X-Y-local-<version>*.<arch>.rpm 3.3.3. Network Repo Installation for RHEL 9 / Rocky 9  Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: rhel9/cross-linux-sbsa rhel9/sbsa rhel9/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. For upgrades, you must also also fetch an updated .repo entry: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Clean Yum repository cache: sudo dnf clean expire-cache 3.3.4. Common Instructions for RHEL 9 / Rocky 9  These instructions apply to both local and network installation. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms\nsudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions. 3.4. KylinOS 10  3.4.1. Prepare KylinOS 10  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Choose an installation method: local repo or network repo . 3.4.2. Local Repo Installation for KylinOS  Install local repository on file system: sudo rpm --install cuda-repo-kylin10-X-Y-local-<version>*.<arch>.rpm 3.4.3. Network Repo Installation for KylinOS  Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/kylin10/x86_64/cuda-$distro.repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. Clean Yum repository cache: sudo dnf clean expire-cache 3.4.4. Common Instructions for KylinOS 10  These instructions apply to both local and network installation. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms\nsudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions. 3.5. Fedora  3.5.1. Prepare Fedora  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.5.2. Local Repo Installation for Fedora  Install local repository on file system: sudo rpm --install cuda-repo-<distro>-X-Y-local-<version>*.x86_64.rpm where distro is fedora37 or fedora39 , for example. 3.5.3. Network Repo Installation for Fedora  Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo where $distro should be replaced by one of the following: fedora37 fedora39 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of Fedora, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. For upgrades, you must also fetch an updated .repo entry: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo Clean DNF repository cache: sudo dnf clean expire-cache 3.5.4. Common Installation Instructions for Fedora  These instructions apply to both local and network installation for Fedora. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms\nsudo dnf install cuda-toolkit Note The CUDA driver installation may fail if the RPMFusion non-free repository is enabled. In this case, CUDA installations should temporarily disable the RPMFusion non-free repository. sudo dnf --disablerepo=\"rpmfusion-nonfree*\" install cuda It may be necessary to rebuild the grub configuration files, particularly if you use a non-default partition scheme. If so, then run this below command, and reboot the system: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Perform the post-installation actions. 3.6. SLES  3.6.1. Prepare SLES  Perform the pre-installation actions. The kernel development packages for the currently running kernel can be installed with: sudo zypper install -y kernel-<variant>-devel=<version> To run the above command, you will need the variant and version of the currently running kernel. Use the output of the uname command to determine the currently running kernel’s variant and version: $ uname -r\n3.16.6-2-default In the above example, the variant is default and version is 3.16.6-2 . The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\\-default//') The kernel headers and development packages for the currently running kernel can be installed with: sudo zypper install -y kernel-<variant>-devel=<version> On SLES12 SP4, install the Mesa-libgl-devel Linux packages before proceeding. See Mesa-libGL-devel. Add the user to the video group: sudo usermod -a -G video <username> Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.6.2. Local Repo Installation for SLES  Install local repository on file system: sudo rpm --install cuda-repo-sles15-X-Y-local-<version>*.x86_64.rpm 3.6.3. Network Repo Installation for SLES  Enable the network repo: sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: sles15/cross-linux-sbsa sles15/sbsa sles15/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of SLES, the zypper package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. For upgrades, you must also also fetch an updated .repo entry: sudo zypper removerepo cuda-$distro-$arch\nsudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Refresh Zypper repository cache: sudo SUSEConnect --product PackageHub/15/<architecture>\nsudo zypper refresh 3.6.4. Common Installation Instructions for SLES  These instructions apply to both local and network installation for SLES. Install CUDA SDK: sudo zypper install cuda-toolkit Install CUDA Samples GL dependencies: Refer to CUDA Cross-Platform Samples . Reboot the system: sudo reboot Perform the post-installation actions. 3.7. OpenSUSE  3.7.1. Prepare OpenSUSE  Perform the pre-installation actions. The kernel development packages for the currently running kernel can be installed with: sudo zypper install -y kernel-<variant>-devel=<version> To run the above command, you will need the variant and version of the currently running kernel. Use the output of the uname command to determine the currently running kernel’s variant and version: $ uname -r\n3.16.6-2-default In the above example, the variant is default and version is 3.16.6-2 . The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\\-default//') Add the user to the video group: sudo usermod -a -G video <username> Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.7.2. Local Repo Installation for OpenSUSE  Install local repository on file system: sudo rpm --install cuda-repo-opensuse15-<version>.x86_64.rpm 3.7.3. Network Repo Installation for OpenSUSE  Enable the network repo: sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On fresh installation of openSUSE, the zypper package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. For upgrades, you must also also fetch an updated .repo entry: sudo zypper removerepo cuda-opensuse15-x86_64\nsudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo Refresh Zypper repository cache: sudo zypper refresh 3.7.4. Common Installation Instructions for OpenSUSE  These instructions apply to both local and network installation for OpenSUSE. Install CUDA SDK: sudo zypper install cuda-toolkit Reboot the system: sudo reboot Perform the post-installation actions. 3.8. WSL  These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case; it is important to not install the cuda-drivers packages within the WSL environment. 3.8.1. Prepare WSL  Perform the pre-installation actions. Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.8.2. Local Repo Installation for WSL  Install local repositiry on file system: sudo dpkg -i cuda-repo-wsl-ubuntu-X-Y-local_<version>*_x86_64.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo-wsl-ubuntu-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ 3.8.3. Network Repo Installation for WSL  The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc . This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended. Install the newcuda-keyring package: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-archive-keyring.gpg\nsudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/ /\" | sudo tee /etc/apt/sources.list.d/cuda-wsl-ubuntu-x86_64.list Add pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.8.4. Common Installation Instructions for WSL  These instructions apply to both local and network installation for WSL. Update the Apt repository cache: sudo apt-get update Install CUDA SDK: sudo apt-get install cuda-toolkit Perform the post-installation actions. 3.9. Ubuntu  3.9.1. Prepare Ubuntu  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.9.2. Local Repo Installation for Ubuntu  Install local repository on file system: sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo-<distro>-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ Add pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/x86_64/cuda-<distro>.pin\n sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.9.3. Network Repo Installation for Ubuntu  The new GPG public key for the CUDA repository is 3bf863cc . This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended. Install the new cuda-keyring package: wget https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb where $distro/$arch should be replaced by one of the following: ubuntu1604/x86_64 ubuntu1804/cross-linux-sbsa ubuntu1804/sbsa ubuntu1804/x86_64 ubuntu2004/cross-linux-aarch64 ubuntu2004/arm64 ubuntu2004/cross-linux-sbsa ubuntu2004/sbsa ubuntu2004/x86_64 ubuntu2204/sbsa ubuntu2204/x86_64 Note arm64-Jetson repos: native: $distro/arm64 cross: $distro/cross-linux-aarch64 sudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-archive-keyring.gpg\nsudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/ /\" | sudo tee /etc/apt/sources.list.d/cuda-<distro>-<arch>.list Add pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-<distro>.pin\nsudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.9.4. Common Installation Instructions for Ubuntu  These instructions apply to both local and network installation for Ubuntu. Update the Apt repository cache: sudo apt-get update Install CUDA SDK: Note These two commands must be executed separately. sudo apt-get install cuda-toolkit To include all GDS packages: sudo apt-get install nvidia-gds Reboot the system sudo reboot Perform the Post-installation Actions 3.10. Debian  3.10.1. Prepare Debian  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Enable the contrib repository: sudo add-apt-repository contrib Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.10.2. Local Repo Installation for Debian  Install local repository on file system: sudo dpkg -i cuda-repo-<distro>-X-Y-local_<version>*_x86_64.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo-<distro>-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ 3.10.3. Network Repo Installation for Debian  The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc . This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended. Install the new cuda-keyring package: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb where $distro/$arch should be replaced by one of the following: debian10/x86_64 debian11/x86_64 sudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/x86_64/cuda-archive-keyring.gpg\nsudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/<distro>/x86_64/ /\" | sudo tee /etc/apt/sources.list.d/cuda-<distro>-x86_64.list 3.10.4. Common Installation Instructions for Debian  These instructions apply to both local and network installation for Debian. Update the Apt repository cache: sudo apt-get update Note If you are using Debian 10, you may instead need to run: sudo apt-get --allow-releaseinfo-change update Install CUDA SDK: sudo apt-get -y install cuda Reboot the system: sudo reboot Perform the post-installation actions. 3.11. Amazon Linux 2023  3.11.1. Prepare Amazon Linux 2023  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo . 3.11.2. Local Repo Installation for Amazon Linux  Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm 3.11.3. Network Repo Installation for Amazon Linux  Enable the network repository: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo Clean DNF repository cache: sudo dnf clean expire-cache 3.11.4. Common Installation Instructions for Amazon Linux  These instructions apply to both local and network installation for Amazon Linux. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms\nsudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions. 3.12. Additional Package Manager Capabilities  Below are some additional capabilities of the package manager that users can take advantage of. 3.12.1. Available Packages  The recommended installation package is the cuda package. This package will install the full set of other CUDA packages required for native development and should cover most scenarios. The cuda package installs all the available packages for native developments. That includes the compiler, the debugger, the profiler, the math libraries, and so on. For x86_64 platforms, this also includes Nsight Eclipse Edition and the visual profilers. It also includes the NVIDIA driver package. On supported platforms, the cuda-cross-aarch64 and cuda-cross-sbsa packages install all the packages required for cross-platform development to arm64-Jetson and arm64-Server, respectively. The libraries and header files of the target architecture’s display driver package are also installed to enable the cross compilation of driver applications. The cuda-cross-<arch> packages do not install the native display driver. Note 32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running existing 32-bit applications on existing GPUs except Hopper. Hopper does not support 32-bit applications. Ada will be the last architecture with driver support for 32-bit applications. The packages installed by the packages above can also be installed individually by specifying their names explicitly. The list of available packages be can obtained with: yum --disablerepo=\"*\" --enablerepo=\"cuda*\" list available    # RedHat dnf --disablerepo=\"*\" --enablerepo=\"cuda*\" list available    # Fedora zypper packages -r cuda                                      # OpenSUSE & SLES cat /var/lib/apt/lists/*cuda*Packages | grep \"Package:\"      # Ubuntu 3.12.2. Meta Packages  Meta packages are RPM/Deb/Conda packages which contain no (or few) files but have multiple dependencies. They are used to install many CUDA packages when you may not know the details of the packages you want. The following table lists the meta packages. Table 5 Meta Packages Available for CUDA 12.4  Meta Package Purpose cuda Installs all CUDA Toolkit and Driver packages. Handles upgrading to the next version of the cuda package when it’s released. cuda-12-5 Installs all CUDA Toolkit and Driver packages. Remains at version 12.5 until an additional version of CUDA is installed. cuda-toolkit-12-5 Installs all CUDA Toolkit packages required to develop CUDA applications. Does not include the driver. cuda-toolkit-15 Installs all CUDA Toolkit packages required to develop applications. Will not upgrade beyond the 12.x series toolkits. Does not include the driver. cuda-toolkit Installs all CUDA Toolkit packages required to develop applications. Handles upgrading to the next 12.x version of CUDA when it’s released. Does not include the driver. cuda-tools-12-5 Installs all CUDA command line and visual tools. cuda-runtime-12-5 Installs all CUDA Toolkit packages required to run CUDA applications, as well as the Driver packages. cuda-compiler-12-5 Installs all CUDA compiler packages. cuda-libraries-12-5 Installs all runtime CUDA Library packages. cuda-libraries-dev-12-5 Installs all development CUDA Library packages. cuda-drivers Installs all NVIDIA Driver packages with proprietary kernel modules. Handles upgrading to the next version of the Driver packages when they’re released. cuda-drivers-555 Installs all NVIDIA Driver packages with proprietary kernel modules. Will not upgrade beyond the 555 branch drivers. 3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpm  These packages provide 32-bit driver libraries needed for things such as Steam (popular game app store/launcher), older video games, and some compute applications. For Debian 10 and Debian 11: sudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libcuda1-i386 nvidia-driver-libs-i386 For Debian 12: sudo dpkg --add-architecture i386\nsudo apt-get update\napt install nvidia-driver-libs:i386 For Ubuntu: sudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libnvidia-compute-<branch>:i386 libnvidia-decode-<branch>:i386 \\\n libnvidia-encode-<branch>:i386 libnvidia-extra-<branch>:i386 libnvidia-fbc1-<branch>:i386 \\\n libnvidia-gl-<branch>:i386 Where <branch> is the driver version, for example 495. For Fedora and RHEL8+: sudo dnf install nvidia-driver-cuda-libs.i686 nvidia-driver-devel.i686 \\\n nvidia-driver-libs.i686 nvidia-driver-NvFBCOpenGL.i686 nvidia-driver-NVML.i686 Note There is no modularity profile support. For openSUSE/SLES: sudo zypper install nvidia-compute-G06-32bit nvidia-gl-G06-32bit nvidia-video-G06-32bit 3.12.4. Package Upgrades  The cuda package points to the latest stable release of the CUDA Toolkit. When a new version is available, use the following commands to upgrade the toolkit and driver: sudo dnf install cuda-toolkit                                # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-toolkit                             # OpenSUSE and SLES sudo apt-get install cuda-toolkit                            # Ubuntu and Debian The cuda-cross-<arch> packages can also be upgraded in the same manner. The cuda-drivers package points to the latest driver release available in the CUDA repository. When a new version is available, use the following commands to upgrade the driver: sudo dnf module install nvidia-driver:latest-dkms             # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-drivers nvidia-gfxG04-kmp-default   # OpenSUSE and SLES sudo apt-get install cuda-drivers                            # Ubuntu and Debian Some desktop environments, such as GNOME or KDE, will display a notification alert when new packages are available. To avoid any automatic upgrade, and lock down the toolkit installation to the X.Y release, install the cuda-X-Y or cuda-cross-<arch>-X-Y package. Side-by-side installations are supported. For instance, to install both the X.Y CUDA Toolkit and the X.Y+1 CUDA Toolkit, install the cuda-X.Y and cuda-X.Y+1 packages. 4. Driver Installation  This section is for users who want to install a specific driver version. For Debian and Ubuntu: sudo apt-get install cuda-drivers-<driver_branch> For example: sudo apt-get install cuda-drivers-535 For OpenSUSE and SLES: sudo zypper -v install cuda-drivers-<driver_branch> For example: sudo zypper -v install cuda-drivers-550 This allows you to get the highest version in the specified branch. For Fedora and RHEL8+: sudo dnf module install nvidia-driver:<stream>/<profile> where profile by default is “ default ” and does not need to be specified. Example dkms streams: 450-dkms or latest-dkms Example precompiled streams: 450 or latest Note Precompiled streams are only supported on RHEL8 x86_64 and RHEL9 x86_64. To uninstall or change streams on Fedora and RHEL8: sudo dnf module remove --all nvidia-driver\nsudo dnf module reset nvidia-driver 5. NVIDIA Open GPU Kernel Modules  The NVIDIA Linux GPU Driver contains several kernel modules: nvidia.ko nvidia-modeset.ko nvidia-uvm.ko nvidia-drm.ko nvidia-peermem.ko Starting in the 515 driver release series, two “flavors” of these kernel modules are provided: Proprietary - this is the flavor that NVIDIA has historically shipped. Open-source - published kernel modules that are dual licensed MIT/GPLv2. These are new starting in release 515. With every driver release, the source code to the open kernel modules will be published on https://github.com/NVIDIA/open-gpu-kernel-modules and a tarball will be provided on https://download.nvidia.com/XFree86/ . Verify that your NVIDIA GPU is at least Turing or newer generation. lspci | grep VGA Experimental support for GeForce and Quadro SKUs can be enabled with: echo \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe.d/nvidia-gsp.conf To install NVIDIA Open GPU Kernel Modules, follow the instructions below. 5.1. CUDA Runfile  Pass the CLI argument to the CUDA runfile to opt in to NVIDIA Open GPU Kernel Modules: sh cuda_<release>_<version>_linux.run -m=kernel-open 5.2. Debian  Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-kernel-open-dkms Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-kernel-open-dkms=<version>-1 Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers-<driver_branch> For example: sudo apt-get install -v nvidia-kernel-open-dkms=550.90.07-1\nsudo apt-get install -v cuda-drivers-550 5.3. Fedora  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:<driver_branch>-open 5.4. KylinOS 10  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:<driver_branch>-open 5.5. RHEL 9 and Rocky 9  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:<driver_branch>-open 5.6. RHEL 8 and Rocky 8  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:<driver_branch>-open 5.7. OpenSUSE and SLES  Install the NVIDIA Open GPU Kernel Modules package: sudo zypper install nvidia-open-driver-G06-kmp-default Install the rest of the NVIDIA driver packages: sudo zypper install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-<flavor> | sed 's| ||g' | awk -F '|' '/<driver_branch>/ {print $2\"=\"$4}') Install the rest of the NVIDIA driver packages: sudo zypper -v install cuda-drivers-<driver_branch> For example: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-default | sed 's| ||g' | awk -F '|' '/550/ {print $2\"=\"$4}')\nsudo zypper -v install cuda-drivers-550 5.8. Ubuntu  Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-driver-<driver_branch>-open Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers-<driver_branch> Note End-users on Ubuntu should upgrade their NVIDIA Open GPU kernel modules using the following: sudo apt-get install --verbose-versions nvidia-kernel-source-550-open cuda-drivers-550 OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-driver-<driver_branch>-open Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers-<driver_branch> For example: sudo apt-get install -v nvidia-driver-550-open\nsudo apt-get install -v cuda-drivers-550 6. Precompiled Streams  Precompiled streams offer an optional method of streamlining the installation process. The advantages of precompiled streams: Precompiled: faster boot up after driver and/or kernel updates Pre-tested: kernel and driver combination has been validated Removes gcc dependency: no compiler installation required Removes dkms dependency: enabling EPEL repository not required Removes kernel-devel and kernel-headers dependencies: no black screen if matching packages are missing When using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale .ko files. To prevent system breakages, the NVIDIA dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists. This can delay the application of security fixes but ensures that a tested kernel and driver combination is always used. A warning is displayed by dnf during that upgrade situation: NOTE:  Skipping kernel installation since no NVIDIA driver kernel module package\n kmod-nvidia-${driver}-${kernel} ... could be found Packaging templates and instructions are provided on GitHub to allow you to maintain your own precompiled kernel module packages for custom kernels and derivative Linux distros: NVIDIA/yum-packaging-precompiled-kmod To use the new driver packages on RHEL 8 or RHEL 9: First, ensure that the Red Hat repositories are enabled: RHEL 8: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms\nsubscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms or RHEL 9: subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms\nsubscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms Choose one of the four options below depending on the desired driver: latest always updates to the highest versioned driver (precompiled): sudo dnf module install nvidia-driver:latest <id> locks the driver updates to the specified driver branch (precompiled): sudo dnf module install nvidia-driver:<id> Note Replace <id> with the appropriate driver branch streams, for example 520, 515, 470, or 450. latest-dkms always updates to the highest versioned driver (non-precompiled): sudo dnf module install nvidia-driver:latest-dkms Note This is the default stream. <id>-dkms locks the driver updates to the specified driver branch (non-precompiled): sudo dnf module install nvidia-driver:<id>-dkms Note Valid streams include 520-dkms , 515-dkms , 470-dkms , and 450-dkms . 6.1. Precompiled Streams Support Matrix  This table shows the supported precompiled and legacy DKMS streams for each driver. NVIDIA Driver Precompiled Stream Legacy DKMS Stream Open DKMS Stream Highest version latest latest-dkms open-dkms Locked at 520.x 520 520-dkms 520-open Locked at 515.x 515 515-dkms 515-open Prior to switching between module streams, first reset: sudo dnf module reset nvidia-driver Note This is also required for upgrading between branch locked streams. Or alternatively: sudo dnf module switch-to nvidia-driver:<stream> 6.2. Modularity Profiles  Modularity profiles work with any supported modularity stream and allow for additional use cases. These modularity profiles are available on RHEL8+ and Fedora. Table 6 Table 5. List of nvidia-driver Module Profiles  Stream Profile Use Case Default /default Installs all the driver packages in a stream. Kickstart /ks Performs unattended Linux OS installation using a config file. NVSwitch Fabric /fm Installs all the driver packages plus components required for bootstrapping an NVSwitch system (including the Fabric Manager and NSCQ telemetry). Source /src Source headers for compilation (precompiled streams only). For example: sudo dnf module nvidia-driver:<stream>/default\nsudo dnf module nvidia-driver:<stream>/ks\nsudo dnf module nvidia-driver:<stream>/fm\nsudo dnf module nvidia-driver:<stream>/src You can install multiple modularity profiles using BASH curly brace expansion, for example: sudo dnf module install nvidia-driver:latest/{default,src} See https://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams in the Developer Blog and https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/precompiled/ for more information. 7. Kickstart Installation  7.1. RHEL 8 / Rocky Linux 8  Enable the EPEL repository: repo --name=epel --baseurl=http://download.fedoraproject.org/pub/epel/8/Everything/x86_64/ Enable the CUDA repository: repo --name=cuda-rhel8 --baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/ In the packages section of the ks.cfg file, make sure you are using the /ks profile and :latest-dkms stream: @nvidia-driver:latest-dkms/ks Perform the post-installation actions. 7.2. RHEL 9 / Rocky Linux 9  Enable the EPEL repository: repo --name=epel --baseurl=http://download.fedoraproject.org/pub/epel/9/Everything/x86_64/ Enable the CUDA repository: repo --name=cuda-rhel9 --baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/ In the packages section of the ks.cfg file, make sure you are using the /ks profile and :latest-dkms stream: @nvidia-driver:latest-dkms/ks Perform the post-installation actions. 8. Runfile Installation  Basic instructions can be found in the Quick Start Guide . Read on for more detailed instructions. This section describes the installation and configuration of CUDA when using the standalone installer. The standalone installer is a “.run” file and is completely self-contained. 8.1. Runfile Overview  The Runfile installation installs the NVIDIA Driver and CUDA Toolkit via an interactive ncurses-based interface. The installation steps are listed below. Distribution-specific instructions on disabling the Nouveau drivers as well as steps for verifying device node creation are also provided. Finally, advanced options for the installer and uninstallation steps are detailed below. The Runfile installation does not include support for cross-platform development. For cross-platform development, see the CUDA Cross-Platform Environment section. 8.2. Installation  Perform the pre-installation actions . Disable the Nouveau drivers . Reboot into text mode (runlevel 3). This can usually be accomplished by adding the number “3” to the end of the system’s kernel boot parameters. Since the NVIDIA drivers are not yet installed, the text terminals may not display correctly. Temporarily adding “nomodeset” to the system’s kernel boot parameters may fix this issue. Consult your system’s bootloader documentation for information on how to make the above boot parameter changes. The reboot is required to completely unload the Nouveau drivers and prevent the graphical interface from loading. The CUDA driver cannot be installed while the Nouveau drivers are loaded or while the graphical interface is active. Verify that the Nouveau drivers are not loaded. If the Nouveau drivers are still loaded, consult your distribution’s documentation to see if further steps are needed to disable Nouveau. Run the installer and follow the on-screen prompts: sudo sh cuda_<version>_linux.run The installer will prompt for the following: EULA Acceptance CUDA Driver installation CUDA Toolkit installation, location, and /usr/local/cuda symbolic link The default installation location for the toolkit is /usr/local/cuda-12.4 : The /usr/local/cuda symbolic link points to the location where the CUDA Toolkit was installed. This link allows projects to use the latest CUDA Toolkit without any configuration file update. The installer must be executed with sufficient privileges to perform some actions. When the current privileges are insufficient to perform an action, the installer will ask for the user’s password to attempt to install with root privileges. Actions that cause the installer to attempt to install with root privileges are: installing the CUDA Driver installing the CUDA Toolkit to a location the user does not have permission to write to creating the /usr/local/cuda symbolic link Running the installer with sudo , as shown above, will give permission to install to directories that require root permissions. Directories and files created while running the installer with sudo will have root ownership. If installing the driver, the installer will also ask if the openGL libraries should be installed. If the GPU used for display is not an NVIDIA GPU, the NVIDIA openGL libraries should not be installed. Otherwise, the openGL libraries used by the graphics driver of the non-NVIDIA GPU will be overwritten and the GUI will not work. If performing a silent installation, the --no-opengl-libs option should be used to prevent the openGL libraries from being installed. See the Advanced Options section for more details. If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf , may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information. Note Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries. Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly. Perform the post-installation actions . 8.3. Disabling Nouveau  To install the Display Driver, the Nouveau drivers must first be disabled. Each distribution of Linux has a different method for disabling Nouveau. The Nouveau drivers are loaded if the following command prints anything: lsmod | grep nouveau 8.3.1. Fedora  Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the following command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system. 8.3.2. RHEL / Rocky and KylinOS  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force 8.3.3. OpenSUSE  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\noptions nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd 8.3.4. SLES  No actions to disable Nouveau are required as Nouveau is not installed on SLES. 8.3.5. WSL  No actions to disable Nouveau are required as Nouveau is not installed on WSL. 8.3.6. Ubuntu  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.3.7. Debian  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.4. Device Node Verification  Check that the device files /dev/nvidia* exist and have the correct (0666) file permissions. These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver. Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuid nvidia-modprobe tool that is bundled with the NVIDIA Driver. However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below: #!/bin/bash\n\n/sbin/modprobe nvidia\n\nif [ \"$?\" -eq 0 ]; then\n  # Count the number of NVIDIA controllers found.\n  NVDEVS=`lspci | grep -i NVIDIA`\n  N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l`\n  NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l`\n\n  N=`expr $N3D + $NVGA - 1`\n  for i in `seq 0 $N`; do\n    mknod -m 666 /dev/nvidia$i c 195 $i\n  done\n\n  mknod -m 666 /dev/nvidiactl c 195 255\n\nelse\n  exit 1\nfi\n\n/sbin/modprobe nvidia-uvm\n\nif [ \"$?\" -eq 0 ]; then\n  # Find out the major device number used by the nvidia-uvm driver\n  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`\n\n  mknod -m 666 /dev/nvidia-uvm c $D 0\nelse\n  exit 1\nfi 8.5. Advanced Options  Action Options Used Explanation Silent Installation --silent Required for any silent installation. Performs an installation with no further user-input and minimal command-line output based on the options provided below. Silent installations are useful for scripting the installation of CUDA. Using this option implies acceptance of the EULA. The following flags can be used to customize the actions taken during installation. At least one of --driver , --uninstall , and --toolkit must be passed if running with non-root permissions. --driver Install the CUDA Driver. --toolkit Install the CUDA Toolkit. --toolkitpath=<path> Install the CUDA Toolkit to the <path> directory. If not provided, the default path of /usr/local/cuda-12.4 is used. --defaultroot=<path> Install libraries to the <path> directory. If the <path> is not provided, then the default path of your distribution is used. This only applies to the libraries installed outside of the CUDA Toolkit path. Extraction --extract=<path> Extracts to the <path> the following: the driver runfile, the raw files of the toolkit to <path>. This is especially useful when one wants to install the driver using one or more of the command-line options provided by the driver installer which are not exposed in this installer. Overriding Installation Checks --override Ignores compiler, third-party library, and toolkit detection checks which would prevent the CUDA Toolkit from installing. No OpenGL Libraries --no-opengl-libs Prevents the driver installation from installing NVIDIA’s GL libraries. Useful for systems where the display is driven by a non-NVIDIA GPU. In such systems, NVIDIA’s GL libraries could prevent X from loading properly. No man pages --no-man-page Do not install the man pages under /usr/share/man . Overriding Kernel Source --kernel-source-path=<path> Tells the driver installation to use <path> as the kernel source directory when building the NVIDIA kernel module. Required for systems where the kernel source is installed to a non-standard location. Running nvidia-xconfig --run-nvidia-xconfig Tells the driver installation to run nvidia-xconfig to update the system X configuration file so that the NVIDIA X driver is used. The pre-existing X configuration file will be backed up. No nvidia-drm kernel module --no-drm Do not install the nvidia-drm kernel module. This option should only be used to work around failures to build or install the nvidia-drm kernel module on systems that do not need the provided features. Custom Temporary Directory Selection --tmpdir=<path> Performs any temporary actions within <path> instead of /tmp . Useful in cases where /tmp cannot be used (doesn’t exist, is full, is mounted with ‘noexec’, etc.). Show Installer Options --help Prints the list of command-line options to stdout. 8.6. Uninstallation  To uninstall the CUDA Toolkit, run the uninstallation script provided in the bin directory of the toolkit. By default, it is located in /usr/local/cuda-12.4/bin : sudo /usr/local/cuda-12.4/bin/cuda-uninstaller To uninstall the NVIDIA Driver, run nvidia-uninstall : sudo /usr/bin/nvidia-uninstall To enable the Nouveau drivers, remove the blacklist file created in the Disabling Nouveau section, and regenerate the kernel initramfs/initrd again as described in that section. 9. Conda Installation  This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia . 9.1. Conda Overview  The Conda installation installs the CUDA Toolkit. The installation steps are listed below. 9.2. Installing CUDA Using Conda  To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia 9.3. Uninstalling CUDA Using Conda  To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 9.4. Installing Previous CUDA Releases  All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as: conda install cuda -c nvidia/label/cuda-11.3.0 9.5. Upgrading from cudatoolkit Package  If you had previously installed CUDA using the cudatoolkit package and want to maintain a similar install footprint, you can limit your installation to the following packages: cuda-libraries-dev cuda-nvcc cuda-nvtx cuda-cupti Note Some extra files, such as headers, will be included in this installation which were not included in the cudatoolkit package. If you need to reduce your installation further, replace cuda-libraries-dev with the specific libraries you need. 10. Pip Wheels  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. python3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.org/simple Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia-<library> Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cuda-runtime-cu12 nvidia-cuda-cccl-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-opencl-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cublas-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 These metapackages install the following packages: nvidia-cuda-runtime-cu125 nvidia-cuda-cccl-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-opencl-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 11. Tarball and Zip Archive Deliverables  In an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community CI/CD systems, tarball and zip archives are available for each component. These tarball and zip archives, known as binary archives, are provided at https://developer.download.nvidia.com/compute/cuda/redist/ . These component .tar.xz and .zip binary archives do not replace existing packages such as .deb, .rpm, runfile, conda, etc. and are not meant for general consumption, as they are not installers. However this standardized approach will replace existing .txz archives. For each release, a JSON manifest is provided such as redistrib_11.4.2.json , which corresponds to the CUDA 11.4.2 release label (CUDA 11.4 update 2) which includes the release date, the name of each component, license name, relative URL for each platform and checksums. Package maintainers are advised to check the provided LICENSE for each component prior to redistribution. Instructions for developers using CMake and Bazel build systems are provided in the next sections. 11.1. Parsing Redistrib JSON  The following example of a JSON manifest contains keys for each component: name, license, version, and a platform array which includes relative_path, sha256, md5, and size (bytes) for each archive. {\n    \"release_date\": \"2021-09-07\",\n    \"cuda_cudart\": {\n        \"name\": \"CUDA Runtime (cudart)\",\n        \"license\": \"CUDA Toolkit\",\n        \"version\": \"11.4.108\",\n        \"linux-x86_64\": {\n            \"relative_path\": \"cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-11.4.108-archive.tar.xz\",\n            \"sha256\": \"d08a1b731e5175aa3ae06a6d1c6b3059dd9ea13836d947018ea5e3ec2ca3d62b\",\n            \"md5\": \"da198656b27a3559004c3b7f20e5d074\",\n            \"size\": \"828300\"\n        },\n        \"linux-ppc64le\": {\n            \"relative_path\": \"cuda_cudart/linux-ppc64le/cuda_cudart-linux-ppc64le-11.4.108-archive.tar.xz\",\n            \"sha256\": \"831dffe062ae3ebda3d3c4010d0ee4e40a01fd5e6358098a87bb318ea7c79e0c\",\n            \"md5\": \"ca73328e3f8e2bb5b1f2184c98c3a510\",\n            \"size\": \"776840\"\n        },\n        \"linux-sbsa\": {\n            \"relative_path\": \"cuda_cudart/linux-sbsa/cuda_cudart-linux-sbsa-11.4.108-archive.tar.xz\",\n            \"sha256\": \"2ab9599bbaebdcf59add73d1f1a352ae619f8cb5ccec254093c98efd4c14553c\",\n            \"md5\": \"aeb5c19661f06b6398741015ba368102\",\n            \"size\": \"782372\"\n        },\n        \"windows-x86_64\": {\n            \"relative_path\": \"cuda_cudart/windows-x86_64/cuda_cudart-windows-x86_64-11.4.108-archive.zip\",\n            \"sha256\": \"b59756c27658d1ea87a17c06d064d1336576431cd64da5d1790d909e455d06d3\",\n            \"md5\": \"7f6837a46b78198402429a3760ab28fc\",\n            \"size\": \"2897751\"\n        }\n    }\n} A JSON schema is provided at https://developer.download.nvidia.com/compute/redist/redistrib-v2.schema.json . A sample script that parses these JSON manifests is available on GitHub : Downloads each archive Validates SHA256 checksums Extracts archives Flattens into a collapsed directory structure Table 7 Available Tarball and Zip Archives  Product Example CUDA Toolkit ./parse_redist.py --product cuda --label 12.3.2 cuBLASMp ./parse_redist.py --product cublasmp --label 0.1.0 cuDNN ./parse_redist.py --product cudnn --label 8.9.6.50 cuDSS ./parse_redist.py --product cudss --label 0.1.0 cuQuantum ./parse_redist.py --product cuquantum --label 23.10.0 cuSPARSELt ./parse_redist.py --product cusparselt --label 0.5.2 cuTENSOR ./parse_redist.py --product cutensor --label 1.7.0 NVIDIA driver ./parse_redist.py --product nvidia-driver --label 535.129.03 nvJPEG2000 ./parse_redist.py --product nvjpeg2000 --label 0.7.5 NVPL ./parse_redist.py --product nvpl --label 23.11 nvTIFF ./parse_redist.py --product nvtiff --label 0.3.0 11.2. Importing Tarballs into CMake  The recommended module for importing these tarballs into the CMake build system is via FindCUDAToolkit (3.17 and newer). Note The FindCUDA module is deprecated. The path to the extraction location can be specified with the CUDAToolkit_ROOT environmental variable. For example CMakeLists.txt and commands, see cmake/1_FindCUDAToolkit/ . For older versions of CMake, the ExternalProject_Add module is an alternative method. For example CMakeLists.txt file and commands, see cmake/2_ExternalProject/ . 11.3. Importing Tarballs into Bazel  The recommended method of importing these tarballs into the Bazel build system is using http_archive and pkg_tar . For an example, see bazel/1_pkg_tar/ . 12. CUDA Cross-Platform Environment  Cross development for arm64-sbsa is supported on Ubuntu 20.04, Ubuntu 22.04, RHEL 8, RHEL 9, and SLES 15. Cross development for arm64-Jetson is only supported on Ubuntu 20.04 We recommend selecting a host development environment that matches the supported cross-target environment. This selection helps prevent possible host/target incompatibilities, such as GCC or GLIBC version mismatches. 12.1. CUDA Cross-Platform Installation  Some of the following steps may have already been performed as part of the native Ubuntu installation . Such steps can safely be skipped. These steps should be performed on the x86_64 host system, rather than the target system. To install the native CUDA Toolkit on the target system, refer to the native Ubuntu installation section. Perform the pre-installation actions. Install repository meta-data package with: sudo dpkg -i cuda-repo-cross-<identifier>_all.deb where <identifier> indicates the operating system, architecture, and/or the version of the package. Update the Apt repository cache: sudo apt-get update Install the appropriate cross-platform CUDA Toolkit: For aarch64: sudo apt-get install cuda-cross-aarch64 For QNX: sudo apt-get install cuda-cross-qnx Perform the post-installation actions. 12.2. CUDA Cross-Platform Samples  CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples. 13. Post-installation Actions  The post-installation actions must be manually performed. These actions are split into mandatory, recommended, and optional sections. 13.1. Mandatory Actions  Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used. 13.1.1. Environment Setup  The PATH variable needs to include export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} . Nsight Compute has moved to /opt/nvidia/nsight-compute/ only in rpm/deb installation method. When using .run installer it is still located under /usr/local/cuda-12.4/ . To add this path to the PATH variable: export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} In addition, when using the runfile installation method, the LD_LIBRARY_PATH variable needs to contain /usr/local/cuda-12.4/lib64 on a 64-bit system, or /usr/local/cuda-12.4/lib on a 32-bit system To change the environment variables for 64-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64\\\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} To change the environment variables for 32-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\\\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Note that the above paths change when using a custom install path with the runfile installation method. 13.2. Recommended Actions  Other actions are recommended to verify the integrity of the installation. 13.2.1. Install Persistence Daemon  NVIDIA is providing a user-space daemon on Linux to support persistence of driver state across CUDA job runs. The daemon approach provides a more elegant and robust solution to this problem than persistence mode. For more details on the NVIDIA Persistence Daemon, see the documentation here . The NVIDIA Persistence Daemon can be started as the root user by running: /usr/bin/nvidia-persistenced --verbose This command should be run on boot. Consult your Linux distribution’s init documentation for details on how to automate this. 13.2.2. Install Writable Samples  CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples. 13.2.3. Verify the Installation  Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the sample programs, located in https://github.com/nvidia/cuda-samples . Note Ensure the PATH and, if using the runfile installation method, LD_LIBRARY_PATH variables are set correctly . 13.2.3.1. Verify the Driver Version  If you installed the driver, verify that the correct version of it is loaded. If you did not install the driver, or are using an operating system where the driver is not loaded via a kernel module, such as L4T, skip this step. When the driver is loaded, the driver version can be found by executing the command cat /proc/driver/nvidia/version Note that this command will not work on an iGPU/dGPU system. 13.2.3.2. Running the Binaries  After compilation, find and run deviceQuery from https://github.com/nvidia/cuda-samples . If the CUDA software is installed and configured correctly, the output for deviceQuery should look similar to that shown in Figure 1 . Figure 1 Figure 1. Valid Results from deviceQuery CUDA Sample  The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found (the first highlighted line), that the device matches the one on your system (the second highlighted line), and that the test passed (the final highlighted line). If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, this likely means that the /dev/nvidia* files are missing or have the wrong permissions. On systems where SELinux is enabled, you might need to temporarily disable this security feature to run deviceQuery . To do this, type: setenforce 0 from the command line as the superuser. Running the bandwidthTest program ensures that the system and the CUDA-capable device are able to communicate correctly. Its output is shown in Figure 2 . Figure 2 Figure 2. Valid Results from bandwidthTest CUDA Sample  Note that the measurements for your CUDA-capable device description will vary from system to system. The important point is that you obtain measurements, and that the second-to-last line (in Figure 2 ) confirms that all necessary tests passed. Should the tests not pass, make sure you have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed. If you run into difficulties with the link step (such as libraries not being found), consult the Linux Release Notes found in https://github.com/nvidia/cuda-samples . 13.2.4. Install Nsight Eclipse Plugins  To install Nsight Eclipse plugins, an installation script is provided: /usr/local/cuda-12.4/bin/nsight_ee_plugins_manage.sh install <eclipse-dir> Refer to Nsight Eclipse Plugins Installation Guide for more details. 13.2.5. Local Repo Removal  Removal of the local repo installer is recommended after installation of CUDA SDK . Ubuntu and Debian sudo apt-get remove --purge \"cuda-repo-<distro>-X-Y-local*\" Fedora sudo dnf remove \"cuda-repo-<distro>-X-Y-local*\" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove \"cuda-repo-<distro>-X-Y-local*\" openSUSE 15 and SLES 15 sudo zypper remove \"cuda-repo-<distro>-X-Y-local*\" Removal of the local repo installer is recommended after installation of NVIDA driver . Ubuntu and Debian sudo apt-get remove --purge \"nvidia-driver-local-repo-<distro>*\" Fedora sudo dnf remove \"nvidia-driver-local-repo-<distro>*\" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove \"nvidia-driver-local-repo-<distro>*\" openSUSE 15 and SLES 15 sudo zypper remove \"nvidia-driver-local-repo-<distro>*\" 13.3. Optional Actions  Other options are not necessary to use the CUDA Toolkit, but are available to provide additional features. 13.3.1. Install Third-party Libraries  Some CUDA samples use third-party libraries which may not be installed by default on your system. These samples attempt to detect any required libraries when building. If a library is not detected, it waives itself and warns you which library is missing. To build and run these samples, you must install the missing libraries. In cases where these dependencies are not installed, follow the instructions below. RHEL 8 / Rocky Linux 8 sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\n    make mesa-libGLU-devel freeimage-devel libglfw3-devel RHEL 9 / Rocky Linux 9 sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\n                    make mesa-libGLU-devel freeimage-devel libglfw3-devel KylinOS 10 sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\n                    make mesa-libGLU-devel freeimage-devel libglfw3-devel Fedora sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\n    make mesa-libGLU-devel freeimage-devel libglfw3-devel SLES sudo zypper install libglut3 libX11-devel libXi6 libXmu6 libGLU1 make OpenSUSE sudo zypper install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\n    make Mesa-libGL-devel freeimage-devel Ubuntu sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \\\n    libxmu-dev libxi-dev libglu1-mesa-dev libfreeimage-dev libglfw3-dev Debian sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \\\n    libxmu-dev libxi-dev libglu1-mesa-dev libfreeimage-dev libglfw3-dev 13.3.2. Install the Source Code for cuda-gdb  The cuda-gdb source must be explicitly selected for installation with the runfile installation method. During the installation, in the component selection page, expand the component “CUDA Tools 12.4” and select cuda-gdb-src for installation. It is unchecked by default. To obtain a copy of the source code for cuda-gdb using the RPM and Debian installation methods, the cuda-gdb-src package must be installed. The source code is installed as a tarball in the /usr/local/cuda-12.4/extras directory. 13.3.3. Select the Active Version of CUDA  For applications that rely on the symlinks /usr/local/cuda and /usr/local/cuda-MAJOR , you may wish to change to a different installed version of CUDA using the provided alternatives. To show the active version of CUDA and all available versions: update-alternatives --display cuda To show the active minor version of a given major CUDA release: update-alternatives --display cuda-12 To update the active version of CUDA: sudo update-alternatives --config cuda 14. Advanced Setup  Below is information on some advanced setup scenarios which are not covered in the basic instructions above. Table 8 Advanced Setup Scenarios when Installing CUDA  Scenario Instructions Install CUDA using the Package Manager installation method without installing the NVIDIA GL libraries. Fedora Install CUDA using the following command: sudo dnf install cuda-toolkit-12-4 \\\n    nvidia-driver-cuda akmod-nvidia Follow the instructions here to ensure that Nouveau is disabled. If performing an upgrade over a previous installation, the NVIDIA kernel module may need to be rebuilt by following the instructions here . OpenSUSE/SLES On some system configurations the NVIDIA GL libraries may need to be locked before installation using: sudo zypper addlock nvidia-glG04 Install CUDA using the following command: sudo zypper install --no-recommends cuda-toolkit-12-4 \\\n    nvidia-computeG04 \\\n    nvidia-gfxG04-kmp-default Follow the instructions here to ensure that Nouveau is disabled. Ubuntu This functionality isn’t supported on Ubuntu. Instead, the driver packages integrate with the Bumblebee framework to provide a solution for users who wish to control what applications the NVIDIA drivers are used for. See Ubuntu’s Bumblebee wiki for more information. Upgrade from a RPM/Deb driver installation which includes the diagnostic driver packages to a driver installation which does not include the diagnostic driver packages. RHEL/CentOS Remove diagnostic packages using the following command: sudo yum remove cuda-drivers-diagnostic \\\n    xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal. Fedora Remove diagnostic packages using the following command: sudo dnf remove cuda-drivers-diagnostic \\\n    xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal. OpenSUSE/SLES Remove diagnostic packages using the following command: sudo zypper remove cuda-drivers-diagnostic \\\n    nvidia-diagnosticG04 Follow the instructions here to continue installation as normal. Ubuntu Remove diagnostic packages using the following command: sudo apt-get purge cuda-drivers-diagnostic \\\n    nvidia-384-diagnostic Follow the instructions here to continue installation as normal. Use a specific GPU for rendering the display. Add or replace a Device entry in your xorg.conf file, located at /etc/X11/xorg.conf . The Device entry should resemble the following: Section \"Device\"\n    Identifier    \"Device0\"\n    Driver        \"driver_name\"\n    VendorName    \"vendor_name\"\n    BusID         \"bus_id\"\nEndSection The details will you will need to add differ on a case-by-case basis. For example, if you have two NVIDIA GPUs and you want the first GPU to be used for display, you would replace “ driver_name ” with “ nvidia ”, “ vendor_name ” with “ NVIDIA Corporation ” and “ bus_id ” with the Bus ID of the GPU.\nThe Bus ID will resemble “PCI:00:02.0” and can be found by running lspci . Install CUDA to a specific directory using the Package Manager installation method. RPM The RPM packages don’t support custom install locations through the package managers (Yum and Zypper), but it is possible to install the RPM packages to a custom location using rpm’s --relocate parameter: sudo rpm --install --relocate /usr/local/cuda-12.4=/new/toolkit package.rpm You will need to install the packages in the correct dependency order; this task is normally taken care of by the package managers. For example, if package “foo” has a dependency on package “bar”, you should install package “bar” first, and package “foo” second. You can check the dependencies of a RPM package as follows: rpm -qRp package.rpm Note that the driver packages cannot be relocated. Deb The Deb packages do not support custom install locations. It is however possible to extract the contents of the Deb packages and move the files to the desired install location. See the next scenario for more details one xtracting Deb packages. Extract the contents of the installers. Runfile The Runfile can be extracted into the standalone Toolkit and Driver Runfiles by using the --extract parameter. The Toolkit standalone Runfiles can be further extracted by running: ./runfile.run --tar mxvf The Driver Runfile can be extracted by running: ./runfile.run -x RPM The RPM packages can be extracted by running: rpm2cpio package.rpm | cpio -idmv Deb The Deb packages can be extracted by running: dpkg-deb -x package.deb output_dir Modify Ubuntu’s apt package manager to query specific architectures for specific repositories. This is useful when a foreign architecture has been added, causing “404 Not Found” errors to appear when the repository meta-data is updated. Each repository you wish to restrict to specific architectures must have its sources.list entry modified. This is done by modifying the /etc/apt/sources.list file and any files containing repositories you wish to restrict under the /etc/apt/sources.list.d/ directory. Normally, it is sufficient to modify only the entries in /etc/apt/sources.list An architecture-restricted repository entry looks like: deb [arch=<arch1>,<arch2>] <url> For example, if you wanted to restrict a repository to only the amd64 and i386 architectures, it would look like: deb [arch=amd64,i386] <url> It is not necessary to restrict the deb-src repositories, as these repositories don’t provide architecture-specific packages. For more details, see the sources.list manpage. The nvidia.ko kernel module fails to load, saying some symbols are unknown. For example: nvidia: Unknown symbol drm_open (err 0) Check to see if there are any optionally installable modules that might provide these symbols which are not currently installed. For the example of the drm_open symbol, check to see if there are any packages which provide drm_open and are not already installed. For instance, on Ubuntu 14.04, the linux-image-extra package provides the DRM kernel module (which provides drm_open ). This package is optional even though the kernel headers reflect the availability of DRM regardless of whether this package is installed or not. The runfile installer fails to extract due to limited space in the TMP directory. This can occur on systems with limited storage in the TMP directory (usually /tmp ), or on systems which use a tmpfs in memory to handle temporary storage. In this case, the --tmpdir command-line option should be used to instruct the runfile to use a directory with sufficient space to extract into. More information on this option can be found here . Re-enable Wayland after installing the RPM driver on Fedora. Wayland is disabled during installation of the Fedora driver RPM due to compatability issues. To re-enable Wayland, comment out this line in /etc/gdm/custom.conf : WaylandEnable=false In case of the error: E: Failed to fetch file:/var/cuda-repo File not found Debian and Ubuntu This can occur when installing CUDA after uninstalling a different version. Use the following command before installation: sudo rm -v /var/lib/apt/lists/*cuda* /var/lib/apt/lists/*nvidia* Verbose installation on Debian and Ubuntu Use the --verbose-versions flag, for example: sudo apt-get install --verbose-versions cuda 15. Frequently Asked Questions  15.1. How do I install the Toolkit in a different location?  The Runfile installation asks where you wish to install the Toolkit during an interactive install. If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location: ./runfile.run --silent \\\n                --toolkit --toolkitpath=/my/new/toolkit The RPM and Deb packages cannot be installed to a custom install location directly using the package managers. See the “Install CUDA to a specific directory using the Package Manager installation method” scenario in the Advanced Setup section for more information. 15.2. Why do I see “nvcc: No such file or directory” when I try to build a CUDA application?  Your PATH environment variable is not set up correctly. Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12.4/bin . export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} 15.3. Why do I see “error while loading shared libraries: <lib name>: cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library?  Your LD_LIBRARY_PATH environment variable is not set up correctly. Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12.4/lib{,64} : export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\\\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 15.4. Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu?  These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the system’s sources.list file. Repositories that do not host packages for the newly added architecture will present this error. While noisy, the error itself does no harm. Please see the Advanced Setup section for details on how to modify your sources.list file to prevent these errors. 15.5. How can I tell X to ignore a GPU for compute-only use?  To make sure X doesn’t use a certain GPU for display, you need to specify which other GPU to use for display. For more information, please refer to the “Use a specific GPU for rendering the display” scenario in the Advanced Setup section. 15.6. Why doesn’t the cuda-repo package install the CUDA Toolkit and Drivers?  When using RPM or Deb, the downloaded package is a repository package. Such a package only informs the package manager where to find the actual installation packages, but will not install them. See the Package Manager Installation section for more details. 15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04?  After installing CUDA, set the driver value for the intel device in /etc/X11/xorg.conf to ‘ modesetting ’ as shown below: Section \"Device\"\n    Identifier \"intel\"\n    Driver \"modesetting\"\n    ...\nEndSection To prevent Ubuntu from reverting the change in xorg.conf, edit /etc/default/grub to add “ nogpumanager ” to GRUB_CMDLINE_LINUX_DEFAULT. Run the following command to update grub before rebooting: sudo update-grub 15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update?  System updates may include an updated Linux kernel. In many cases, a new Linux kernel will be installed without properly updating the required Linux kernel headers and development packages. To ensure the CUDA driver continues to work when performing a system update, rerun the commands in the Kernel Headers and Development Packages section. Additionally, on Fedora, the Akmods framework will sometimes fail to correctly rebuild the NVIDIA kernel module packages when a new Linux kernel is installed. When this happens, it is usually sufficient to invoke Akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting: sudo akmods --force\nsudo depmod You can reach a virtual console by hitting ctrl+alt+f2 at the same time. 15.9. How do I install a CUDA driver with a version less than 367 using a network repo?  To install a CUDA driver at a version earlier than 367 using a network repo, the required packages will need to be explicitly installed at the desired version. For example, to install 352.99, instead of installing the cuda-drivers metapackage at version 352.99, you will need to install all required packages of cuda-drivers at version 352.99. 15.10. How do I install an older CUDA version using a network repo?  Depending on your system configuration, you may not be able to install old versions of CUDA using the cuda metapackage. In order to install a specific version of CUDA, you may need to specify all of the packages that would normally be installed by the cuda metapackage at the version you want to install. If you are using yum to install certain packages at an older version, the dependencies may not resolve as expected. In this case you may need to pass “ --setopt=obsoletes=0 ” to yum to allow an install of packages which are obsoleted at a later version than you are trying to install. 15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency?  This dependency comes from the SUSE repositories and shouldn’t affect the use of the NVIDIA driver or the CUDA Toolkit. To disable this dependency, you can lock that package with the following command: sudo zypper al Mesa-dri-nouveau 15.12. How do I handle “Errors were encountered while processing: glx-diversions”?  This sometimes occurs when trying to uninstall CUDA after a clean .deb installation. Run the following commands: sudo apt-get install glx-diversions --reinstall\nsudo apt-get remove nvidia-alternative Then re-run the commands from Removing CUDA Toolkit and Driver . 16. Additional Considerations  Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C++ Programming Guide, located in /usr/local/cuda-12.4/doc . A number of helpful development tools are included in the CUDA Toolkit to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK. For technical support on programming questions, consult and participate in the developer forums at https://forums.developer.nvidia.com/c/accelerated-computing/cuda/206 . 17. Switching between Driver Module Flavors  Use the following steps to switch between the NVIDIA driver legacy and open module flavors on your system. Note If switching to open module, experimental support for GeForce and Quadro SKUs can be enabled with: echo \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe.d/nvidia-gsp.conf Note Replace XXX with the NVIDIA driver branch number such as 550. Fedora, RHEL 9 / Rocky Linux 9, RHEL 8 / Rocky Linux 8 To switch between legacy and open: uninstall, then reinstall. Kylin OS To switch between legacy and open: uninstall, then reinstall. Ubuntu To switch from legacy to open: sudo apt-get --purge remove nvidia-kernel-source-XXX\nsudo apt-get install --verbose-versions nvidia-kernel-open-XXX\nsudo apt-get install --verbose-versions cuda-drivers-XXX To switch from open to legacy: sudo apt-get remove --purge nvidia-kernel-open-XXX\nsudo apt-get install --verbose-versions cuda-drivers-XXX Debian To switch from legacy to open: sudo apt-get --purge remove nvidia-kernel-dkms\nsudo apt-get install --verbose-versions nvidia-kernel-open-dkms\nsudo apt-get install --verbose-versions cuda-drivers-XXX To switch from open to legacy: sudo apt-get remove --purge nvidia-kernel-open-dkms\nsudo apt-get install --verbose-versions cuda-drivers-XXX OpenSUSE To switch from legacy to open: sudo zypper remove nvidia-driver-G06-kmp-default\nsudo zypper install --details nvidia-open-driver-G06-kmp-default\nsudo zypper install --details cuda-drivers-XXX To switch from open to legacy: sudo zypper remove nvidia-open-driver-G06-kmp-default\nsudo zypper install --details cuda-drivers-XXX SLES To switch from legacy to open: sudo zypper remove nvidia-driver-G06-kmp-default nvidia-driver-G06-kmp-azure\nsudo zypper install --details nvidia-open-driver-G06-kmp-default nvidia-open-driver-G06-kmp-azure\nsudo zypper install --details cuda-drivers-XXX To switch from open to legacy: sudo zypper remove nvidia-open-driver-G06-kmp-default nvidia-driver-G06-open-kmp-azure\nsudo zypper install --details cuda-drivers-XXX Note The Azure package is only available for SLES (x86_64). 18. Removing CUDA Toolkit and Driver  Follow the below steps to properly uninstall the CUDA Toolkit and NVIDIA Drivers from your system. These steps will ensure that the uninstallation will be clean. KylinOS 10 To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\n                    \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver RHEL 9 / Rocky Linux 9 To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver RHEL 8 / Rocky Linux 8 To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver Fedora To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver To remove 3rd party NVIDIA Drivers: sudo dnf remove \"*nvidia*\" OpenSUSE / SLES To remove CUDA Toolkit: sudo zypper remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo zypper remove \"*nvidia*\" Ubuntu and Debian To remove CUDA Toolkit: sudo apt-get --purge remove \"*cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo apt-get --purge remove \"*nvidia*\" \"libxnvctrl*\" To clean up the uninstall: sudo apt-get autoremove 19. Notices  19.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 19.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 19.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 20. Copyright  © 2009-2024 NVIDIA Corporation & affiliates. All rights reserved. This product includes software developed by the Syncro Soft SRL ( http://www.sync.ro/ ). Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html", "content_type": "text/html", "text": "CUDA Installation Guide for Microsoft Windows 1. Introduction 1.1. System Requirements 1.2. About This Document 2. Installing CUDA Development Tools 2.1. Verify You Have a CUDA-Capable GPU 2.2. Download the NVIDIA CUDA Toolkit 2.3. Install the CUDA Software 2.3.1. Uninstalling the CUDA Software 2.4. Using Conda to Install the CUDA Software 2.4.1. Conda Overview 2.4.2. Installation 2.4.3. Uninstallation 2.4.4. Installing Previous CUDA Releases 2.5. Use a Suitable Driver Model 2.6. Verify the Installation 2.6.1. Running the Compiled Examples 3. Pip Wheels 4. Compiling CUDA Programs 4.1. Compiling Sample Projects 4.2. Sample Projects 4.3. Build Customizations for New Projects 4.4. Build Customizations for Existing Projects 5. Additional Considerations 6. Notices 6.1. Notice 6.2. OpenCL 6.3. Trademarks Installation Guide Windows » 1. Introduction v12.5 | PDF | Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems. 1. Introduction  CUDA ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU). CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources. CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements  To use CUDA on your system, you will need the following installed: A CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) Supported Microsoft Windows ® operating systems: Microsoft Windows 11 21H2 Microsoft Windows 11 22H2-SV2 Microsoft Windows 11 23H2 Microsoft Windows 10 21H2 Microsoft Windows 10 22H2 Microsoft Windows Server 2022 Table 1 Windows Compiler Support in CUDA 12.5  Compiler* IDE Native x86_64 Cross-compilation (32-bit on 64-bit) C++ Dialect MSVC Version 193x Visual Studio 2022 17.x YES Not supported C++14 (default), C++17, C++20 MSVC Version 192x Visual Studio 2019 16.x YES C++14 (default), C++17 MSVC Version 191x Visual Studio 2017 15.x (RTW and all updates) YES C++14 (default), C++17 * Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5. 32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries  on  GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications. Support for running x86 32-bit applications on x86_64 Windows is limited to use with: CUDA Driver CUDA Runtime (cudart) CUDA Math Library (math.h) 1.2. About This Document  This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation. 2. Installing CUDA Development Tools  Basic instructions can be found in the Quick Start Guide . Read on for more detailed instructions. The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps: Verify the system has a CUDA-capable GPU. Download the NVIDIA CUDA Toolkit. Install the NVIDIA CUDA Toolkit. Test that the installed software runs correctly and communicates with the hardware. 2.1. Verify You Have a CUDA-Capable GPU  You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager . Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus , that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products. The Windows Device Manager can be opened via the following steps: Open a run window from the Start Menu Run: control /name Microsoft.DeviceManager 2.2. Download the NVIDIA CUDA Toolkit  The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads . Choose the platform you are using and one of the following installer formats: Network Installer: A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time. Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment. The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources. Download Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. 2.3. Install the CUDA Software  Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality. Note The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit. Note The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again. Graphical Installation Install the CUDA Software by executing the CUDA installer and following the on-screen prompts. Silent Installation The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names. Table 2 Possible Subpackage Names  Subpackage Name Subpackage Description Toolkit Subpackages (defaults to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5) cuda_profiler_api_12.5 CUDA Profiler API. cudart_12.5 CUDA Runtime libraries. cuobjdump_12.5 Extracts information from cubin files. cupti_12.5 The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications. cuxxfilt_12.5 The CUDA cu++ filt demangler tool. demo_suite_12.5 Prebuilt demo applications using CUDA. documentation_12.5 CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc. nvcc_12.5 CUDA compiler. nvdisasm_12.5 Extracts information from standalone cubin files. nvfatbin_12.5 Library for creating fatbinaries at runtime. nvjitlink_12.5 nvJitLink library. nvml_dev_12.5 NVML development libraries and headers. nvprof_12.5 Tool for collecting and viewing CUDA application profiling data from the command-line. nvprune_12.5 Prunes host object files and libraries to only contain device code for the specified targets. nvrtc_12.5 nvrtc_dev_12.5 NVRTC runtime libraries. nvtx_12.5 NVTX on Windows. opencl_12.5 OpenCL library. visual_profiler_12.5 Visual Profiler. sanitizer_12.5 Compute Sanitizer API. thrust_12.5 CUDA Thrust. cublas_12.5 cublas_dev_12.5 cuBLAS runtime libraries. cufft_12.5 cufft_dev_12.5 cuFFT runtime libraries. curand_12.5 curand_dev_12.5 cuRAND runtime libraries. cusolver_12.5 cusolver_dev_12.5 cuSOLVER runtime libraries. cusparse_12.5 cusparse_dev_12.5 cuSPARSE runtime libraries. npp_12.5 npp_dev_12.5 NPP runtime libraries. nvjpeg_12.5 nvjpeg_dev_12.5 nvJPEG libraries. nsight_compute_12.5 Nsight Compute. nsight_systems_12.5 Nsight Systems. nsight_vse_12.5 Installs the Nsight Visual Studio Edition plugin in all VS. occupancy_calculator_12.5 Installs the CUDA_Occupancy_Calculator.xls tool. visual_studio_integration_12.5 Installs CUDA project wizard and builds customization files in VS. Driver Subpackages Display.Driver The NVIDIA Display Driver. Required to run CUDA applications. For example, to install only the compiler and driver components: <PackageName>.exe -s nvcc_12.1 Display.Driver Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required. Extracting and Inspecting the Files Manually Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip . Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files. Note Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment. 2.3.1. Uninstalling the CUDA Software  All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget. 2.4. Using Conda to Install the CUDA Software  This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia . 2.4.1. Conda Overview  The Conda installation installs the CUDA Toolkit. The installation steps are listed below. 2.4.2. Installation  To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda - c nvidia 2.4.3. Uninstallation  To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 2.4.4. Installing Previous CUDA Releases  All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as: conda install cuda - c nvidia / label / cuda -11.3.0 Note Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as: conda install cuda - c nvidia / label / cuda -11.3.0 - c nvidia / label / cuda -11.3.1 This example will install all packages released as part of CUDA 11.3.1. 2.5. Use a Suitable Driver Model  On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate: The WDDM driver model is used for display devices. The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model. TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details). Note Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device. Note NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode. 2.6. Verify the Installation  Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs. 2.6.1. Running the Compiled Examples  The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to: Start > All Programs > Accessories > Command Prompt CUDA Samples are located in https://github.com/nvidia/cuda-samples . To use the samples, clone the project, build the samples, and run them using the instructions on the Github page. To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder. This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1 . Figure 1 Valid Results from deviceQuery CUDA Sample  The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed. If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed. Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2 . Figure 2 Valid Results from bandwidthTest CUDA Sample  The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed. If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed. To see a graphical representation of what CUDA can do, run the particles sample at https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles 3. Pip Wheels  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. py -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia-<library> Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 nvidia-opencl-cu12 These metapackages install the following packages: nvidia-cublas-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 nvidia-opencl-cu125 4. Compiling CUDA Programs  The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code. To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples 4.1. Compiling Sample Projects  The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest . If you elected to use the default installation location, the output is placed in CUDA Samples\\v12.5\\bin\\win64\\Release . Build the program using the appropriate solution file and run the executable. If all works correctly, the output should be similar to Figure 2 . 4.2. Sample Projects  The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects. A few of the example projects require some additional setup. These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are. The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process. Table 3 CUDA Visual Studio .props locations  Visual Studio CUDA 12.5 .props file Install Directory Visual Studio 2015 (deprecated) C:Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\BuildCustomizations Visual Studio 2017 <Visual Studio Install Dir>\\Common7\\IDE\\VC\\VCTargets\\BuildCustomizations Visual Studio 2019 C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Microsoft\\VC\\v160\\BuildCustomizations Visual Studio 2022 C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations You can reference this CUDA 12.5.props file when building your own CUDA applications. 4.3. Build Customizations for New Projects  When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Project… NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the “CUDA 12.5 Runtime” template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIA’s Build Customizations. All standard capabilities of Visual Studio C++ projects will be available. To specify a custom CUDA Toolkit location, under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations. Note A supported version of MSVC must be installed to use this feature. 4.4. Build Customizations for Existing Projects  When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods: Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizations… , then select the CUDA Toolkit version you would like to target. Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties . Under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer. While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2. If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps: Open a run window from the Start Menu. Run: control sysdm.cpl Select the Advanced tab at the top of the window. Click Environment Variables at the bottom of the window. Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item , selecting NVIDIA CUDA 12.5\\CodeCUDA C/C++ File , and then selecting the file you wish to add. For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following: msbuild <projectname.extension> /t:Rebuild /p:CudaToolkitDir=\"drive:/path/to/new/toolkit/\" 5. Additional Considerations  Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C Programming Guide, located in the CUDA Toolkit documentation directory. A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Visual Studio Edition, and NVIDIA Visual Profiler. For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/ . 6. Notices  6.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 6.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html", "content_type": "text/html", "text": "1. Introduction — Quick Start Guide 12.5 documentation 1. Introduction 2. Windows 2.1. Network Installer 2.2. Local Installer 2.3. Pip Wheels - Windows 2.4. Conda 3. Linux 3.1. Linux x86_64 3.1.1. Redhat / CentOS 3.1.1.1. RPM Installer 3.1.1.2. Runfile Installer 3.1.2. Fedora 3.1.2.1. RPM Installer 3.1.2.2. Runfile Installer 3.1.3. SUSE Linux Enterprise Server 3.1.3.1. RPM Installer 3.1.3.2. Runfile Installer 3.1.4. OpenSUSE 3.1.4.1. RPM Installer 3.1.4.2. Runfile Installer 3.1.5. Amazon Linux 2023 3.1.5.1. Prepare Amazon Linux 2023 3.1.5.2. Local Repo Installation for Amazon Linux 3.1.5.3. Network Repo Installation for Amazon Linux 3.1.5.4. Common Installation Instructions for Amazon Linux 3.1.6. Pip Wheels - Linux 3.1.7. Conda 3.1.8. WSL 3.1.9. Ubuntu 3.1.9.1. Debian Installer 3.1.9.2. Runfile Installer 3.1.10. Debian 3.1.10.1. Debian Installer 3.1.10.2. Runfile Installer 4. Notices 4.1. Notice 4.2. OpenCL 4.3. Trademarks Quick Start Guide » 1. Introduction v12.5 | PDF | Archive CUDA Quick Start Guide Minimal first-steps instructions to get CUDA running on a standard system. 1. Introduction  This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform. These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide . The CUDA installation packages can be found on the CUDA Downloads Page . 2. Windows  When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide . 2.1. Network Installer  Perform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to download and install all components. Once the download completes, the installation will begin automatically. Once the installation completes, click “next” to acknowledge the Nsight Visual Studio Edition installation summary. Click close to close the installer. Navigate to the Samples’ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody . Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln . Open the Build menu within Visual Studio and click Build Solution . Navigate to the CUDA Samples build directory and run the nbody sample. Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 2.2. Local Installer  Perform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to install all components. Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary. Click close to close the installer. Navigate to the Samples’ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody . Open the nbody Visual Studio solution file for the version of Visual Studio you have installed. Open the Build menu within Visual Studio and click Build Solution . Navigate to the CUDA Samples build directory and run the nbody sample. Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 2.3. Pip Wheels - Windows  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. py -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia-<library> Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 2.4. Conda  The Conda packages are available at https://anaconda.org/nvidia . Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3. Linux  CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on. 3.1. Linux x86_64  For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details. 3.1.1. Redhat / CentOS  When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide . 3.1.1.1. RPM Installer  Perform the following steps to install CUDA and verify the installation. Install EPEL to satisfy the DKMS dependency by following the instructions at EPEL’s website . Enable optional repos : On RHEL 8 Linux only, execute the following steps to enable optional repositories. On x86_64 workstation: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms\r\nsubscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms\r\nsubscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Install the repository meta-data, clean the yum cache, and install CUDA: sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm\r\nsudo rpm --erase gpg-pubkey-7fa2af80*\r\nsudo yum clean expire-cache\r\nsudo yum install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.1.2. Runfile Installer  Perform the following steps to install CUDA and verify the installation. Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\r\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda_<version>_linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.2. Fedora  When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide . 3.1.2.1. RPM Installer  Perform the following steps to install CUDA and verify the installation. Install the RPMFusion free repository to satisfy the Akmods dependency: su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm' Install the repository meta-data, clean the dnf cache, and install CUDA: sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm\r\nsudo rpm --erase gpg-pubkey-7fa2af80*\r\nsudo dnf clean expire-cache\r\nsudo dnf install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.2.2. Runfile Installer  Perform the following steps to install CUDA and verify the installation. Disable the Nouveau drivers: Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\r\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the below command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda_<version>_linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface. Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.3. SUSE Linux Enterprise Server  When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide . 3.1.3.1. RPM Installer  Perform the following steps to install CUDA and verify the installation. Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA: sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm\r\nsudo SUSEConnect --product PackageHub/15/x86_64\r\nsudo zypper refresh\r\nsudo rpm --erase gpg-pubkey-7fa2af80*\r\nsudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo\r\nsudo zypper install cuda Add the user to the video group: sudo usermod -a -G video <username> Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.3.2. Runfile Installer  Perform the following steps to install CUDA and verify the installation. Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda_<version>_linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.4. OpenSUSE  When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide . 3.1.4.1. RPM Installer  Perform the following steps to install CUDA and verify the installation. Install the repository meta-data, refresh the Zypper cache, and install CUDA: sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm\r\nsudo rpm --erase gpg-pubkey-7fa2af80*\r\nsudo zypper refresh\r\nsudo zypper install cuda Add the user to the video group: sudo usermod -a -G video <username> Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.4.2. Runfile Installer  Perform the following steps to install CUDA and verify the installation. Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\r\noptions nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda_<version>_linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                    ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.5. Amazon Linux 2023  3.1.5.1. Prepare Amazon Linux 2023  Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo . 3.1.5.2. Local Repo Installation for Amazon Linux  Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm 3.1.5.3. Network Repo Installation for Amazon Linux  Enable the network repository and clean the DN cache: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo\r\nsudo dnf clean expire-cache 3.1.5.4. Common Installation Instructions for Amazon Linux  These instructions apply to both local and network installation for Amazon Linux. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms\r\nsudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions. 3.1.6. Pip Wheels - Linux  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. python3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia-<library> Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 nvidia-opencl-cu12 nvidia-nvjitlink-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 nvidia-opencl-cu125 nvidia-nvjitlink-cu125 3.1.7. Conda  The Conda packages are available at https://anaconda.org/nvidia . Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.1.8. WSL  These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case. Install repository meta-data sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb Update the CUDA public GPG key sudo apt-key del 7fa2af80 When installing using the local repo: sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/ When installing using the network repo: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb\r\nsudo dpkg -i cuda-keyring_1.1-1_all.deb Pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<architecture>/cuda-<distro>.pin\r\nsudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600 Update the Apt repository cache and install CUDA sudo apt-get update\r\nsudo apt-get install cuda 3.1.9. Ubuntu  When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the Debian installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide . 3.1.9.1. Debian Installer  Perform the following steps to install CUDA and verify the installation. Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA: sudo dpkg --install cuda-repo-<distro>-<version>.<architecture>.deb\r\nsudo apt-key del 7fa2af80\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb\r\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\r\nsudo add-apt-repository contrib\r\nsudo apt-get update\r\nsudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.9.2. Runfile Installer  Perform the following steps to install CUDA and verify the installation. Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\r\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda_<version>_linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.10. Debian  When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Linux Installation Guide . 3.1.10.1. Debian Installer  Perform the following steps to install CUDA and verify the installation. Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA: sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub\r\nsudo apt-key del 7fa2af80\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb\r\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\r\nsudo add-apt-repository contrib\r\nsudo apt-get update\r\nsudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 3.1.10.2. Runfile Installer  Perform the following steps to install CUDA and verify the installation. Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau\r\noptions nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda_<version>_linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 4. Notices  4.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 4.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 4.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jun 25, 2024."}, {"url": "https://docs.nvidia.com/cuda/eula/index.html", "parent_url": "https://docs.nvidia.com/cuda/eula/index.html", "content_type": "text/html", "text": "EULA 1. License Agreement for NVIDIA Software Development Kits 1.1. License 1.1.1. License Grant 1.1.2. Distribution Requirements 1.1.3. Authorized Users 1.1.4. Pre-Release SDK 1.1.5. Updates 1.1.6. Components Under Other Licenses 1.1.7. Reservation of Rights 1.2. Limitations 1.3. Ownership 1.4. No Warranties 1.5. Limitation of Liability 1.6. Termination 1.7. General 2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits 2.1. License Scope 2.2. Distribution 2.3. Operating Systems 2.4. Audio and Video Encoders and Decoders 2.5. Licensing 2.6. Attachment A 2.7. Attachment B EULA » 1. License Agreement for NVIDIA Software Development Kits v12.5 | PDF | Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. Last updated: October 8, 2021. Preface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein. NVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs. NVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references. Default Install Location of CUDA Toolkit Windows platform: %ProgramFiles%\\NVIDIA GPU Computing Toolkit\\CUDA\\v#.# Linux platform: /usr/local/cuda-#.# Mac platform: /Developer/NVIDIA/CUDA-#.# NVIDIA CUDA Samples Description CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit. NVIDIA Nsight Visual Studio Edition (Windows only) Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications. Default Install Location of Nsight Visual Studio Edition Windows platform: %ProgramFiles(x86)%\\NVIDIA Corporation\\Nsight Visual Studio Edition #.# 1. License Agreement for NVIDIA Software Development Kits  Important Notice—Read before downloading, installing, copying or using the licensed software: This license agreement, including exhibits attached (“Agreement”) is a legal agreement between you and NVIDIA Corporation (“NVIDIA”) and governs your use of a NVIDIA software development kit (“SDK”). Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation. This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used. If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case “you” will mean the entity you represent. If you don’t have the required age or authority to accept this Agreement, or if you don’t accept all the terms and conditions of this Agreement, do not download, install or use the SDK. You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions. 1.1. License  1.1.1. License Grant  Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to: Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement. 1.1.2. Distribution Requirements  These are the distribution requirements for you to exercise the distribution grant: Your application must have material additional functionality, beyond the included portions of the SDK. The distributable portions of the SDK shall only be accessed by your application. The following notice shall be included in modifications and derivative works of sample source code distributed: “This software contains source code provided by NVIDIA Corporation.” Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only. The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA’s intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users. You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK. 1.1.3. Authorized Users  You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf. If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network. You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didn’t follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences. 1.1.4. Pre-Release SDK  The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss. You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems. NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability. 1.1.5. Updates  NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK. 1.1.6. Components Under Other Licenses  The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict. Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses. 1.1.7. Reservation of Rights  NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement. 1.2. Limitations  The following license limitations apply to your use of the SDK: You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK. Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product. Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA. You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK. You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be: Disclosed or distributed in source code form; Licensed for the purpose of making derivative works; or Redistributable at no charge. You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a “Critical Application”). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements. You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney’s fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms. You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform. 1.3. Ownership  NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2 . This SDK may include software and materials from NVIDIA’s licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights. You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIA’s rights under Section 1.3.1 . You may, but don’t have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com . 1.4. No Warranties  THE SDK IS PROVIDED BY NVIDIA “AS IS” AND “WITH ALL FAULTS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE. 1.5. Limitation of Liability  TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIA’S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT. These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different. 1.6. Termination  This Agreement will continue to apply until terminated by either you or NVIDIA as described below. If you want to terminate this Agreement, you may do so by stopping to use the SDK. NVIDIA may, at any time, terminate this Agreement if: (i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIA’s intellectual property rights); (ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or (iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIA’s sole discretion, the continued use of it is no longer commercially viable. Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions. 1.7. General  If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified. You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement. This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language. The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction. If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. Unless otherwise specified, remedies are cumulative. Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement. The SDK has been developed entirely at private expense and is “commercial items” consisting of “commercial computer software” and “commercial computer software documentation” provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051. The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasury’s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law. Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department. This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties. 2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits  The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (“Agreement”) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement. This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern. 2.1. License Scope  The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs. 2.2. Distribution  The portions of the SDK that are distributable under the Agreement are listed in Attachment A. 2.3. Operating Systems  Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files). 2.4. Audio and Video Encoders and Decoders  You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders. 2.5. Licensing  If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions @ nvidia . com . 2.6. Attachment A  The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable. Component CUDA Runtime Windows cudart.dll, cudart_static.lib, cudadevrt.lib Mac OSX libcudart.dylib, libcudart_static.a, libcudadevrt.a Linux libcudart.so, libcudart_static.a, libcudadevrt.a Android libcudart.so, libcudart_static.a, libcudadevrt.a Component CUDA FFT Library Windows cufft.dll, cufftw.dll, cufft.lib, cufftw.lib Mac OSX libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a Linux libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Android libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Component CUDA BLAS Library Windows cublas.dll, cublasLt.dll Mac OSX libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a Linux libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Android libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Component NVIDIA “Drop-in” BLAS Library Windows nvblas.dll Mac OSX libnvblas.dylib Linux libnvblas.so Component CUDA Sparse Matrix Library Windows cusparse.dll, cusparse.lib Mac OSX libcusparse.dylib, libcusparse_static.a Linux libcusparse.so, libcusparse_static.a Android libcusparse.so, libcusparse_static.a Component CUDA Linear Solver Library Windows cusolver.dll, cusolver.lib Mac OSX libcusolver.dylib, libcusolver_static.a Linux libcusolver.so, libcusolver_static.a Android libcusolver.so, libcusolver_static.a Component CUDA Random Number Generation Library Windows curand.dll, curand.lib Mac OSX libcurand.dylib, libcurand_static.a Linux libcurand.so, libcurand_static.a Android libcurand.so, libcurand_static.a Component NVIDIA Performance Primitives Library Windows nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib Mac OSX libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a Linux libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Android libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Component NVIDIA JPEG Library Windows nvjpeg.lib, nvjpeg.dll Linux libnvjpeg.so, libnvjpeg_static.a Component Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP Mac OSX libculibos.a Linux libculibos.a Component NVIDIA Runtime Compilation Library and Header All nvrtc.h Windows nvrtc.dll, nvrtc-builtins.dll Mac OSX libnvrtc.dylib, libnvrtc-builtins.dylib Linux libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a Component NVIDIA Optimizing Compiler Library Windows nvvm.dll Mac OSX libnvvm.dylib Linux libnvvm.so Component NVIDIA JIT Linking Library Windows libnvJitLink.dll, libnvJitLink.lib Linux libnvJitLink.so, libnvJitLink_static.a Component NVIDIA Common Device Math Functions Library Windows libdevice.10.bc Mac OSX libdevice.10.bc Linux libdevice.10.bc Component CUDA Occupancy Calculation Header Library All cuda_occupancy.h Component CUDA Half Precision Headers All cuda_fp16.h, cuda_fp16.hpp Component CUDA Profiling Tools Interface (CUPTI) Library Windows cupti.dll Mac OSX libcupti.dylib Linux libcupti.so Component NVIDIA Tools Extension Library Windows nvToolsExt.dll, nvToolsExt.lib Mac OSX libnvToolsExt.dylib Linux libnvToolsExt.so Component NVIDIA CUDA Driver Libraries Linux libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a Component NVIDIA CUDA File IO Libraries and Header All cufile.h Linux libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply: The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software. 2.7. Attachment B  Additional Licensing Obligations The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions: Licensee’s use of the GDB third party component is subject to the terms and conditions of GNU GPL v3: This product includes copyrighted third-party software licensed\nunder the terms of the GNU General Public License v3 (\"GPL v3\").\nAll third-party software packages are copyright by their respective\nauthors. GPL v3 terms and conditions are hereby incorporated into\nthe Agreement by this reference: http://www.gnu.org/licenses/gpl.txt Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests @ nvidia . com . This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION. Component          License\nCUDA-GDB           GPL v3 Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licensee’s use of the H.264 video codecs are solely the responsibility of Licensee. Licensee’s use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. All third-party software packages are copyright by their respective authors. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries. Boost Software License - Version 1.0 - August 17th, 2003\n. . . .\n\nPermission is hereby granted, free of charge, to any person or\norganization obtaining a copy of the software and accompanying\ndocumentation covered by this license (the \"Software\") to use,\nreproduce, display, distribute, execute, and transmit the Software,\nand to prepare derivative works of the Software, and to permit\nthird-parties to whom the Software is furnished to do so, all\nsubject to the following:\n\nThe copyright notices in the Software and this entire statement,\nincluding the above license grant, this restriction and the following\ndisclaimer, must be included in all copies of the Software, in whole\nor in part, and all derivative works of the Software, unless such\ncopies or derivative works are solely in the form of machine-executable\nobject code generated by a source language processor.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND\nNON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR\nANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR\nOTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE. Licensee’s use of the LLVM third party component is subject to the following terms and conditions: ======================================================\nLLVM Release License\n======================================================\nUniversity of Illinois/NCSA\nOpen Source License\n\nCopyright (c) 2003-2010 University of Illinois at Urbana-Champaign.\nAll rights reserved.\n\nDeveloped by:\n\n    LLVM Team\n\n    University of Illinois at Urbana-Champaign\n\n    http://llvm.org\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to\ndeal with the Software without restriction, including without limitation the\nrights to use, copy, modify, merge, publish, distribute, sublicense, and/or\nsell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n*  Redistributions of source code must retain the above copyright notice,\n   this list of conditions and the following disclaimers.\n\n*  Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimers in the\n   documentation and/or other materials provided with the distribution.\n\n*  Neither the names of the LLVM Team, University of Illinois at Urbana-\n   Champaign, nor the names of its contributors may be used to endorse or\n   promote products derived from this Software without specific prior\n   written permission.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\nTHE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS WITH THE SOFTWARE. Licensee’s use of the PCRE third party component is subject to the following terms and conditions: ------------\nPCRE LICENCE\n------------\nPCRE is a library of functions to support regular expressions whose syntax\nand semantics are as close as possible to those of the Perl 5 language.\nRelease 8 of PCRE is distributed under the terms of the \"BSD\" licence, as\nspecified below. The documentation for PCRE, supplied in the \"doc\"\ndirectory, is distributed under the same terms as the software itself. The\nbasic library functions are written in C and are freestanding. Also\nincluded in the distribution is a set of C++ wrapper functions, and a just-\nin-time compiler that can be used to optimize pattern matching. These are\nboth optional features that can be omitted when the library is built.\n\nTHE BASIC LIBRARY FUNCTIONS\n---------------------------\nWritten by:       Philip Hazel\nEmail local part: ph10\nEmail domain:     cam.ac.uk\nUniversity of Cambridge Computing Service,\nCambridge, England.\nCopyright (c) 1997-2012 University of Cambridge\nAll rights reserved.\n\nPCRE JUST-IN-TIME COMPILATION SUPPORT\n-------------------------------------\nWritten by:       Zoltan Herczeg\nEmail local part: hzmester\nEmain domain:     freemail.hu\nCopyright(c) 2010-2012 Zoltan Herczeg\nAll rights reserved.\n\nSTACK-LESS JUST-IN-TIME COMPILER\n--------------------------------\nWritten by:       Zoltan Herczeg\nEmail local part: hzmester\nEmain domain:     freemail.hu\nCopyright(c) 2009-2012 Zoltan Herczeg\nAll rights reserved.\n\nTHE C++ WRAPPER FUNCTIONS\n-------------------------\nContributed by:   Google Inc.\nCopyright (c) 2007-2012, Google Inc.\nAll rights reserved. THE \"BSD\" LICENCE\n-----------------\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n  * Redistributions of source code must retain the above copyright notice,\n    this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n  * Neither the name of the University of Cambridge nor the name of Google\n    Inc. nor the names of their contributors may be used to endorse or\n    promote products derived from this software without specific prior\n    written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2007-2009, Regents of the University of California\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the following\n      disclaimer in the documentation and/or other materials provided\n      with the distribution.\n    * Neither the name of the University of California, Berkeley nor\n      the names of its contributors may be used to endorse or promote\n      products derived from this software without specific prior\n      written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\nSTRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\nIN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the following\n      disclaimer in the documentation and/or other materials provided\n      with the distribution.\n    * The name of the author may not be used to endorse or promote\n      products derived from this software without specific prior\n      written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\nSTRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\nIN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2010 The University of Tennessee.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the following\n      disclaimer listed in this license in the documentation and/or\n      other materials provided with the distribution.\n    * Neither the name of the copyright holders nor the names of its\n      contributors may be used to endorse or promote products derived\n      from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2012, The Science and Technology Facilities Council (STFC).\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the following\n      disclaimer in the documentation and/or other materials provided\n      with the distribution.\n    * Neither the name of the STFC nor the names of its contributors\n      may be used to endorse or promote products derived from this\n      software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE STFC BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\nBUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\nOR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\nIF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows: -- (C) Copyright 2013 King Abdullah University of Science and Technology\n Authors:\n Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa)\n David Keyes (david.keyes@kaust.edu.sa)\n Hatem Ltaief (hatem.ltaief@kaust.edu.sa)\n\n Redistribution  and  use  in  source and binary forms, with or without\n modification,  are  permitted  provided  that the following conditions\n are met:\n\n * Redistributions  of  source  code  must  retain  the above copyright\n   notice,  this  list  of  conditions  and  the  following  disclaimer.\n * Redistributions  in  binary  form must reproduce the above copyright\n   notice,  this list of conditions and the following disclaimer in the\n   documentation  and/or other materials provided with the distribution.\n * Neither  the  name of the King Abdullah University of Science and\n   Technology nor the names of its contributors may be used to endorse\n   or promote products derived from this software without specific prior\n   written permission.\n\n THIS  SOFTWARE  IS  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n ``AS IS''  AND  ANY  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n A  PARTICULAR  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT NOT\n LIMITED  TO,  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n DATA,  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n THEORY  OF  LIABILITY,  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n OF  THIS  SOFTWARE,  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows: Copyright (c) 2012, University of Illinois.\n\nAll rights reserved.\n\nDeveloped by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal with the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the following\n      disclaimers in the documentation and/or other materials provided\n      with the distribution.\n    * Neither the names of IMPACT Group, University of Illinois, nor\n      the names of its contributors may be used to endorse or promote\n      products derived from this Software without specific prior\n      written permission.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR\nIN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE\nSOFTWARE. Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license: Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima\nUniversity. All rights reserved.\n\nCopyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima\nUniversity and University of Tokyo.  All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the following\n      disclaimer in the documentation and/or other materials provided\n      with the distribution.\n    * Neither the name of the Hiroshima University nor the names of\n      its contributors may be used to endorse or promote products\n      derived from this software without specific prior written\n      permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license: Copyright 2010-2011, D. E. Shaw Research.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions, and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions, and the following\n      disclaimer in the documentation and/or other materials provided\n      with the distribution.\n    * Neither the name of D. E. Shaw Research nor the names of its\n      contributors may be used to endorse or promote products derived\n      from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license: Copyright (c) 2015-2017, Norbert Juffa\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Licensee’s use of the lz4 third party component is subject to the following terms and conditions: Copyright (C) 2011-2013, Yann Collet.\nBSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The NPP library uses code from the Boost Math Toolkit, and is subject to the following license: Boost Software License - Version 1.0 - August 17th, 2003\n. . . .\n\nPermission is hereby granted, free of charge, to any person or\norganization obtaining a copy of the software and accompanying\ndocumentation covered by this license (the \"Software\") to use,\nreproduce, display, distribute, execute, and transmit the Software,\nand to prepare derivative works of the Software, and to permit\nthird-parties to whom the Software is furnished to do so, all\nsubject to the following:\n\nThe copyright notices in the Software and this entire statement,\nincluding the above license grant, this restriction and the following\ndisclaimer, must be included in all copies of the Software, in whole\nor in part, and all derivative works of the Software, unless such\ncopies or derivative works are solely in the form of machine-executable\nobject code generated by a source language processor.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND\nNON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR\nANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR\nOTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE. Portions of the Nsight Eclipse Edition is subject to the following license: The Eclipse Foundation makes available all content in this plug-in\n(\"Content\"). Unless otherwise indicated below, the Content is provided\nto you under the terms and conditions of the Eclipse Public License\nVersion 1.0 (\"EPL\"). A copy of the EPL is available at http://\nwww.eclipse.org/legal/epl-v10.html. For purposes of the EPL, \"Program\"\nwill mean the Content.\n\nIf you did not receive this Content directly from the Eclipse\nFoundation, the Content is being redistributed by another party\n(\"Redistributor\") and different terms and conditions may apply to your\nuse of any object code in the Content. Check the Redistributor's\nlicense that was provided with the Content. If no such license exists,\ncontact the Redistributor. Unless otherwise indicated below, the terms\nand conditions of the EPL still apply to any source code in the\nContent and such source code may be obtained at http://www.eclipse.org. Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license: License URL\nhttps://github.com/openai/openai-gemm/blob/master/LICENSE\n\nLicense Text\nThe MIT License\n\nCopyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE. Licensee’s use of the Visual Studio Setup Configuration Samples is subject to the following license: The MIT License (MIT)\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \"Software\"), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge,\npublish, distribute, sublicense, and/or sell copies of the Software,\nand to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Licensee’s use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0. The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license . Components of the driver and compiler used for binary management, including nvFatBin, nvcc,\nand cuobjdump, use the Zstandard library which is subject to the following license: BSD License\n\nFor Zstandard software\n\nCopyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted\nprovided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice, this\n      list of conditions and the following disclaimer.\n\n    * Redistributions in binary form must reproduce the above copyright notice,\n      this list of conditions and the following disclaimer in the documentation\n      and/or other materials provided with the distribution.\n\n    * Neither the name Facebook, nor Meta, nor the names of its contributors may\n      be used to endorse or promote products derived from this software without\n      specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-features-archive/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-features-archive/index.html", "content_type": "text/html", "text": "CUDA Features Archive 1. CUDA 11.6 Features 1.1. Compiler 1.1.1. VS2022 Support 1.1.2. New instructions in public PTX 1.1.3. Unused Kernel Optimization 1.1.4. New -arch=native option 1.1.5. Generate PTX from nvlink: 1.1.6. Bullseye support 1.1.7. INT128 developer tool support 2. Notices 2.1. Notice 2.2. OpenCL 2.3. Trademarks CUDA Features Archive » 1. CUDA 11.6 Features v12.5 | PDF | Archive NVIDIA CUDA Features Archive The list of CUDA features by release. 1. CUDA 11.6 Features  1.1. Compiler  1.1.1. VS2022 Support  CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here . A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it. 1.1.2. New instructions in public PTX  New instructions for bit mask creation—BMSK, and sign extension—SZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT . 1.1.3. Unused Kernel Optimization  In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an opt-in feature but in 11.6, this feature is enabled by default. As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations. $ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info 1.1.4. New -arch=native option  In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1. This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in. 1.1.5. Generate PTX from nvlink:  Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN: nvcc -dlto -dlink -ptx Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file. With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization. 1.1.6. Bullseye support  NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye. 1.1.7. INT128 developer tool support  In 11.5, CUDA C++ support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu++, int 128 data datype is supported by math functions. 2. Notices  2.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html", "content_type": "text/html", "text": "CUDA C++ Best Practices Guide 1. Preface 1.1. What Is This Document? 1.2. Who Should Read This Guide? 1.3. Assess, Parallelize, Optimize, Deploy 1.3.1. Assess 1.3.2. Parallelize 1.3.3. Optimize 1.3.4. Deploy 1.4. Recommendations and Best Practices 1.5. Assessing Your Application 2. Heterogeneous Computing 2.1. Differences between Host and Device 2.2. What Runs on a CUDA-Enabled Device? 3. Application Profiling 3.1. Profile 3.1.1. Creating the Profile 3.1.2. Identifying Hotspots 3.1.3. Understanding Scaling 3.1.3.1. Strong Scaling and Amdahl’s Law 3.1.3.2. Weak Scaling and Gustafson’s Law 3.1.3.3. Applying Strong and Weak Scaling 4. Parallelizing Your Application 5. Getting Started 5.1. Parallel Libraries 5.2. Parallelizing Compilers 5.3. Coding to Expose Parallelism 6. Getting the Right Answer 6.1. Verification 6.1.1. Reference Comparison 6.1.2. Unit Testing 6.2. Debugging 6.3. Numerical Accuracy and Precision 6.3.1. Single vs. Double Precision 6.3.2. Floating Point Math Is not Associative 6.3.3. IEEE 754 Compliance 6.3.4. x86 80-bit Computations 7. Optimizing CUDA Applications 8. Performance Metrics 8.1. Timing 8.1.1. Using CPU Timers 8.1.2. Using CUDA GPU Timers 8.2. Bandwidth 8.2.1. Theoretical Bandwidth Calculation 8.2.2. Effective Bandwidth Calculation 8.2.3. Throughput Reported by Visual Profiler 9. Memory Optimizations 9.1. Data Transfer Between Host and Device 9.1.1. Pinned Memory 9.1.2. Asynchronous and Overlapping Transfers with Computation 9.1.3. Zero Copy 9.1.4. Unified Virtual Addressing 9.2. Device Memory Spaces 9.2.1. Coalesced Access to Global Memory 9.2.1.1. A Simple Access Pattern 9.2.1.2. A Sequential but Misaligned Access Pattern 9.2.1.3. Effects of Misaligned Accesses 9.2.1.4. Strided Accesses 9.2.2. L2 Cache 9.2.2.1. L2 Cache Access Window 9.2.2.2. Tuning the Access Window Hit-Ratio 9.2.3. Shared Memory 9.2.3.1. Shared Memory and Memory Banks 9.2.3.2. Shared Memory in Matrix Multiplication (C=AB) 9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT) 9.2.3.4. Asynchronous Copy from Global Memory to Shared Memory 9.2.4. Local Memory 9.2.5. Texture Memory 9.2.5.1. Additional Texture Capabilities 9.2.6. Constant Memory 9.2.7. Registers 9.2.7.1. Register Pressure 9.3. Allocation 9.4. NUMA Best Practices 10. Execution Configuration Optimizations 10.1. Occupancy 10.1.1. Calculating Occupancy 10.2. Hiding Register Dependencies 10.3. Thread and Block Heuristics 10.4. Effects of Shared Memory 10.5. Concurrent Kernel Execution 10.6. Multiple contexts 11. Instruction Optimization 11.1. Arithmetic Instructions 11.1.1. Division Modulo Operations 11.1.2. Loop Counters Signed vs. Unsigned 11.1.3. Reciprocal Square Root 11.1.4. Other Arithmetic Instructions 11.1.5. Exponentiation With Small Fractional Arguments 11.1.6. Math Libraries 11.1.7. Precision-related Compiler Flags 11.2. Memory Instructions 12. Control Flow 12.1. Branching and Divergence 12.2. Branch Predication 13. Deploying CUDA Applications 14. Understanding the Programming Environment 14.1. CUDA Compute Capability 14.2. Additional Hardware Data 14.3. Which Compute Capability Target 14.4. CUDA Runtime 15. CUDA Compatibility Developer’s Guide 15.1. CUDA Toolkit Versioning 15.2. Source Compatibility 15.3. Binary Compatibility 15.3.1. CUDA Binary (cubin) Compatibility 15.4. CUDA Compatibility Across Minor Releases 15.4.1. Existing CUDA Applications within Minor Versions of CUDA 15.4.1.1. Handling New CUDA Features and Driver APIs 15.4.1.2. Using PTX 15.4.1.3. Dynamic Code Generation 15.4.1.4. Recommendations for building a minor-version compatible library 15.4.1.5. Recommendations for taking advantage of minor version compatibility in your application 16. Preparing for Deployment 16.1. Testing for CUDA Availability 16.2. Error Handling 16.3. Building for Maximum Compatibility 16.4. Distributing the CUDA Runtime and Libraries 16.4.1. CUDA Toolkit Library Redistribution 16.4.1.1. Which Files to Redistribute 16.4.1.2. Where to Install Redistributed CUDA Libraries 17. Deployment Infrastructure Tools 17.1. Nvidia-SMI 17.1.1. Queryable state 17.1.2. Modifiable state 17.2. NVML 17.3. Cluster Management Tools 17.4. Compiler JIT Cache Management Tools 17.5. CUDA_VISIBLE_DEVICES 18. Recommendations and Best Practices 18.1. Overall Performance Optimization Strategies 19. nvcc Compiler Switches 19.1. nvcc 20. Notices 20.1. Notice 20.2. OpenCL 20.3. Trademarks CUDA C++ Best Practices Guide » 1. Preface v12.5 | PDF | Archive CUDA C++ Best Practices Guide The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs. 1. Preface  1.1. What Is This Document?  This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA ® CUDA ® GPUs. It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored. As a result, it is recommended that first-time readers proceed through the guide sequentially. This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later. 1.2. Who Should Read This Guide?  The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code. This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website https://docs.nvidia.com/cuda/ . The following documents are especially important resources: CUDA Installation Guide CUDA C++ Programming Guide CUDA Toolkit Reference Manual In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide). 1.3. Assess, Parallelize, Optimize, Deploy  This guide introduces the Assess, Parallelize, Optimize, Deploy(APOD) design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible. APOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production. 1.3.1. Assess  For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration. By understanding the end-user’s requirements and constraints and by applying Amdahl’s and Gustafson’s laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application. 1.3.2. Parallelize  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS , cuFFT , or Thrust , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. On the other hand, some applications’ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput. 1.3.3. Optimize  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned. Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developer’s optimization efforts and provide references into the relevant portions of the optimization section of this guide. 1.3.4. Deploy  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application. 1.4. Recommendations and Best Practices  Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C++ code. These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority. Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied. This approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization. The criteria of benefit and scope for establishing priority will vary depending on the nature of the program. In this guide, they represent a typical case. Your code might reflect different priority factors. Regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items. Note Code samples throughout the guide omit error checking for conciseness. Production code should, however, systematically check the error code returned by each API call and check for failures in kernel launches by calling cudaGetLastError() . 1.5. Assessing Your Application  From supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance. The core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network. As a result, all modern processors require parallel code in order to achieve good utilization of their computational power. While processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using MPI). In order to profit from any modern processor architecture, GPUs included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future. 2. Heterogeneous Computing  CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices . While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. This capability makes them well suited to computations that can leverage parallel execution. However, the device is based on a distinctly different design from the host system, and it’s important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively. 2.1. Differences between Host and Device  The primary differences are in threading model and in separate physical memories: Threading resources Execution pipelines on host systems can support a limited number of concurrent threads. For example, servers that have two 32 core processors can run only 64 threads concurrently (or small multiple of that if the CPUs support simultaneous multithreading). By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads). Modern NVIDIA GPUs can support up to 2048 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads to more than 160,000 concurrently active threads. Threads Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches (when two threads are swapped) are therefore slow and expensive. By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work (in warps of 32 threads each). If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution. In short, CPU cores are designed to minimize latency for a small number of threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput . RAM The host system and the device each have their own distinct attached physical memories 1 . As the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in What Runs on a CUDA-Enabled Device? . These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming. Other differences are discussed as they arise elsewhere in this document. Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device. 2.2. What Runs on a CUDA-Enabled Device?  The following issues should be considered when determining what parts of an application to run on the device: The device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel. This typically involves arithmetic on large data sets (such as matrices) where the same operation can be performed across thousands, if not millions, of elements at the same time. This is a requirement for good performance on CUDA: the software must use a large number (generally thousands or tens of thousands) of concurrent threads. The support for running numerous threads in parallel derives from CUDA’s use of a lightweight threading model described above. To use CUDA, data values must be transferred from the host to the device. These transfers are costly in terms of performance and should be minimized. (See Data Transfer Between Host and Device .) This cost has several ramifications: The complexity of operations should justify the cost of moving data to and from the device. Code that transfers data for brief use by a small number of threads will see little or no performance benefit. The ideal scenario is one in which many threads perform a substantial amount of work. For example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit. The issue here is the number of operations performed per data element transferred. For the preceding procedure, assuming matrices of size NxN, there are N 2 operations (additions) and 3N 2 elements transferred, so the ratio of operations to elements transferred is 1:3 or O(1). Performance benefits can be more readily achieved when this ratio is higher. For example, a matrix multiplication of the same matrices requires N 3 operations (multiply-add), so the ratio of operations to elements transferred is O(N), in which case the larger the matrix the greater the performance benefit. The types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions. It is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device. Data should be kept on the device as long as possible. Because transfers should be minimized, programs that run multiple kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations. So, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in some subsequent calculation, the matrix addition should be performed locally on the device. This approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host. Even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory. Data Transfer Between Host and Device provides further details, including the measurements of bandwidth between the host and the device versus within the device proper. For best performance, there should be some coherence in memory access by adjacent threads running on the device. Certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation. Data that cannot be laid out so as to enable coalescing , or that doesn’t have enough locality to use the L1 or texture caches effectively, will tend to see lesser speedups when used in computations on GPUs. A noteworthy exception to this are completely random memory access patterns. In general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency. However, compared to cache based architectures, like CPUs, latency hiding architectures, like GPUs, tend to cope better with completely random memory access patterns. 1 On Systems on a Chip with integrated GPUs, such as NVIDIA® Tegra®, host and device memory are physically the same, but there is still a logical distinction between host and device memory. See the Application Note on CUDA for Tegra for details. 3. Application Profiling  3.1. Profile  Many codes accomplish a significant portion of the work with a relatively small amount of code. Using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization. 3.1.1. Creating the Profile  There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time. Note High Priority: To maximize developer productivity, profile the application to determine hotspots and bottlenecks. The most important consideration with any profiling activity is to ensure that the workload is realistic - i.e., that information gained from the test and decisions based upon that information are relevant to real data. Using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions. There are a number of tools that can be used to generate the profile. The following example is based on gprof , which is an open-source profiler for Linux platforms from the GNU Binutils collection. $ gcc -O2 -g -pg myprog.c\n$ gprof ./a.out > profile.txt\nEach sample counts as 0.01 seconds.\n  %   cumulative   self              self     total\n time   seconds   seconds    calls  ms/call  ms/call  name\n 33.34      0.02     0.02     7208     0.00     0.00  genTimeStep\n 16.67      0.03     0.01      240     0.04     0.12  calcStats\n 16.67      0.04     0.01        8     1.25     1.25  calcSummaryData\n 16.67      0.05     0.01        7     1.43     1.43  write\n 16.67      0.06     0.01                             mcount\n  0.00      0.06     0.00      236     0.00     0.00  tzset\n  0.00      0.06     0.00      192     0.00     0.00  tolower\n  0.00      0.06     0.00       47     0.00     0.00  strlen\n  0.00      0.06     0.00       45     0.00     0.00  strchr\n  0.00      0.06     0.00        1     0.00    50.00  main\n  0.00      0.06     0.00        1     0.00     0.00  memcpy\n  0.00      0.06     0.00        1     0.00    10.11  print\n  0.00      0.06     0.00        1     0.00     0.00  profil\n  0.00      0.06     0.00        1     0.00    50.00  report 3.1.2. Identifying Hotspots  In the example above, we can clearly see that the function genTimeStep() takes one-third of the total running time of the application. This should be our first candidate function for parallelization. Understanding Scaling discusses the potential benefit we might expect from such parallelization. It is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as calcStats() and calcSummaryData() . Parallelizing these functions as well should increase our speedup potential. However, since APOD is a cyclical process, we might opt to parallelize these functions in a subsequent APOD pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes. 3.1.3. Understanding Scaling  The amount of performance benefit an application will realize by running on CUDA depends entirely on the extent to which it can be parallelized. Code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device. Note High Priority: To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code. By understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy. Strong Scaling and Amdahl’s Law describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size. Weak Scaling and Gustafson’s Law describes weak scaling, where the speedup is attained by growing the problem size. In many applications, a combination of strong and weak scaling is desirable. 3.1.3.1. Strong Scaling and Amdahl’s Law  Strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system. An application that exhibits linear strong scaling has a speedup equal to the number of processors used. Strong scaling is usually equated with Amdahl’s Law, which specifies the maximum speedup that can be expected by parallelizing portions of a serial program. Essentially, it states that the maximum speedup S of a program is: \\(S = \\frac{1}{(1 - P) + \\frac{P}{N}}\\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs. The larger N is(that is, the greater the number of processors), the smaller the P/N fraction. It can be simpler to view N as a very large number, which essentially transforms the equation into \\(S = 1/(1 - P)\\) . Now, if 3/4 of the running time of a sequential program is parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4. In reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some degree of strong scaling. For most purposes, the key point is that the larger the parallelizable portion P is, the greater the potential speedup. Conversely, if P is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors N does little to improve performance. Therefore, to get the largest speedup for a fixed problem size, it is worthwhile to spend effort on increasing P , maximizing the amount of code that can be parallelized. 3.1.3.2. Weak Scaling and Gustafson’s Law  Weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size per processor ; i.e., where the overall problem size increases as the number of processors is increased. Weak scaling is often equated with Gustafson’s Law, which states that in practice, the problem size scales with the number of processors. Because of this, the maximum speedup S of a program is: \\(S = N + (1 - P)(1 - N)\\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs. Another way of looking at Gustafson’s Law is that it is not the problem size that remains constant as we scale up the system but rather the execution time. Note that Gustafson’s Law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem. 3.1.3.3. Applying Strong and Weak Scaling  Understanding which type of scaling is most applicable to an application is an important part of estimating speedup. For some applications the problem size will remain constant and hence only strong scaling is applicable. An example would be modeling how two molecules interact with each other, where the molecule sizes are fixed. For other applications, the problem size will grow to fill the available processors. Examples include modeling fluids or structures as meshes or grids and some Monte Carlo simulations, where increasing the problem size provides increased accuracy. Having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either Amdahl’s or Gustafson’s Law to determine an upper bound for the speedup. 4. Parallelizing Your Application  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS , cuFFT , or Thrust , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. On the other hand, some applications’ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput. 5. Getting Started  There are several key strategies for parallelizing sequential code. While the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore CPUs or for use on CUDA GPUs. 5.1. Parallel Libraries  The most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf. The CUDA Toolkit includes a number of such libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as cuBLAS , cuFFT , and so on. The key here is that libraries are most useful when they match well with the needs of the application. Applications already using other BLAS libraries can often quite easily switch to cuBLAS , for example, whereas applications that do little to no linear algebra will have little use for cuBLAS . The same goes for other CUDA Toolkit libraries: cuFFT has an interface similar to that of FFTW , etc. Also of note is the Thrust library, which is a parallel C++ template library similar to the C++ Standard Template Library. Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically. As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial. 5.2. Parallelizing Compilers  Another common approach to parallelization of sequential codes is to make use of parallelizing compilers. Often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself. By exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture. The OpenACC standard provides a set of compiler directives to specify loops and regions of code in standard C, C++ and Fortran that should be offloaded from a host CPU to an attached accelerator such as a CUDA GPU. The details of managing the accelerator device are handled implicitly by an OpenACC-enabled compiler and runtime. See http://www.openacc.org/ for details. 5.3. Coding to Expose Parallelism  For applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as CUDA C++ that integrate seamlessly with existing sequential code are essential. Once we have located a hotspot in our application’s profile assessment and determined that custom code is the best approach, we can use CUDA C++ to expose the parallelism in that portion of our code as a CUDA kernel. We can then launch this kernel onto the GPU and retrieve the results without requiring major rewrites to the rest of our application. This approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code. More difficult to parallelize are applications with a very flat profile - i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base. For the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, CPU and GPU alike, so it is well worth the effort should it become necessary. 6. Getting the Right Answer  Obtaining the right answer is clearly the principal goal of all computation. On parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming. These include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way CPU and GPU processors operate. This chapter examines issues that can affect the correctness of returned data and points to appropriate solutions. 6.1. Verification  6.1.1. Reference Comparison  A key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results. After each change is made, ensure that the results match using whatever criteria apply to the particular algorithm. Some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; see Numerical Accuracy and Precision regarding numerical accuracy. For other algorithms, implementations may be considered correct if they match the reference within some small epsilon. Note that the process used for validating numerical results can easily be extended to validate performance results as well. We want to ensure that each change we make is correct and that it improves performance (and by how much). Checking these things frequently as an integral part of our cyclical APOD process will help ensure that we achieve the desired results as rapidly as possible. 6.1.2. Unit Testing  A useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level. For example, we can write our CUDA kernels as a collection of many short __device__ functions rather than one large monolithic __global__ function; each device function can be tested independently before hooking them all together. For example, many kernels have complex addressing logic for accessing memory in addition to their actual computation. If we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts. (Note that the CUDA compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least write something out to global memory as a result of our addressing logic in order to successfully apply this strategy.) Going a step further, if most functions are defined as __host__ __device__ rather than just __device__ functions, then these functions can be tested on both the CPU and the GPU, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results. If there are differences, then those differences will be seen early and can be understood in the context of a simple function. As a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both CPU and GPU execution paths in our application: if the bulk of the work of our CUDA kernels is done in __host__ __device__ functions, we can easily call those functions from both the host code and the device code without duplication. 6.2. Debugging  CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see: https://developer.nvidia.com/cuda-gdb . The NVIDIA Nsight Visual Studio Edition for Microsoft Windows 7, Windows HPC Server 2008, Windows 8.1, and Windows 10 is available as a free plugin for Microsoft Visual Studio; see: https://developer.nvidia.com/nsight-visual-studio-edition . Several third-party debuggers support CUDA debugging as well; see: https://developer.nvidia.com/debugging-solutions for more details. 6.3. Numerical Accuracy and Precision  Incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored. The following sections explain the principal items of interest. Other peculiarities of floating-point arithmetic are presented in Features and Technical Specifications of the CUDA C++ Programming Guide as well as in a whitepaper and accompanying webinar on floating-point precision and performance available from https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus . 6.3.1. Single vs. Double Precision  Devices of compute capability 1.3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide). Results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues. Therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact. 6.3.2. Floating Point Math Is not Associative  Each floating-point arithmetic operation involves a certain amount of rounding. Consequently, the order in which arithmetic operations are performed is important. If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math. When you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results. This limitation is not specific to CUDA, but an inherent part of parallel computation on floating-point values. 6.3.3. IEEE 754 Compliance  All CUDA compute devices follow the IEEE 754 standard for binary floating-point representation, with some small exceptions. These exceptions, which are detailed in Features and Technical Specifications of the CUDA C++ Programming Guide, can lead to results that differ from IEEE 754 values computed on the host system. One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. Its result will often differ slightly from results obtained by doing the two operations separately. 6.3.4. x86 80-bit Computations  x86 processors can use an 80-bit double extended precision math when performing floating-point calculations. The results of these calculations can frequently differ from pure 64-bit operations performed on the CUDA device. To get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively). This is done with the FLDCW x86 assembly instruction or the equivalent operating system API. 7. Optimizing CUDA Applications  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned. Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developer’s optimization efforts and provide references into the relevant portions of the optimization section of this guide. 8. Performance Metrics  When attempting to optimize CUDA code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement. This chapter discusses how to correctly measure performance using CPU timers and CUDA events. It then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses. 8.1. Timing  CUDA calls and kernel executions can be timed using either CPU or GPU timers. This section examines the functionality, advantages, and pitfalls of both approaches. 8.1.1. Using CPU Timers  Any CPU timer can be used to measure the elapsed time of a CUDA call or kernel execution. The details of various CPU timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide. When using CPU timers, it is critical to remember that many CUDA API functions are asynchronous; that is, they return control back to the calling CPU thread prior to completing their work. All kernel launches are asynchronous, as are memory-copy functions with the Async suffix on their names. Therefore, to accurately measure the elapsed time for a particular call or sequence of CUDA calls, it is necessary to synchronize the CPU thread with the GPU by calling cudaDeviceSynchronize() immediately before starting and stopping the CPU timer. cudaDeviceSynchronize() blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed. Although it is also possible to synchronize the CPU thread with a particular stream or event on the GPU, these synchronization functions are not suitable for timing code in streams other than the default stream. cudaStreamSynchronize() blocks the CPU thread until all CUDA calls previously issued into the given stream have completed. cudaEventSynchronize() blocks until a given event in a particular stream has been recorded by the GPU. Because the driver may interleave execution of CUDA calls from other non-default streams, calls in other streams may be included in the timing. Because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream. Be aware that CPU-to-GPU synchronization points such as those mentioned in this section imply a stall in the GPU’s processing pipeline and should thus be used sparingly to minimize their performance impact. 8.1.2. Using CUDA GPU Timers  The CUDA event API provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds. How to time code using CUDA events illustrates their use. How to time code using CUDA events cudaEvent_t start , stop ; float time ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); cudaEventRecord ( start , 0 ); kernel <<< grid , threads >>> ( d_odata , d_idata , size_x , size_y , NUM_REPS ); cudaEventRecord ( stop , 0 ); cudaEventSynchronize ( stop ); cudaEventElapsedTime ( & time , start , stop ); cudaEventDestroy ( start ); cudaEventDestroy ( stop ); Here cudaEventRecord() is used to place the start and stop events into the default stream, stream 0. The device will record a timestamp for the event when it reaches that event in the stream. The cudaEventElapsedTime() function returns the time elapsed between the recording of the start and stop events. This value is expressed in milliseconds and has a resolution of approximately half a microsecond. Like the other calls in this listing, their specific operation, parameters, and return values are described in the CUDA Toolkit Reference Manual . Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent. 8.2. Bandwidth  Bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance. Almost all changes to code should be made in the context of how they affect bandwidth. As described in Memory Optimizations of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors. To measure performance accurately, it is useful to calculate theoretical and effective bandwidth. When the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it. Note High Priority: Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits. 8.2.1. Theoretical Bandwidth Calculation  Theoretical bandwidth can be calculated using hardware specifications available in the product literature. For example, the NVIDIA Tesla V100 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz and a 4096-bit-wide memory interface. Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s: \\(\\left. \\left( 0.877 \\times 10^{9} \\right. \\times (4096/8) \\times 2 \\right) \\div 10^{9} = 898\\text{GB/s}\\) In this calculation, the memory clock rate is converted in to Hz, multiplied by the interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate. Finally, this product is divided by 10 9 to convert the result to GB/s. Note Some calculations use 1024 3 instead of 10 9 for the final calculation. In such a case, the bandwidth would be 836.4 GiB/s. It is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid. Note On GPUs with GDDR memory with ECC enabled the available DRAM is reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled, though the exact impact of ECC on bandwidth can be higher and depends on the memory access pattern. HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection. 2 8.2.2. Effective Bandwidth Calculation  Effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program. To do so, use this equation: \\(\\text{Effective\\ bandwidth} = \\left( {\\left( B_{r} + B_{w} \\right) \\div 10^{9}} \\right) \\div \\text{time}\\) Here, the effective bandwidth is in units of GB/s, B r is the number of bytes read per kernel, B w is the number of bytes written per kernel, and time is given in seconds. For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used: \\(\\text{Effective\\ bandwidth} = \\left( {\\left( 2048^{2} \\times 4 \\times 2 \\right) \\div 10^{9}} \\right) \\div \\text{time}\\) The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the read and write), divided by 10 9 (or 1,024 3 ) to obtain GB of memory transferred. This number is divided by the time in seconds to obtain GB/s. 8.2.3. Throughput Reported by Visual Profiler  For devices with compute capability of 2.0 or greater, the Visual Profiler can be used to collect several different memory throughput measures. The following throughput metrics can be displayed in the Details or Detail Graphs view: Requested Global Load Throughput Requested Global Store Throughput Global Load Throughput Global Store Throughput DRAM Read Throughput DRAM Write Throughput The Requested Global Load Throughput and Requested Global Store Throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under Effective Bandwidth Calculation . Because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel. For global memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store Throughput values. It’s important to note that both numbers are useful. The actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (see Coalesced Access to Global Memory ). For global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global Memory Load Efficiency and Global Memory Store Efficiency metrics. 2 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory. 9. Memory Optimizations  Memory optimizations are the most important area for performance. The goal is to maximize the use of the hardware by maximizing bandwidth. Bandwidth is best served by using as much fast memory and as little slow-access memory as possible. This chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively. 9.1. Data Transfer Between Host and Device  The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory and device memory (16 GB/s on the PCIe x16 Gen3). Hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU. Note High Priority: Minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host CPU. Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory. Also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer. Finally, higher bandwidth between the host and the device is achieved when using page-locked (or pinned ) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section of this document. 9.1.1. Pinned Memory  Page-locked or pinned memory transfers attain the highest bandwidth between the host and the device. On PCIe x16 Gen3 cards, for example, pinned memory can attain roughly 12 GB/s transfer rates. Pinned memory is allocated using the cudaHostAlloc() functions in the Runtime API. The bandwidthTest CUDA Sample shows how to use these functions as well as how to measure memory transfer performance. For regions of system memory that have already been pre-allocated, cudaHostRegister() can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it. Pinned memory should not be overused. Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance. Furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters. 9.1.2. Asynchronous and Overlapping Transfers with Computation  Data transfers between the host and the device using cudaMemcpy() are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete. The cudaMemcpyAsync() function is a non-blocking variant of cudaMemcpy() in which control is returned immediately to the host thread. In contrast with cudaMemcpy() , the asynchronous transfer version requires pinned host memory (see Pinned Memory ), and it contains an additional argument, a stream ID. A stream is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device. Asynchronous transfers enable overlap of data transfers with computation in two different ways. On all CUDA-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations. For example, Overlapping computation and data transfers demonstrates how host computation in the routine cpuFunction() is performed while data is transferred to the device and a kernel using the device is executed. Overlapping computation and data transfers cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , 0 ); kernel <<< grid , block >>> ( a_d ); cpuFunction (); The last argument to the cudaMemcpyAsync() function is the stream ID, which in this case uses the default stream, stream 0. The kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed. Because the memory copy and the kernel both return control to the host immediately, the host function cpuFunction() overlaps their execution. In Overlapping computation and data transfers , the memory copy and kernel execution occur sequentially. On devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device. Whether a device has this capability is indicated by the asyncEngineCount field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample). On devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream IDs). Non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished. Concurrent copy and execute illustrates the basic technique. Concurrent copy and execute cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , stream1 ); kernel <<< grid , block , 0 , stream2 >>> ( otherData_d ); In this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of the cudaMemcpyAsync call and the kernel’s execution configuration. Concurrent copy and execute demonstrates how to overlap kernel execution with asynchronous data transfer. This technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives. Sequential copy and execute and Staged concurrent copy and execute demonstrate this. They produce equivalent results. The first segment shows the reference sequential implementation, which transfers and operates on an array of N floats (where N is assumed to be evenly divisible by nThreads). Sequential copy and execute cudaMemcpy ( a_d , a_h , N * sizeof ( float ), dir ); kernel <<< N / nThreads , nThreads >>> ( a_d ); Staged concurrent copy and execute shows how the transfer and kernel execution can be broken up into nStreams stages. This approach permits some overlapping of the data transfer and execution. Staged concurrent copy and execute size = N * sizeof ( float ) / nStreams ; for ( i = 0 ; i < nStreams ; i ++ ) { offset = i * N / nStreams ; cudaMemcpyAsync ( a_d + offset , a_h + offset , size , dir , stream [ i ]); kernel <<< N / ( nThreads * nStreams ), nThreads , 0 , stream [ i ] >>> ( a_d + offset ); } (In Staged concurrent copy and execute , it is assumed that N is evenly divisible by nThreads*nStreams .) Because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete. Current GPUs can simultaneously process asynchronous data transfers and execute kernels. GPUs with a single copy engine can perform one asynchronous data transfer and execute kernels whereas GPUs with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels. The number of copy engines on a GPU is given by the asyncEngineCount field of the cudaDeviceProp structure, which is also listed in the output of the deviceQuery CUDA Sample. (It should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous CUDA calls complete. It will not allow any other CUDA call to begin until it has completed.) A diagram depicting the timeline of execution for the two code segments is shown in Figure 1 , and nStreams is equal to 4 for Staged concurrent copy and execute in the bottom half of the figure. Timeline comparison for copy and kernel execution  Top Sequential Bottom Concurrent For this example, it is assumed that the data transfer and kernel execution times are comparable. In such cases, and when the execution time ( tE ) exceeds the transfer time ( tT ), a rough estimate for the overall time is tE + tT/nStreams for the staged version versus tE + tT for the sequential version. If the transfer time exceeds the execution time, a rough estimate for the overall time is tT + tE/nStreams . 9.1.3. Zero Copy  Zero copy is a feature that was added in version 2.2 of the CUDA Toolkit. It enables GPU threads to directly access host memory. For this purpose, it requires mapped pinned (non-pageable) memory. On integrated GPUs (i.e., GPUs with the integrated field of the CUDA device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated GPU and CPU memory are physically the same. On discrete GPUs, mapped pinned memory is advantageous only in certain cases. Because the data is not cached on the GPU, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced. Zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams. Note Low Priority: Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later. The host code in Zero-copy host code shows how zero copy is typically set up. Zero-copy host code float * a_h , * a_map ; ... cudaGetDeviceProperties ( & prop , 0 ); if ( ! prop . canMapHostMemory ) exit ( 0 ); cudaSetDeviceFlags ( cudaDeviceMapHost ); cudaHostAlloc ( & a_h , nBytes , cudaHostAllocMapped ); cudaHostGetDevicePointer ( & a_map , a_h , 0 ); kernel <<< gridSize , blockSize >>> ( a_map ); In this code, the canMapHostMemory field of the structure returned by cudaGetDeviceProperties() is used to check that the device supports mapping host memory to the device’s address space. Page-locked memory mapping is enabled by calling cudaSetDeviceFlags() with cudaDeviceMapHost . Note that cudaSetDeviceFlags() must be called prior to setting a device or making a CUDA call that requires state (that is, essentially, before a context is created). Page-locked mapped host memory is allocated using cudaHostAlloc() , and the pointer to the mapped device address space is obtained via the function cudaHostGetDevicePointer() . In the code in Zero-copy host code , kernel() can reference the mapped pinned host memory using the pointer a_map in exactly the same was as it would if a_map referred to a location in device memory. Note Mapped pinned host memory allows you to overlap CPU-GPU memory transfers with computation while avoiding the use of CUDA streams. But since any repeated access to such memory areas causes repeated CPU-GPU transfers, consider creating a second area in device memory to manually cache the previously read host memory data. 9.1.4. Unified Virtual Addressing  Devices of compute capability 2.0 and later support a special addressing mode called Unified Virtual Addressing (UVA) on 64-bit Linux and Windows. With UVA, the host memory and the device memories of all installed supported devices share a single virtual address space. Prior to UVA, an application had to keep track of which pointers referred to device memory (and for which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer. Using UVA, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using cudaPointerGetAttributes() . Under UVA, pinned host memory allocated with cudaHostAlloc() will have identical host and device pointers, so it is not necessary to call cudaHostGetDevicePointer() for such allocations. Host memory allocations pinned after-the-fact via cudaHostRegister() , however, will continue to have different device pointers than their host pointers, so cudaHostGetDevicePointer() remains necessary in that case. UVA is also a necessary precondition for enabling peer-to-peer (P2P) transfer of data directly across the PCIe bus or NVLink for supported GPUs in supported configurations, bypassing host memory. See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P. 9.2. Device Memory Spaces  CUDA devices use several memory spaces, which have different characteristics that reflect their distinct usages in CUDA applications. These memory spaces include global, local, shared, texture, and registers, as shown in Figure 2 . Memory spaces on a CUDA device  Of these different memory spaces, global memory is the most plentiful; see Features and Technical Specifications of the CUDA C++ Programming Guide for the amounts of memory available in each memory space at each compute capability level. Global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file. The various principal traits of the memory types are shown in Table 1 . Table 1. Salient Features of Device Memory  Memory Location on/off chip Cached Access Scope Lifetime Register On n/a R/W 1 thread Thread Local Off Yes†† R/W 1 thread Thread Shared On n/a R/W All threads in block Block Global Off † R/W All threads + host Host allocation Constant Off Yes R All threads + host Host allocation Texture Off Yes R All threads + host Host allocation † Cached in L1 and L2 by default on devices of compute capability 6.0 and 7.x; cached only in L2 by default on devices of lower compute capabilities, though some allow opt-in to caching in L1 as well via compilation flags. †† Cached in L1 and L2 by default except on devices of compute capability 5.x; devices of compute capability 5.x cache locals only in L2. In the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array. Texture references that are bound to CUDA arrays can be written to via surface-write operations by binding a surface to the same underlying CUDA array storage). Reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified. 9.2.1. Coalesced Access to Global Memory  A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions. Note High Priority: Ensure global memory accesses are coalesced whenever possible. The access requirements for coalescing depend on the compute capability of the device and are documented in the CUDA C++ Programming Guide. For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp. For certain devices of compute capability 5.2, L1-caching of accesses to global memory can be optionally enabled. If L1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments. Note On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not. On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory. Coalescing concepts are illustrated in the following simple examples. These examples assume compute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted. 9.2.1.1. A Simple Access Pattern  The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the k -th thread accesses the k -th word in a 32-byte aligned array. Not all threads need to participate. For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent float values), four coalesced 32-byte transactions will service that memory access. Such a pattern is shown in Figure 3 . Coalesced access  This access pattern results in four 32-byte transactions, indicated by the red rectangles. If from any of the four 32-byte segments only a subset of the words are requested (e.g. if several threads had accessed the same word or if some threads did not participate in the access), the full segment is fetched anyway. Furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would have been performed by a device with compute capability 6.0 or higher. 9.2.1.2. A Sequential but Misaligned Access Pattern  If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments will be requested, as shown in Figure 4 . Misaligned sequential addresses that fall within five 32-byte segments  Memory allocated through the CUDA Runtime API, such as via cudaMalloc() , is guaranteed to be aligned to at least 256 bytes. Therefore, choosing sensible thread block sizes, such as multiples of the warp size (i.e., 32 on current GPUs), facilitates memory accesses by warps that are properly aligned. (Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example.) 9.2.1.3. Effects of Misaligned Accesses  It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in A copy kernel that illustrates misaligned accesses . A copy kernel that illustrates misaligned accesses __global__ void offsetCopy ( float * odata , float * idata , int offset ) { int xid = blockIdx . x * blockDim . x + threadIdx . x + offset ; odata [ xid ] = idata [ xid ]; } In A copy kernel that illustrates misaligned accesses , data is copied from the input array idata to the output array, both of which exist in global memory. The kernel is executed within a loop in host code that varies the parameter offset from 0 to 32. (e.g. Figure 4 corresponds to this misalignments) The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 ( compute capability 7.0) is shown in Figure 5 . Performance of offsetCopy kernel  For the NVIDIA Tesla V100, global memory accesses with no offset or with offsets that are multiples of 8 words result in four 32-byte transactions. The achieved bandwidth is approximately 790 GB/s. Otherwise, five 32-byte segments are loaded per warp, and we would expect approximately 4/5 th of the memory throughput achieved with no offsets. In this particular example, the offset memory throughput achieved is, however, approximately 9/10 th , because adjacent warps reuse the cache lines their neighbors fetched. So while the impact is still evident it is not as large as we might have expected. It would have been more so if adjacent warps had not exhibited such a high degree of reuse of the over-fetched cache lines. 9.2.1.4. Strided Accesses  As seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact. It may be different with non-unit-strided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices. For this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices. To illustrate the effect of strided access on effective bandwidth, see the kernel strideCopy() in A kernel to illustrate non-unit stride data copy , which copies data with a stride of stride elements between threads from idata to odata . A kernel to illustrate non-unit stride data copy __global__ void strideCopy ( float * odata , float * idata , int stride ) { int xid = ( blockIdx . x * blockDim . x + threadIdx . x ) * stride ; odata [ xid ] = idata [ xid ]; } Figure 6 illustrates such a situation; in this case, threads within a warp access words in memory with a stride of 2. This action leads to a load of eight L2 cache segments per warp on the Tesla V100 (compute capability 7.0). Adjacent threads accessing memory with a stride of 2  A stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth. As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp, as indicated in Figure 7 . Performance of strideCopy kernel  As illustrated in Figure 7 , non-unit-stride global memory accesses should be avoided whenever possible. One method for doing so utilizes shared memory, which is discussed in the next section. 9.2.2. L2 Cache  Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache. Because L2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory. For more details refer to the L2 Access Management section in the CUDA C++ Programming Guide . 9.2.2.1. L2 Cache Access Window  When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting . On the other hand, if the data is only accessed once, such data accesses can be considered to be streaming . A portion of the L2 cache can be set aside for persistent accesses to a data region in global memory. If this set-aside portion is not used by persistent accesses, then streaming or normal data accesses can use it. The L2 cache set-aside size for persisting accesses may be adjusted, within limits: cudaGetDeviceProperties ( & prop , device_id ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , prop . persistingL2CacheMaxSize ); /* Set aside max possible size of L2 cache for persisting accesses */ Mapping of user data to L2 set-aside portion can be controlled using an access policy window on a CUDA stream or CUDA graph kernel node. The example below shows how to use the access policy window on a CUDA stream. cudaStreamAttrValue stream_attribute ; // Stream level attributes data structure stream_attribute . accessPolicyWindow . base_ptr = reinterpret_cast < void *> ( ptr ); // Global Memory data pointer stream_attribute . accessPolicyWindow . num_bytes = num_bytes ; // Number of bytes for persisting accesses. // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize) stream_attribute . accessPolicyWindow . hitRatio = 1.0 ; // Hint for L2 cache hit ratio for persisting accesses in the num_bytes region stream_attribute . accessPolicyWindow . hitProp = cudaAccessPropertyPersisting ; // Type of access property on cache hit stream_attribute . accessPolicyWindow . missProp = cudaAccessPropertyStreaming ; // Type of access property on cache miss. //Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); The access policy window requires a value for hitRatio and num_bytes . Depending on the value of the num_bytes parameter and the size of L2 cache, one may need to tune the value of hitRatio to avoid thrashing of L2 cache lines. 9.2.2.2. Tuning the Access Window Hit-Ratio  The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property. For example, if the hitRatio value is 0.6, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property. To understand the effect of hitRatio and num_bytes , we use a sliding window micro benchmark. This microbenchmark uses a 1024 MB region in GPU global memory. First, we set aside 30 MB of the L2 cache for persisting accesses using cudaDeviceSetLimit() , as discussed above. Then, as shown in the figure below, we specify that the accesses to the first freqSize * sizeof(int) bytes of the memory region are persistent. This data will thus use the L2 set-aside portion. In our experiment, we vary the size of this persistent data region from 10 MB to 60 MB to model various scenarios where data fits in or exceeds the available L2 set-aside portion of 30 MB. Note that the NVIDIA Tesla A100 GPU has 40 MB of total L2 cache capacity. Accesses to the remaining data of the memory region (i.e., streaming data) are considered normal or streaming accesses and will thus use the remaining 10 MB of the non set-aside L2 portion (unless part of the L2 set-aside portion is unused). Mapping Persistent data accesses to set-aside L2 in sliding window experiment  Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment. __global__ void kernel ( int * data_persistent , int * data_streaming , int dataSize , int freqSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; /*Each CUDA thread accesses one element in the persistent data section and one element in the streaming data section. Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data in the persistent region is accessed more frequently*/ data_persistent [ tid % freqSize ] = 2 * data_persistent [ tid % freqSize ]; data_streaming [ tid % dataSize ] = 2 * data_streaming [ tid % dataSize ]; } stream_attribute . accessPolicyWindow . base_ptr = reinterpret_cast < void *> ( data_persistent ); stream_attribute . accessPolicyWindow . num_bytes = freqSize * sizeof ( int ); //Number of bytes for persisting accesses in range 10-60 MB stream_attribute . accessPolicyWindow . hitRatio = 1.0 ; //Hint for cache hit ratio. Fixed value 1.0 The performance of the above kernel is shown in the chart below. When the persistent data region fits well into the 30 MB set-aside portion of the L2 cache, a performance increase of as much as 50% is observed. However, once the size of this persistent data region exceeds the size of the L2 set-aside cache portion, approximately 10% performance drop is observed due to thrashing of L2 cache lines. The performance of the sliding-window benchmark with fixed hit-ratio of 1.0  In order to optimize the performance, when the size of the persistent data is more than the size of the set-aside L2 cache portion, we tune the num_bytes and hitRatio parameters in the access window as below. stream_attribute . accessPolicyWindow . base_ptr = reinterpret_cast < void *> ( data_persistent ); stream_attribute . accessPolicyWindow . num_bytes = 20 * 1024 * 1024 ; //20 MB stream_attribute . accessPolicyWindow . hitRatio = ( 20 * 1024 * 1024 ) / (( float ) freqSize * sizeof ( int )); //Such that up to 20MB of data is resident. We fix the num_bytes in the access window to 20 MB and tune the hitRatio such that a random 20 MB of the total persistent data is resident in the L2 set-aside cache portion. The remaining portion of this persistent data will be accessed using the streaming property. This helps in reducing cache thrashing. The results are shown in the chart below, where we see good performance regardless of whether the persistent data fits in the L2 set-aside or not. The performance of the sliding-window benchmark with tuned hit-ratio  9.2.3. Shared Memory  Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section. 9.2.3.1. Shared Memory and Memory Banks  To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules ( banks ) that can be accessed simultaneously. Therefore, any memory load or store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single bank. However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized. The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests. The one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads. To minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests. On devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle, and successive 32-bit words are assigned to successive banks. The warp size is 32 threads and the number of banks is also 32, so bank conflicts can occur between any threads in the warp. See Compute Capability 5.x in the CUDA C++ Programming Guide for further details. 9.2.3.2. Shared Memory in Matrix Multiplication (C=AB)  Shared memory enables cooperation between threads in a block. When multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once. Shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and then reordering it in shared memory. Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a warp in shared memory. The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN. To keep the kernels simple, M and N are multiples of 32, since the warp size (w) is 32 for current devices. A natural decomposition of the problem is to use a block and tile size of wxw threads. Therefore, in terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11 . A grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different tile in C from a single tile of A and a single tile of B. Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C).  To do this, the simpleMultiply kernel ( Unoptimized matrix multiplication ) calculates the output elements of a tile of matrix C. Unoptimized matrix multiplication __global__ void simpleMultiply ( float * a , float * b , float * c , int N ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; float sum = 0.0f ; for ( int i = 0 ; i < TILE_DIM ; i ++ ) { sum += a [ row * TILE_DIM + i ] * b [ i * N + col ]; } c [ row * N + col ] = sum ; } In Unoptimized matrix multiplication , a , b , and c are pointers to global memory for the matrices A, B, and C, respectively; blockDim.x , blockDim.y , and TILE_DIM are all equal to w. Each thread in the wxw-thread block calculates one element in a tile of C. row and col are the row and column of the element in C being calculated by a particular thread. The for loop over i multiplies a row of A by a column of B, which is then written to C. The effective bandwidth of this kernel is 119.9 GB/s on an NVIDIA Tesla V100. To analyze performance, it is necessary to consider how warps access global memory in the for loop. Each warp of threads calculates one row of a tile of C, which depends on a single row of A and an entire tile of B as illustrated in Figure 12 . Computing a row of a tile. Computing a row of a tile in C using one row of A and an entire tile of B.  For each iteration i of the for loop, the threads in a warp read a row of the B tile, which is a sequential and coalesced access for all compute capabilities. However, for each iteration i , all threads in a warp read the same value from global memory for matrix A, as the index row*TILE_DIM+i is constant within a warp. Even though such an access requires only 1 transaction on devices of compute capability 2.0 or higher, there is wasted bandwidth in the transaction, because only one 4-byte word out of 8 words in a 32-byte cache segment is used. We can reuse this cache line in subsequent iterations of the loop, and we would eventually utilize all 8 words; however, when many warps execute on the same multiprocessor simultaneously, as is generally the case, the cache line may easily be evicted from the cache between iterations i and i+1 . The performance on a device of any compute capability can be improved by reading a tile of A into shared memory as shown in Using shared memory to improve the global memory load efficiency in matrix multiplication . Using shared memory to improve the global memory load efficiency in matrix multiplication __global__ void coalescedMultiply ( float * a , float * b , float * c , int N ) { __shared__ float aTile [ TILE_DIM ][ TILE_DIM ]; int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; float sum = 0.0f ; aTile [ threadIdx . y ][ threadIdx . x ] = a [ row * TILE_DIM + threadIdx . x ]; __syncwarp (); for ( int i = 0 ; i < TILE_DIM ; i ++ ) { sum += aTile [ threadIdx . y ][ i ] * b [ i * N + col ]; } c [ row * N + col ] = sum ; } In Using shared memory to improve the global memory load efficiency in matrix multiplication , each element in a tile of A is read from global memory only once, in a fully coalesced fashion (with no wasted bandwidth), to shared memory. Within each iteration of the for loop, a value in shared memory is broadcast to all threads in a warp. Instead of a __syncthreads() synchronization barrier call, a __syncwarp() is sufficient after reading the tile of A into shared memory because only threads within the warp that write the data into shared memory read this data. This kernel has an effective bandwidth of 144.4 GB/s on an NVIDIA Tesla V100. This illustrates the use of the shared memory as a user-managed cache when the hardware L1 cache eviction policy does not match up well with the needs of the application or when L1 cache is not used for reads from global memory. A further improvement can be made to how Using shared memory to improve the global memory load efficiency in matrix multiplication deals with matrix B. In calculating each of the rows of a tile of matrix C, the entire tile of B is read. The repeated reading of the B tile can be eliminated by reading it into shared memory once ( Improvement by reading additional data into shared memory ). Improvement by reading additional data into shared memory __global__ void sharedABMultiply ( float * a , float * b , float * c , int N ) { __shared__ float aTile [ TILE_DIM ][ TILE_DIM ], bTile [ TILE_DIM ][ TILE_DIM ]; int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; float sum = 0.0f ; aTile [ threadIdx . y ][ threadIdx . x ] = a [ row * TILE_DIM + threadIdx . x ]; bTile [ threadIdx . y ][ threadIdx . x ] = b [ threadIdx . y * N + col ]; __syncthreads (); for ( int i = 0 ; i < TILE_DIM ; i ++ ) { sum += aTile [ threadIdx . y ][ i ] * bTile [ i ][ threadIdx . x ]; } c [ row * N + col ] = sum ; } Note that in Improvement by reading additional data into shared memory , a __syncthreads() call is required after reading the B tile because a warp reads data from shared memory that were written to shared memory by different warps. The effective bandwidth of this routine is 195.5 GB/s on an NVIDIA Tesla V100. Note that the performance improvement is not due to improved coalescing in either case, but to avoiding redundant transfers from global memory. The results of the various optimizations are summarized in Table 2 . Table 2. Performance Improvements Optimizing C = AB Matrix Multiply\n:class table-no-stripes  Optimization NVIDIA Tesla V100 No optimization 119.9 GB/s Coalesced using shared memory to store a tile of A 144.4 GB/s Using shared memory to eliminate redundant reads of a tile of B 195.5 GB/s Note Medium Priority: Use shared memory to avoid redundant transfers from global memory. 9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT)  A variant of the previous matrix multiplication can be used to illustrate how strided accesses to global memory, as well as shared memory bank conflicts, are handled. This variant simply uses the transpose of A in place of B, so C = AA T . A simple implementation for C = AA T is shown in Unoptimized handling of strided accesses to global memory Unoptimized handling of strided accesses to global memory __global__ void simpleMultiply ( float * a , float * c , int M ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; float sum = 0.0f ; for ( int i = 0 ; i < TILE_DIM ; i ++ ) { sum += a [ row * TILE_DIM + i ] * a [ col * TILE_DIM + i ]; } c [ row * M + col ] = sum ; } In Unoptimized handling of strided accesses to global memory , the row -th, col -th element of C is obtained by taking the dot product of the row -th and col -th rows of A. The effective bandwidth for this kernel is 12.8 GB/s on an NVIDIA Tesla V100. These results are substantially lower than the corresponding measurements for the C = AB kernel. The difference is in how threads in a half warp access elements of A in the second term, a[col*TILE_DIM+i] , for each iteration i . For a warp of threads, col represents sequential columns of the transpose of A, and therefore col*TILE_DIM represents a strided access of global memory with a stride of w, resulting in plenty of wasted bandwidth. The way to avoid strided access is to use shared memory as before, except in this case a warp reads a row of A into a column of a shared memory tile, as shown in An optimized handling of strided accesses using coalesced reads from global memory . An optimized handling of strided accesses using coalesced reads from global memory __global__ void coalescedMultiply ( float * a , float * c , int M ) { __shared__ float aTile [ TILE_DIM ][ TILE_DIM ], transposedTile [ TILE_DIM ][ TILE_DIM ]; int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; float sum = 0.0f ; aTile [ threadIdx . y ][ threadIdx . x ] = a [ row * TILE_DIM + threadIdx . x ]; transposedTile [ threadIdx . x ][ threadIdx . y ] = a [( blockIdx . x * blockDim . x + threadIdx . y ) * TILE_DIM + threadIdx . x ]; __syncthreads (); for ( int i = 0 ; i < TILE_DIM ; i ++ ) { sum += aTile [ threadIdx . y ][ i ] * transposedTile [ i ][ threadIdx . x ]; } c [ row * M + col ] = sum ; } An optimized handling of strided accesses using coalesced reads from global memory uses the shared transposedTile to avoid uncoalesced accesses in the second term in the dot product and the shared aTile technique from the previous example to avoid uncoalesced accesses in the first term. The effective bandwidth of this kernel is 140.2 GB/s on an NVIDIA Tesla V100.These results are lower than those obtained by the final kernel for C = AB. The cause of the difference is shared memory bank conflicts. The reads of elements in transposedTile within the for loop are free of conflicts, because threads of each half warp read across rows of the tile, resulting in unit stride across the banks. However, bank conflicts occur when copying the tile from global memory into shared memory. To enable the loads from global memory to be coalesced, data are read from global memory sequentially. However, this requires writing to shared memory in columns, and because of the use of wxw tiles in shared memory, this results in a stride between threads of w banks - every thread of the warp hits the same bank (Recall that w is selected as 32). These many-way bank conflicts are very expensive. The simple remedy is to pad the shared memory array so that it has an extra column, as in the following line of code. __shared__ float transposedTile [ TILE_DIM ][ TILE_DIM + 1 ]; This padding eliminates the conflicts entirely, because now the stride between threads is w+1 banks (i.e., 33 for current devices), which, due to modulo arithmetic used to compute bank indices, is equivalent to a unit stride. After this change, the effective bandwidth is 199.4 GB/s on an NVIDIA Tesla V100, which is comparable to the results from the last C = AB kernel. The results of these optimizations are summarized in Table 3 . Table 3. Performance Improvements Optimizing C = AA T Matrix Multiplication  Optimization NVIDIA Tesla V100 No optimization 12.8 GB/s Using shared memory to coalesce global reads 140.2 GB/s Removing bank conflicts 199.4 GB/s These results should be compared with those in Table 2 . As can be seen from these tables, judicious use of shared memory can dramatically improve performance. The examples in this section have illustrated three reasons to use shared memory: To enable coalesced accesses to global memory, especially to avoid large strides (for general matrices, strides are much larger than 32) To eliminate (or reduce) redundant loads from global memory To avoid wasted bandwidth 9.2.3.4. Asynchronous Copy from Global Memory to Shared Memory  CUDA 11.0 introduces an async-copy feature that can be used within device code to explicitly manage the asynchronous copying of data from global memory to shared memory. This feature enables CUDA kernels to overlap copying data from global to shared memory with computation. It also avoids an intermediary register file access traditionally present between the global memory read and the shared memory write. For more details refer to the memcpy_async section in the CUDA C++ Programming Guide . To understand the performance difference between synchronous copy and asynchronous copy of data from global memory to shared memory, consider the following micro benchmark CUDA kernels for demonstrating the synchronous and asynchronous approaches. Asynchronous copies are hardware accelerated for NVIDIA A100 GPU. template < typename T > __global__ void pipeline_kernel_sync ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast < T *> ( s ); uint64_t clock_start = clock64 (); for ( size_t i = 0 ; i < copy_count ; ++ i ) { shared [ blockDim . x * i + threadIdx . x ] = global [ blockDim . x * i + threadIdx . x ]; } uint64_t clock_end = clock64 (); atomicAdd ( reinterpret_cast < unsigned long long *> ( clock ), clock_end - clock_start ); } template < typename T > __global__ void pipeline_kernel_async ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast < T *> ( s ); uint64_t clock_start = clock64 (); //pipeline pipe; for ( size_t i = 0 ; i < copy_count ; ++ i ) { __pipeline_memcpy_async ( & shared [ blockDim . x * i + threadIdx . x ], & global [ blockDim . x * i + threadIdx . x ], sizeof ( T )); } __pipeline_commit (); __pipeline_wait_prior ( 0 ); uint64_t clock_end = clock64 (); atomicAdd ( reinterpret_cast < unsigned long long *> ( clock ), clock_end - clock_start ); } The synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory. In the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as __pipeline_memcpy_async() function is called. The __pipeline_wait_prior(0) will wait until all the instructions in the pipe object have been executed. Using asynchronous copies does not use any intermediate register. Not using intermediate registers can help reduce register pressure and can increase kernel occupancy. Data copied from global memory to shared memory using asynchronous copy instructions can be cached in the L1 cache or the L1 cache can be optionally bypassed. If individual CUDA threads are copying elements of 16 bytes, the L1 cache can be bypassed. This difference is illustrated in Figure 13 . Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory  We evaluate the performance of both kernels using elements of size 4B, 8B and 16B per thread i.e., using int , int2 and int4 for the template parameter. We adjust the copy_count in the kernels such that each thread block copies from 512 bytes up to 48 MB. The performance of the kernels is shown in Figure 14 . Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory  From the performance chart, the following observations can be made for this experiment. Best performance with synchronous copy is achieved when the copy_count parameter is a multiple of 4 for all three element sizes. The compiler can optimize groups of 4 load and store instructions. This is evident from the saw tooth curves. Asynchronous copy achieves better performance in nearly all cases. The async-copy does not require the copy_count parameter to be a multiple of 4, to maximize performance through compiler optimizations. Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes. 9.2.4. Local Memory  Local memory is so named because its scope is local to the thread, not because of its physical location. In fact, local memory is off-chip. Hence, access to local memory is as expensive as access to global memory. In other words, the term local in the name does not imply faster access. Local memory is used only to hold automatic variables. This is done by the nvcc compiler when it determines that there is insufficient register space to hold the variable. Automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically. Inspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line options to nvcc ) reveals whether a variable has been placed in local memory during the first compilation phases. If it has, it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics. If it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture. There is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the --ptxas-options=-v option. 9.2.5. Texture Memory  The read-only texture memory space is cached. Therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache. The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance. Texture memory is also designed for streaming fetches with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency. In certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory. 9.2.5.1. Additional Texture Capabilities  If textures are fetched using tex1D() , tex2D() , or tex3D() rather than tex1Dfetch() , the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in Table 4 . Table 4. Useful Features for tex1D(), tex2D(), and tex3D() Fetches  Feature Use Caveat Filtering Fast, low-precision interpolation between texels Valid only if the texture reference returns floating-point data Normalized texture coordinates Resolution-independent coding None Addressing modes Automatic handling of boundary cases 1 Can be used only with normalized texture coordinates 1 The automatic handling of boundary cases in the bottom row of Table 4 refers to how a texture coordinate is resolved when it falls outside the valid addressing range. There are two options: clamp and wrap . If x is the coordinate and N is the number of texels for a one-dimensional texture, then with clamp, x is replaced by 0 if x < 0 and by 1-1/ N if 1 < x . With wrap, x is replaced by frac(x) where frac(x) = x - floor(x) . Floor returns the largest integer less than or equal to x . So, in clamp mode where N = 1, an x of 1.3 is clamped to 1.0; whereas in wrap mode, it is converted to 0.3 Within a kernel call, the texture cache is not kept coherent with respect to global memory writes, so texture fetches from addresses that have been written via global stores in the same kernel call return undefined data. That is, a thread can safely read a memory location via texture if the location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread within the same kernel call. 9.2.6. Constant Memory  There is a total of 64 KB constant memory on a device. The constant memory space is cached. As a result, a read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp accesses only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. 9.2.7. Registers  Generally, accessing a register consumes zero extra clock cycles per instruction, but delays may occur due to register read-after-write dependencies and register memory bank conflicts. The compiler and hardware thread scheduler will schedule instructions as optimally as possible to avoid register memory bank conflicts. An application has no direct control over these bank conflicts. In particular, there is no register-related reason to pack data into vector data types such as float4 or int4 types. 9.2.7.1. Register Pressure  Register pressure occurs when there are not enough registers available for a given task. Even though each multiprocessor contains thousands of 32-bit registers (see Features and Technical Specifications of the CUDA C++ Programming Guide), these are partitioned among concurrent threads. To prevent the compiler from allocating too many registers, use the -maxrregcount=N compiler command-line option (see nvcc ) or the launch bounds kernel definition qualifier (see Execution Configuration of the CUDA C++ Programming Guide) to control the maximum number of registers to allocated per thread. 9.3. Allocation  Device memory allocation and de-allocation via cudaMalloc() and cudaFree() are expensive operations. It is recommended to use cudaMallocAsync() and cudaFreeAsync() which are stream ordered pool allocators to manage device memory. 9.4. NUMA Best Practices  Some recent Linux distributions enable automatic NUMA balancing (or “ AutoNUMA ”) by default. In some instances, operations performed by automatic NUMA balancing may degrade the performance of applications running on NVIDIA GPUs. For optimal performance, users should manually tune the NUMA characteristics of their application. The optimal NUMA tuning will depend on the characteristics and desired hardware affinities of each application and node, but in general applications computing on NVIDIA GPUs are advised to choose a policy that disables automatic NUMA balancing. For example, on IBM Newell POWER9 nodes (where the CPUs correspond to NUMA nodes 0 and 8), use: numactl --membind=0,8 to bind memory allocations to the CPUs. 10. Execution Configuration Optimizations  One of the keys to good performance is to keep the multiprocessors on the device as busy as possible. A device in which work is poorly balanced across the multiprocessors will deliver suboptimal performance. Hence, it’s important to design your application to use threads and blocks in a way that maximizes hardware utilization and to limit practices that impede the free distribution of work. A key concept in this effort is occupancy, which is explained in the following sections. Hardware utilization can also be improved in some cases by designing your application so that multiple, independent kernels can execute at the same time. Multiple kernels executing at the same time is known as concurrent kernel execution. Concurrent kernel execution is described below. Another important concept is the management of system resources allocated for a particular task. How to manage this resource utilization is discussed in the final sections of this chapter. 10.1. Occupancy  Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. This metric is occupancy . Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. (To determine the latter number, see the deviceQuery CUDA Sample or refer to Compute Capabilities in the CUDA C++ Programming Guide.) Another way to view occupancy is the percentage of the hardware’s ability to process warps that is actively in use. Higher occupancy does not always equate to higher performance-there is a point above which additional occupancy does not improve performance. However, low occupancy always interferes with the ability to hide memory latency, resulting in performance degradation. Per thread resources required by a CUDA kernel might limit the maximum block size in an unwanted way. In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument __launch_bounds__(maxThreadsPerBlock) which specifies the largest block size that the kernel will be launched with. Failure to do so could lead to “too many resources requested for launch” errors. Providing the two argument version of __launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor) can improve performance in some cases. The right value for minBlocksPerMultiprocessor should be determined using a detailed per kernel analysis. 10.1.1. Calculating Occupancy  One of several factors that determine occupancy is register availability. Register storage enables threads to keep local variables nearby for low-latency access. However, the set of registers (known as the register file ) is a limited commodity that all threads resident on a multiprocessor must share. Registers are allocated to an entire block all at once. So, if each thread block uses many registers, the number of thread blocks that can be resident on a multiprocessor is reduced, thereby lowering the occupancy of the multiprocessor. The maximum number of registers per thread can be set manually at compilation time per-file using the -maxrregcount option or per-kernel using the __launch_bounds__ qualifier (see Register Pressure ). For purposes of calculating occupancy, the number of registers used by each thread is one of the key factors. For example, on devices of compute capability 7.0 each multiprocessor has 65,536 32-bit registers and can have a maximum of 2048 simultaneous threads resident (64 warps x 32 threads per warp). This means that in one of these devices, for a multiprocessor to have 100% occupancy, each thread can use at most 32 registers. However, this approach of determining how register count affects occupancy does not take into account the register allocation granularity. For example, on a device of compute capability 7.0, a kernel with 128-thread blocks using 37 registers per thread results in an occupancy of 75% with 12 active 128-thread blocks per multi-processor, whereas a kernel with 320-thread blocks using the same 37 registers per thread results in an occupancy of 63% because only four 320-thread blocks can reside on a multiprocessor. Furthermore, register allocations are rounded up to the nearest 256 registers per warp. The number of registers available, the maximum number of simultaneous threads resident on each multiprocessor, and the register allocation granularity vary over different compute capabilities. Because of these nuances in register allocation and the fact that a multiprocessor’s shared memory is also partitioned between resident thread blocks, the exact relationship between register usage and occupancy can be difficult to determine. The --ptxas options=v option of nvcc details the number of registers used per thread for each kernel. See Hardware Multithreading of the CUDA C++ Programming Guide for the register allocation formulas for devices of various compute capabilities and Features and Technical Specifications of the CUDA C++ Programming Guide for the total number of registers available on those devices. Alternatively, NVIDIA provides an occupancy calculator in the form of an Excel spreadsheet that enables developers to hone in on the optimal balance and to test different possible scenarios more easily. This spreadsheet, shown in Figure 15 , is called CUDA_Occupancy_Calculator.xls and is located in the tools subdirectory of the CUDA Toolkit installation. Using the CUDA Occupancy Calculator to project GPU multiprocessor occupancy  In addition to the calculator spreadsheet, occupancy can be determined using the NVIDIA Nsight Compute Profiler. Details about occupancy are displayed in the Occupancy section. An application can also use the Occupancy API from the CUDA Runtime, e.g. cudaOccupancyMaxActiveBlocksPerMultiprocessor , to dynamically select launch configurations based on runtime parameters. 10.2. Hiding Register Dependencies  Note Medium Priority: To hide latency arising from register dependencies, maintain sufficient numbers of active threads per multiprocessor (i.e., sufficient occupancy). Register dependencies arise when an instruction uses a result stored in a register written by an instruction before it. The latency of most arithmetic instructions is typically 4 cycles on devices of compute capability 7.0. So threads must wait approximatly 4 cycles before using an arithmetic result. However, this latency can be completely hidden by the execution of threads in other warps. See Registers for details. 10.3. Thread and Block Heuristics  Note Medium Priority: The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing. The dimension and size of blocks per grid and the dimension and size of threads per block are both important factors. The multidimensional aspect of these parameters allows easier mapping of multidimensional problems to CUDA and does not play a role in performance. As a result, this section discusses size but not dimension. Latency hiding and occupancy depend on the number of active warps per multiprocessor, which is implicitly determined by the execution parameters along with resource (register and shared memory) constraints. Choosing execution parameters is a matter of striking a balance between latency hiding (occupancy) and resource utilization. Choosing the execution configuration parameters should be done in tandem; however, there are certain heuristics that apply to each parameter individually. When choosing the first execution configuration parameter-the number of blocks per grid, or grid size - the primary concern is keeping the entire GPU busy. The number of blocks in a grid should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. Furthermore, there should be multiple active blocks per multiprocessor so that blocks that aren’t waiting for a __syncthreads() can keep the hardware busy. This recommendation is subject to resource availability; therefore, it should be determined in the context of the second execution parameter - the number of threads per block, or block size - as well as shared memory usage. To scale to future devices, the number of blocks per kernel launch should be in the thousands. When choosing the block size, it is important to remember that multiple concurrent blocks can reside on a multiprocessor, so occupancy is not determined by block size alone. In particular, a larger block size does not imply a higher occupancy. As mentioned in Occupancy , higher occupancy does not always equate to better performance. For example, improving occupancy from 66 percent to 100 percent generally does not translate to a similar increase in performance. A lower occupancy kernel will have more registers available per thread than a higher occupancy kernel, which may result in less register spilling to local memory; in particular, with a high degree of exposed instruction-level parallelism (ILP) it is, in some cases, possible to fully cover latency with a low occupancy. There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed: Threads per block should be a multiple of warp size to avoid wasting computation on under-populated warps and to facilitate coalescing. A minimum of 64 threads per block should be used, and only if there are multiple concurrent blocks per multiprocessor. Between 128 and 256 threads per block is a good initial range for experimentation with different block sizes. Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call __syncthreads() . Note that when a thread block allocates more registers than are available on a multiprocessor, the kernel launch fails, as it will when too much shared memory or too many threads are requested. 10.4. Effects of Shared Memory  Shared memory can be helpful in several situations, such as helping to coalesce or eliminate redundant access to global memory. However, it also can act as a constraint on occupancy. In many cases, the amount of shared memory required by a kernel is related to the block size that was chosen, but the mapping of threads to shared memory elements does not need to be one-to-one. For example, it may be desirable to use a 64x64 element shared memory array in a kernel, but because the maximum number of threads per block is 1024, it is not possible to launch a kernel with 64x64 threads per block. In such cases, kernels with 32x32 or 64x16 threads can be launched with each thread processing four elements of the shared memory array. The approach of using a single thread to process multiple elements of a shared memory array can be beneficial even if limits such as threads per block are not an issue. This is because some operations common to each element can be performed by the thread once, amortizing the cost over the number of shared memory elements processed by the thread. A useful technique to determine the sensitivity of performance to occupancy is through experimentation with the amount of dynamically allocated shared memory, as specified in the third parameter of the execution configuration. By simply increasing this parameter (without modifying the kernel), it is possible to effectively reduce the occupancy of the kernel and measure its effect on performance. 10.5. Concurrent Kernel Execution  As described in Asynchronous and Overlapping Transfers with Computation , CUDA streams can be used to overlap kernel execution with data transfers. On devices that are capable of concurrent kernel execution, streams can also be used to execute multiple kernels simultaneously to more fully take advantage of the device’s multiprocessors. Whether a device has this capability is indicated by the concurrentKernels field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample). Non-default streams (streams other than stream 0) are required for concurrent execution because kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished. The following example illustrates the basic technique. Because kernel1 and kernel2 are executed in different, non-default streams, a capable device can execute the kernels at the same time. cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); kernel1 <<< grid , block , 0 , stream1 >>> ( data_1 ); kernel2 <<< grid , block , 0 , stream2 >>> ( data_2 ); 10.6. Multiple contexts  CUDA work occurs within a process space for a particular GPU known as a context . The context encapsulates kernel launches and memory allocations for that GPU as well as supporting constructs such as the page tables. The context is explicit in the CUDA Driver API but is entirely implicit in the CUDA Runtime API, which creates and manages contexts automatically. With the CUDA Driver API, a CUDA application process can potentially create more than one context for a given GPU. If multiple CUDA application processes access the same GPU concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless Multi-Process Service is in use. While multiple contexts (and their associated resources such as global memory allocations) can be allocated concurrently on a given GPU, only one of these contexts can execute work at any given moment on that GPU; contexts sharing the same GPU are time-sliced. Creating additional contexts incurs memory overhead for per-context data and time overhead for context switching. Furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently (see also Concurrent Kernel Execution ). Therefore, it is best to avoid multiple contexts per GPU within the same CUDA application. To assist with this, the CUDA Driver API provides methods to access and manage a special context on each GPU called the primary context . These are the same contexts used implicitly by the CUDA Runtime when there is not already a current context for a thread. // When initializing the program/library CUcontext ctx ; cuDevicePrimaryCtxRetain ( & ctx , dev ); // When the program/library launches work cuCtxPushCurrent ( ctx ); kernel <<< ... >>> (...); cuCtxPopCurrent ( & ctx ); // When the program/library is finished with the context cuDevicePrimaryCtxRelease ( dev ); Note NVIDIA-SMI can be used to configure a GPU for exclusive process mode , which limits the number of contexts per GPU to one. This context can be current to as many threads as desired within the creating process, and cuDevicePrimaryCtxRetain will fail if a non-primary context that was created with the CUDA driver API already exists on the device. 11. Instruction Optimization  Awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program). Best practices suggest that this optimization be performed after all higher-level optimizations have been completed. 11.1. Arithmetic Instructions  Single-precision floats provide the best performance, and their use is highly encouraged. The throughput of individual arithmetic operations is detailed in the CUDA C++ Programming Guide. 11.1.1. Division Modulo Operations  Note Low Priority: Use shift operations to avoid expensive division and modulo calculations. Integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: If \\(n\\) is a power of 2, ( \\(i/n\\) ) is equivalent to ( \\(i \\gg {log2}(n)\\) ) and ( \\(i\\% n\\) ) is equivalent to ( \\(i\\&\\left( {n - 1} \\right)\\) ). The compiler will perform these conversions if n is literal. (For further information, refer to Performance Guidelines in the CUDA C++ Programming Guide). 11.1.2. Loop Counters Signed vs. Unsigned  Note Low Medium Priority: Use signed integers rather than unsigned integers as loop counters. In the C language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results. Therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic. This is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned. For slightly better performance, however, they should instead be declared as signed. For example, consider the following code: for ( i = 0 ; i < n ; i ++ ) { out [ i ] = in [ offset + stride * i ]; } Here, the sub-expression stride*i could overflow a 32-bit integer, so if i is declared as unsigned, the overflow semantics prevent the compiler from using some optimizations that might otherwise have applied, such as strength reduction. If instead i is declared as signed, where the overflow semantics are undefined, the compiler has more leeway to use these optimizations. 11.1.3. Reciprocal Square Root  The reciprocal square root should always be invoked explicitly as rsqrtf() for single precision and rsqrt() for double precision. The compiler optimizes 1.0f/sqrtf(x) into rsqrtf() only when this does not violate IEEE-754 semantics. 11.1.4. Other Arithmetic Instructions  Note Low Priority: Avoid automatic conversion of doubles to floats. The compiler must on occasion insert conversion instructions, introducing additional execution cycles. This is the case for: Functions operating on char or short whose operands generally need to be converted to an int Double-precision floating-point constants (defined without any type suffix) used as input to single-precision floating-point computations The latter case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3.141592653589793f , 1.0f , 0.5f . For single-precision code, use of the float type and the single-precision math functions are highly recommended. It should also be noted that the CUDA math library’s complementary error function, erfcf() , is particularly fast with full single-precision accuracy. 11.1.5. Exponentiation With Small Fractional Arguments  For some fractional exponents, exponentiation can be accelerated significantly compared to the use of pow() by using square roots, cube roots, and their inverses. For those exponentiations where the exponent is not exactly representable as a floating-point number, such as 1/3, this can also provide much more accurate results, as use of pow() magnifies the initial representational error. The formulas in the table below are valid for x >= 0, x != -0 , that is, signbit(x) == 0 . Table 5. Formulae for exponentiation by small fractions  Computation Formula x 1/9 r = rcbrt(rcbrt(x)) x -1/9 r = cbrt(rcbrt(x)) x 1/6 r = rcbrt(rsqrt(x)) x -1/6 r = rcbrt(sqrt(x)) x 1/4 r = rsqrt(rsqrt(x)) x -1/4 r = sqrt(rsqrt(x)) x 1/3 r = cbrt(x) x -1/3 r = rcbrt(x) x 1/2 r = sqrt(x) x -1/2 r = rsqrt(x) x 2/3 r = cbrt(x); r = r*r x -2/3 r = rcbrt(x); r = r*r x 3/4 r = sqrt(x); r = r*sqrt(r) x -3/4 r = rsqrt(x); r = r*sqrt(r) x 7/6 r = x*rcbrt(rsqrt(x)) x -7/6 r = (1/x) * rcbrt(sqrt(x)) x 5/4 r = x*rsqrt(rsqrt(x)) x -5/4 r = (1/x)*sqrt(rsqrt(x)) x 4/3 r = x*cbrt(x) x -4/3 r = (1/x)*rcbrt(x) x 3/2 r = x*sqrt(x) x -3/2 r = (1/x)*rsqrt(x) 11.1.6. Math Libraries  Note Medium Priority: Use the fast math library whenever speed trumps precision. Two types of runtime math operations are supported. They can be distinguished by their names: some have names with prepended underscores, whereas others do not (e.g., __functionName() versus functionName() ). Functions following the __functionName() naming convention map directly to the hardware level. They are faster but provide somewhat lower accuracy (e.g., __sinf(x) and __expf(x) ). Functions following functionName() naming convention are slower but have higher accuracy (e.g., sinf(x) and expf(x) ). The throughput of __sinf(x) , __cosf(x) , and __expf(x) is much greater than that of sinf(x) , cosf(x) , and expf(x) . The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument x needs to be reduced. Moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory. More details are available in the CUDA C++ Programming Guide . Note also that whenever sine and cosine of the same argument are computed, the sincos family of instructions should be used to optimize performance: __sincosf() for single-precision fast math (see next paragraph) sincosf() for regular single-precision sincos() for double precision The -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. It also disables single-precision denormal support and lowers the precision of single-precision division in general. This is an aggressive optimization that can both reduce numerical accuracy and alter special case handling. A more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated. Note this switch is effective only on single-precision floating point. Note Medium Priority: Prefer faster, more specialized math functions over slower, more general ones when possible. For small integer powers (e.g., x2 or x3 ), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow() . While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage. This advantage is increased when several powers of the same base are needed (e.g., where both x2 and x5 are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization. For exponentiation using base 2 or 10, use the functions exp2() or expf2() and exp10() or expf10() rather than the functions pow() or powf() . Both pow() and powf() are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent. The functions exp2() , exp2f() , exp10() , and exp10f() , on the other hand, are similar to exp() and expf() in terms of performance, and can be as much as ten times faster than their pow() / powf() equivalents. For exponentiation with an exponent of 1/3, use the cbrt() or cbrtf() function rather than the generic exponentiation functions pow() or powf() , as the former are significantly faster than the latter. Likewise, for exponentation with an exponent of -1/3, use rcbrt() or rcbrtf() . Replace sin(π*<expr>) with sinpi(<expr>) , cos(π*<expr>) with cospi(<expr>) , and sincos(π*<expr>) with sincospi(<expr>) . This is advantageous with regard to both accuracy and performance. As a particular example, to evaluate the sine function in degrees instead of radians, use sinpi(x/180.0) . Similarly, the single-precision functions sinpif() , cospif() , and sincospif() should replace calls to sinf() , cosf() , and sincosf() when the function argument is of the form π*<expr> . (The performance advantage sinpi() has over sin() is due to simplified argument reduction; the accuracy advantage is because sinpi() multiplies by π only implicitly, effectively using an infinitely precise mathematical π rather than a single- or double-precision approximation thereof.) 11.1.7. Precision-related Compiler Flags  By default, the nvcc compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster: -ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) Another, more aggressive, option is -use_fast_math , which coerces every functionName() call to the equivalent __functionName() call. This makes the code run faster at the cost of diminished precision and accuracy. See Math Libraries . 11.2. Memory Instructions  Note High Priority: Minimize the use of global memory. Prefer shared memory access where possible. Memory instructions include any instruction that reads from or writes to shared, local, or global memory. When accessing uncached local or global memory, there are hundreds of clock cycles of memory latency. As an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory: __shared__ float shared [ 32 ]; __device__ float device [ 32 ]; shared [ threadIdx . x ] = device [ threadIdx . x ]; Much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete. However, it is best to avoid accessing global memory whenever possible. 12. Control Flow  12.1. Branching and Divergence  Note High Priority: Avoid different execution paths within the same warp. Flow control instructions ( if , switch , do , for , while ) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp. To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps. This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture of the CUDA C++ Programming Guide. A trivial example is when the controlling condition depends only on ( threadIdx / WSIZE ) where WSIZE is the warp size. In this case, no warp diverges because the controlling condition is perfectly aligned with the warps. For branches including just a few instructions, warp divergence generally results in marginal performance losses. For example, the compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Threads with a false predicate do not write results, and also do not evaluate addresses or read operands. Starting with the Volta architecture, Independent Thread Scheduling allows a warp to remain diverged outside of the data-dependent conditional block. An explicit __syncwarp() can be used to guarantee that the warp has reconverged for subsequent instructions. 12.2. Branch Predication  Note Low Priority: Make it easy for the compiler to use branch predication in lieu of loops or control statements. Sometimes, the compiler may unroll loops or optimize out if or switch statements by using branch predication instead. In these cases, no warp can ever diverge. The programmer can also control loop unrolling using #pragma unroll For more information on this pragma, refer to the CUDA C++ Programming Guide. When using branch predication, none of the instructions whose execution depends on the controlling condition is skipped. Instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition. Although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed. Instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands. The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold. 13. Deploying CUDA Applications  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application. 14. Understanding the Programming Environment  With each generation of NVIDIA processors, new features are added to the GPU that CUDA can leverage. Consequently, it’s important to understand the characteristics of the architecture. Programmers should be aware of two version numbers. The first is the compute capability , and the second is the version number of the CUDA Runtime and CUDA Driver APIs. 14.1. CUDA Compute Capability  The compute capability describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor. Higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible. The compute capability of the GPU in the device can be queried programmatically as illustrated in the deviceQuery CUDA Sample. The output for that program is shown in Figure 16 . This information is obtained by calling cudaGetDeviceProperties() and accessing the information in the structure it returns. Sample CUDA configuration data reported by deviceQuery  The major and minor revision numbers of the compute capability are shown on the seventh line of Figure 16 . Device 0 of this system has compute capability 7.0. More details about the compute capabilities of various GPUs are in CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming Guide. In particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device. 14.2. Additional Hardware Data  Certain hardware features are not described by the compute capability. For example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all GPUs irrespective of the compute capability. In such cases, call cudaGetDeviceProperties() to determine whether the device is capable of a certain feature. For example, the asyncEngineCount field of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, the canMapHostMemory field indicates whether zero-copy data transfers can be performed. 14.3. Which Compute Capability Target  To target specific versions of NVIDIA hardware and CUDA software, use the -arch , -code , and -gencode options of nvcc . Code that uses the warp shuffle operation, for example, must be compiled with -arch=sm_30 (or higher compute capability). See Building for Maximum Compatibility for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously. 14.4. CUDA Runtime  The host runtime component of the CUDA software environment can be used only by host functions. It provides functions to handle the following: Device management Context management Memory management Code module management Execution control Texture reference management Interoperability with OpenGL and Direct3D As compared to the lower-level CUDA Driver API, the CUDA Runtime greatly eases device management by providing implicit initialization, context management, and device code module management. The C++ host code generated by nvcc utilizes the CUDA Runtime, so applications that link to this code will depend on the CUDA Runtime; similarly, any code that uses the cuBLAS , cuFFT , and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries. The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual. The CUDA Runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched. The implicit driver version checking, code initialization, CUDA context management, CUDA module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the CUDA Runtime. It comprises two principal parts: A C-style function interface ( cuda_runtime_api.h ). C++-style convenience wrappers ( cuda_runtime.h ) built on top of the C-style functions. For more information on the Runtime API, refer to CUDA Runtime of the CUDA C++ Programming Guide. 15. CUDA Compatibility Developer’s Guide  CUDA Toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes. CUDA compatibility allows users to update the latest CUDA Toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack. The CUDA software environment consists of three parts: CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications. CUDA driver - User-mode driver component used to run CUDA applications (e.g. libcuda.so on Linux systems). NVIDIA GPU device driver - Kernel-mode driver component for NVIDIA GPUs. On Linux systems, the CUDA driver and kernel mode components are delivered together in the NVIDIA display driver package. This is shown in Figure 1. Components of CUDA  The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA code (by splitting and steering compilation), along with the CUDA runtime, is part of the CUDA compiler toolchain. The CUDA Runtime API provides developers with high-level C++ interface for simplified management of devices, kernel executions etc., While the CUDA driver API provides ( CUDA Driver API ) a low-level programming interface for applications to target NVIDIA hardware. Built on top of these technologies are CUDA libraries, some of which are included in the CUDA Toolkit, while others such as cuDNN may be released independently of the CUDA Toolkit. 15.1. CUDA Toolkit Versioning  Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .X.Y.Z, where: .X stands for the major version - APIs have changed and binary compatibility is broken. .Y stands for the minor version - Introduction of new APIs, deprecation of old APIs, and source compatibility might be broken but binary compatibility is maintained. .Z stands for the release/patch version - new updates and patches will increment this. Each component in the toolkit is recommended to be semantically versioned. From CUDA 11.3 NVRTC is also semantically versioned. We will note some of them later on in the document. The versions of the components in the toolkit are available in this table . Compatibility of the CUDA platform is thus intended to address a few scenarios: NVIDIA driver upgrades to systems with GPUs running in production for enterprises or datacenters can be complex and may need advance planning. Delays in rolling out new NVIDIA drivers could mean that users of such systems may not have access to new features available in CUDA releases. Not requiring driver updates for new CUDA releases can mean that new versions of the software can be made available faster to users. Many software libraries and applications built on top of CUDA (e.g. math libraries or deep learning frameworks) do not have a direct dependency on the CUDA runtime, compiler or driver. In such cases, users or developers can still benefit from not having to upgrade the entire CUDA Toolkit or driver to use these libraries or frameworks. Upgrading dependencies is error-prone and time consuming, and in some corner cases, can even change the semantics of a program. Constantly recompiling with the latest CUDA Toolkit means forcing upgrades on the end-customers of an application product. Package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process. CUDA supports several compatibility choices: First introduced in CUDA 10, the CUDA Forward Compatible Upgrade is designed to allow users to get access to new CUDA features and run applications built with new CUDA releases on systems with older installations of the NVIDIA datacenter driver. First introduced in CUDA 11.1, CUDA Enhanced Compatibility provides two benefits: By leveraging semantic versioning across components in the CUDA Toolkit, an application can be built for one CUDA minor release (for example 11.1) and work across all future minor releases within the major family (i.e. 11.x). The CUDA runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release. The CUDA driver ensures backward Binary Compatibility is maintained for compiled CUDA applications. Applications compiled with CUDA toolkit versions as old as 3.2 will run on newer drivers. 15.2. Source Compatibility  We define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the SDK) will continue to build and run without errors when a newer version of the SDK is installed. Both the CUDA driver and the CUDA runtime are not source compatible across the different SDK releases. APIs can be deprecated and removed. Therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit. Developers are notified through deprecation and documentation mechanisms of any current or upcoming changes. This does not mean that application binaries compiled using an older toolkit will not be supported anymore. Application binaries rely on CUDA Driver API interface and even though the CUDA Driver API itself may also have changed across toolkit versions, CUDA guarantees Binary Compatibility of the CUDA Driver API interface. 15.3. Binary Compatibility  We define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library. The CUDA Driver API has a versioned C-style ABI, which guarantees that applications that were running against an older driver (for example CUDA 3.2) will still run and function correctly against a modern driver (for example one shipped with CUDA 11.0). This means that even though an application source might need to be changed if it has to be recompiled against a newer CUDA Toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions. The CUDA Driver API thus is binary-compatible (the OS loader can pick up a newer version and the application continues to work) but not source-compatible (rebuilding your application against a newer SDK might require source changes). CUDA Toolkit and Minimum Driver Versions  Before we proceed further on this topic, it’s important for developers to understand the concept of Minimum Driver Version and how that may affect them. Each version of the CUDA Toolkit (and runtime) requires a minimum version of the NVIDIA driver. Applications compiled against a CUDA Toolkit version will only run on systems with the specified minimum driver version for that toolkit version. Prior to CUDA 11.0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the CUDA Toolkit. So, when an application is built with CUDA 11.0, it can only run on a system with an R450 or later driver. If such an application is run on a system with the R418 driver installed, CUDA initialization will return an error as can be seen in the example below. In this example, the deviceQuery sample is compiled with CUDA 11.1 and is run on a system with R418. In this scenario, CUDA initialization returns an error due to the minimum driver requirement. ubuntu@:~/samples/1_Utilities/deviceQuery\n$ make\n/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp\n\n/usr/local/cuda-11.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o\n\n$ nvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   42C    P0    28W /  70W |      0MiB / 15079MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n$ samples/bin/x86_64/linux/release/deviceQuery\nsamples/bin/x86_64/linux/release/deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\ncudaGetDeviceCount returned 3\n-> initialization error\nResult = FAIL Refer to the CUDA Toolkit Release Notes for details for the minimum driver version and the version of the driver shipped with the toolkit. 15.3.1. CUDA Binary (cubin) Compatibility  A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA. CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual. It is however usually more effective to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device. The cubins are architecture-specific. Binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions. In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where z≥y . To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability. For portability, that is, to be able to execute code on future GPU architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled by the NVIDIA driver for these future devices. More information on cubins, PTX and application compatibility can be found in the CUDA C++ Programming Guide . 15.4. CUDA Compatibility Across Minor Releases  By leveraging the semantic versioning, starting with CUDA 11, components in the CUDA Toolkit will remain binary compatible across the minor versions of the toolkit. In order to maintain binary compatibility across minor versions, the CUDA runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a major release is shipped. One of the main reasons a new toolchain requires a new minimum driver is to handle the JIT compilation of PTX code and the JIT linking of binary code. In this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the CUDA platform. 15.4.1. Existing CUDA Applications within Minor Versions of CUDA  $ nvidia - smi +-----------------------------------------------------------------------------+ | NVIDIA - SMI 450.80.02 Driver Version : 450.80.02 CUDA Version : 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence - M | Bus - Id Disp . A | Volatile Uncorr . ECC | | Fan Temp Perf Pwr : Usage / Cap | Memory - Usage | GPU - Util Compute M . | | | | MIG M . | |===============================+======================+======================| | 0 Tesla T4 On | 00000000 : 00 : 1 E .0 Off | 0 | | N / A 39 C P8 9 W / 70 W | 0 MiB / 15109 MiB | 0 % Default | | | | N / A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes : | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ When our CUDA 11.1 application (i.e. cudart 11.1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11.0 version - that is, without requiring the driver or other toolkit components to be updated on the system. $ samples / bin / x86_64 / linux / release / deviceQuery samples / bin / x86_64 / linux / release / deviceQuery Starting ... CUDA Device Query ( Runtime API ) version ( CUDART static linking ) Detected 1 CUDA Capable device ( s ) Device 0 : \"Tesla T4\" CUDA Driver Version / Runtime Version 11.0 / 11.1 CUDA Capability Major / Minor version number : 7.5 ... < snip > ... deviceQuery , CUDA Driver = CUDART , CUDA Driver Version = 11.0 , CUDA Runtime Version = 11.1 , NumDevs = 1 Result = PASS By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features. The following sections discuss some caveats and considerations. 15.4.1.1. Handling New CUDA Features and Driver APIs  A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies. For example, cuMemMap APIs or any of APIs introduced prior to CUDA 11.0, such as cudaDeviceSynchronize , do not require a driver upgrade. To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully. This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions. Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release. When working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older CUDA driver. Users wishing to take advantage of such a feature should query its availability with a dynamic check in the code: static bool hostRegisterFeatureSupported = false ; static bool hostRegisterIsDeviceAddress = false ; static error_t cuFooFunction ( int * ptr ) { int * dptr = null ; if ( hostRegisterFeatureSupported ) { cudaHostRegister ( ptr , size , flags ); if ( hostRegisterIsDeviceAddress ) { qptr = ptr ; } else { cudaHostGetDevicePointer ( & qptr , ptr , 0 ); } } else { // cudaMalloc(); // cudaMemcpy(); } gemm <<< 1 , 1 >>> ( dptr ); cudaDeviceSynchronize (); } int main () { // rest of code here cudaDeviceGetAttribute ( & hostRegisterFeatureSupported , cudaDevAttrHostRegisterSupported , 0 ); cudaDeviceGetAttribute ( & hostRegisterIsDeviceAddress , cudaDevAttrCanUseHostPointerForRegisteredMem , 0 ); cuFooFunction ( /* malloced pointer */ ); } Alternatively the application’s interface might not work at all without a new CUDA driver and then its best to return an error right away: #define MIN_VERSION 11010 cudaError_t foo () { int version = 0 ; cudaGetDriverVersion ( & version ); if ( version < MIN_VERSION ) { return CUDA_ERROR_INSUFFICIENT_DRIVER ; } // proceed as normal } A new error code is added to indicate that the functionality is missing from the driver you are running against: cudaErrorCallRequiresNewerDriver . 15.4.1.2. Using PTX  PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs are translated at load time to the target hardware instruction set via the JIT Compiler which is part of the CUDA driver. As PTX is compiled by the CUDA driver, new toolchains will generate PTX that is not compatible with the older CUDA driver. This is not a problem when PTX is used for future device compatibility (the most common case), but can lead to issues when used for runtime compilation. For codes continuing to make use of PTX, in order to support compiling on an older driver, your code must be first transformed into device code via the static ptxjitcompiler library or NVRTC with the option of generating code for a specific architecture (e.g. sm_80) rather than a virtual architecture (e.g. compute_80). For this workflow, a new nvptxcompiler_static library is shipped with the CUDA Toolkit. We can see this usage in the following example: char * compilePTXToNVElf () { nvPTXCompilerHandle compiler = NULL ; nvPTXCompileResult status ; size_t elfSize , infoSize , errorSize ; char * elf , * infoLog , * errorLog ; int minorVer , majorVer ; const char * compile_options [] = { \"--gpu-name=sm_80\" , \"--device-debug\" }; nvPTXCompilerGetVersion ( & majorVer , & minorVer ); nvPTXCompilerCreate ( & compiler , ( size_t ) strlen ( ptxCode ), ptxCode ); status = nvPTXCompilerCompile ( compiler , 2 , compile_options ); if ( status != NVPTXCOMPILE_SUCCESS ) { nvPTXCompilerGetErrorLogSize ( compiler , ( void * ) & errorSize ); if ( errorSize != 0 ) { errorLog = ( char * ) malloc ( errorSize + 1 ); nvPTXCompilerGetErrorLog ( compiler , ( void * ) errorLog ); printf ( \"Error log: %s \\n \" , errorLog ); free ( errorLog ); } exit ( 1 ); } nvPTXCompilerGetCompiledProgramSize ( compiler , & elfSize )); elf = ( char * ) malloc ( elfSize ); nvPTXCompilerGetCompiledProgram ( compiler , ( void * ) elf ); nvPTXCompilerGetInfoLogSize ( compiler , ( void * ) & infoSize ); if ( infoSize != 0 ) { infoLog = ( char * ) malloc ( infoSize + 1 ); nvPTXCompilerGetInfoLog ( compiler , ( void * ) infoLog ); printf ( \"Info log: %s \\n \" , infoLog ); free ( infoLog ); } nvPTXCompilerDestroy ( & compiler ); return elf ; } 15.4.1.3. Dynamic Code Generation  NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx. Dealing with relocatable objects is not yet supported, therefore the cuLink * set of APIs in the CUDA driver will not work with enhanced compatibility. An upgraded driver matching the CUDA runtime version is currently required for those APIs. As mentioned in the PTX section, the compilation of PTX to device code lives along with the CUDA driver, hence the generated PTX might be newer than what is supported by the driver on the deployment system. When using NVRTC, it is recommended that the resulting PTX code is first transformed to the final device code via the steps outlined by the PTX user workflow. This ensures your code is compatible. Alternatively, NVRTC can generate cubins directly starting with CUDA 11.1. Applications using the new API can load the final device code directly using driver APIs cuModuleLoadData and cuModuleLoadDataEx . NVRTC used to support only virtual architectures through the option -arch, since it was only emitting PTX. It will now support actual architectures as well to emit SASS. The interface is augmented to retrieve either the PTX or cubin if an actual architecture is specified. The example below shows how an existing example can be adapted to use the new features, guarded by the USE_CUBIN macro in this case: #include <nvrtc.h> #include <cuda.h> #include <iostream> void NVRTC_SAFE_CALL ( nvrtcResult result ) { if ( result != NVRTC_SUCCESS ) { std :: cerr << \" \\n nvrtc error: \" << nvrtcGetErrorString ( result ) << '\\n' ; std :: exit ( 1 ); } } void CUDA_SAFE_CALL ( CUresult result ) { if ( result != CUDA_SUCCESS ) { const char * msg ; cuGetErrorName ( result , & msg ); std :: cerr << \" \\n cuda error: \" << msg << '\\n' ; std :: exit ( 1 ); } } const char * hello = \" \\n \\ extern \\\" C \\\" __global__ void hello() { \\n \\ printf( \\\" hello world \\\\ n \\\" ); \\n \\ } \\n \" ; int main () { nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog , hello , \"hello.cu\" , 0 , NULL , NULL )); #ifdef USE_CUBIN const char * opts [] = { \"-arch=sm_70\" }; #else const char * opts [] = { \"-arch=compute_70\" }; #endif nvrtcResult compileResult = nvrtcCompileProgram ( prog , 1 , opts ); size_t logSize ; NVRTC_SAFE_CALL ( nvrtcGetProgramLogSize ( prog , & logSize )); char * log = new char [ logSize ]; NVRTC_SAFE_CALL ( nvrtcGetProgramLog ( prog , log )); std :: cout << log << '\\n' ; delete [] log ; if ( compileResult != NVRTC_SUCCESS ) exit ( 1 ); size_t codeSize ; #ifdef USE_CUBIN NVRTC_SAFE_CALL ( nvrtcGetCUBINSize ( prog , & codeSize )); char * code = new char [ codeSize ]; NVRTC_SAFE_CALL ( nvrtcGetCUBIN ( prog , code )); #else NVRTC_SAFE_CALL ( nvrtcGetPTXSize ( prog , & codeSize )); char * code = new char [ codeSize ]; NVRTC_SAFE_CALL ( nvrtcGetPTX ( prog , code )); #endif NVRTC_SAFE_CALL ( nvrtcDestroyProgram ( & prog )); CUdevice cuDevice ; CUcontext context ; CUmodule module ; CUfunction kernel ; CUDA_SAFE_CALL ( cuInit ( 0 )); CUDA_SAFE_CALL ( cuDeviceGet ( & cuDevice , 0 )); CUDA_SAFE_CALL ( cuCtxCreate ( & context , 0 , cuDevice )); CUDA_SAFE_CALL ( cuModuleLoadDataEx ( & module , code , 0 , 0 , 0 )); CUDA_SAFE_CALL ( cuModuleGetFunction ( & kernel , module , \"hello\" )); CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , 1 , 1 , 1 , 1 , 1 , 1 , 0 , NULL , NULL , 0 )); CUDA_SAFE_CALL ( cuCtxSynchronize ()); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); delete [] code ; } 15.4.1.4. Recommendations for building a minor-version compatible library  We recommend that the CUDA runtime be statically linked to minimize dependencies. Verify that your library doesn’t leak dependencies, breakages, namespaces, etc. outside your established ABI contract. Follow semantic versioning for your library’s soname. Having a semantically versioned ABI means the interfaces need to be maintained and versioned. The library should follow semantic rules and increment the version number when a change is made that affects this ABI contract. Missing dependencies is also a binary compatibility break, hence you should provide fallbacks or guards for functionality that depends on those interfaces. Increment major versions when there are ABI breaking changes such as API deprecation and modifications. New APIs can be added in minor versions. Conditionally use features to remain compatible against older drivers. If no new features are used (or if they are used conditionally with fallbacks provided) you’ll be able to remain compatible. Don’t expose ABI structures that can change. A pointer to a structure with a size embedded is a better solution. When linking with dynamic libraries from the toolkit, the library must be equal to or newer than what is needed by any one of the components involved in the linking of your application. For example, if you link against the CUDA 11.1 dynamic runtime, and use functionality from 11.1, as well as a separate shared library that was linked against the CUDA 11.2 dynamic runtime that requires 11.2 functionality, the final link step must include a CUDA 11.2 or newer dynamic runtime. 15.4.1.5. Recommendations for taking advantage of minor version compatibility in your application  Certain functionality might not be available so you should query where applicable. This is common for building applications that are GPU architecture, platform and compiler agnostic. However we now add “the underlying driver” to that mix. As with the previous section on library building recommendations, if using the CUDA runtime, we recommend linking to the CUDA runtime statically when building your application. When using the driver APIs directly, we recommend using the new driver entry point access API ( cuGetProcAddress ) documented here: CUDA Driver API :: CUDA Toolkit Documentation . When using a shared or static library, follow the release notes of said library to determine if the library supports minor version compatibility. 16. Preparing for Deployment  16.1. Testing for CUDA Availability  When deploying a CUDA application, it is often desirable to ensure that the application will continue to function properly even if the target machine does not have a CUDA-capable GPU and/or a sufficient version of the NVIDIA Driver installed. (Developers targeting a single machine with known configuration may choose to skip this section.) Detecting a CUDA-Capable GPU When an application will be deployed to target machines of arbitrary/unknown configuration, the application should explicitly test for the existence of a CUDA-capable GPU in order to take appropriate action when no such device is available. The cudaGetDeviceCount() function can be used to query for the number of available devices. Like all CUDA Runtime API functions, this function will fail gracefully and return cudaErrorNoDevice to the application if there is no CUDA-capable GPU or cudaErrorInsufficientDriver if there is not an appropriate version of the NVIDIA Driver installed. If cudaGetDeviceCount() reports an error, the application should fall back to an alternative code path. A system with multiple GPUs may contain GPUs of different hardware versions and capabilities. When using multiple GPUs from the same application, it is recommended to use GPUs of the same type, rather than mixing hardware generations. The cudaChooseDevice() function can be used to select the device that most closely matches a desired set of features. Detecting Hardware and Software Configuration When an application depends on the availability of certain hardware or software capabilities to enable certain functionality, the CUDA API can be queried for details about the configuration of the available device and for the installed software versions. The cudaGetDeviceProperties() function reports various features of the available devices, including the CUDA Compute Capability of the device (see also the Compute Capabilities section of the CUDA C++ Programming Guide). See Version Management for details on how to query the available CUDA software API versions. 16.2. Error Handling  All CUDA Runtime API calls return an error code of type cudaError_t ; the return value will be equal to cudaSuccess if no errors have occurred. (The exceptions to this are kernel launches, which return void, and cudaGetErrorString() , which returns a character string describing the cudaError_t code that was passed into it.) The CUDA Toolkit libraries ( cuBLAS , cuFFT , etc.) likewise return their own sets of error codes. Since some CUDA API calls and all kernel launches are asynchronous with respect to the host code, errors may be reported to the host asynchronously as well; often this occurs the next time the host and device synchronize with each other, such as during a call to cudaMemcpy() or to cudaDeviceSynchronize() . Always check the error return values on all CUDA API functions, even for functions that are not expected to fail, as this will allow the application to detect and recover from errors as soon as possible should they occur. To check for errors occurring during kernel launches using the <<<...>>> syntax, which does not return any error code, the return code of cudaGetLastError() should be checked immediately after the kernel launch. Applications that do not check for CUDA API errors could at times run to completion without having noticed that the data calculated by the GPU is incomplete, invalid, or uninitialized. Note The CUDA Toolkit Samples provide several helper functions for error checking with the various CUDA APIs; these helper functions are located in the samples/common/inc/helper_cuda.h file in the CUDA Toolkit. 16.3. Building for Maximum Compatibility  Each generation of CUDA-capable device has an associated compute capability version that indicates the feature set supported by the device (see CUDA Compute Capability ). One or more compute capability versions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target GPU(s) of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of GPU. When an application is built for multiple compute capabilities simultaneously (using several instances of the -gencode flag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the CUDA Driver selects the most appropriate binary at runtime according to the compute capability of the present device. If an appropriate native binary ( cubin ) is not available, but the intermediate PTX code (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiled Just In Time (JIT) (see Compiler JIT Cache Management Tools ) from the PTX to the native cubin for the device. If the PTX is also not available, then the kernel launch will fail. Windows nvcc.exe -ccbin \"C:\\vs2008\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\"\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  --compile -o \"Release\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc\n  -gencode=arch=compute_30,code=sm_30\n  -gencode=arch=compute_35,code=sm_35\n  -gencode=arch=compute_50,code=sm_50\n  -gencode=arch=compute_60,code=sm_60\n  -gencode=arch=compute_70,code=sm_70\n  -gencode=arch=compute_75,code=sm_75\n  -gencode=arch=compute_75,code=compute_75\n  -O2 -o mykernel.o -c mykernel.cu Alternatively, the nvcc command-line option -arch=sm_XX can be used as a shorthand equivalent to the following more explicit -gencode= command-line options described above: -gencode=arch=compute_XX,code=sm_XX\n-gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default (due to the code=compute_XX target it implies), it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 16.4. Distributing the CUDA Runtime and Libraries  CUDA applications are built against the CUDA Runtime library, which handles device, memory, and kernel management. Unlike the CUDA Driver, the CUDA Runtime guarantees neither forward nor backward binary compatibility across versions. It is therefore best to redistribute the CUDA Runtime library with the application when using dynamic linking or else to statically link against the CUDA Runtime. This will ensure that the executable will be able to run even if the user does not have the same CUDA Toolkit installed that the application was built against. Note When statically linking to the CUDA Runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously; for example, if an application uses one version of the CUDA Runtime, and a plugin to that application is statically linked to a different version, that is perfectly acceptable, as long as the installed NVIDIA Driver is sufficient for both. Statically-linked CUDA Runtime The easiest option is to statically link against the CUDA Runtime. This is the default if using nvcc to link in CUDA 5.5 and later. Static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the CUDA Runtime library. Dynamically-linked CUDA Runtime If static linking against the CUDA Runtime is impractical for some reason, then a dynamically-linked version of the CUDA Runtime library is also available. (This was the default and only option provided in CUDA versions 5.0 and earlier.) To use dynamic linking with the CUDA Runtime when using the nvcc from CUDA 5.5 or later to link the application, add the --cudart=shared flag to the link command line; otherwise the statically-linked CUDA Runtime library is used by default. After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be bundled with the application. It can be copied into the same directory as the application executable or into a subdirectory of that installation path. Other CUDA Libraries Although the CUDA Runtime provides the option of static linking, some libraries included in the CUDA Toolkit are available only in dynamically-linked form. As with the dynamically-linked version of the CUDA Runtime library , these libraries should be bundled with the application executable when distributing that application. 16.4.1. CUDA Toolkit Library Redistribution  The CUDA Toolkit’s End-User License Agreement (EULA) allows for redistribution of many of the CUDA libraries under certain terms and conditions. This allows applications that depend on these libraries to redistribute the exact versions of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the CUDA Toolkit (or perhaps none at all) installed on their machines. Please refer to the EULA for details. Note This does not apply to the NVIDIA Driver; the end user must still download and install an NVIDIA Driver appropriate to their GPU(s) and operating system. 16.4.1.1. Which Files to Redistribute  When redistributing the dynamically-linked versions of one or more CUDA libraries, it is important to identify the exact files that need to be redistributed. The following examples use the cuBLAS library from CUDA Toolkit 5.5 as an illustration: Linux In a shared library on Linux, there is a string field called the SONAME that indicates the binary compatibility level of the library. The SONAME of the library against which the application was built must match the filename of the library that is redistributed with the application. For example, in the standard CUDA Toolkit installation, the files libcublas.so and libcublas.so.5.5 are both symlinks pointing to a specific build of cuBLAS, which is named like libcublas.so.5.5.x , where x is the build number (e.g., libcublas.so.5.5.17 ). However, the SONAME of this library is given as “ libcublas.so.5.5 ”: $ objdump -p /usr/local/cuda/lib64/libcublas.so | grep SONAME\n   SONAME               libcublas.so.5.5 Because of this, even if -lcublas (with no version number specified) is used when linking the application, the SONAME found at link time implies that “ libcublas.so.5.5 ” is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file (or a symlink to the same) that is redistributed with the application. The ldd tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the dynamic loader would select when loading the application given the current library search path: $ ldd a.out | grep libcublas\n   libcublas.so.5.5 => /usr/local/cuda/lib64/libcublas.so.5.5 Mac In a shared library on Mac OS X, there is a field called the install name that indicates the expected installation path and filename the library; the CUDA libraries also use this filename to indicate binary compatibility. The value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime. For example, if the install name of the cuBLAS library is given as @rpath/libcublas.5.5.dylib , then the library is version 5.5 and the copy of this library redistributed with the application must be named libcublas.5.5.dylib , even though only -lcublas (with no version number specified) is used at link time. Furthermore, this file should be installed into the @rpath of the application; see Where to Install Redistributed CUDA Libraries . To view a library’s install name, use the otool -L command: $ otool -L a.out\na.out:\n        @rpath/libcublas.5.5.dylib (...) Windows The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename. For example, a 64-bit application linked to cuBLAS 5.5 will look for cublas64_55.dll at runtime, so this is the file that should be redistributed with that application, even though cublas.lib is the file that the application is linked against. For 32-bit applications, the file would be cublas32_55.dll . To verify the exact DLL filename that the application expects to find at runtime, use the dumpbin tool from the Visual Studio command prompt: $ dumpbin /IMPORTS a.exe\nMicrosoft (R) COFF/PE Dumper Version 10.00.40219.01\nCopyright (C) Microsoft Corporation.  All rights reserved.\n\n\nDump of file a.exe\n\nFile Type: EXECUTABLE IMAGE\n\n  Section contains the following imports:\n\n    ...\n    cublas64_55.dll\n    ... 16.4.1.2. Where to Install Redistributed CUDA Libraries  Once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them. On Windows, if the CUDA Runtime or other dynamically-linked CUDA Toolkit library is placed in the same directory as the executable, Windows will locate it automatically. On Linux and Mac, the -rpath linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths: Linux/Mac nvcc -I $(CUDA_HOME)/include\n  -Xlinker \"-rpath '$ORIGIN'\" --cudart=shared\n  -o myprogram myprogram.cu Windows nvcc.exe -ccbin \"C:\\vs2008\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" --cudart=shared\n  -o \"Release\\myprogram.exe\" \"myprogram.cu\" Note It may be necessary to adjust the value of -ccbin to reflect the location of your Visual Studio installation. To specify an alternate path where the libraries will be distributed, use linker options similar to those below: Linux/Mac nvcc -I $(CUDA_HOME)/include\n  -Xlinker \"-rpath '$ORIGIN/lib'\" --cudart=shared\n  -o myprogram myprogram.cu Windows nvcc.exe -ccbin \"C:\\vs2008\\VC\\bin\"\n  -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT /DELAY\" --cudart=shared\n  -o \"Release\\myprogram.exe\" \"myprogram.cu\" For Linux and Mac, the -rpath option is used as before. For Windows, the /DELAY option is used; this requires that the application call SetDllDirectory() before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs. Note For Windows 8, SetDefaultDLLDirectories() and AddDllDirectory() should be used instead of SetDllDirectory() . Please see the MSDN documentation for these routines for more information. 17. Deployment Infrastructure Tools  17.1. Nvidia-SMI  The NVIDIA System Management Interface ( nvidia-smi ) is a command line utility that aids in the management and monitoring of NVIDIA GPU devices. This utility allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state. nvidia-smi is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs. nvidia-smi ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7. nvidia-smi can output queried information as XML or as human-readable plain text either to standard output or to a file. See the nvidia-smi documenation for details. Please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions. 17.1.1. Queryable state  ECC error counts Both correctable single-bit and detectable double-bit errors are reported. Error counts are provided for both the current boot cycle and the lifetime of the GPU. GPU utilization Current utilization rates are reported for both the compute resources of the GPU and the memory interface. Active compute process The list of active processes running on the GPU is reported, along with the corresponding process name/ID and allocated GPU memory. Clocks and performance state Max and current clock rates are reported for several important clock domains, as well as the current GPU performance state ( pstate ). Temperature and fan speed The current GPU core temperature is reported, along with fan speeds for products with active cooling. Power management The current board power draw and power limits are reported for products that report these measurements. Identification Various dynamic and static information is reported, including board serial numbers, PCI device IDs, VBIOS/Inforom version numbers and product names. 17.1.2. Modifiable state  ECC mode Enable and disable ECC reporting. ECC reset Clear single-bit and double-bit ECC error counts. Compute mode Indicate whether compute processes can run on the GPU and whether they run exclusively or concurrently with other compute processes. Persistence mode Indicate whether the NVIDIA driver stays loaded when no applications are connected to the GPU. It is best to enable this option in most circumstances. GPU reset Reinitialize the GPU hardware and software state via a secondary bus reset. 17.2. NVML  The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications. The NVML API is shipped with the CUDA Toolkit (since version 8.0) and is also available standalone on the NVIDIA developer website as part of the GPU Deployment Kit through a single header file accompanied by PDF documentation, stub libraries, and sample applications; see https://developer.nvidia.com/gpu-deployment-kit . Each new version of NVML is backward-compatible. An additional set of Perl and Python bindings are provided for the NVML API. These bindings expose the same features as the C-based interface and also provide backwards compatibility. The Perl bindings are provided via CPAN and the Python bindings via PyPI. All of these products ( nvidia-smi , NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality. See https://developer.nvidia.com/nvidia-management-library-nvml for additional information. 17.3. Cluster Management Tools  Managing your GPU cluster will help achieve maximum GPU utilization and help you and your users extract the best possible performance. Many of the industry’s most popular cluster management tools support CUDA GPUs via NVML. For a listing of some of these tools, see https://developer.nvidia.com/cluster-management . 17.4. Compiler JIT Cache Management Tools  Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver. This is called just-in-time compilation ( JIT ). Just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements. It is also the only way for applications to run on devices that did not exist at the time the application was compiled. When JIT compilation of PTX device code is used, the NVIDIA driver caches the resulting binary code on disk. Some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see Just in Time Compilation of the CUDA C++ Programming Guide. 17.5. CUDA_VISIBLE_DEVICES  It is possible to rearrange the collection of installed CUDA devices that will be visible to and enumerated by a CUDA application prior to the start of that application by way of the CUDA_VISIBLE_DEVICES environment variable. Devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices. For example, to use only devices 0 and 2 from the system-wide list of devices, set CUDA_VISIBLE_DEVICES=0,2 before launching the application. The application will then enumerate these devices as device 0 and device 1, respectively. 18. Recommendations and Best Practices  This chapter contains a summary of the recommendations for optimization that are explained in this document. 18.1. Overall Performance Optimization Strategies  Performance optimization revolves around three basic strategies: Maximizing parallel execution Optimizing memory usage to achieve maximum memory bandwidth Optimizing instruction usage to achieve maximum instruction throughput Maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible. Once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible. This is done by carefully choosing the execution configuration of each kernel launch. The application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device. Optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers. Kernel access to global memory also should be minimized by maximizing the use of shared memory on the device. Sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed. The effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory. The next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns. This optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles. Shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts. As for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided. This suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision. Finally, particular attention must be paid to control flow instructions due to the SIMT (single instruction multiple thread) nature of the device. 19. nvcc Compiler Switches  19.1. nvcc  The NVIDIA nvcc compiler driver converts .cu files into C++ for the host system and CUDA assembly or binary instructions for the device. It supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices: -maxrregcount=N specifies the maximum number of registers kernels can use at a per-file level. See Register Pressure . (See also the __launch_bounds__ qualifier discussed in Execution Configuration of the CUDA C++ Programming Guide to control the number of registers used on a per-kernel basis.) --ptxas-options=-v or -Xptxas=-v lists per-kernel register, shared, and constant memory usage. -ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. This makes the code run faster at the cost of diminished precision and accuracy. See Math Libraries . 20. Notices  20.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 20.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 20.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html", "content_type": "text/html", "text": "CUDA 12.5 Update 1 Release Notes 1. CUDA 12.5 Update 1 Release Notes 1.1. CUDA Toolkit Major Component Versions 1.2. New Features 1.2.1. General CUDA 1.2.2. CUDA Compiler 1.2.3. CUDA Developer Tools 1.3. Resolved Issues 1.3.1. CUDA Compiler 1.4. Known Issues and Limitations 1.5. Deprecated or Dropped Features 1.5.1. Deprecated or Dropped Architectures 1.5.2. Deprecated Operating Systems 1.5.3. Deprecated Toolchains 1.5.4. CUDA Tools 2. CUDA Libraries 2.1. cuBLAS Library 2.1.1. cuBLAS: Release 12.5 Update 1 2.1.2. cuBLAS: Release 12.5 2.1.3. cuBLAS: Release 12.4 Update 1 2.1.4. cuBLAS: Release 12.4 2.1.5. cuBLAS: Release 12.3 Update 1 2.1.6. cuBLAS: Release 12.3 2.1.7. cuBLAS: Release 12.2 Update 2 2.1.8. cuBLAS: Release 12.2 2.1.9. cuBLAS: Release 12.1 Update 1 2.1.10. cuBLAS: Release 12.0 Update 1 2.1.11. cuBLAS: Release 12.0 2.2. cuFFT Library 2.2.1. cuFFT: Release 12.5 2.2.2. cuFFT: Release 12.4 Update 1 2.2.3. cuFFT: Release 12.4 2.2.4. cuFFT: Release 12.3 Update 1 2.2.5. cuFFT: Release 12.3 2.2.6. cuFFT: Release 12.2 2.2.7. cuFFT: Release 12.1 Update 1 2.2.8. cuFFT: Release 12.1 2.2.9. cuFFT: Release 12.0 Update 1 2.2.10. cuFFT: Release 12.0 2.3. cuSOLVER Library 2.3.1. cuSOLVER: Release 12.5 Update 1 2.3.2. cuSOLVER: Release 12.5 2.3.3. cuSOLVER: Release 12.4 Update 1 2.3.4. cuSOLVER: Release 12.4 2.3.5. cuSOLVER: Release 12.2 Update 2 2.3.6. cuSOLVER: Release 12.2 2.4. cuSPARSE Library 2.4.1. cuSPARSE: Release 12.5 Update 1 2.4.2. cuSPARSE: Release 12.5 2.4.3. cuSPARSE: Release 12.4 2.4.4. cuSPARSE: Release 12.3 Update 1 2.4.5. cuSPARSE: Release 12.3 2.4.6. cuSPARSE: Release 12.2 Update 1 2.4.7. cuSPARSE: Release 12.1 Update 1 2.4.8. cuSPARSE: Release 12.0 Update 1 2.4.9. cuSPARSE: Release 12.0 2.5. Math Library 2.5.1. CUDA Math: Release 12.5 2.5.2. CUDA Math: Release 12.4 2.5.3. CUDA Math: Release 12.3 2.5.4. CUDA Math: Release 12.2 2.5.5. CUDA Math: Release 12.1 2.5.6. CUDA Math: Release 12.0 2.6. NVIDIA Performance Primitives (NPP) 2.6.1. NPP: Release 12.4 2.6.2. NPP: Release 12.0 2.7. nvJPEG Library 2.7.1. nvJPEG: Release 12.4 2.7.2. nvJPEG: Release 12.3 Update 1 2.7.3. nvJPEG: Release 12.2 2.7.4. nvJPEG: Release 12.0 3. Notices 3.1. Notice 3.2. OpenCL 3.3. Trademarks Release Notes » 1. CUDA 12.5 Update 1 Release Notes v12.5 | PDF | Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit. 1. CUDA 12.5 Update 1 Release Notes  The release notes for the NVIDIA® CUDA® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html . Note The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases. 1.1. CUDA Toolkit Major Component Versions  CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently. For CUDA 12.5 Update 1, the table below indicates the versions: Table 1 CUDA 12.5 Update 1 Component Versions  Component Name Version Information Supported Architectures Supported Platforms CUDA C++ Core Compute Libraries Thrust 2.4.0 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUB 2.4.0 libcu++ 2.4.0 Cooperative Groups 12.5.82 CUDA Compatibility 12.5.36505571 aarch64-jetson Linux CUDA Runtime (cudart) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuobjdump 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUPTI 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuxxfilt (demangler) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA Demo Suite 12.5.82 x86_64 Linux, Windows CUDA GDB 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, WSL CUDA Nsight Eclipse Plugin 12.5.82 x86_64 Linux CUDA NVCC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvdisasm 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA NVML Headers 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvprof 12.5.82 x86_64 Linux, Windows CUDA nvprune 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVRTC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL NVTX 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVVP 12.5.82 x86_64, Linux, Windows CUDA OpenCL 12.5.39 x86_64 Linux, Windows CUDA Profiler API 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA Compute Sanitizer API 12.5.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuBLAS 12.5.3.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuDLA 12.5.82 aarch64-jetson Linux CUDA cuFFT 11.2.3.61 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuFile 1.10.1.7 x86_64, arm64-sbsa, aarch64-jetson Linux CUDA cuRAND 10.3.6.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSOLVER 11.6.3.83 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSPARSE 12.5.1.3 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NPP 12.3.0.159 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvFatbin 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJitLink 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJPEG 12.3.2.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL Nsight Compute 2024.2.1.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL (Windows 11) Nsight Systems 2024.2.3.38 x86_64, arm64-sbsa, Linux, Windows, WSL Nsight Visual Studio Edition (VSE) 2024.2.1.24155 x86_64 (Windows) Windows nvidia_fs 1 2.20.6 x86_64, arm64-sbsa, aarch64-jetson Linux Visual Studio Integration 12.5.82 x86_64 (Windows) Windows NVIDIA Linux Driver 555.42.06 x86_64, arm64-sbsa Linux NVIDIA Windows Driver 555.85 x86_64 (Windows) Windows, WSL CUDA Driver Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3 . For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus . Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases. More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades . Note : Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below. The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility  CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x >=525.60.13 >=528.33 CUDA 11.8.x\r\nCUDA 11.7.x\r\nCUDA 11.6.x\r\nCUDA 11.5.x\r\nCUDA 11.4.x\r\nCUDA 11.3.x\r\nCUDA 11.2.x\r\nCUDA 11.1.x >=450.80.02 >=452.39 CUDA 11.0 (11.0.3) >=450.36.06** >=451.22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode – please read the CUDA Compatibility Guide for details. ** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits. The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below. Table 3 CUDA Toolkit and Corresponding Driver Versions  CUDA Toolkit Toolkit Driver Version Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.5 Update 1 >=555.42.06 >=555.85 CUDA 12.5 GA >=555.42.02 >=555.85 CUDA 12.4 Update 1 >=550.54.15 >=551.78 CUDA 12.4 GA >=550.54.14 >=551.61 CUDA 12.3 Update 1 >=545.23.08 >=546.12 CUDA 12.3 GA >=545.23.06 >=545.84 CUDA 12.2 Update 2 >=535.104.05 >=537.13 CUDA 12.2 Update 1 >=535.86.09 >=536.67 CUDA 12.2 GA >=535.54.03 >=536.25 CUDA 12.1 Update 1 >=530.30.02 >=531.14 CUDA 12.1 GA >=530.30.02 >=531.14 CUDA 12.0 Update 1 >=525.85.12 >=528.33 CUDA 12.0 GA >=525.60.13 >=527.41 CUDA 11.8 GA >=520.61.05 >=520.06 CUDA 11.7 Update 1 >=515.48.07 >=516.31 CUDA 11.7 GA >=515.43.04 >=516.01 CUDA 11.6 Update 2 >=510.47.03 >=511.65 CUDA 11.6 Update 1 >=510.47.03 >=511.65 CUDA 11.6 GA >=510.39.01 >=511.23 CUDA 11.5 Update 2 >=495.29.05 >=496.13 CUDA 11.5 Update 1 >=495.29.05 >=496.13 CUDA 11.5 GA >=495.29.05 >=496.04 CUDA 11.4 Update 4 >=470.82.01 >=472.50 CUDA 11.4 Update 3 >=470.82.01 >=472.50 CUDA 11.4 Update 2 >=470.57.02 >=471.41 CUDA 11.4 Update 1 >=470.57.02 >=471.41 CUDA 11.4.0 GA >=470.42.01 >=471.11 CUDA 11.3.1 Update 1 >=465.19.01 >=465.89 CUDA 11.3.0 GA >=465.19.01 >=465.89 CUDA 11.2.2 Update 2 >=460.32.03 >=461.33 CUDA 11.2.1 Update 1 >=460.32.03 >=461.09 CUDA 11.2.0 GA >=460.27.03 >=460.82 CUDA 11.1.1 Update 1 >=455.32 >=456.81 CUDA 11.1 GA >=455.23 >=456.38 CUDA 11.0.3 Update 1 >= 450.51.06 >= 451.82 CUDA 11.0.2 GA >= 450.51.05 >= 451.48 CUDA 11.0.1 RC >= 450.36.06 >= 451.22 CUDA 10.2.89 >= 440.33 >= 441.22 CUDA 10.1 (10.1.105 general release, and updates) >= 418.39 >= 418.96 CUDA 10.0.130 >= 410.48 >= 411.31 CUDA 9.2 (9.2.148 Update 1) >= 396.37 >= 398.26 CUDA 9.2 (9.2.88) >= 396.26 >= 397.44 CUDA 9.1 (9.1.85) >= 390.46 >= 391.29 CUDA 9.0 (9.0.76) >= 384.81 >= 385.54 CUDA 8.0 (8.0.61 GA2) >= 375.26 >= 376.51 CUDA 8.0 (8.0.44) >= 367.48 >= 369.30 CUDA 7.5 (7.5.16) >= 352.31 >= 353.66 CUDA 7.0 (7.0.28) >= 346.46 >= 347.62 For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs. For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers . During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages). For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software . For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas . 1.2. New Features  This section lists new general CUDA and CUDA compilers features. 1.2.1. General CUDA  In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.\r\nEnd-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules. MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here . 1.2.2. CUDA Compiler  For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5 . 1.2.3. CUDA Developer Tools  For changes to nvprof and Visual Profiler, see the changelog . For new features, improvements, and bug fixes in Nsight Systems, see the changelog . For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog . For new features, improvements, and bug fixes in CUPTI, see the changelog . For new features, improvements, and bug fixes in Nsight Compute, see the changelog . For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog . For new features, improvements, and bug fixes in CUDA-GDB, see the changelog . 1.3. Resolved Issues  1.3.1. CUDA Compiler  Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device. Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler. Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag. Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops. Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”. Fix to correct the calculation of write-after-read hazard latency. 1.4. Known Issues and Limitations  Runfile will not be supported for Amazon Linux 2023. Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features. Launching Cooperative Group kernels with MPS is not supported on Tegra platforms. 1.5. Deprecated or Dropped Features  Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software. 1.5.1. Deprecated or Dropped Architectures  NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5. 1.5.2. Deprecated Operating Systems  NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5. CUDA 12.5 is the last release to support Debian 10. Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated. 1.5.3. Deprecated Toolchains  CUDA Toolkit 12.4 deprecated support for the following host compilers: Microsoft Visual C/C++ (MSVC) 2017 All GCC versions prior to GCC 7.3 1.5.4. CUDA Tools  Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release. 2. CUDA Libraries  This section covers CUDA Libraries release notes for 12.x releases. CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host. 2.1. cuBLAS Library  2.1.1. cuBLAS: Release 12.5 Update 1  New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs. Known Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release. cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release. Resolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error. cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D). 2.1.2. cuBLAS: Release 12.5  New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details. Known Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types. Resolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results. cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv. 2.1.3. cuBLAS: Release 12.4 Update 1  Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release. cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release. Resolved Issues cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 ( CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 ). Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul() , cublasLtMatmulAlgoCheck() , and cublasLtMatmulAlgoGetHeuristic() . The issue was introduced in CUDA Toolkit 12.4. cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8. 2.1.4. cuBLAS: Release 12.4  New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH .  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details. Known Issues When the current context has been created using cuGreenCtxCreate() , cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget() . BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE . This is the same known issue documented in cuBLAS 12.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6. When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync . However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.5. cuBLAS: Release 12.3 Update 1  New Features Improved performance of heuristics cache for workloads that have a high eviction rate. Known Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE . The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST . Resolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute() . Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS). cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient. 2.1.6. cuBLAS: Release 12.3  New Features Improved performance on NVIDIA L40S Ada GPUs. Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute() . To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit() . This will be fixed in an upcoming release. 2.1.7. cuBLAS: Release 12.2 Update 2  New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable. Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. 2.1.8. cuBLAS: Release 12.2  Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue. Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE .  The kernels apply the first batch’s bias vector to all batches. This will be fixed in a future release. 2.1.9. cuBLAS: Release 12.1 Update 1  New Features Support for FP8 on NVIDIA Ada GPUs. Improved performance on NVIDIA L4 Ada GPUs. Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions . Known Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t . The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes. 2.1.10. cuBLAS: Release 12.0 Update 1  New Features Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs. Known Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture. Resolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release. Added forward compatible single precision complex GEMM that does not require workspace. 2.1.11. cuBLAS: Release 12.0  New Features cublasLtMatmul now supports FP8 with a non-zero beta. Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface . Added more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux. Known Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release. Resolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE ) could return incorrect results for the bias gradient. cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues. Deprecations Disallow including cublas.h and cublas_v2.h in the same translation unit. Removed: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t . No kernels utilize these stages anymore. cublasLt3mMode_t , CUBLASLT_MATMUL_PREF_MATH_MODE_MASK , and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t . Instead, use the corresponding flags from cublasLtNumericalImplFlags_t . CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK , CUBLASLT_MATMUL_PREF_EPILOGUE_MASK , and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t . The corresponding parameters are taken directly from cublasLtMatmulDesc_t . CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t . This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. 2.2. cuFFT Library  2.2.1. cuFFT: Release 12.5  New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes . We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API. 2.2.2. cuFFT: Release 12.4 Update 1  Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ) in CUDA 12.4. This routine has now been removed from the header. 2.2.3. cuFFT: Release 12.4  New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing. Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs. Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes. Known Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ). This routine is not supported by cuFFT, and will be removed from the header in a future release. Resolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API ). Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL . From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension. 2.2.4. cuFFT: Release 12.3 Update 1  Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT. Resolved Issues Complex-to-complex (C2C) execution functions ( cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context. 2.2.5. cuFFT: Release 12.3  New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers. Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127. Slightly improved planning times for some FFT sizes. 2.2.6. cuFFT: Release 12.2  New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs . Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT. Reduced the size of the static libraries when compared to cuFFT in the 12.1 release. Resolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive. cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently. 2.2.7. cuFFT: Release 12.1 Update 1  Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy ) and another thread calls any API (except cufftCreate or cufftDestroy ), and when the total number of plans alive exceeds 1023. cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans. 2.2.8. cuFFT: Release 12.1  New Features Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout. Known Issues Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. Resolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit. 2.2.9. cuFFT: Release 12.0 Update 1  Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced. 2.2.10. cuFFT: Release 12.0  New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures. Known Issues cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme . Resolved Issues cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved. 2.3. cuSOLVER Library  2.3.1. cuSOLVER: Release 12.5 Update 1  Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved. 2.3.2. cuSOLVER: Release 12.5  New Features Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N' . Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR . Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices. Known Issues With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice . As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)) , with auto ALIGN_32 = []( int64_t val ) { return (( val + 31 ) / 32 ) * 32 ; }; and auto sizeofCudaDataType = []( cudaDataType dt ) { if ( dt == CUDA_R_32F ) return sizeof ( float ); if ( dt == CUDA_R_64F ) return sizeof ( double ); if ( dt == CUDA_C_32F ) return sizeof ( cuComplex ); if ( dt == CUDA_C_64F ) return sizeof ( cuDoubleComplex ); }; 2.3.3. cuSOLVER: Release 12.4 Update 1  New Features The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr , cusolverDn<t>ormtr , and cusolverDnXsyevd . The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster. Resolved Issues cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes. Deprecations Using long-deprecated cusolverDnPotrf , cusolverDnPotrs , cusolverDnGeqrf , cusolverDnGetrf , cusolverDnGetrs , cusolverDnSyevd , cusolverDnSyevdx , cusolverDnGesvd ,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf , cusolverDnXpotrs , cusolverDnXgeqrf , cusolverDnXgetrf , cusolverDnXgetrs , cusolverDnXsyevd , cusolverDnXsyevdx , cusolverDnXgesvd , and the corresponding bufferSize functions instead. 2.3.4. cuSOLVER: Release 12.4  New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes. Known Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size. 2.3.5. cuSOLVER: Release 12.2 Update 2  Resolved Issues Fixed an issue with cusolverDn<t>gesvd() , cusolverDnGesvd() , and cusolverDnXgesvd() , which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘ N ’. 2.3.6. cuSOLVER: Release 12.2  New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode() . Affected functions are: cusolverDn<t>geqrf() , cusolverDn<t>syevd() , cusolverDn<t>syevdx() , cusolverDn<t>gesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvdr() , and cusolverDnXgesvdp() . Known Issues Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock. 2.4. cuSPARSE Library  2.4.1. cuSPARSE: Release 12.5 Update 1  New Features Added support for BSR format in cusparseSpMM . Resolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0 , num_batches>1 , batch_stride indicates that there is padding between batches. cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1). cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \\*= beta . The bug behavior was not modifying C at all. cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows. Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices. Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes. 2.4.2. cuSPARSE: Release 12.5  New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector. Resolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. 2.4.3. cuSPARSE: Release 12.4  New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess() . Added support for mixed real and complex types for cusparseSpMM() . Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM() . Known Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. Resolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros. 2.4.4. cuSPARSE: Release 12.3 Update 1  New Features Added support for block sizes of 64 and 128 in cusparseSDDMM() . Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage. 2.4.5. cuSPARSE: Release 12.3  New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector. The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values. Known Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous. Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A. Resolved Issues cusparseSpSV() provided indeterministic results in some cases. Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment. Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN. 2.4.6. cuSPARSE: Release 12.2 Update 1  New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api . Resolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process. Clarified the supported operations for cusparseSDDMM() . cusparseCreateConstSlicedEll() now uses const pointers. Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing. cusparseSpSM_bufferSize() could ask slightly less memory than needed. cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed. Deprecations Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them. 2.4.7. cuSPARSE: Release 12.1 Update 1  New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine ( cusparseSDDMM ). Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication ( cusparseSpMV ) and triangular solver with a single right-hand side ( cusparseSpSV ). Added a new API call ( cusparseSpSV_updateMatrix ) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step. 2.4.8. cuSPARSE: Release 12.0 Update 1  New Features cusparseSDDMM() now supports mixed precision computation. Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs. Improved cusparseSpMV() performance with a new load balancing algorithm. cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address. Resolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows. 2.4.9. cuSPARSE: Release 12.0  New Features JIT LTO functionalities ( cusparseSpMMOp() ) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so , see cuSPARSE documentation . JIT LTO performance has also been improved for cusparseSpMMOpPlan() . Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet() . Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions. Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks. Added int8_t support to cusparseGather() , cusparseScatter() , and cusparseCsr2cscEx2() . Improved cusparseSpSV() performance for both the analysis and the solving phases. Improved cusparseSpSM() performance for both the analysis and the solving phases. Improved cusparseSDDMM() performance and added support for batch computation. Improved cusparseCsr2cscEx2() performance. Resolved Issues cusparseSpSV() and cusparseSpSM() could produce wrong results. cusparseDnMatGetStridedBatch() did not accept batchStride == 0 . Deprecations Removed deprecated CUDA 11.x APIs, enumerators, and descriptors. 2.5. Math Library  2.5.1. CUDA Math: Release 12.5  Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions. 2.5.2. CUDA Math: Release 12.4  Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules. 2.5.3. CUDA Math: Release 12.3  New Features Performance of SIMD Integer CUDA Math APIs was improved. Resolved Issues The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3. Known Issues Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half , __half2 , __nv_bfloat16 , __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing . This behavior may improve in future versions of the headers. 2.5.4. CUDA Math: Release 12.2  New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions. __half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware. Updated the observed worst case error bounds for single precision intrinsic functions __expf() , __exp10f() and double precision functions asinh() , acosh() . 2.5.5. CUDA Math: Release 12.1  New Features Performance and accuracy improvements in atanf , acosf , asinf , sinpif , cospif , powf , erff , and tgammaf . 2.5.6. CUDA Math: Release 12.0  New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html . Known Issues Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn() . Affected CUDA language operation: double precision / operation in the device code. Deprecations All previously deprecated undocumented APIs are removed from CUDA 12.0. 2.6. NVIDIA Performance Primitives (NPP)  2.6.1. NPP: Release 12.4  New Features Enhanced large file support with size_t . 2.6.2. NPP: Release 12.0  Deprecations Deprecating non-CTX API support from next release. Resolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance. 2.7. nvJPEG Library  2.7.1. nvJPEG: Release 12.4  New Features IDCT performance optimizations for single image CUDA decode. Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE . 2.7.2. nvJPEG: Release 12.3 Update 1  New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them. 2.7.3. nvJPEG: Release 12.2  New Features Added support for JPEG Lossless decode (process 14, FO prediction). nvJPEG is now supported on L4T. 2.7.4. nvJPEG: Release 12.0  New Features Immproved the GPU Memory optimisation for the nvJPEG codec. Resolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved. An issue with CMYK four component color conversion is now resolved. Known Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths. Deprecations The reuse of Huffman table in Encoder ( nvjpegEncoderParamsCopyHuffmanTables ). 1 Only available on select Linux distros 3. Notices  3.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-gdb/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-gdb/index.html", "content_type": "text/html", "text": "CUDA-GDB 1. Introduction 1.1. What is CUDA-GDB? 1.2. Supported Features 1.3. About This Document 2. Release Notes 3. Getting Started 3.1. Setting Up the Debugger Environment 3.1.1. Temporary Directory 3.1.2. Using the CUDA-GDB debugger on Jetson and Drive Tegra devices 3.2. Compiling the Application 3.2.1. Debug Compilation 3.2.2. Compilation With Linenumber Information 3.2.3. Compiling For Specific GPU architectures 3.3. Using the Debugger 3.3.1. Single-GPU Debugging with the Desktop Manager Running 3.3.2. Multi-GPU Debugging 3.3.3. Remote Debugging 3.3.4. Multiple Debuggers 3.3.5. Attaching/Detaching 4. CUDA-GDB Extensions 4.1. Command Naming Convention 4.2. Getting Help 4.3. Initialization File 4.4. GUI Integration 4.5. GPU core dump support 5. Kernel Focus 5.1. Software Coordinates vs. Hardware Coordinates 5.2. Current Focus 5.3. Switching Focus 6. Program Execution 6.1. Interrupting the Application 6.2. Single Stepping 7. Breakpoints and Watchpoints 7.1. Symbolic Breakpoints 7.2. Line Breakpoints 7.3. Address Breakpoints 7.4. Kernel Entry Breakpoints 7.5. Conditional Breakpoints 7.6. Watchpoints 8. Inspecting Program State 8.1. Memory and Variables 8.2. Variable Storage and Accessibility 8.3. Info CUDA Commands 8.3.1. info cuda devices 8.3.2. info cuda sms 8.3.3. info cuda warps 8.3.4. info cuda lanes 8.3.5. info cuda kernels 8.3.6. info cuda blocks 8.3.7. info cuda threads 8.3.8. info cuda launch trace 8.3.9. info cuda launch children 8.3.10. info cuda contexts 8.3.11. info cuda managed 8.4. Disassembly 8.5. Registers 8.6. Const banks 9. Event Notifications 9.1. Context Events 9.2. Kernel Events 10. Automatic Error Checking 10.1. Checking API Errors 10.2. GPU Error Reporting 10.3. Autostep 11. Walk-Through Examples 11.1. Example: bitreverse 11.1.1. Walking through the Code 11.2. Example: autostep 11.2.1. Debugging with Autosteps 11.3. Example: MPI CUDA Application 12. Tips and Tricks 12.1. set cuda break_on_launch 12.2. set cuda launch_blocking 12.3. set cuda notify 12.4. set cuda ptx_cache 12.5. set cuda single_stepping_optimizations 12.6. set cuda thread_selection 12.7. set cuda value_extrapolation 12.8. Debugging Docker Containers 12.9. Switching to Classic Debugger Backend 12.10. Thread Block Clusters 12.11. Debugging OptiX/RTCore applications 12.12. Debugging on Windows Subsystem for Linux 13. Supported Platforms 14. Common Issues on Supported Operating Systems 15. Known Issues 16. Notices 16.1. Notice 16.2. OpenCL 16.3. Trademarks CUDA-GDB » 1. Introduction v12.5 | PDF | Archive CUDA-GDB The user manual for CUDA-GDB, the NVIDIA tool for debugging CUDA applications on Linux and QNX systems. 1. Introduction  This document introduces CUDA-GDB, the NVIDIA ® CUDA ® debugger for Linux and QNX targets. 1.1. What is CUDA-GDB?  CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Linux and QNX. CUDA-GDB is an extension to GDB, the GNU Project debugger. The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware. This enables developers to debug applications without the potential variations introduced by simulation and emulation environments. 1.2. Supported Features  CUDA-GDB is designed to present the user with a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application. Just as programming in CUDA C is an extension to C programming, debugging with CUDA-GDB is a natural extension to debugging with GDB. The existing GDB debugging features are inherently present for debugging the host code, and additional features have been provided to support debugging CUDA device code. CUDA-GDB supports debugging C/C++ and Fortran CUDA applications. Fortran debugging support is limited to 64-bit Linux operating system. CUDA-GDB allows the user to set breakpoints, to single-step CUDA applications, and also to inspect and modify the memory and variables of any given thread running on the hardware. CUDA-GDB supports debugging all CUDA applications, whether they use the CUDA driver API, the CUDA runtime API, or both. CUDA-GDB supports debugging kernels that have been compiled for specific CUDA architectures, such as sm_75 or sm_80 , but also supports debugging kernels compiled at runtime, referred to as just-in-time compilation, or JIT compilation for short. 1.3. About This Document  This document is the main documentation for CUDA-GDB and is organized more as a user manual than a reference manual. The rest of the document will describe how to install and use CUDA-GDB to debug CUDA kernels and how to use the new CUDA commands that have been added to GDB. Some walk-through examples are also provided. It is assumed that the user already knows the basic GDB commands used to debug host applications. 2. Release Notes  12.5 Release Updated GDB version Moved from GDB 13.1 to 13.2.  See GDB 13.2 changes Support removal notice Support for the macOS host client of CUDA-GDB has been removed. Support for Android has been removed. Support for Python 3.6 and 3.7 has been removed. Features Multi build feature that supports native Python and TUI mode across all supported platforms. The cuda-gdb program is now a wrapper script that calls the appropriate cuda-gdb binary. If no supported Python or libncurses is detected, the wrapper will fallback to a cuda-gdb binary with Python and TUI support disabled. Added support for TUI mode. Added support for Python 3.10, 3.11, and 3.12. Added support for detecting and printing exceptions encountered in exited warps. This can occur when debugging an application with optimizations enabled. Added new gdb/mi command equivalents for info cuda managed and info cuda line. Fixed Issues Fixed issue with printing reference parameter arguments to CUDA functions. Fixed issues resulting in crashes/errors when reading/writing from/to CUDA generic memory. Fixed issue where break_on_launch breakpoints were missed for back to back launches of the same kernel. Fixed issue with incorrectly reporting breakpoint hit events as SIGTRAP when breakpoint is hit in divergent thread. Fixed crash on QNX when cuda-gdbserver packets arrive out-of-order. Better error handling when encountering an error when reading CUDA disassembly. Better exit handling when resuming execution from a fatal CUDA exception. 12.4 Release Updated GDB version Moved from GDB 12.1 to 13.1.  See GDB 13.1 changes Android deprecation notice Support for Android is deprecated. It will be dropped in an upcoming release. Python 3.6 and 3.7 deprecation notice Support for end-of-life Python 3.6 and 3.7 versions is deprecated. It will be dropped in an upcoming release. Features Performance enhancement which reduces the number of overall CUDA Debugger API calls. Performance enhancement when loading large cubins with device functions using a large number of GPU registers. Performance enhancement when single stepping over warp wide barriers. Added support for printing values contained within constant banks from GPU core dumps. Fixed Issues Prevented shell expansion on cloned function names when disassembling. Fixed crash when setting a conditional breakpoint on an unknown symbol name. Fixed issue with setting a watchpoint on a global pointer. Fixed assertion in switch_to_thread_1 during inferior teardown. Fixed attach failures encountered with newer Intel processors. Refactored the libpython layer to avoid unnecessary gdb code changes. 12.3 Release macOS host client deprecation notice Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release. Features Added support for printing values contained within constant banks. New $_cuda_const_bank(bank, offset) convenience function to obtain address of offset in constant bank. See Const Banks . Performance enhancements added which reduce overhead when running applications with many CUDA threads. Added support for CUDA function pointers. Fixed Issues Fixed issue when detaching from attached process that can result in a crash. Fixed thread ordering issues present with several info cuda commands. Added support for opening of GPU core dumps when no valid warps are present on the device. Added missing DWARF operators used by OptiX. Fixed issue with parsing CUDA Fortran pointer types. Fixed issue where CUDA Cluster coordinates were being displayed when no CUDA Cluster was present. 12.2 Release Features Enabled printing of extended error messages when a CUDA Debugger API error is encountered. Enabled support for debugging with Confidential Compute mode with devtools mode. See Confidential Computing Deployment Guide <https://docs.nvidia.com/confidential-computing-deployment-guide.pdf> for more details on how to enable the mode. Fixed Issues Fixed “??” appearing in backtrace in OptiX applications. Host shadow breakpoints are now handled correctly with CUDA Lazy Loading enabled. Fixed name mangling issue when debugging LLVM generated cubins. CUDA Cluster coordinates are now displayed correctly. Fixed issue with attaching to an application using CUDA Lazy Loading when debugging remotely with cuda-gdbserver. 12.1 Release CUDA Driver API added for controlling core dump behavior CTK 12.1 and the r530 driver adds new APIs that allow developers to enable/configure core dump settings programmatically inside their application instead of using environment variables. See the CUDA Driver API manual for more information. Features Performance improvements for applications using CUDA Lazy Loading. Added support for ELF cubins with a large number of sections (more than 32767). Added break_on_launch support for CUDA Graphs. Fixed Issues Removed unsupported set/show gpu_busy_check command. On QNX fixed an issue where info threads incorrectly reported dead host threads. Performance fixes for stepping/next over inline function calls. Performance fixes when using the info cuda managed command. Fixed issue when using set follow-fork-mode child . Fixed issue when parsing DWARF for self referential structures. 12.0 Release Updated GDB version Moved from GDB 10.2 to 12.1.  See GDB 12.1 changes Texture and surface reference support removed CTK 12.0 removed support for the Texture and Surface Reference API. Support for printing texture and surface references has been removed. CUDA Memory Checker integration removed cuda-memcheck has been deprecated in CUDA 11.x and replaced by Compute Sanitizer. The new memory checking workflow is to use Compute Sanitizer from the CLI. This will support coredumps when issues are detected which can then be opened and inspected with CUDA-GDB, similar to other coredumps. Support for cuda-memcheck has been removed with the CUDA 12.0 release. Debugging of applications using CUDA Dynamic Parallelism Support for debugging applications using CUDA Dynamic Parallelism with the classic debugger backend or on Maxwell GPUs has been removed by default for applications compiled with the CTK 12.0 or newer. Debugging can be accomplished in these situations by recompiling the application while passing the -DCUDA_FORCE_CDP1_IF_SUPPORTED flag. Features Moved from base gdb/10.2 to gdb/12.1. Added initial support for Thread Block Clusters. Changed the default behavior of --cuda-use-lockfile to 0 . Lockfiles are no longer created by default. Fixed Issues Addressed a hang that could be encountered when stepping through device system calls. Fixed an overflow issue with displaying active warp masks in info cuda commands. Changed internal CUDA Dynamic Parallelsim detection breakpoint to be set only when break_on_launch is enabled. Removed unsupported gpu_busy_check setting . 11.8 Release Features Uses the new Unified Debugger (UD) debugging backend by default. Added support for debugging applications using CUDA Lazy Loading. Debugger is now enabled on Windows Subsystem for Linux (WSL). Added basic type support for printing FP8 values (E4M3 and E5M2). Notes By default, CUDA-GDB will use the new Unified Debugger (UD) backend. This change is transparent to most users using Pascal or newer cards. For Maxwell debugging, or to force the old classic debugging backend, set CUDBG_USE_LEGACY_DEBUGGER to 1 in your environment. WSL is not supported on GH100 platforms with this release. 11.7 Release Features Major break_on_launch performance enhancements to use new KERNEL_READY notification mechanism instead of setting manual breakpoints. Refactored info cuda command output to be more condensed. Omitted printing of inactive messages. Added new --disable-python commandline option to disable Python interpreter dlopen. Fixed Issues Fixed follow-fork child to avoid hanging behavior when both parent and child processes use CUDA. Added a missing dlsym of a libpython function that was causing errors with some versions of libpython. 11.6 Release Updated GDB version Moved from GDB 10.1 to 10.2. See GDB 10.2 changes Features Added errorpc instruction prefix to the disassembly view. If an error PC is set, prefix the instruction with *> . Fixed Issues Fixed lineinfo frames to properly display the source filename. Fixed writing to gpu global memory that was allocated from the host. Fixed bug that was preventing reading host variables during certain situations. Fixed cuda-gdbserver init check that prevented QNX from starting. 11.5 Release Python 3 support on Jetson and Drive Tegra devices Support for Python 2 has been removed. CUDA-GDB now supports Python 3 on Jetson and Drive Tegra devices. Fixed Issues Added robust version checks when dynamic loading the libpython3 library. The loaded libpython3 will match the version of the python3 runtime in PATH. Added support for checking PEP-3149 flag names when loading libpython3 libraries. Added support for dynamic loading of Python 3.9. Fixed overriding PYTHONPATH on certain RHEL distributions. 11.4 Update 1 Release Known Issues with Fedora 34 CUDA-GDB has known issues with debugging on Fedora 34 and may not be reliable. Fixed Issues Enabled python integration for ppc64le and aarch64 SBSA. Fixed a performance regression when debugging CUDA apps. Fixed an intermitent hang with remote debugging via cuda-gdbserver. Fixed bug with set cuda api_failures stop not triggering breakpoints on failure. Changed python behavior to dlopen libpython libraries that match the version of the python3 interpreter in PATH. OpenMP Fortran: Fixed a crash when setting breakpoints inside an OpenMP parallel region. OpenMP: Better support for printing local variables within a parallel region. Fortran: Added updated support for printing assumed shape arrays and array slices. Fixed selecting between host and device thread focus in cudacore debugging. Various fixes for QNX remote debugging. 11.4 Release Updated GDB version Moved from GDB 8.3 to 10 (based on GDB 10.1). See GDB 10.1 changes Python 3 support Support for Python 2 has been removed. CUDA-GDB now supports Python 3. GDB TUI mode disabled Support for GDB TUI mode has been disabled. This avoids cross platform dependency mismatches for OSes that lack ncurses-5.5 support. Kepler deprecation notice Support for Kepler devices (sm_35 and sm_37) is deprecated. Kepler support will be dropped in an upcoming release. Coredump support Added support for writing coredumps to named pipe using CUDA_COREDUMP_FILE . Fixed Issues Added support for displaying SIGTRAP exception in coredumps. Disabled ability to enable scheduler-locking when debugging CUDA targets. Fixed cuda_register_name and cuda_special_register_name to avoid returning old cached result on error. Fixed intermitent race condition when creating the CUDA temporary directory. Various fixes for QNX remote debugging. 11.3 Release Python 2 deprecation notice Support for Python 2 is being deprecated. CUDA-GDB will move to build with Python 3 support in an upcoming release. Fixed Issues Improvements to late attach for remote debugging. 11.2 Update 1 Release GDB TUI deprecation notice Support for GDB TUI mode is being deprecated. This will avoid cross platform dependency mismatches for OSes that lack ncurses-5.5 support. GDB TUI mode will be disabled in an upcoming release. Fixed Issues Fixed printing of strings in the global GPU memory while running CPU code. Fixed a bug with extended debug_line handling. Fixed truncation with builtin gdb variables such as gridDim . Fixed a segfault during startup for DWARF dies missing names. Fixed a segfault when a CUDA kernel calls assert . Fixed a bug that prevented debugging cubins > 2GB. Added minor usability enhancements for cubins compiled with --lineinfo . Fixed a segfault cause by a pretty printer when using CUDA-GDB within CLion. 11.1 Release Updated GDB version Moved from GDB 8.2 to 8.3 (based on gdb 8.3.1). See gdb 8.3.1 changes Support for SM 8.6 CUDA-GDB now supports Devices with Compute Capability 8.6. Updated DWARF parser Old binaries might need to be recompiled in order to ensure CUDA-specific DWARF info are up to date. Fixed Issues Fixed an intermittent deadlock when attaching to a running CUDA process. Fixed a bug when inspecting the value of half registers. 11.0 Release Updated GDB version CUDA-GDB has been upgraded from GDB/7.12 to GDB/8.2. Support for SM8.0 CUDA-GDB now supports Devices with Compute Capability 8.0. Support for Bfloat16 Support for Bfloat16 (__nv_bfloat16) types have been added. MIG support CUDA-GDB supports MIG. There can be a separate debugger session on each MIG instance. Refer to Multiple Debuggers in case multiple debuggers are needed. Mac support Debugging on macOS is no longer supported. However, macOS can still be used as the host system (where CUDA-GDB runs under macOS, using cuda-gdbserver to debug a remote target). The download for the macOS version of CUDA-GDB can be found at the following location: Download Here 10.1 Release Enhanced debugging with only linenumber information Several enhancements were made to CUDA-GDB support for debugging programs compiled with -lineinfo but not with -G . This is intended primarily for debugging programs built with OptiX/RTCore. See also Compilation With Linenumber Information 10.0 Release Turing Uniform Register Support Support added for examining and modifying uniform registers on Turing GPUs. 9.2 Release User induced core dump support For the devices that support compute preemption, user induced core dump support is added. New environment variable: CUDA_ENABLE_USER_TRIGGERED_COREDUMP can be used to enable this feature. 9.1 Release Volta-MPS core dump support GPU core dump generation is supported on Volta-MPS. Lightweight GPU core dump support CUDA-GDB supports reading lightweight GPU core dump files. New environment variable: CUDA_ENABLE_LIGHTWEIGHT_COREDUMP can be used to enable this feature. 7.0 Release GPU core dump support CUDA-GDB supports reading GPU and GPU+CPU core dump files. New environment variables: CUDA_ENABLE_COREDUMP_ON_EXCEPTION , CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION and CUDA_COREDUMP_FILE can be used to enable and configure this feature. 6.5 Release CUDA Fortran Support CUDA-GDB supports CUDA Fortran debugging on 64-bit Linux operating systems. GDB 7.6.2 Code Base The code base for CUDA-GDB was upgraded to GDB 7.6.2. 6.0 Release Unified Memory Support Managed variables can be read and written from either a host thread or a device thread. The debugger also annotates memory addresses that reside in managed memory with @managed . The list of statically allocated managed variables can be accessed through a new info cuda managed command. GDB 7.6 Code Base The code base for CUDA-GDB was upgraded from GDB 7.2 to GDB 7.6. Android Support CUDA-GDB can now be used to debug Android native applications either locally or remotely. Single-Stepping Optimizations CUDA-GDB can now use optimized methods to single-step the program, which accelerate single-stepping most of the time. This feature can be disabled by issuing set cuda single_stepping_optimizations off . Faster Remote Debugging A lot of effort has gone into making remote debugging considerably faster, up to 2 orders of magnitude. The effort also made local debugging faster. Kernel Entry Breakpoints The set cuda break_on_launch option will now break on kernels launched from the GPU. Also, enabling this option does not affect kernel launch notifications. Precise Error Attribution On Maxwell architecture (SM 5.0), the instruction that triggers an exception will be reported accurately. The application keeps making forward progress and the PC at which the debugger stops may not match that address but an extra output message identifies the origin of the exception. Live Range Optimizations To mitigate the issue of variables not being accessible at some code addresses, the debugger offers two new options. With set cuda value_extrapolation , the latest known value is displayed with (possibly) prefix. With set cuda ptx_cache , the latest known value of the PTX register associated with a source variable is displayed with the (cached) prefix. Event Notifications Kernel event notifications are not displayed by default any more. New kernel events verbosity options have been added: set cuda kernel_events , set cuda kernel_events_depth . Also set cuda defer_kernel_launch_notifications has been deprecated and has no effect any more. 5.5 Release Kernel Launch Trace Two new commands, info cuda launch trace and info cuda launch children , are introduced to display the kernel launch trace and the children kernel of a given kernel when Dynamic Parallelism is used. Single-GPU Debugging (BETA) CUDA-GDB can now be used to debug a CUDA application on the same GPU that is rendering the desktop GUI. This feature also enables debugging of long-running or indefinite CUDA kernels that would otherwise encounter a launch timeout. In addition, multiple CUDA-GDB sessions can debug CUDA applications context-switching on the same GPU. This feature is available on Linux with SM3.5 devices. For information on enabling this, please see Single-GPU Debugging with the Desktop Manager Running and Multiple Debuggers . Remote GPU Debugging CUDA-GDB in conjunction with CUDA-GDBSERVER can now be used to debug a CUDA application running on the remote host. 5.0 Release Dynamic Parallelism Support CUDA-GDB fully supports Dynamic Parallelism, a new feature introduced with the 5.0 toolkit. The debugger is able to track the kernels launched from another kernel and to inspect and modify variables like any other CPU-launched kernel. Attach/Detach It is now possible to attach to a CUDA application that is already running. It is also possible to detach from the application before letting it run to completion. When attached, all the usual features of the debugger are available to the user, as if the application had been launched from the debugger. This feature is also supported with applications using Dynamic Parallelism. Attach on exception Using the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION , the application will run normally until a device exception occurs. Then the application will wait for the debugger to attach itself to it for further debugging. API Error Reporting Checking the error code of all the CUDA driver API and CUDA runtime API function calls is vital to ensure the correctness of a CUDA application. Now the debugger is able to report, and even stop, when any API call returns an error. See set cuda api_failures for more information. Inlined Subroutine Support Inlined subroutines are now accessible from the debugger on SM 2.0 and above. The user can inspect the local variables of those subroutines and visit the call frame stack as if the routines were not inlined. 4.2 Release Kepler Support The primary change in Release 4.2 of CUDA-GDB is the addition of support for the new Kepler architecture. There are no other user-visible changes in this release. 4.1 Release Source Base Upgraded to GDB 7.2 Until now, CUDA-GDB was based on GDB 6.6 on Linux, and GDB 6.3.5 on Darwin (the Apple branch). Now, both versions of CUDA-GDB are using the same 7.2 source base. Now CUDA-GDB supports newer versions of GCC (tested up to GCC 4.5), has better support for DWARF3 debug information, and better C++ debugging support. Simultaneous Sessions Support With the 4.1 release, the single CUDA-GDB process restriction is lifted. Now, multiple CUDA-GDB sessions are allowed to co-exist as long as the GPUs are not shared between the applications being processed. For instance, one CUDA-GDB process can debug process foo using GPU 0 while another CUDA-GDB process debugs process bar using GPU 1. The exclusive of GPUs can be enforced with the CUDA_VISIBLE_DEVICES environment variable. New Autostep Command A new ‘autostep’ command was added. The command increases the precision of CUDA exceptions by automatically single-stepping through portions of code. Under normal execution, the thread and instruction where an exception occurred may be imprecisely reported. However, the exact instruction that generates the exception can be determined if the program is being single-stepped when the exception occurs. Manually single-stepping through a program is a slow and tedious process. Therefore ‘autostep’ aides the user by allowing them to specify sections of code where they suspect an exception could occur. These sections are automatically single-stepped through when the program is running, and any exception that occurs within these sections is precisely reported. Type ‘help autostep’ from CUDA-GDB for the syntax and usage of the command. Multiple Context Support On GPUs with compute capability of SM20 or higher, debugging multiple contexts on the same GPU is now supported. It was a known limitation until now. Device Assertions Support The R285 driver released with the 4.1 version of the toolkit supports device assertions. CUDA_GDB supports the assertion call and stops the execution of the application when the assertion is hit. Then the variables and memory can be inspected as usual. The application can also be resumed past the assertion if needed. Use the ‘set cuda hide_internal_frames’ option to expose/hide the system call frames (hidden by default). Temporary Directory By default, the debugger API will use /tmp as the directory to store temporary files. To select a different directory, the $TMPDIR environment variable and the API CUDBG_APICLIENT_PID variable must be set. 3. Getting Started  The CUDA toolkit can be installed by following instructions in the Quick Start Guide . Further steps should be taken to set up the debugger environment, build the application, and run the debugger. 3.1. Setting Up the Debugger Environment  3.1.1. Temporary Directory  By default, CUDA-GDB uses /tmp as the directory to store temporary files. To select a different directory, set the $TMPDIR environment variable. Note The user must have write and execute permission to the temporary directory used by CUDA-GDB. Otherwise, the debugger will fail with an internal error. Note The value of $TMPDIR must be the same in the environment of the application and CUDA-GDB. If they do not match, CUDA-GDB will fail to attach onto the application process. Note Since /tmp folder does not exist on Android device, the $TMPDIR environment variable must be set and point to a user-writeable folder before launching cuda-gdb. 3.1.2. Using the CUDA-GDB debugger on Jetson and Drive Tegra devices  By default, on Jetson and Drive Tegra devices, GPU debugging is supported only if cuda-gdb and cuda-gdbserver are launched by a user who is a member of the debug group. To add the current user to the debug group run this command: sudo usermod -a -G debug $USER 3.2. Compiling the Application  3.2.1. Debug Compilation  NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly. The -g -G option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example, nvcc -g -G foo.cu -o foo Using this line to compile the CUDA application foo.cu forces -O0 compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations. makes the compiler include debug information in the executable To compile your CUDA Fortran code with debgging information necessary for CUDA-GDB to work properly, pgfortran, the PGI CUDA Fortran compiler, must be invoked with -g option. Also, for the ease of debugging and forward compatibility with the future GPU architectures, it is recommended to compile the code with -Mcuda=nordc option; for example, pgfortran -g -Mcuda=nordc foo.cuf -o foo For more information about the available compilation flags, please consult the PGI compiler documentation. 3.2.2. Compilation With Linenumber Information  Several enhancements were made to cuda-gdb’s support for debugging programs compiled with -lineinfo but not with -G . This is intended primarily for debugging programs built with OptiX/RTCore. Note that -lineinfo can be used when trying to debug optimized code. In this case, debugger stepping and breakpoint behavior may appear somewhat erratic. The PC may jump forward and backward unexpectedly while stepping. The user may step into code that has no linenumber information, leading to an inability to determine which source-file/linenumber the code at the PC belongs to. Breakpoints may break on a different line than they were originally set on. When debugging OptiX/RTCore code, the following should be kept in mind: NVIDIA internal code cannot be debugged or examined by the user. OptiX/RTCode debugging is limited to -lineinfo , and building this code with full debug infomation ( -G ) is not supported. OptiX/RTCode code is highly optimized, and as such the notes above about debugging optimized code apply. 3.2.3. Compiling For Specific GPU architectures  By default, the compiler will only generate code for the compute_52 PTX and sm_52 cubins. For later GPUs, the kernels are recompiled at runtime from the PTX for the architecture of the target GPU(s). Compiling for a specific virtual architecture guarantees that the application will work for any GPU architecture after that, for a trade-off in performance. This is done for forward-compatibility. It is highly recommended to compile the application once and for all for the GPU architectures targeted by the application, and to generate the PTX code for the latest virtual architecture for forward compatibility. A GPU architecture is defined by its compute capability. The list of GPUs and their respective compute capability, see https://developer.nvidia.com/cuda-gpus . The same application can be compiled for multiple GPU architectures. Use the -gencode compilation option to dictate which GPU architecture to compile for. The option can be specified multiple times. For instance, to compile an application for a GPU with compute capability 7.0, add the following flag to the compilation command: -gencode arch=compute_70,code=sm_70 To compile PTX code for any future architecture past the compute capability 7.0, add the following flag to the compilation command: -gencode arch=compute_70,code=compute_70 For additional information, please consult the compiler documentation at https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#extended-notation 3.3. Using the Debugger  CUDA-GDB can be used in the following system configurations: 3.3.1. Single-GPU Debugging with the Desktop Manager Running  For devices with compute capability 6.0 and higher CUDA-GDB can be used to debug CUDA applications on the same GPU that is running the desktop GUI. Additionally for devices with compute capability less than 6.0 software preemption can be used to debug CUDA applications on the same GPU that is running the desktop GUI. There are two ways to enable this functionality: Note This is a BETA feature available on Linux and is only supported on Maxwell. The options listed below are ignored for GPUs with SM6.0 compute capability and higher. Use the following command: set cuda software_preemption on Export the following environment variable: CUDA_DEBUGGER_SOFTWARE_PREEMPTION=1 Either of the options above will activate software preemption. These options must be set prior to running the application. When the GPU hits a breakpoint or any other event that would normally cause the GPU to freeze, CUDA-GDB releases the GPU for use by the desktop or other applications. This enables CUDA-GDB to debug a CUDA application on the same GPU that is running the desktop GUI, and also enables debugging of multiple CUDA applications context-switching on the same GPU. 3.3.2. Multi-GPU Debugging  Multi-GPU debugging designates the scenario where the application is running on more than one CUDA-capable device. Multi-GPU debugging is not much different than single-GPU debugging except for a few additional CUDA-GDB commands that let you switch between the GPUs. Any GPU hitting a breakpoint will pause all the GPUs running CUDA on that system. Once paused, you can use info cuda kernels to view all the active kernels and the GPUs they are running on. When any GPU is resumed, all the GPUs are resumed. Note If the CUDA_VISIBLE_DEVICES environment is used, only the specified devices are suspended and resumed. All CUDA-capable GPUs may run one or more kernels. To switch to an active kernel, use cuda kernel <n> , where n is the ID of the kernel retrieved from info cuda kernels . Note The same kernel can be loaded and used by different contexts and devices at the same time. When a breakpoint is set in such a kernel, by either name or file name and line number, it will be resolved arbitrarily to only one instance of that kernel. With the runtime API, the exact instance to which the breakpoint will be resolved cannot be controlled. With the driver API, the user can control the instance to which the breakpoint will be resolved to by setting the breakpoint right after its module is loaded. 3.3.3. Remote Debugging  There are multiple methods to remote debug an application with CUDA-GDB. In addition to using SSH or VNC from the host system to connect to the target system, it is also possible to use the target remote GDB feature. Using this option, the local cuda-gdb (client) connects to the cuda-gdbserver process (the server) running on the target system. This option is supported with a Linux client and a Linux or QNX server. Setting remote debugging that way is a 2-step process: Launch the cuda-gdbserver on the remote host cuda-gdbserver can be launched on the remote host in different operation modes. Option 1: Launch a new application in debug mode. To launch a new application in debug mode, invoke cuda-gdb server as follows: $ cuda-gdbserver :1234 app_invocation Where 1234 is the TCP port number that cuda-gdbserver will listen to for incoming connections from cuda-gdb , and app-invocation is the invocation command to launch the application, arguments included. Option 2: Attach cuda-gdbserver to the running process To attach cuda-gdbserver to an already running process, the --attach option followed by process identification number (PID) must be used: $ cuda-gdbserver :1234 --attach 5678 Where 1234 is the TCP port number and 5678 is process identifier of the application cuda-gdbserver must be attached to. Attaching to an already running process is not supported on QNX platforms. Launch cuda-gdb on the client Configure cuda-gdb to connect to the remote target using either: (cuda-gdb) target remote or (cuda-gdb) target extended-remote It is recommended to use set sysroot command if libraries installed on the debug target might differ from the ones installed on the debug host. For example, cuda-gdb could be configured to connect to remote target as follows: (cuda-gdb) set sysroot remote://\n(cuda-gdb) target remote 192.168.0.2:1234 Where 192.168.0.2 is the IP address or domain name of the remote target, and 1234 is the TCP port previously previously opened by cuda-gdbserver . 3.3.4. Multiple Debuggers  For devices with compute capability 6.0 and higher several debugging sessions may take place simultaneously. For devices with compute capability less than 6.0, several debugging sessions may take place simultaneously as long as the CUDA devices are used exclusively. For instance, one instance of CUDA-GDB can debug a first application that uses the first GPU while another instance of CUDA-GDB debugs a second application that uses the second GPU. The exclusive use of a GPU is achieved by specifying which GPU is visible to the application by using the CUDA_VISIBLE_DEVICES environment variable. $ CUDA_VISIBLE_DEVICES=1 cuda-gdb my_app Additionally for devices with compute capability less than 6.0, with software preemption enabled ( set cuda software_preemption on ), multiple CUDA-GDB instances can be used to debug CUDA applications context-switching on the same GPU. 3.3.5. Attaching/Detaching  CUDA-GDB can attach to and detach from a CUDA application running on GPUs with compute capability 2.0 and beyond, using GDB’s built-in commands for attaching to or detaching from a process. Additionally, if the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION is set to 1 prior to running the CUDA application, the application will run normally until a device exception occurs. The application will then wait for CUDA-GDB to attach itself to it for further debugging. This feature is not supported on WSL. Note By default on some Linux distributions, the debugger cannot attach to an already running processes due to security settings. In order to enable the attach feature of the CUDA debugger, either cuda-gdb should be launched as root, or /proc/sys/kernel/yama/ptrace_scope should be set to zero, using the following command: $ sudo sh -c \"echo 0 >/proc/sys/kernel/yama/ptrace_scope\" To make the change permanent, edit /etc/sysctl.d/10-ptrace.conf . 4. CUDA-GDB Extensions  4.1. Command Naming Convention  The existing GDB commands are unchanged. Every new CUDA command or option is prefixed with the CUDA keyword. As much as possible, CUDA-GDB command names will be similar to the equivalent GDB commands used for debugging host code. For instance, the GDB command to display the host threads and switch to host thread 1 are, respectively: (cuda-gdb) info threads\n(cuda-gdb) thread 1 To display the CUDA threads and switch to cuda thread 1, the user only has to type: (cuda-gdb) info cuda threads\n(cuda-gdb) cuda thread 1 4.2. Getting Help  As with GDB commands, the built-in help for the CUDA commands is accessible from the cuda-gdb command line by using the help command: (cuda-gdb) help cuda name_of_the_cuda_command\n(cuda-gdb) help set cuda name_of_the_cuda_option\n(cuda-gdb) help info cuda name_of_the_info_cuda_command Moreover, all the CUDA commands can be auto-completed by pressing the TAB key, as with any other GDB command. CUDA commands can also be queried using the apropos command. 4.3. Initialization File  The initialization file for CUDA-GDB is named .cuda-gdbinit and follows the same rules as the standard .gdbinit file used by GDB. The initialization file may contain any CUDA- GDB command. Those commands will be processed in order when CUDA-GDB is launched. 4.4. GUI Integration  Emacs CUDA-GDB works with GUD in Emacs and XEmacs. No extra step is required other than pointing to the right binary. To use CUDA-GDB, the gud-gdb-command-name variable must be set to cuda-gdb annotate=3 . Use M-x customize-variable to set the variable. Ensure that cuda-gdb is present in the Emacs/XEmacs $PATH . DDD CUDA-GDB works with DDD. To use DDD with CUDA-GDB, launch DDD with the following command: ddd --debugger cuda-gdb cuda-gdb must be in your $PATH . 4.5. GPU core dump support  There are two ways to configure the core dump options for CUDA applications. Environment variables set in the application environment or programmatically from the application with the CUDA Driver API . Compilation for GPU core dump generation GPU core dumps will be generated regardless of compilation flags used to generate the GPU application. For the best debugging experience, it is recommended to compile the application with the -g -G or the -lineinfo option with NVCC. See Compiling the Application for more information on passing compilation flags for debugging. Enabling GPU core dump generation on exception with environment variables Set the CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable to 1 in order to enable generating a GPU core dump when a GPU exception is encountered. This option is disabled by default. Set the CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION environment variable to 0 in order to disable generating a CPU core dump when a GPU exception is encountered. This option is enabled by default when GPU core dump generation is enabled. Set the CUDA_ENABLE_LIGHTWEIGHT_COREDUMP environment variable to 1 in order to enable generating lightweight corefiles instead of full corefiles. When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. This option is disabled by default. Controlling behavior of GPU core dump generation The CUDA_COREDUMP_GENERATION_FLAGS environment variable can be used when generating GPU core dumps to deviate from default generation behavior. Multiple flags can be provided to this environment variable and are delimited by , . These flags can be used to accomplish tasks such as reducing the size of the generated GPU core dump or other desired behaviors that deviate from the defaults. The table below lists each flag and the behavior when present. GPU core dump CUDA_COREDUMP_GENERATION_FLAGS  Environment Variable flag Description skip_nonrelocated_elf_images Disables including copies of nonrelocated elf images in the GPU core dump. Only the relocated images will be present. skip_global_memory Disables dumping of GPU global and constbank memory segments. skip_shared_memory Disables dumping of GPU shared memory segments. skip_local_memory Disables dumping of GPU local memory segments. skip_abort Disables calling abort() at the end of the GPU core dump generation process. Note Setting the CUDA_ENABLE_LIGHTWEIGHT_COREDUMP environment variable to 1 is equivalent to CUDA_COREDUMP_GENERATION_FLAGS=\"skip_nonrelocated_elf_images,skip_global_memory,skip_shared_memory,skip_local_memory\" . Note Setting the CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION environment variable to 0 is equivalent to CUDA_COREDUMP_GENERATION_FLAGS=\"skip_abort\" . Limitations and notes for core dump generation The following limitations apply to core dump support: For Windows WDDM, GPU core dump is only supported on a GPU with compute capability 6.0 or higher. Windows TCC supports GPU core dump on all supported compute capabilities. GPU core dump is unsupported for the Windows Subsystem for Linux on GPUs running in SLI mode. Multi-GPU setups are supported, but SLI mode cannot be enabled in the Driver Control Panel. GPU core dump is supported for the Windows Subsystem for Linux only when the hardware scheduling mode is enabled. Generating a CPU core dump with CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION is currently unsupported on the QNX platform. GPU core dump is unsupported for the NVIDIA CMP product line. Per-context core dump can only be enabled on a GPU with compute capability 6.0 or higher. GPUs with compute capability less than 6.0 will return CUDA_ERROR_NOT_SUPPORTED when using the Coredump Attributes Control API. If an MPS client triggers a core dump, every other client running on the same MPS server will fault. The indirectly faulting clients will also generate a core dump if they have core dump generation enabled. GPU core dump is unsupported when other developer tools, including CUDA-GDB, are interacting with the application. Unless explicitly documented as a supported use case (e.g generate-cuda-core-file command). When generating a coredump on exception, if the kernel exits before the exception has been recognized it may result in failure to generate the corefile. See the note in GPU Error Reporting for strategies on how to work around this issue. Note The user should not send the application process a signal and ensure that the application process does not automatically terminate while the coredump generation is in process. Doing so may cause GPU coredump generation to abort. Note Starting from CUDA 11.6, the compute-sanitizer tool can generate a GPU core dump when an error is detected by using the --generate-coredump yes option. Once the core dump is generated, the target application will abort. See the compute-sanitizer documentation for more information: https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#coredump Note CPU core dumps will be located in a distribution specific location. Examining the /proc/sys/kernel/core_pattern file will typically hint at the name/location of the CPU core dump. Note NVIDIA vGPU platforms must explicitly enable debugging support to perform GPU core dump generation. Please reference the Virtual GPU Software User Guide for information on how to enable debugging on vGPU. Note NVIDIA Jetson and Drive Tegra devices must explicitly enable debugging support to perform GPU core dump generation. Refer to the Using the CUDA-GDB debugger on Jetson and Drive Tegra devices section. Note When generating core dumps on NVIDIA Drive Tegra devices running QNX, core dump generation may hang when generating CPU core dumps. If a hang is encountered, set CUDA_ENABLE_CPU_COREDUMP_EXCEPTION to 0. Note If core dumps are not generated when running programs built with OptiX/RTCore, try setting the environment variable OPTIX_FORCE_DEPRECATED_LAUNCHER to 1. Refer to the Debugging OptiX/RTCore applications section. Note If core dumps are not generated when running programs on Windows Subsystem for Linux, ensure the debug interface is enabled via setting the registry key >HKEY_LOCAL_MACHINE\\SOFTWARE\\NVIDIA Corporation\\GPUDebugger\\EnableInterface to (DWORD) 1 . Refer to the Debugging on Windows Subsystem for Linux section. Note GPU core dump is supported on GPUs running with Confidential Compute mode only with devtools mode. See Confidential Computing Deployment Guide <https://docs.nvidia.com/confidential-computing-deployment-guide.pdf> for more details on how to enable the mode. Naming of GPU core dump files By default, a GPU core dump is created in the current working directory. It is named core_TIME_HOSTNAME_PID.nvcudmp where TIME is the number of seconds since the Epoch, HOSTNAME is the host name of the machine running the CUDA application and PID is the process identifier of the CUDA application. The CUDA_COREDUMP_FILE environment variable can be used to define a template that is used to change the name of a GPU core dump file. The template can either be an absolute path or a relative path to the current working directory. The template can contain % specifiers which are substituted by the following patterns when a GPU core dump is created: Specifier Description %h Host name of the machine running the CUDA application %p Process identifier of the CUDA application %t Time as the number of seconds since the Epoch, 1970-01-01 00:00:00 +0000 (UTC) As an example, setting CUDA_COREDUMP_FILE to: export CUDA_COREDUMP_FILE=newName.%h.%p Would result in GPU core dumps being written to newName.myhost.1234 relative to the current working directory. Here myhost and 1234 are replaced with the real host name and pid respectively. Setting CUDA_COREDUMP_FILE to: export CUDA_COREDUMP_FILE=\"/home/$USER/newName.%h.%p\" Would result in GPU core dumps being written to the user’s home directory with the same name logic as in the above example. If CUDA_COREDUMP_FILE points to an existing file of FIFO type (e.g named pipe), the core dump will be streamed to it. Coredumps may be piped to shell commands via CUDA_COREDUMP_FILE with the following format: export CUDA_COREDUMP_FILE='| cmd > file' For example, to pipe a coredump to gzip use: export CUDA_COREDUMP_FILE='| gzip -9 > cuda-coredump.gz' Note When piping a coredump, the % specifiers will not be recognized. Enabling user induced GPU core dump generation For the devices that support compute preemption, the user can interrupt a running CUDA process to generate the GPU core dump. Set the CUDA_ENABLE_USER_TRIGGERED_COREDUMP environment variable to 1 in order to enable generating a user induced GPU core dump. This option is disabled by default. Setting this environment variable will open a communication pipe for each subsequently running CUDA process. To induce the GPU core dump, the user simply writes to the pipe. To change the default pipe file name, set the CUDA_COREDUMP_PIPE environment variable to a specific pipe name. The default pipe name is in the following format: corepipe.cuda.HOSTNAME.PID where HOSTNAME is the host name of machine running the CUDA application and PID is the process identifier of the CUDA application. This environment variable can take % specifiers as decribed in the above section. Displaying core dump generation progress By default, when an application crashes and generates a GPU core dump, the application may appear to be unresponsive or frozen until fully generated. Set the CUDA_COREDUMP_SHOW_PROGRESS environment variable to 1 in order to print core dump generation progress messages to stderr . This can be used to determine how far along the coredump generation is: coredump: SM 1/14 has finished state collection\ncoredump: SM 2/14 has finished state collection\ncoredump: SM 3/14 has finished state collection\ncoredump: SM 4/14 has finished state collection\ncoredump: SM 5/14 has finished state collection\ncoredump: SM 6/14 has finished state collection\ncoredump: SM 7/14 has finished state collection\ncoredump: SM 8/14 has finished state collection\ncoredump: SM 9/14 has finished state collection\ncoredump: SM 10/14 has finished state collection\ncoredump: SM 11/14 has finished state collection\ncoredump: SM 12/14 has finished state collection\ncoredump: SM 13/14 has finished state collection\ncoredump: SM 14/14 has finished state collection\ncoredump: Device 1/1 has finished state collection\ncoredump: Calculating ELF file layout\ncoredump: ELF file layout calculated\ncoredump: Writing ELF file to core_TIME_HOSTNAME_PID.nvcudmp\ncoredump: Writing out global memory (1073741824 bytes)\ncoredump: 5%...\ncoredump: 10%...\ncoredump: 15%...\ncoredump: 20%...\ncoredump: 25%...\ncoredump: 30%...\ncoredump: 35%...\ncoredump: 40%...\ncoredump: 45%...\ncoredump: 50%...\ncoredump: 55%...\ncoredump: 60%...\ncoredump: 65%...\ncoredump: 70%...\ncoredump: 75%...\ncoredump: 80%...\ncoredump: 85%...\ncoredump: 90%...\ncoredump: 95%...\ncoredump: 100%...\ncoredump: Writing out device table\ncoredump: Finalizing\ncoredump: All done Enabling GPU core dump generation with the CUDA Driver API The Driver API has equivalent settings for all of the environment variables, with the added feature of being able to set different core dump settings per-context instead of globally. This API can be called directly inside your application. Use cuCoredumpGetAttributeGlobal and cuCoredumpSetAttributeGlobal to fetch or set the global attribute. Use cuCoredumpGetAttribute and cuCoredumpSetAttribute to fetch or set the per context attribute. See the Coredump Attributes Control API manual for more information. The table below lists the environment variables and the equivalent CUcoredumpSettings flags that are available to manage core dump settings with the Coredump Attributes Control API. Note The CU_COREDUMP_ENABLE_USER_TRIGGER setting can only be set globally in the driver API and CU_COREDUMP_PIPE must be set (if desired) before user-triggered core dumps are enabled. GPU core dump configuration parameters  Environment Variable Description Environment Variable: CUDA_ENABLE_COREDUMP_ON_EXCEPTION CUcoredumpSettings Flag: CU_COREDUMP_ENABLE_ON_EXCEPTION Enables GPU core dump generation for exceptions. Disabled by default. Environment Variable: CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION CUcoredumpSettings Flag: CU_COREDUMP_TRIGGER_HOST Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default. Environment Variable: CUDA_ENABLE_LIGHTWEIGHT_COREDUMP CUcoredumpSettings Flag: CU_COREDUMP_LIGHTWEIGHT When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default. Environment Variable: CUDA_ENABLE_USER_TRIGGERED_COREDUMP CUcoredumpSettings Flag: CU_COREDUMP_ENABLE_USER_TRIGGER Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default. Environment Variable: CUDA_COREDUMP_FILE CUcoredumpSettings Flag: CU_COREDUMP_FILE Filename template for the GPU core dump. Environment Variable: CUDA_COREDUMP_PIPE CUcoredumpSettings Flag: CU_COREDUMP_PIPE Filename template for the user pipe trigger. Inspecting GPU and GPU+CPU core dumps in cuda-gdb Use the following command to load the GPU core dump into the debugger (cuda-gdb) target cudacore core.cuda.localhost.1234 This will open the core dump file and print the exception encountered during program execution. Then, issue standard cuda-gdb commands to further investigate application state on the device at the moment it was aborted. Use the following command to load CPU and GPU core dumps into the debugger (cuda-gdb) target core core.cpu core.cuda This will open the core dump file and print the exception encountered during program execution. Then, issue standard cuda-gdb commands to further investigate application state on the host and the device at the moment it was aborted. Note Coredump inspection does not require that a GPU be installed on the system 5. Kernel Focus  A CUDA application may be running several host threads and many device threads. To simplify the visualization of information about the state of application, commands are applied to the entity in focus. When the focus is set to a host thread, the commands will apply only to that host thread (unless the application is fully resumed, for instance). On the device side, the focus is always set to the lowest granularity level–the device thread. 5.1. Software Coordinates vs. Hardware Coordinates  A device thread belongs to a block, which in turn belongs to a kernel. Thread, block, and kernel are the software coordinates of the focus. A device thread runs on a lane. A lane belongs to a warp, which belongs to an SM, which in turn belongs to a device. Lane, warp, SM, and device are the hardware coordinates of the focus. Software and hardware coordinates can be used interchangeably and simultaneously as long as they remain coherent. Another software coordinate is sometimes used: the grid. The difference between a grid and a kernel is the scope. The grid ID is unique per GPU whereas the kernel ID is unique across all GPUs. Therefore there is a 1:1 mapping between a kernel and a (grid,device) tuple. Note If software preemption is enabled ( set cuda software_preemption on ), hardware coordinates corresponding to a device thread are likely to change upon resuming execution on the device. However, software coordinates will remain intact and will not change for the lifetime of the device thread. 5.2. Current Focus  To inspect the current focus, use the cuda command followed by the coordinates of interest: (cuda-gdb) cuda device sm warp lane block thread\nblock (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0\n(cuda-gdb) cuda kernel block thread\nkernel 1, block (0,0,0), thread (0,0,0)\n(cuda-gdb) cuda kernel\nkernel 1 5.3. Switching Focus  To switch the current focus, use the cuda command followed by the coordinates to be changed: (cuda-gdb) cuda device 0 sm 1 warp 2 lane 3\n[Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread\n(67,0,0), device 0, sm 1, warp 2, lane 3]\n374 int totalThreads = gridDim.x * blockDim.x; If the specified focus is not fully defined by the command, the debugger will assume that the omitted coordinates are set to the coordinates in the current focus, including the subcoordinates of the block and thread. (cuda-gdb) cuda thread (15)\n[Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread\n(15,0,0), device 0, sm 1, warp 0, lane 15]\n374 int totalThreads = gridDim.x * blockDim.x; The parentheses for the block and thread arguments are optional. (cuda-gdb) cuda block 1 thread 3\n[Switching focus to CUDA kernel 1, grid 2, block (1,0,0), thread (3,0,0),\ndevice 0, sm 3, warp 0, lane 3]\n374 int totalThreads = gridDim.x * blockDim. 6. Program Execution  Applications are launched the same way in CUDA-GDB as they are with GDB by using the run command. This chapter describes how to interrupt and single-step CUDA applications 6.1. Interrupting the Application  If the CUDA application appears to be hanging or stuck in an infinite loop, it is possible to manually interrupt the application by pressing CTRL+C. When the signal is received, the GPUs are suspended and the cuda-gdb prompt will appear. At that point, the program can be inspected, modified, single-stepped, resumed, or terminated at the user’s discretion. This feature is limited to applications running within the debugger. It is not possible to break into and debug applications that have been launched outside the debugger. 6.2. Single Stepping  Single-stepping device code is supported. However, unlike host code single-stepping, device code single-stepping works at the warp level. This means that single-stepping a device kernel advances all the active threads in the warp currently in focus. The divergent threads in the warp are not single-stepped. In order to advance the execution of more than one warp, a breakpoint must be set at the desired location and then the application must be fully resumed. A special case is single-stepping over thread barrier calls like: __syncthreads() or cluster-wide barriers. In this case, an implicit temporary breakpoint is set immediately after the barrier and all threads are resumed until the temporary breakpoint is hit. You can step in, over, or out of the device functions as long as they are not inlined. To force a function to not be inlined by the compiler, the __noinline__ keyword must be added to the function declaration. Asynchronous SASS instructions executed on the device, such as the warpgroup instructions, at prior PCs are not guaranteed to be complete. With Dynamic Parallelism, several CUDA APIs can be called directly from device code. The following list defines single-step behavior when encountering these APIs: When encountering device side kernel launches (denoted by the <<<>>> launch syntax), the step and next commands will have the same behavior, and both will step over the launch call. On devices prior to Hopper (SM 9.0), stepping into the deprecated cudaDeviceSynchronize() results in undefined behavior. Users shall step over this call instead. When stepping a device grid launch to completion, focus will automatically switch back to the CPU. The cuda kernel focus switching command must be used to switch to another grid of interest (if one is still resident). Note It is not possible to step into a device launch call (nor the routine launched by the call). 7. Breakpoints and Watchpoints  There are multiple ways to set a breakpoint on a CUDA application. These methods are described below. The commands used to set a breakpoint on device code are the same as the commands used to set a breakpoint on host code. If a breakpoint is set on device code, the breakpoint will be marked pending until the ELF image of the kernel is loaded. At that point, the breakpoint will be resolved and its address will be updated. When a breakpoint is set, it forces all resident GPU threads to stop at this location when it reaches the corresponding PC. When a breakpoint is hit by one thread, there is no guarantee that the other threads will hit the breakpoint at the same time. Therefore the same breakpoint may be hit several times, and the user must be careful with checking which thread(s) actually hit(s) the breakpoint. The disable command can be used to prevent hitting the breakpoint by additional threads. 7.1. Symbolic Breakpoints  To set a breakpoint at the entry of a function, use the break command followed by the name of the function or method: (cuda-gdb) break my_function\n(cuda-gdb) break my_class::my_method For templatized functions and methods, the full signature must be given: (cuda-gdb) break int my_templatized_function<int>(int) The mangled name of the function can also be used. To find the mangled name of a function, you can use the following command: (cuda-gdb) set demangle-style none\n(cuda-gdb) info function my_function_name\n(cuda-gdb) set demangle-style auto 7.2. Line Breakpoints  To set a breakpoint on a specific line number, use the following syntax: (cuda-gdb) break my_file.cu:185 If the specified line corresponds to an instruction within templatized code, multiple breakpoints will be created, one for each instance of the templatized code. 7.3. Address Breakpoints  To set a breakpoint at a specific address, use the break command with the address as argument: (cuda-gdb) break *0x1afe34d0 The address can be any address on the device or the host. 7.4. Kernel Entry Breakpoints  To break on the first instruction of every launched kernel, set the break_on_launch option to application: (cuda-gdb) set cuda break_on_launch application See set cuda break_on_launch for more information. 7.5. Conditional Breakpoints  To make the breakpoint conditional, use the optional if keyword or the cond command. (cuda-gdb) break foo.cu:23 if threadIdx.x == 1 && i < 5\n(cuda-gdb) cond 3 threadIdx.x == 1 && i < 5 Conditional expressions may refer any variable, including built-in variables such as threadIdx and blockIdx . Function calls are not allowed in conditional expressions. Note that conditional breakpoints are always hit and evaluated, but the debugger reports the breakpoint as being hit only if the conditional statement is evaluated to TRUE. The process of hitting the breakpoint and evaluating the corresponding conditional statement is time-consuming. Therefore, running applications while using conditional breakpoints may slow down the debugging session. Moreover, if the conditional statement is always evaluated to FALSE, the debugger may appear to be hanging or stuck, although it is not the case. You can interrupt the application with CTRL-C to verify that progress is being made. Conditional breakpoints can be set on code from CUDA modules that are not already loaded. The verification of the condition will then only take place when the ELF image of that module is loaded. Therefore any error in the conditional expression will be deferred until the CUDA module is loaded. To double check the desired conditional expression, first set an unconditional breakpoint at the desired location and continue. When the breakpoint is hit, evaluate the desired conditional statement by using the cond command. 7.6. Watchpoints  Watchpoints on CUDA code are not supported. Watchpoints on host code are supported. The user is invited to read the GDB documentation for a tutorial on how to set watchpoints on host code. 8. Inspecting Program State  8.1. Memory and Variables  The GDB print command has been extended to decipher the location of any program variable and can be used to display the contents of any CUDA program variable including: data allocated via cudaMalloc() data that resides in various GPU memory regions, such as shared, local, and global memory special CUDA runtime variables, such as threadIdx 8.2. Variable Storage and Accessibility  Depending on the variable type and usage, variables can be stored either in registers or in local , shared , const or global memory. You can print the address of any variable to find out where it is stored and directly access the associated memory. The example below shows how the variable array, which is of type shared int * , can be directly accessed in order to see what the stored values are in the array. (cuda-gdb) print &array\n$1 = (@shared int (*)[0]) 0x20\n(cuda-gdb) print array[0]@4\n$2 = {0, 128, 64, 192} You can also access the shared memory indexed into the starting offset to see what the stored values are: (cuda-gdb) print *(@shared int*)0x20\n$3 = 0\n(cuda-gdb) print *(@shared int*)0x24\n$4 = 128\n(cuda-gdb) print *(@shared int*)0x28\n$5 = 64 The example below shows how to access the starting address of the input parameter to the kernel. (cuda-gdb) print &data\n$6 = (const @global void * const @parameter *) 0x10\n(cuda-gdb) print *(@global void * const @parameter *) 0x10\n$7 = (@global void * const @parameter) 0x110000</> 8.3. Info CUDA Commands  These are commands that display information about the GPU and the application’s CUDA state. The available options are: devices information about all the devices sms information about all the active SMs in the current device warps information about all the active warps in the current SM lanes information about all the active lanes in the current warp kernels information about all the active kernels blocks information about all the active blocks in the current kernel threads information about all the active threads in the current kernel launch trace information about the parent kernels of the kernel in focus launch children information about the kernels launched by the kernels in focus contexts information about all the contexts A filter can be applied to every info cuda command. The filter restricts the scope of the command. A filter is composed of one or more restrictions. A restriction can be any of the following: device n sm n warp n lane n kernel n grid n block x[,y] or block (x[,y]) thread x[,y[,z]] or thread (x[,y[,z]]) breakpoint all and breakpoint n where n , x , y , z are integers, or one of the following special keywords: current , any , and all . current indicates that the corresponding value in the current focus should be used. any and all indicate that any value is acceptable. Note The breakpoint all and breakpoint n filter are only effective for the info cuda threads command. 8.3.1. info cuda devices  This command enumerates all the GPUs in the system sorted by device index. A * indicates the device currently in focus. This command supports filters. The default is device all . This command prints No CUDA Devices if no active GPUs are found. A device is not considered active until the first kernel launch has been encountered. (cuda-gdb) info cuda devices\n  Dev PCI Bus/Dev ID                Name Description SM Type SMs Warps/SM Lanes/Warp Max Regs/Lane Active SMs Mask\n    0        06:00.0 GeForce GTX TITAN Z      GK110B   sm_35  15       64         32           256 0x00000000\n    1        07:00.0 GeForce GTX TITAN Z      GK110B   sm_35  15       64         32           256 0x00000000 8.3.2. info cuda sms  This command shows all the SMs for the device and the associated active warps on the SMs. This command supports filters and the default is device current sm all . A * indicates the SM is focus. The results are grouped per device. (cuda-gdb) info cuda sms\n SM Active Warps Mask\nDevice 0\n* 0 0xffffffffffffffff\n  1 0xffffffffffffffff\n  2 0xffffffffffffffff\n  3 0xffffffffffffffff\n  4 0xffffffffffffffff\n  5 0xffffffffffffffff\n  6 0xffffffffffffffff\n  7 0xffffffffffffffff\n  8 0xffffffffffffffff\n... 8.3.3. info cuda warps  This command takes you one level deeper and prints all the warps information for the SM in focus. This command supports filters and the default is device current sm current warp all . The command can be used to display which warp executes what block. (cuda-gdb) info cuda warps\nWp /Active Lanes Mask/ Divergent Lanes Mask/Active Physical PC/Kernel/BlockIdx\nDevice 0 SM 0\n* 0    0xffffffff    0x00000000 0x000000000000001c    0    (0,0,0)\n  1    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n  2    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n  3    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n  4    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n  5    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n  6    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n  7    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)\n ... 8.3.4. info cuda lanes  This command displays all the lanes (threads) for the warp in focus. This command supports filters and the default is device current sm current warp current lane all . In the example below you can see that all the lanes are at the same physical PC. The command can be used to display which lane executes what thread. (cuda-gdb) info cuda lanes\n  Ln    State  Physical PC        ThreadIdx\nDevice 0 SM 0 Warp 0\n*  0    active 0x000000000000008c   (0,0,0)\n   1    active 0x000000000000008c   (1,0,0)\n   2    active 0x000000000000008c   (2,0,0)\n   3    active 0x000000000000008c   (3,0,0)\n   4    active 0x000000000000008c   (4,0,0)\n   5    active 0x000000000000008c   (5,0,0)\n   6    active 0x000000000000008c   (6,0,0)\n   7    active 0x000000000000008c   (7,0,0)\n   8    active 0x000000000000008c   (8,0,0)\n   9    active 0x000000000000008c   (9,0,0)\n  10    active 0x000000000000008c  (10,0,0)\n  11    active 0x000000000000008c  (11,0,0)\n  12    active 0x000000000000008c  (12,0,0)\n  13    active 0x000000000000008c  (13,0,0)\n  14    active 0x000000000000008c  (14,0,0)\n  15    active 0x000000000000008c  (15,0,0)\n  16    active 0x000000000000008c  (16,0,0)\n ... 8.3.5. info cuda kernels  This command displays on all the active kernels on the GPU in focus. It prints the SM mask, kernel ID, and the grid ID for each kernel with the associated dimensions and arguments. The kernel ID is unique across all GPUs whereas the grid ID is unique per GPU. The Parent column shows the kernel ID of the parent grid. This command supports filters and the default is kernel all . (cuda-gdb) info cuda kernels\n  Kernel Parent Dev Grid Status   SMs Mask   GridDim  BlockDim      Name Args\n*      1      -   0    2 Active 0x00ffffff (240,1,1) (128,1,1) acos_main parms=... This command will also show grids that have been launched on the GPU with Dynamic Parallelism. Kernels with a negative grid ID have been launched from the GPU, while kernels with a positive grid ID have been launched from the CPU. 8.3.6. info cuda blocks  This command displays all the active or running blocks for the kernel in focus. The results are grouped per kernel. This command supports filters and the default is kernel current block all . The outputs are coalesced by default. (cuda-gdb) info cuda blocks\n   BlockIdx   To BlockIdx  Count  State\nKernel 1\n*  (0,0,0)    (191,0,0)    192    running Coalescing can be turned off as follows in which case more information on the Device and the SM get displayed: (cuda-gdb) set cuda coalescing off The following is the output of the same command when coalescing is turned off. (cuda-gdb) info cuda blocks\n  BlockIdx   State    Dev SM\nKernel 1\n*   (0,0,0)   running   0   0\n    (1,0,0)   running   0   3\n    (2,0,0)   running   0   6\n    (3,0,0)   running   0   9\n    (4,0,0)   running   0  12\n    (5,0,0)   running   0  15\n    (6,0,0)   running   0  18\n    (7,0,0)   running   0  21\n    (8,0,0)   running   0   1\n ... 8.3.7. info cuda threads  This command displays the application’s active CUDA blocks and threads with the total count of threads in those blocks. Also displayed are the virtual PC and the associated source file and the line number information. The results are grouped per kernel. The command supports filters with default being kernel current block all thread all . The outputs are coalesced by default as follows: (cuda-gdb) info cuda threads\n  BlockIdx ThreadIdx To BlockIdx ThreadIdx Count   Virtual PC    Filename   Line\nDevice 0 SM 0\n* (0,0,0  (0,0,0)    (0,0,0)  (31,0,0)    32  0x000000000088f88c   acos.cu   376\n  (0,0,0)(32,0,0)  (191,0,0) (127,0,0) 24544  0x000000000088f800   acos.cu   374\n ... Coalescing can be turned off as follows in which case more information is displayed with the output. (cuda-gdb) info cuda threads\n   BlockIdx  ThreadIdx  Virtual PC         Dev SM Wp Ln   Filename  Line\nKernel 1\n*  (0,0,0)    (0,0,0)  0x000000000088f88c   0  0  0  0    acos.cu    376\n   (0,0,0)    (1,0,0)  0x000000000088f88c   0  0  0  1    acos.cu    376\n   (0,0,0)    (2,0,0)  0x000000000088f88c   0  0  0  2    acos.cu    376\n   (0,0,0)    (3,0,0)  0x000000000088f88c   0  0  0  3    acos.cu    376\n   (0,0,0)    (4,0,0)  0x000000000088f88c   0  0  0  4    acos.cu    376\n   (0,0,0)    (5,0,0)  0x000000000088f88c   0  0  0  5    acos.cu    376\n   (0,0,0)    (6,0,0)  0x000000000088f88c   0  0  0  6    acos.cu    376\n   (0,0,0)    (7,0,0)  0x000000000088f88c   0  0  0  7    acos.cu    376\n   (0,0,0)    (8,0,0)  0x000000000088f88c   0  0  0  8    acos.cu    376\n   (0,0,0)    (9,0,0)  0x000000000088f88c   0  0  0  9    acos.cu    376\n ... Note In coalesced form, threads must be contiguous in order to be coalesced. If some threads are not currently running on the hardware, they will create holes in the thread ranges. For instance, if a kernel consist of 2 blocks of 16 threads, and only the 8 lowest threads are active, then 2 coalesced ranges will be printed: one range for block 0 thread 0 to 7, and one range for block 1 thread 0 to 7. Because threads 8-15 in block 0 are not running, the 2 ranges cannot be coalesced. The command also supports breakpoint all and breakpoint breakpoint_number as filters. The former displays the threads that hit all CUDA breakpoints set by the user. The latter displays the threads that hit the CUDA breakpoint breakpoint_number . (cuda-gdb) info cuda threads breakpoint all\n  BlockIdx ThreadIdx         Virtual PC Dev SM Wp Ln        Filename  Line\nKernel 0\n   (1,0,0)   (0,0,0) 0x0000000000948e58   0 11  0  0 infoCommands.cu    12\n   (1,0,0)   (1,0,0) 0x0000000000948e58   0 11  0  1 infoCommands.cu    12\n   (1,0,0)   (2,0,0) 0x0000000000948e58   0 11  0  2 infoCommands.cu    12\n   (1,0,0)   (3,0,0) 0x0000000000948e58   0 11  0  3 infoCommands.cu    12\n   (1,0,0)   (4,0,0) 0x0000000000948e58   0 11  0  4 infoCommands.cu    12\n   (1,0,0)   (5,0,0) 0x0000000000948e58   0 11  0  5 infoCommands.cu    12\n\n(cuda-gdb) info cuda threads breakpoint 2 lane 1\n  BlockIdx ThreadIdx         Virtual PC Dev SM Wp Ln        Filename  Line\nKernel 0\n   (1,0,0)   (1,0,0) 0x0000000000948e58   0 11  0  1 infoCommands.cu    12 8.3.8. info cuda launch trace  This command displays the kernel launch trace for the kernel in focus. The first element in the trace is the kernel in focus. The next element is the kernel that launched this kernel. The trace continues until there is no parent kernel. In that case, the kernel is CPU-launched. For each kernel in the trace, the command prints the level of the kernel in the trace, the kernel ID, the device ID, the grid Id, the status, the kernel dimensions, the kernel name, and the kernel arguments. (cuda-gdb) info cuda launch trace\n  Lvl Kernel Dev Grid     Status   GridDim  BlockDim Invocation\n*   0      3   0   -7     Active  (32,1,1)  (16,1,1) kernel3(c=5)\n    1      2   0   -5 Terminated (240,1,1) (128,1,1) kernel2(b=3)\n    2      1   0    2     Active (240,1,1) (128,1,1) kernel1(a=1) A kernel that has been launched but that is not running on the GPU will have a Pending status. A kernel currently running on the GPU will be marked as Active . A kernel waiting to become active again will be displayed as Sleeping . When a kernel has terminated, it is marked as Terminated . For the few cases, when the debugger cannot determine if a kernel is pending or terminated, the status is set to Undetermined . This command supports filters and the default is kernel all . Note With set cuda software_preemption on , no kernel will be reported as active. 8.3.9. info cuda launch children  This command displays the list of non-terminated kernels launched by the kernel in focus. For each kernel, the kernel ID, the device ID, the grid Id, the kernel dimensions, the kernel name, and the kernel parameters are displayed. (cuda-gdb) info cuda launch children\n  Kernel Dev Grid GridDim BlockDim Invocation\n*      3   0   -7 (1,1,1)  (1,1,1) kernel5(a=3)\n      18   0   -8 (1,1,1) (32,1,1) kernel4(b=5) This command supports filters and the default is kernel all . 8.3.10. info cuda contexts  This command enumerates all the CUDA contexts running on all GPUs. A * indicates the context currently in focus. This command shows whether a context is currently active on a device or not. (cuda-gdb) info cuda contexts\n     Context Dev    State\n  0x080b9518   0 inactive\n* 0x08067948   0   active 8.3.11. info cuda managed  This command shows all the static managed variables on the device or on the host depending on the focus. (cuda-gdb) info cuda managed\nStatic managed variables on device 0 are:\nmanaged_var = 3\nmanaged_consts = {one = 1, e = 2.71000004, pi = 3.1400000000000001} 8.4. Disassembly  The device SASS code can be disassembled using the standard GDB disassembly instructions such as x/i and display/i . (cuda-gdb) x/4i $pc-32\n   0xa689a8 <acos_main(acosParams)+824>: MOV R0, c[0x0][0x34]\n   0xa689b8 <acos_main(acosParams)+840>: MOV R3, c[0x0][0x28]\n   0xa689c0 <acos_main(acosParams)+848>: IMUL R2, R0, R3\n=> 0xa689c8 <acos_main(acosParams)+856>: MOV R0, c[0x0][0x28] Note For disassembly instruction to work properly, cuobjdump must be installed and present in your $PATH . In the disassembly view, the current pc is prefixed with => . For Maxwell (SM 5.0) and newer architectures, if an instruction triggers an exception it will be prefixed with *> . If the pc and errorpc are the same instruction it will be prefixed with *=> . For example, consider the following exception: CUDA Exception: Warp Illegal Address\nThe exception was triggered at PC 0x555555c08620 (memexceptions_kernel.cu:17)\n\nThread 1 \"memexceptions\" received signal CUDA_EXCEPTION_14, Warp Illegal Address.\n[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]\n0x0000555555c08fb0 in exception_kernel<<<(1,1,1),(1,1,1)>>> (data=0x7fffccc00000, exception=MMU_FAULT) at memexceptions_kernel.cu:50\n50  }\n(cuda-gdb) The disas command can be used to view both the PC and the error PC that triggered the exception. (cuda-gdb) disas $pc,+16\nDump of assembler code from 0x555555c08fb0 to 0x555555c08fc0:\n=> 0x0000555555c08fb0 <_Z16exception_kernelPv11exception_t+3504>:  ERRBAR\nEnd of assembler dump. (cuda-gdb) disas $errorpc,+16\nDump of assembler code from 0x555555c08620 to 0x555555c08630:\n*> 0x0000555555c08620 <_Z16exception_kernelPv11exception_t+1056>:  ST.E.U8.STRONG.SYS [R6.64], R5\nEnd of assembler dump. 8.5. Registers  The device registers code can be inspected/modified using the standard GDB commands such as info registers . (cuda-gdb) info registers $R0 $R1 $R2 $R3\nR0             0xf0 240\nR1             0xfffc48 16776264\nR2             0x7800   30720\nR3             0x80 128 The registers are also accessible as $R<regnum> built-in variables, for example: (cuda-gdb) printf \"%d %d\\n\", $R0*$R3, $R2\n30720 30720 Values of predicate and CC registers can be inspecting by printing system registers group or by using their respective pseudo-names: $P0 .. $P6 and $CC . (cuda-gdb) info registers system\nP0             0x1  1\nP1             0x1  1\nP2             0x0  0\nP3             0x0  0\nP4             0x0  0\nP5             0x0  0\nP6             0x1  1\nCC             0x0  0 8.6. Const banks  Memory allocated in the constant address space of GPU memory resides in two dimensional arrays called constant banks.\nConstant banks are noted c[X][Y] where X is the bank number and Y the offset.\nThe memory address of a given bank/offset pair is obtained via the convenience function $_cuda_const_bank(bank, offset) . (cuda-gdb) disass $pc,+16\nDump of assembler code from 0x7fffd5043d40 to 0x7fffd5043d50:\n=> 0x00007fffd5043d40 <_Z9acos_main10acosParams+1856>:  MOV R0, c[0x0][0xc]\nEnd of assembler dump.\n(cuda-gdb) p *$_cuda_const_bank(0x0,0xc)\n$1 = 8 9. Event Notifications  As the application is making forward progress, CUDA-GDB notifies the users about kernel events and context events. Within CUDA-GDB, kernel refers to the device code that executes on the GPU, while context refers to the virtual address space on the GPU for the kernel. You can enable output of CUDA context and kernel events to review the flow of the active contexts and kernels. By default, only context event messages are displayed. 9.1. Context Events  Any time a CUDA context is created, pushed, popped, or destroyed by the application, CUDA-GDB can optionally display a notification message. The message includes the context id and the device id to which the context belongs. [Context Create of context 0xad2fe60 on Device 0]\n[Context Destroy of context 0xad2fe60 on Device 0] By default, context event notification is disabled. The context event notification policy is controlled with the context_events option. (cuda-gdb) set cuda context_events off CUDA-GDB does not display the context event notification messages (default). (cuda-gdb) set cuda context_events on CUDA-GDB displays the context event notification messages. 9.2. Kernel Events  Any time CUDA-GDB is made aware of the launch or the termination of a CUDA kernel, a notification message can be displayed. The message includes the kernel id, the kernel name, and the device to which the kernel belongs. [Launch of CUDA Kernel 1 (kernel3) on Device 0]\n[Termination of CUDA Kernel 1 (kernel3) on Device 0] The kernel event notification policy is controlled with kernel_events and kernel_events_depth options. (cuda-gdb) set cuda kernel_events none Possible options are: none no kernel, application or system (default) application kernel launched by the user application system any kernel launched by the driver, such as memset all any kernel, application and system (cuda-gdb) set cuda kernel_events_depth 0 Controls the maximum depth of the kernels after which no kernel event notifications will be displayed. A value of zero means that there is no maximum and that all the kernel notifications are displayed. A value of one means that the debugger will display kernel event notifications only for kernels launched from the CPU (default). 10. Automatic Error Checking  10.1. Checking API Errors  CUDA-GDB can automatically check the return code of any driver API or runtime API call. If the return code indicates an error, the debugger will stop or warn the user. The behavior is controlled with the set cuda api_failures option. Three modes are supported: hide CUDA API call failures are not reported ignore Warning message is printed for every fatal CUDA API call failure (default) stop The application is stopped when a CUDA API call returns a fatal error ignore_all Warning message is printed for every CUDA API call failure stop_all The application is stopped when a CUDA API call returns any error Note The success return code and other non-error return codes are ignored. For the driver API, those are: CUDA_SUCCESS and CUDA_ERROR_NOT_READY . For the runtime API, they are cudaSuccess and cudaErrorNotReady . 10.2. GPU Error Reporting  With improved GPU error reporting in CUDA-GDB, application bugs are now easier to identify and easy to fix. The following table shows the new errors that are reported on GPUs with compute capability sm_20 and higher. Note Continuing the execution of your application after these errors are found can lead to application termination or indeterminate results. Note Warp errors may result in instructions to continue executing before the exception is recognized and reported. The reported $errorpc shall contain the precise address of the instruction that caused the exception. If the warp exits after the instruction causing exception has executed, but before the exception has been recognized and reported, it may result in the exception not being reported. CUDA-GDB relies on an active warp present on the device in order to report exceptions. To help avoid this scenario of unreported exceptions: For Volta+ architectures, compile the application with -G . See Compiling the Application for more information. Add while(1); before kernel exit. This shall ensure the exception is recognized and reported. Rely on the compute-sanitizer memcheck tool to catch accesses that can lead to an exception. CUDA Exception Codes  Exception Code Precision of the Error Scope of the Error Description CUDA_EXCEPTION_0 : “Device Unknown Exception” Unknown Global error on the GPU This is a global GPU error caused by the application which does not match any of the listed error codes below. This should be a rare occurrence. Potentially, this may be due to Device Hardware Stack overflows or a kernel generating an exception very close to its termination. CUDA_EXCEPTION_1 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 . CUDA_EXCEPTION_2 : “Lane User Stack Overflow” Precise Per lane/thread error This occurs when a thread exceeds its stack memory limit. CUDA_EXCEPTION_3 : “Device Hardware Stack Overflow” Precise Global error on the GPU This occurs when the application triggers a global hardware stack overflow. The main cause of this error is large amounts of divergence in the presence of function calls. CUDA_EXCEPTION_4 : “Warp Illegal Instruction” Precise Warp error This occurs when any thread within a warp has executed an illegal instruction. CUDA_EXCEPTION_5 : “Warp Out-of-range Address” Precise Warp error This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions. CUDA_EXCEPTION_6 : “Warp Misaligned Address” Precise Warp error This occurs when any thread within a warp accesses an address in the local or shared memory segments that is not correctly aligned. CUDA_EXCEPTION_7 : “Warp Invalid Address Space” Precise Warp error This occurs when any thread within a warp executes an instruction that accesses a memory space not permitted for that instruction. CUDA_EXCEPTION_8 : “Warp Invalid PC” Precise Warp error This occurs when any thread within a warp advances its PC beyond the 40-bit address space. CUDA_EXCEPTION_9 : “Warp Hardware Stack Overflow” Precise Warp error This occurs when any thread in a warp triggers a hardware stack overflow. This should be a rare occurrence. CUDA_EXCEPTION_10 : “Device Illegal Address” Precise Global error This occurs when a thread accesses an illegal(out of bounds) global address. CUDA_EXCEPTION_11 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 . CUDA_EXCEPTION_12 : “Warp Assert” Precise Per warp This occurs when any thread in the warp hits a device side assertion. CUDA_EXCEPTION_13 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 . CUDA_EXCEPTION_14 : “Warp Illegal Address” Precise Per warp This occurs when a thread accesses an illegal(out of bounds) global/local/shared address. CUDA_EXCEPTION_15 : “Invalid Managed Memory Access” Precise Per host thread This occurs when a host thread attempts to access managed memory currently used by the GPU. CUDA_EXCEPTION_13 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 . CUDA_EXCEPTION_17 : “Cluster Out-of-range Address” Not precise Per Cuda Cluster This occurs when any thread within a block accesses an address that is outside the valid range of shared memory regions belonging to the cluster. CUDA_EXCEPTION_18 : “Cluster Target Block Not Present” Not precise Per Cuda Cluster This occurs when any thread within a block accesses another block that is outside the valid range of blocks belonging to the cluster. 10.3. Autostep  Autostep is a command to increase the precision of CUDA exceptions to the exact lane and instruction, when they would not have been otherwise. Under normal execution, an exception may be reported several instructions after the exception occurred, or the exact thread where an exception occurred may not be known unless the exception is a lane error. However, the precise origin of the exception can be determined if the program is being single-stepped when the exception occurs. Single- stepping manually is a slow and tedious process; stepping takes much longer than normal execution and the user has to single-step each warp individually. Autostep aides the user by allowing them to specify sections of code where they suspect an exception could occur, and these sections are automatically and transparently single- stepped the program is running. The rest of the program is executed normally to minimize the slow-down caused by single-stepping. The precise origin of an exception will be reported if the exception occurs within these sections. Thus the exact instruction and thread where an exception occurred can be found quickly and with much less effort by using autostep. Autostep Usage autostep [LOCATION]\nautostep [LOCATION] for LENGTH [lines|instructions] LOCATION may be anything that you use to specify the location of a breakpoint, such as a line number, function name, or an instruction address preceded by an asterisk. If no LOCATION is specified, then the current instruction address is used. LENGTH specifies the size of the autostep window in number of lines or instructions ( lines and instructions can be shortened, e.g., l or i ). If the length type is not specified, then lines is the default. If the for clause is omitted, then the default is 1 line. astep can be used as an alias for the autostep command. Calls to functions made during an autostep will be stepped over. In case of divergence, the length of the autostep window is determined by the number of lines or instructions the first active lane in each warp executes. Divergent lanes are also single stepped, but the instructions they execute do not count towards the length of the autostep window. If a breakpoint occurs while inside an autostep window, the warp where the breakpoint was hit will not continue autostepping when the program is resumed. However, other warps may continue autostepping. Overlapping autosteps are not supported. If an autostep is encountered while another autostep is being executed, then the second autostep is ignored. If an autostep is set before the location of a memory error and no memory error is hit, then it is possible that the chosen window is too small. This may be caused by the presence of function calls between the address of the autostep location and the instruction that triggers the memory error. In that situation, either increase the size of the window to make sure that the faulty instruction is included, or move to the autostep location to an instruction that will be executed closer in time to the faulty instruction. Related Commands Autosteps and breakpoints share the same numbering so most commands that work with breakpoints will also work with autosteps. info autosteps shows all breakpoints and autosteps. It is similar to info breakpoints . (cuda-gdb) info autosteps\nNum  Type      Disp Enb Address            What\n1    autostep  keep y   0x0000000000401234 in merge at sort.cu:30 for 49 instructions\n3    autostep  keep y   0x0000000000489913 in bubble at sort.cu:94 for 11 lines disable autosteps disables an autostep. It is equivalent to disable breakpoints n . delete autosteps n deletes an autostep. It is quivalent to delete breakpoints n . ignore n i tells the debugger to not single-step the next i times the debugger enters the window for autostep n . This command already exists for breakpoints. 11. Walk-Through Examples  The chapter contains two CUDA-GDB walk-through examples: Example: bitreverse Example: autostep Example: MPI CUDA Application 11.1. Example: bitreverse  This section presents a walk-through of CUDA-GDB by debugging a sample application–called bitreverse –that performs a simple 8 bit reversal on a data set. Source Code 1  #include <stdio.h>\n2  #include <stdlib.h>\n3\n4  // Simple 8-bit bit reversal Compute test\n5\n6  #define N 256\n7\n8  __global__ void bitreverse(void *data) {\n9     unsigned int *idata = (unsigned int*)data;\n10    extern __shared__ int array[];\n11\n12    array[threadIdx.x] = idata[threadIdx.x];\n13\n14    array[threadIdx.x] = ((0xf0f0f0f0 & array[threadIdx.x]) >> 4) |\n15                        ((0x0f0f0f0f & array[threadIdx.x]) << 4);\n16    array[threadIdx.x] = ((0xcccccccc & array[threadIdx.x]) >> 2) |\n17                        ((0x33333333 & array[threadIdx.x]) << 2);\n18    array[threadIdx.x] = ((0xaaaaaaaa & array[threadIdx.x]) >> 1) |\n19                         ((0x55555555 & array[threadIdx.x]) << 1);\n20\n21    idata[threadIdx.x] = array[threadIdx.x];\n22 }\n23\n24 int main(void) {\n25     void *d = NULL; int i;\n26     unsigned int idata[N], odata[N];\n27\n28     for (i = 0; i < N; i++)\n29         idata[i] = (unsigned int)i;\n30\n31     cudaMalloc((void**)&d, sizeof(int)*N);\n32     cudaMemcpy(d, idata, sizeof(int)*N,\n33                cudaMemcpyHostToDevice);\n34\n35     bitreverse<<<1, N, N*sizeof(int)>>>(d);\n36\n37     cudaMemcpy(odata, d, sizeof(int)*N,\n38                cudaMemcpyDeviceToHost);\n39\n40     for (i = 0; i < N; i++)\n41        printf(\"%u -> %u\\n\", idata[i], odata[i]);\n42\n43     cudaFree((void*)d);\n44     return 0;\n45 } 11.1.1. Walking through the Code  Begin by compiling the bitreverse.cu CUDA application for debugging by entering the following command at a shell prompt: $ nvcc -g -G bitreverse.cu -o bitreverse This command assumes that the source file name is bitreverse.cu and that no additional compiler flags are required for compilation. See also Debug Compilation Start the CUDA debugger by entering the following command at a shell prompt: $ cuda-gdb bitreverse Set breakpoints. Set both the host ( main ) and GPU ( bitreverse ) breakpoints here. Also, set a breakpoint at a particular line in the device function ( bitreverse.cu:18 ). (cuda-gdb) break main\nBreakpoint 1 at 0x18e1: file bitreverse.cu, line 25.\n(cuda-gdb) break bitreverse\nBreakpoint 2 at 0x18a1: file bitreverse.cu, line 8.\n(cuda-gdb) break 21\nBreakpoint 3 at 0x18ac: file bitreverse.cu, line 21. Run the CUDA application, and it executes until it reaches the first breakpoint ( main ) set in 3 . (cuda-gdb) run\nStarting program: /Users/CUDA_User1/docs/bitreverse\nReading symbols for shared libraries\n..++........................................................... done\n\nBreakpoint 1, main () at bitreverse.cu:25\n25  void *d = NULL; int i; At this point, commands can be entered to advance execution or to print the program state. For this walkthrough, let’s continue until the device kernel is launched. (cuda-gdb) continue\nContinuing.\nReading symbols for shared libraries .. done\nReading symbols for shared libraries .. done\n[Context Create of context 0x80f200 on Device 0]\n[Launch of CUDA Kernel 0 (bitreverse<<<(1,1,1),(256,1,1)>>>) on Device 0]\nBreakpoint 3 at 0x8667b8: file bitreverse.cu, line 21.\n[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]\n\nBreakpoint 2, bitreverse<<<(1,1,1),(256,1,1)>>> (data=0x110000) at bitreverse.cu:9\n9   unsigned int *idata = (unsigned int*)data; CUDA−GDB has detected that a CUDA device kernel has been reached. The debugger prints the current CUDA thread of focus. Verify the CUDA thread of focus with the info cuda threads command and switch between host thread and the CUDA threads: (cuda-gdb) info cuda threads\n  BlockIdx ThreadIdx To BlockIdx ThreadIdx Count            Virtual PC\nFilename   Line\n\nKernel 0\n*  (0,0,0)    (0,0,0)    (0,0,0) (255,0,0)    256 0x0000000000866400 bitreverse.cu     9\n(cuda-gdb) thread\n[Current thread is 1 (process 16738)]\n(cuda-gdb) thread 1\n[Switching to thread 1 (process 16738)]\n#0  0x000019d5 in main () at bitreverse.cu:34\n34    bitreverse<<<1, N, N*sizeof(int)>>>(d);\n(cuda-gdb) backtrace\n#0  0x000019d5 in main () at bitreverse.cu:34\n(cuda-gdb) info cuda kernels\nKernel Dev Grid   SMs Mask GridDim  BlockDim        Name Args\n     0   0    1 0x00000001 (1,1,1) (256,1,1) bitreverse data=0x110000\n(cuda-gdb) cuda kernel 0\n[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]\n9    unsigned int *idata = (unsigned int*)data;\n(cuda-gdb) backtrace\n#0   bitreverse<<<(1,1,1),(256,1,1)>>> (data=0x110000) at bitreverse.cu:9 Corroborate this information by printing the block and thread indexes: (cuda-gdb) print blockIdx\n$1 = {x = 0, y = 0}\n(cuda-gdb) print threadIdx\n$2 = {x = 0, y = 0, z = 0) The grid and block dimensions can also be printed: (cuda-gdb) print gridDim\n$3 = {x = 1, y = 1}\n(cuda-gdb) print blockDim\n$4 = {x = 256, y = 1, z = 1) Advance kernel execution and verify some data: (cuda-gdb) next\n12       array[threadIdx.x] = idata[threadIdx.x];\n(cuda-gdb) next\n14       array[threadIdx.x] = ((0xf0f0f0f0 & array[threadIdx.x]) >> 4) |\n(cuda-gdb) next\n16       array[threadIdx.x] = ((0xcccccccc & array[threadIdx.x]) >> 2) |\n(cuda-gdb) next\n18       array[threadIdx.x] = ((0xaaaaaaaa & array[threadIdx.x]) >> 1) |\n(cuda-gdb) next\n\nBreakpoint 3, bitreverse <<<(1,1),(256,1,1)>>> (data=0x100000) at bitreverse.cu:21\n21             idata[threadIdx.x] = array[threadIdx.x];\n(cuda-gdb) print array[0]@12\n$7 = {0, 128, 64, 192, 32, 160, 96, 224, 16, 144, 80, 208}\n(cuda-gdb) print/x array[0]@12\n$8 = {0x0, 0x80, 0x40, 0xc0, 0x20, 0xa0, 0x60, 0xe0, 0x10, 0x90, 0x50,\n0xd0}\n\n(cuda-gdb) print &data\n$9 = (@global void * @parameter *) 0x10\n(cuda-gdb) print *(@global void * @parameter *) 0x10\n$10 = (@global void * @parameter) 0x100000 The resulting output depends on the current content of the memory location. Since thread ( 0,0,0 ) reverses the value of 0 , switch to a different thread to show more interesting data: (cuda-gdb) cuda thread 170\n[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread\n(170,0,0), device 0, sm 0, warp 5, lane 10] Delete the breakpoints and continue the program to completion: (cuda-gdb) delete breakpoints\nDelete all breakpoints? (y or n) y\n(cuda-gdb) continue\nContinuing.\n\nProgram exited normally.\n(cuda-gdb) 11.2. Example: autostep  This section shows how to use the autostep command and demonstrates how it helps increase the precision of memory error reporting. Source Code 1  #define NUM_BLOCKS 8\n2  #define THREADS_PER_BLOCK 64\n3\n4  __global__ void example(int **data) {\n5    int value1, value2, value3, value4, value5;\n6    int idx1, idx2, idx3;\n7\n8    idx1 = blockIdx.x * blockDim.x;\n9    idx2 = threadIdx.x;\n10   idx3 = idx1 + idx2;\n11   value1 = *(data[idx1]);\n12   value2 = *(data[idx2]);\n13   value3 = value1 + value2;\n14   value4 = value1 * value2;\n15   value5 = value3 + value4;\n16   *(data[idx3]) = value5;\n17   *(data[idx1]) = value3;\n18   *(data[idx2]) = value4;\n19   idx1 = idx2 = idx3 = 0;\n20 }\n21\n22 int main(int argc, char *argv[]) {\n23   int *host_data[NUM_BLOCKS * THREADS_PER_BLOCK];\n24   int **dev_data;\n25   const int zero = 0;\n26\n27   /* Allocate an integer for each thread in each block */\n28   for (int block = 0; block < NUM_BLOCKS; block++) {\n29     for (int thread = 0; thread < THREADS_PER_BLOCK; thread++) {\n30       int idx = thread + block * THREADS_PER_BLOCK;\n31       cudaMalloc(&host_data[idx], sizeof(int));\n32       cudaMemcpy(host_data[idx], &zero, sizeof(int),\n33                  cudaMemcpyHostToDevice);\n34     }\n35   }\n36\n37   /* This inserts an error into block 3, thread 39*/\n38   host_data[3*THREADS_PER_BLOCK  + 39] = NULL;\n39\n40   /* Copy the array of pointers to the device */\n41   cudaMalloc((void**)&dev_data,  sizeof(host_data));\n42   cudaMemcpy(dev_data, host_data, sizeof(host_data), cudaMemcpyHostToDevice);\n43\n44   /* Execute example */\n45   example <<< NUM_BLOCKS, THREADS_PER_BLOCK >>> (dev_data);\n46   cudaThreadSynchronize();\n47 } In this small example, we have an array of pointers to integers, and we want to do some operations on the integers. Suppose, however, that one of the pointers is NULL as shown in line 38. This will cause CUDA_EXCEPTION_10 \"Device Illegal Address\" to be thrown when we try to access the integer that corresponds with block 3, thread 39. This exception should occur at line 16 when we try to write to that value. 11.2.1. Debugging with Autosteps  Compile the example and start CUDA−GDB as normal. We begin by running the program: (cuda-gdb) run\nStarting program: /home/jitud/cudagdb_test/autostep_ex/example\n[Thread debugging using libthread_db enabled] [New Thread 0x7ffff5688700 (LWP 9083)]\n[Context Create of context 0x617270 on Device 0]\n[Launch of CUDA Kernel 0 (example<<<(8,1,1),(64,1,1)>>>) on Device 0]\n\nProgram received signal CUDA_EXCEPTION_10, Device Illegal Address.\n[Switching focus to CUDA kernel 0, grid 1, block (1,0,0), thread (0,0,0), device 0, sm 1, warp 0, lane 0]\n0x0000000000796f60 in example (data=0x200300000) at example.cu:17\n17        *(data[idx1]) = value3; As expected, we received a CUDA_EXCEPTION_10 . However, the reported thread is block 1, thread 0 and the line is 17. Since CUDA_EXCEPTION_10 is a Global error, there is no thread information that is reported, so we would manually have to inspect all 512 threads. Set autosteps . To get more accurate information, we reason that since CUDA_EXCEPTION_10 is a memory access error, it must occur on code that accesses memory. This happens on lines 11, 12, 16, 17, and 18, so we set two autostep windows for those areas: (cuda-gdb) autostep 11 for 2 lines\nBreakpoint 1 at 0x796d18: file example.cu, line 11.\nCreated autostep of length 2 lines\n(cuda-gdb) autostep 16 for 3 lines\nBreakpoint 2 at 0x796e90: file example.cu, line 16.\nCreated autostep of length 3 lines Finally, we run the program again with these autosteps: (cuda-gdb) run\nThe program being debugged has been started already.\nStart it from the beginning? (y or n) y\n[Termination of CUDA Kernel 0 (example<<<(8,1,1),(64,1,1)>>>) on Device 0]\nStarting program: /home/jitud/cudagdb_test/autostep_ex/example\n[Thread debugging using libthread_db enabled]\n[New Thread 0x7ffff5688700 (LWP 9089)]\n[Context Create of context 0x617270 on Device 0]\n[Launch of CUDA Kernel 1 (example<<<(8,1,1),(64,1,1)>>>) on Device 0]\n[Switching focus to CUDA kernel 1, grid 1, block (0,0,0), thread (0,0,0),\ndevice 0, sm 0, warp 0, lane 0]\n\nProgram received signal CUDA_EXCEPTION_10, Device Illegal Address.\n[Current focus set to CUDA kernel 1, grid 1, block (3,0,0), thread\n(32,0,0), device 0, sm 1, warp 3, lane 0]\nAutostep precisely caught exception at example.cu:16 (0x796e90) This time we correctly caught the exception at line 16. Even though CUDA_EXCEPTION_10 is a global error, we have now narrowed it down to a warp error, so we now know that the thread that threw the exception must have been in the same warp as block 3, thread 32. In this example, we have narrowed down the scope of the error from 512 threads down to 32 threads just by setting two autosteps and re−running the program. 11.3. Example: MPI CUDA Application  For large scale MPI CUDA application debugging, NVIDIA recommends using parallel debuggers supplied by our partners Allinea and Totalview. Both make excellent parallel debuggers with extended support for CUDA. However, for debugging smaller applications, or for debugging just a few processes in a large application, CUDA-GDB can be used. If the cluster nodes have xterm support, launch CUDA-GDB in the same way you would launch gdb with your job launcher. For example: $ mpirun -np 4 -host nv1,nv2 xterm -e cuda-gdb a.out You may have to export the DISPLAY variable to make sure that the xterm finds its way back to your display. For example: $ mpirun -np 4 -host nv1,nv2 -x DISPLAY=host.nvidia.com:0 xterm -e cuda-gdb a.out Job launchers have different ways of exporting environment variables to the cluster nodes. Consult your job launcher documentation for more details. When xterm is not supported by your cluster environment, you can insert a spin loop inside your program, ssh to the compute node(s), and attach onto the MPI processes. Somewhere near the start of your program, add a code snippet similar to the following: { int i = 0 ; char host [ 256 ]; printf ( \"PID %d on node %s is ready for attach \\n \" , getpid (), host ); fflush ( stdout ); while ( 0 == i ) { sleep ( 5 ); } } Recompile and launch the application. After it starts, ssh to the node(s) of interest and attach to the process using CUDA-GDB. Set the variable i to 1 to break out of the loop: $ mpirun -np 2 -host nv1,nv2 a.out\nPID 20060 on node nv1 is ready for attach\nPID 5488 on node nv2 is ready for attach $ ssh nv1\n[nv1]$ cuda-gdb --pid 5488 $ ssh nv2\n[nv2]$ cuda-gdb --pid 20060 For larger applications, you can conditionalize the spin loop based on the MPI rank using the MPI_Comm_rank function. For devices with compute capability below 6.0, the software preemption workaround described in Multiple Debuggers does not work with MPI applications. For those GPUs, ensure each MPI rank targets a unique GPU. If CUDA_VISIBLE_DEVICES is set, it may cause problems with the GPU selection logic in the MPI application. It may also prevent CUDA IPC working between GPUs on a node. 12. Tips and Tricks  This section serves as reference to advanced settings and various tips and tricks users of CUDA-GDB can utilize which are not documented elsewhere. 12.1. set cuda break_on_launch  To break on the first instruction of every launched kernel, set the break_on_launch option to application: (cuda-gdb) set cuda break_on_launch application Possible options are: none no kernel, application or system (default) application kernel launched by the user application system any kernel launched by the driver, such as memset all any kernel, application and system Those automatic breakpoints are not displayed by the info breakpoints command and are managed separately from individual breakpoints. Turning off the option will not delete other individual breakpoints set to the same address and vice-versa. 12.2. set cuda launch_blocking  When enabled, the kernel launches are synchronous as if the environment variable CUDA_LAUNCH_BLOCKING had been set to 1. Once blocking, the launches are effectively serialized and may be easier to debug. (cuda-gdb) set cuda launch_blocking off The kernel launches are launched synchronously or asynchronously as dictacted by the application. This is the default. (cuda-gdb) set cuda launch_blocking on The kernel launches are synchronous. If the application has already started, the change will only take affect after the current session has terminated. 12.3. set cuda notify  Any time a CUDA event occurs, the debugger needs to be notified. The notification takes place in the form of a signal being sent to a host thread. The host thread to receive that special signal is determined with the set cuda notify option. (cuda-gdb) set cuda notify youngest The host thread with the smallest thread id will receive the notification signal (default). (cuda-gdb) set cuda notify random An arbitrary host thread will receive the notification signal. 12.4. set cuda ptx_cache  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC. On CUDA devices, the variables may not be live all the time and will be reported as “Optimized Out”. CUDA-GDB offers an option to circumvent this limitation by caching the value of the variable at the PTX register level. Each source variable is compiled into a PTX register, which is later mapped to one or more hardware registers. Using the debug information emitted by the compiler, the debugger may be able cache the value of a PTX register based on the latest hardware register it was mapped to at an earlier time. This optimization is always correct. When enabled, the cached value will be displayed as the normal value read from an actual hardware register and indicated with the (cached) prefix. The optimization will only kick in while single-stepping the code. (cuda-gdb) set cuda ptx_cache off The debugger only read the value of live variables. (cuda-gdb) set cuda ptx_cache on The debugger will use the cached value when possible. This setting is the default and is always safe. 12.5. set cuda single_stepping_optimizations  Single-stepping can take a lot of time. When enabled, this option tells the debugger to use safe tricks to accelerate single-stepping. (cuda-gdb) set cuda single_stepping_optimizations off The debugger will not try to accelerate single-stepping. This is the unique and default behavior in the 5.5 release and earlier. (cuda-gdb) set cuda single_stepping_optimizations on The debugger will use safe techniques to accelerate single-stepping. This is the default starting with the 6.0 release. 12.6. set cuda thread_selection  When the debugger must choose an active thread to focus on, the decision is guided by a heuristics. The set cuda thread_selection guides those heuristics. (cuda-gdb) set cuda thread_selection logical The thread with the lowest blockIdx/threadIdx coordinates is selected. (cuda-gdb) set cuda thread_selection physical The thread with the lowest dev/sm/warp/lane coordinates is selected. 12.7. set cuda value_extrapolation  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC. On CUDA devices, the variables may not be live all the time and will be reported as “Optimized Out”. CUDA-GDB offers an option to opportunistically circumvent this limitation by extrapolating the value of a variable when the debugger would otherwise mark it as optimized out. The extrapolation is not guaranteed to be accurate and must be used carefully. If the register that was used to store the value of a variable has been reused since the last time the variable was seen as live, then the reported value will be wrong. Therefore, any value printed using the option will be marked as \"(possibly)\" . (cuda-gdb) set cuda value_extrapolation off The debugger only read the value of live variables. This setting is the default and is always safe. (cuda-gdb) set cuda value_extrapolation on The debugger will attempt to extrapolate the value of variables beyound their respecitve live ranges. This setting may report erroneous values. 12.8. Debugging Docker Containers  When debugging an application within a Docker container, the PTRACE capability needs to be enabled. The user needs to also ensure that the root file system has both read/write permissions set. To enable the PTRACE capability, add the following to your Docker run command: --cap-add=SYS_PTRACE 12.9. Switching to Classic Debugger Backend  A new debugger backend named the Unified Debugger (UD) has been introduced on Linux platforms with the CTK 11.8 release. UD allows for a unified debugger backend shared with debugging tools such as cuda-gdb and NVIDIA® Nsight™ VSE. UD is supported across multiple platforms including both Windows and Linux. The end user experience with UD is transparent to existing tool use. The previous debugger backend, known as the classic debugger backend, can still be used by setting CUDBG_USE_LEGACY_DEBUGGER to 1 in the environment before starting CUDA-GDB. UD is not supported on Maxwell GPUs. Users must switch to the classic debugger backend to debug their applications on Maxwell GPUs. 12.10. Thread Block Clusters  CUDA applications that make use of Thread Block Clusters will see the cluster index displayed in the CUDA focus. Both cluster index and cluster dimension can be queried by printing the convenience variables clusterIdx and clusterDim . 12.11. Debugging OptiX/RTCore applications  When debugging programs built with OptiX/RTCore, it may be necessary to set the environment variable OPTIX_FORCE_DEPRECATED_LAUNCHER to 1. If breakpoints are unable to be hit, try setting this environment variable before starting your application. 12.12. Debugging on Windows Subsystem for Linux  If you are unable to use the debugger on Windows Subsystem for Linux, make sure the debug interface is enabled by setting the registry key >HKEY_LOCAL_MACHINE\\SOFTWARE\\NVIDIA Corporation\\GPUDebugger\\EnableInterface to (DWORD) 1 13. Supported Platforms  Host Platform Requirements CUDA-GDB is supported on all the platforms supported by the CUDA toolkit with which it is shipped. See the CUDA Toolkit release notes for more information. GPU Requirements Debugging is supported on all CUDA-capable GPUs supported by the current CUDA release. GDB Python integration GDB Python integration is supported in cuda-gdb with a multiple builds mechanism in order to support multiple python3 interpreters across different platforms. The cuda-gdb program is a shell script that selects the associated supported cuda-gdb binary based on the version of python available on the system. Support exists for the following Python versions: Python 3.8 , Python 3.9 , Python 3.10 , Python 3.11 , and Python 3.12 Windows Subsystem for Linux (WSL) cuda-gdb supports debugging CUDA application on WSL2. Make sure this capability is enabled via the registry key >HKEY_LOCAL_MACHINE\\SOFTWARE\\NVIDIA Corporation\\GPUDebugger\\EnableInterface set to (DWORD) 1 . Debugging compute-intensive apps may require to increase or disable TDR . 14. Common Issues on Supported Operating Systems  The following are known issues with the current release on supported operating systems and how to fix them. Python not initialized This happens due to a missing Python 3.x library on the machine, installing it fixes the issue. This can also be caused by having a mismatched major.minor version of libpython installed with the default python3 interpreter in PATH. A libpython version matching the default python3 interpreter in PATH must be available. The libpython version can be determined with the python3 --version command. For example, the following command would tell us that a libpython3.8.so* needs to be installed in a default library search path: $ python3 --version\nPython 3.8.10 Specific commands to install the proper libpython are below. RHEL 8/9 $ sudo yum -y install python3-libs Debian 10/11/12 $ sudo apt-get -y install libpython3-stdlib Fedora 39 $ sudo yum -y install python3-libs OpenSUSE 15 $ sudo zypper install -y libpython3 Ubuntu 20.04/22.04 $ sudo apt-get -y install python3.8 $ sudo apt-get -y install libpython3.8 15. Known Issues  The following are known issues with the current release. Debugging on Hopper architecture with MCDM enabled is not supported for WSL. On Windows Subsystem for Linux (WSL), the r555 WDDM driver released with the CUDA Toolkit 12.5 has a known issue when CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 that will prevent coredump file generation and may cause a segmentation fault. This is not an issue with earlier drivers (ie. r551) or other driver modes (TCC, MCDM). This WDDM issue is expected to be resolved in an upcoming driver release. Setting a breakpoint on a line within a __device__ or __global__ function before its module is loaded may result in the breakpoint being temporarily set on the first line of a function below in the source code. As soon as the module for the targeted function is loaded, the breakpoint will be reset properly. In the meantime, the breakpoint may be hit, depending on the application. In those situations, the breakpoint can be safely ignored, and the application can be resumed. The scheduler-locking option cannot be set to on . Stepping again after stepping out of a kernel results in undetermined behavior. It is recommended to use the ‘continue’ command instead. When remotely debugging 32-bit applications on a 64-bit server, the cuda-gdbserver binary used must be 32-bit. Attaching to a CUDA application with Software Preemption enabled in cuda-gdb is not supported. Attaching to a CUDA application on QNX is not supported. Attaching to a CUDA application running in MPS client mode is not supported. Attaching to the MPS server process (nvidia-cuda-mps-server) using cuda-gdb, or starting the MPS server with cuda-gdb is not supported. If a CUDA application is started in the MPS client mode with cuda-gdb, the MPS client will wait until all other MPS clients have terminated, and will then run as non-MPS application. Significant performance degradation will occur when the debugger steps over inlined routines. Because inlined code blocks may have multiple exit points, under the hood, the debugger steps every single instruction until an exit point is reached, which incurs considerable cost for large routines. The following actions are recommended to avoid this problem: Avoid using __forceinline__ when declaring a function. (For code is compiled with debug information, only routines declared with the __forceinline__ keyword are actually inlined) Use the until <line#> command to step over inlined subroutines. On Jetson, calls to the cuda API might result in the debugger jumping to _dl_catch_exception(). A workaround is to continue. On Jetson and Drive devices GPU debugging works correctly only if the debugger is run with the root permissions. Changes to devfs node permissions are required for the debugger to work without running as root. Debugger can miss reporting an induced trap( __trap() ) in case it is the next instruction executed after the device resumes from a breakpoint. Debugger can miss reporting breakpoints or exceptions during resume in case new warps are launched on a previously empty SM. Debugger uses the libpython installed on the system. Use of Python scripting functionality will expose cuda-gdb to the same vulnerabilities as those in the system libpython version. It is recommended to always keep the system libpython library up-to-date. Debugger doesn’t support accesses to shared memory allocations that are imported from other processes using the CUDA IPC APIs. Attempts to access these shared memory allocations by the debugger will result in an error stating access to memory allocations shared via IPC is not supported. break_on_launch will not function with OptiX/RTCore programs unless OPTIX_FORCE_DEPRECATED_LAUNCHER is set to 1 . It is not possible to generate CUDA coredumps on WSL in WDDM mode. 16. Notices  16.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 16.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 16.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html", "parent_url": "https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html", "content_type": "text/html", "text": "NVIDIA CUDA Compiler Driver 1. Introduction 1.1. Overview 1.1.1. CUDA Programming Model 1.1.2. CUDA Sources 1.1.3. Purpose of NVCC 1.2. Supported Host Compilers 2. Compilation Phases 2.1. NVCC Identification Macro 2.2. NVCC Phases 2.3. Supported Input File Suffixes 2.4. Supported Phases 3. The CUDA Compilation Trajectory 4. NVCC Command Options 4.1. Command Option Types and Notation 4.2. Command Option Description 4.2.1. File and Path Specifications 4.2.1.1. --output-file file ( -o ) 4.2.1.2. --objdir-as-tempdir ( -objtemp ) 4.2.1.3. --pre-include file,... ( -include ) 4.2.1.4. --library library,... ( -l ) 4.2.1.5. --define-macro def,... ( -D ) 4.2.1.6. --undefine-macro def,... ( -U ) 4.2.1.7. --include-path path,... ( -I ) 4.2.1.8. --system-include path,... ( -isystem ) 4.2.1.9. --library-path path,... ( -L ) 4.2.1.10. --output-directory directory ( -odir ) 4.2.1.11. --dependency-output file ( -MF ) 4.2.1.12. --generate-dependency-targets ( -MP ) 4.2.1.13. --compiler-bindir directory ( -ccbin ) 4.2.1.14. --allow-unsupported-compiler ( -allow-unsupported-compiler ) 4.2.1.15. --archiver-binary executable ( -arbin ) 4.2.1.16. --cudart { none | shared | static } ( -cudart ) 4.2.1.17. --cudadevrt { none | static } ( -cudadevrt ) 4.2.1.18. --libdevice-directory directory ( -ldir ) 4.2.1.19. --target-directory string ( -target-dir ) 4.2.2. Options for Specifying the Compilation Phase 4.2.2.1. --link ( -link ) 4.2.2.2. --lib ( -lib ) 4.2.2.3. --device-link ( -dlink ) 4.2.2.4. --device-c ( -dc ) 4.2.2.5. --device-w ( -dw ) 4.2.2.6. --cuda ( -cuda ) 4.2.2.7. --compile ( -c ) 4.2.2.8. --fatbin ( -fatbin ) 4.2.2.9. --cubin ( -cubin ) 4.2.2.10. --ptx ( -ptx ) 4.2.2.11. --preprocess ( -E ) 4.2.2.12. --generate-dependencies ( -M ) 4.2.2.13. --generate-nonsystem-dependencies ( -MM ) 4.2.2.14. --generate-dependencies-with-compile ( -MD ) 4.2.2.15. --generate-nonsystem-dependencies-with-compile ( -MMD ) 4.2.2.16. --optix-ir ( -optix-ir ) 4.2.2.17. --run ( -run ) 4.2.3. Options for Specifying Behavior of Compiler/Linker 4.2.3.1. --profile ( -pg ) 4.2.3.2. --debug ( -g ) 4.2.3.3. --device-debug ( -G ) 4.2.3.4. --extensible-whole-program ( -ewp ) 4.2.3.5. --no-compress ( -no-compress ) 4.2.3.6. --generate-line-info ( -lineinfo ) 4.2.3.7. --optimization-info kind,... ( -opt-info ) 4.2.3.8. --optimize level ( -O ) 4.2.3.9. --dopt kind ( -dopt ) 4.2.3.10. --dlink-time-opt ( -dlto ) 4.2.3.11. --gen-opt-lto ( -gen-opt-lto ) 4.2.3.12. --split-compile number ( -split-compile ) 4.2.3.13. --ftemplate-backtrace-limit limit ( -ftemplate-backtrace-limit ) 4.2.3.14. --ftemplate-depth limit ( -ftemplate-depth ) 4.2.3.15. --no-exceptions ( -noeh ) 4.2.3.16. --shared ( -shared ) 4.2.3.17. --x { c | c++ | cu } ( -x ) 4.2.3.18. --std { c++03 | c++11 | c++14 | c++17 | c++20 } ( -std ) 4.2.3.19. --no-host-device-initializer-list ( -nohdinitlist ) 4.2.3.20. --expt-relaxed-constexpr ( -expt-relaxed-constexpr ) 4.2.3.21. --extended-lambda ( -extended-lambda ) 4.2.3.22. --expt-extended-lambda ( -expt-extended-lambda ) 4.2.3.23. --machine { 64 } ( -m ) 4.2.3.24. --m64 ( -m64 ) 4.2.3.25. --host-linker-script { use-lcs | gen-lcs } ( -hls ) 4.2.3.26. --augment-host-linker-script ( -aug-hls ) 4.2.3.27. --host-relocatable-link ( -r ) 4.2.4. Options for Passing Specific Phase Options 4.2.4.1. --compiler-options options,... ( -Xcompiler ) 4.2.4.2. --linker-options options,... ( -Xlinker ) 4.2.4.3. --archive-options options,... ( -Xarchive ) 4.2.4.4. --ptxas-options options,... ( -Xptxas ) 4.2.4.5. --nvlink-options options,... ( -Xnvlink ) 4.2.5. Options for Guiding the Compiler Driver 4.2.5.1. --forward-unknown-to-host-compiler ( -forward-unknown-to-host-compiler ) 4.2.5.2. --forward-unknown-to-host-linker ( -forward-unknown-to-host-linker ) 4.2.5.3. --dont-use-profile ( -noprof ) 4.2.5.4. --threads number ( -t ) 4.2.5.5. --dryrun ( -dryrun ) 4.2.5.6. --verbose ( -v ) 4.2.5.7. --keep ( -keep ) 4.2.5.8. --keep-dir directory ( -keep-dir ) 4.2.5.9. --save-temps ( -save-temps ) 4.2.5.10. --clean-targets ( -clean ) 4.2.5.11. --run-args arguments,... ( -run-args ) 4.2.5.12. --use-local-env ( -use-local-env ) 4.2.5.13. --input-drive-prefix prefix ( -idp ) 4.2.5.14. --dependency-drive-prefix prefix ( -ddp ) 4.2.5.15. --drive-prefix prefix ( -dp ) 4.2.5.16. --dependency-target-name target ( -MT ) 4.2.5.17. --no-align-double 4.2.5.18. --no-device-link ( -nodlink ) 4.2.5.19. --allow-unsupported-compiler ( -allow-unsupported-compiler ) 4.2.6. Options for Steering CUDA Compilation 4.2.6.1. --default-stream { legacy | null | per-thread } ( -default-stream ) 4.2.7. Options for Steering GPU Code Generation 4.2.7.1. --gpu-architecture ( -arch ) 4.2.7.2. --gpu-code code,... ( -code ) 4.2.7.3. --generate-code specification ( -gencode ) 4.2.7.4. --relocatable-device-code { true | false } ( -rdc ) 4.2.7.5. --entries entry,... ( -e ) 4.2.7.6. --maxrregcount amount ( -maxrregcount ) 4.2.7.7. --use_fast_math ( -use_fast_math ) 4.2.7.8. --ftz { true | false } ( -ftz ) 4.2.7.9. --prec-div { true | false } ( -prec-div ) 4.2.7.10. --prec-sqrt { true | false } ( -prec-sqrt ) 4.2.7.11. --fmad { true | false } ( -fmad ) 4.2.7.12. --extra-device-vectorization ( -extra-device-vectorization ) 4.2.7.13. --compile-as-tools-patch ( -astoolspatch ) 4.2.7.14. --keep-device-functions ( -keep-device-functions ) 4.2.7.15. --jump-table-density percentage ( -jtd ) 4.2.8. Generic Tool Options 4.2.8.1. --disable-warnings ( -w ) 4.2.8.2. --source-in-ptx ( -src-in-ptx ) 4.2.8.3. --restrict ( -restrict ) 4.2.8.4. --Wno-deprecated-gpu-targets ( -Wno-deprecated-gpu-targets ) 4.2.8.5. --Wno-deprecated-declarations ( -Wno-deprecated-declarations ) 4.2.8.6. --Wreorder ( -Wreorder ) 4.2.8.7. --Wdefault-stream-launch ( -Wdefault-stream-launch ) 4.2.8.8. --Wmissing-launch-bounds ( -Wmissing-launch-bounds ) 4.2.8.9. --Wext-lambda-captures-this ( -Wext-lambda-captures-this ) 4.2.8.10. --Werror kind,... ( -Werror ) 4.2.8.11. --display-error-number ( -err-no ) 4.2.8.12. --no-display-error-number ( -no-err-no ) 4.2.8.13. --diag-error errNum,... ( -diag-error ) 4.2.8.14. --diag-suppress errNum,... ( -diag-suppress ) 4.2.8.15. --diag-warn errNum,... ( -diag-warn ) 4.2.8.16. --resource-usage ( -res-usage ) 4.2.8.17. --help ( -h ) 4.2.8.18. --version ( -V ) 4.2.8.19. --options-file file,... ( -optf ) 4.2.8.20. --time filename ( -time ) 4.2.8.21. --qpp-config config ( -qpp-config ) 4.2.8.22. --list-gpu-code ( -code-ls ) 4.2.8.23. --list-gpu-arch ( -arch-ls ) 4.2.9. Phase Options 4.2.9.1. Ptxas Options 4.2.9.1.1. --allow-expensive-optimizations ( -allow-expensive-optimizations ) 4.2.9.1.2. --compile-only ( -c ) 4.2.9.1.3. --def-load-cache ( -dlcm ) 4.2.9.1.4. --def-store-cache ( -dscm ) 4.2.9.1.5. --device-debug ( -g ) 4.2.9.1.6. --disable-optimizer-constants ( -disable-optimizer-consts ) 4.2.9.1.7. --entry entry,... ( -e ) 4.2.9.1.8. --fmad ( -fmad ) 4.2.9.1.9. --force-load-cache ( -flcm ) 4.2.9.1.10. --force-store-cache ( -fscm ) 4.2.9.1.11. --generate-line-info ( -lineinfo ) 4.2.9.1.12. --gpu-name gpuname ( -arch ) 4.2.9.1.13. --help ( -h ) 4.2.9.1.14. --machine ( -m ) 4.2.9.1.15. --maxrregcount amount ( -maxrregcount ) 4.2.9.1.16. --opt-level N ( -O ) 4.2.9.1.17. --options-file file,... ( -optf ) 4.2.9.1.18. --position-independent-code ( -pic ) 4.2.9.1.19. --preserve-relocs ( -preserve-relocs ) 4.2.9.1.20. --sp-bound-check ( -sp-bound-check ) 4.2.9.1.21. --suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning ) 4.2.9.1.22. --verbose ( -v ) 4.2.9.1.23. --version ( -V ) 4.2.9.1.24. --warning-as-error ( -Werror ) 4.2.9.1.25. --warn-on-double-precision-use ( -warn-double-usage ) 4.2.9.1.26. --warn-on-local-memory-usage ( -warn-lmem-usage ) 4.2.9.1.27. --warn-on-spills ( -warn-spills ) 4.2.9.1.28. --compile-as-tools-patch ( -astoolspatch ) 4.2.9.1.29. --maxntid ( -maxntid ) 4.2.9.1.30. --minnctapersm ( -minnctapersm ) 4.2.9.1.31. --override-directive-values ( -override-directive-values ) 4.2.9.1.32. --make-errors-visible-at-exit ( -make-errors-visible-at-exit ) 4.2.9.2. NVLINK Options 4.2.9.2.1. --disable-warnings ( -w ) 4.2.9.2.2. --preserve-relocs ( -preserve-relocs ) 4.2.9.2.3. --verbose ( -v ) 4.2.9.2.4. --warning-as-error ( -Werror ) 4.2.9.2.5. --suppress-arch-warning ( -suppress-arch-warning ) 4.2.9.2.6. --suppress-stack-size-warning ( -suppress-stack-size-warning ) 4.2.9.2.7. --dump-callgraph ( -dump-callgraph ) 4.2.9.2.8. --dump-callgraph-no-demangle ( -dump-callgraph-no-demangle ) 4.2.9.2.9. --Xptxas ( -Xptxas ) 4.2.9.2.10. --cpu-arch ( -cpu-arch ) 4.2.9.2.11. --extra-warnings ( -extrawarn ) 4.2.9.2.12. --gen-host-linker-script ( -ghls ) 4.2.9.2.13. --ignore-host-info ( -ignore-host-info ) 4.2.9.2.14. --keep-system-libraries ( -keep-system-libraries ) 4.2.9.2.15. --kernels-used ( -kernels-used ) 4.2.9.2.16. --options-file ( -optf ) 4.2.9.2.17. --report-arch ( -report-arch ) 4.2.9.2.18. --suppress-debug-info ( -suppress-debug-info ) 4.2.9.2.19. --variables-used ( -variables used ) 4.3. NVCC Environment Variables 5. GPU Compilation 5.1. GPU Generations 5.2. GPU Feature List 5.3. Application Compatibility 5.4. Virtual Architectures 5.5. Virtual Architecture Feature List 5.6. Further Mechanisms 5.6.1. Just-in-Time Compilation 5.6.2. Fatbinaries 5.7. NVCC Examples 5.7.1. Base Notation 5.7.2. Shorthand 5.7.2.1. Shorthand 1 5.7.2.2. Shorthand 2 5.7.2.3. Shorthand 3 5.7.3. Extended Notation 5.7.4. Virtual Architecture Macros 6. Using Separate Compilation in CUDA 6.1. Code Changes for Separate Compilation 6.2. NVCC Options for Separate Compilation 6.3. Libraries 6.4. Examples 6.5. Optimization Of Separate Compilation 6.6. Potential Separate Compilation Issues 6.6.1. Object Compatibility 6.6.2. JIT Linking Support 6.6.3. Implicit CUDA Host Code 6.6.4. Using __CUDA_ARCH__ 6.6.5. Device Code in Libraries 7. Miscellaneous NVCC Usage 7.1. Cross Compilation 7.2. Keeping Intermediate Phase Files 7.3. Cleaning Up Generated Files 7.4. Printing Code Generation Statistics 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks NVIDIA CUDA Compiler Driver » 1. Introduction v12.5 | PDF | Archive NVIDIA CUDA Compiler Driver NVCC The documentation for nvcc , the CUDA compiler driver. 1. Introduction  1.1. Overview  1.1.1. CUDA Programming Model  The CUDA Toolkit targets a class of applications whose control part runs as a process on a general purpose computing device, and which use one or more NVIDIA GPUs as coprocessors for accelerating single program, multiple data (SPMD) parallel jobs. Such jobs are self-contained, in the sense that they can be executed and completed by a batch of GPU threads entirely without intervention by the host process, thereby gaining optimal benefit from the parallel graphics hardware. The GPU code is implemented as a collection of functions in a language that is essentially C++, but with some annotations for distinguishing them from the host code, plus annotations for distinguishing different types of data memory that exists on the GPU. Such functions may have parameters, and they can be called using a syntax that is very similar to regular C function calling, but slightly extended for being able to specify the matrix of GPU threads that must execute the called function. During its life time, the host process may dispatch many parallel GPU tasks. For more information on the CUDA programming model, consult the CUDA C++ Programming Guide . 1.1.2. CUDA Sources  Source files for CUDA applications consist of a mixture of conventional C++ host code, plus GPU device functions. The CUDA compilation trajectory separates the device functions from the host code, compiles the device functions using the proprietary NVIDIA compilers and assembler, compiles the host code using a C++ host compiler that is available, and afterwards embeds the compiled GPU functions as fatbinary images in the host object file. In the linking stage, specific CUDA runtime libraries are added for supporting remote SPMD procedure calling and for providing explicit GPU manipulation such as allocation of GPU memory buffers and host-GPU data transfer. 1.1.3. Purpose of NVCC  The compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file. It is the purpose of nvcc , the CUDA compiler driver, to hide the intricate details of CUDA compilation from developers. It accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. All non-CUDA compilation steps are forwarded to a C++ host compiler that is supported by nvcc , and nvcc translates its options to appropriate host compiler command line options. 1.2. Supported Host Compilers  A general purpose C++ host compiler is needed by nvcc in the following situations: During non-CUDA phases (except the run phase), because these phases will be forwarded by nvcc to this compiler. During CUDA phases, for several preprocessing stages and host code compilation (see also The CUDA Compilation Trajectory ). nvcc assumes that the host compiler is installed with the standard method designed by the compiler provider. If the host compiler installation is non-standard, the user must make sure that the environment is set appropriately and use relevant nvcc compile options. The following documents provide detailed information about supported host compilers: NVIDIA CUDA Installation Guide for Linux NVIDIA CUDA Installation Guide for Microsoft Windows On all platforms, the default host compiler executable ( gcc and g++ on Linux and cl.exe on Windows) found in the current execution search path will be used, unless specified otherwise with appropriate options (see File and Path Specifications ). Note, nvcc does not support the compilation of file paths that exceed the maximum path length limitations of the host system. To support the compilation of long file paths, please refer to the documentation for your system. 2. Compilation Phases  2.1. NVCC Identification Macro  nvcc predefines the following macros: __NVCC__ Defined when compiling C/C++/CUDA source files. __CUDACC__ Defined when compiling CUDA source files. __CUDACC_RDC__ Defined when compiling CUDA source files in relocatable device code mode (see NVCC Options for Separate Compilation ). __CUDACC_EWP__ Defined when compiling CUDA source files in extensible whole program mode (see Options for Specifying Behavior of Compiler/Linker ). __CUDACC_DEBUG__ Defined when compiling CUDA source files in the device-debug mode (see Options for Specifying Behavior of Compiler/Linker ). __CUDACC_RELAXED_CONSTEXPR__ Defined when the --expt-relaxed-constexpr flag is specified on the command line. Refer to the CUDA C++ Programming Guide for more details. __CUDACC_EXTENDED_LAMBDA__ Defined when the --expt-extended-lambda or --extended-lambda flag is specified on the command line. Refer to the CUDA C++ Programming Guide for more details. __CUDACC_VER_MAJOR__ Defined with the major version number of nvcc . __CUDACC_VER_MINOR__ Defined with the minor version number of nvcc . __CUDACC_VER_BUILD__ Defined with the build version number of nvcc . __NVCC_DIAG_PRAGMA_SUPPORT__ Defined when the CUDA frontend compiler supports diagnostic control with the nv_diag_suppress , nv_diag_error , nv_diag_warning , nv_diag_default , nv_diag_once , and nv_diagnostic pragmas. 2.2. NVCC Phases  A compilation phase is a logical translation step that can be selected by command line options to nvcc . A single compilation phase can still be broken up by nvcc into smaller steps, but these smaller steps are just implementations of the phase: they depend on seemingly arbitrary capabilities of the internal tools that nvcc uses, and all of these internals may change with a new release of the CUDA Toolkit. Hence, only compilation phases are stable across releases, and although nvcc provides options to display the compilation steps that it executes, these are for debugging purposes only and must not be copied and used in build scripts. nvcc phases are selected by a combination of command line options and input file name suffixes, and the execution of these phases may be modified by other command line options. In phase selection, the input file suffix defines the phase input, while the command line option defines the required output of the phase. The following paragraphs list the recognized file name suffixes and the supported compilation phases. A full explanation of the nvcc command line options can be found in NVCC Command Options . 2.3. Supported Input File Suffixes  The following table defines how nvcc interprets its input files: Input File Suffix Description .cu CUDA source file, containing host code and device functions .c C source file .cc , .cxx , .cpp C++ source file .ptx PTX intermediate assembly file (see Figure 1 ) .cubin CUDA device code binary file (CUBIN) for a single GPU architecture (see Figure 1 ) .fatbin CUDA fat binary file that may contain multiple PTX and CUBIN files (see Figure 1 ) .o , .obj Object file .a , .lib Library file .res Resource file .so Shared object file Note that nvcc does not make any distinction between object, library or resource files. It just passes files of these types to the linker when the linking phase is executed. 2.4. Supported Phases  The following table specifies the supported compilation phases, plus the option to nvcc that enables the execution of each phase. It also lists the default name of the output file generated by each phase, which takes effect when no explicit output file name is specified using the option --output-file : Phase nvcc Option Default Output File Name Long Name Short Name CUDA compilation to C/C++ source file --cuda -cuda .cpp.ii appended to source file name, as in x.cu.cpp.ii . This output file can be compiled by the host compiler that was used by nvcc to preprocess the .cu file. C/C++ preprocessing --preprocess -E < result on standard output > C/C++ compilation to object file --compile -c Source file name with suffix replaced by o on Linux or obj on Windows Cubin generation from CUDA source files --cubin -cubin Source file name with suffix replaced by cubin Cubin generation from PTX intermediate files. --cubin -cubin Source file name with suffix replaced by cubin PTX generation from CUDA source files --ptx -ptx Source file name with suffix replaced by ptx Fatbinary generation from source, PTX or cubin files --fatbin -fatbin Source file name with suffix replaced by fatbin Linking relocatable device code. --device-link -dlink a_dlink.obj on Windows or a_dlink.o on other platforms Cubin generation from linked relocatable device code. --device-link --cubin -dlink -cubin a_dlink.cubin Fatbinary generation from linked relocatable device code --device-link --fatbin -dlink -fatbin a_dlink.fatbin Linking an executable < no phase option > a.exe on Windows or a.out on other platforms Constructing an object file archive, or library --lib -lib a.lib on Windows or a.a on other platforms make dependency generation --generate-dependencies -M < result on standard output > make dependency generation without headers in system paths. --generate-nonsystem-dependencies -MM < result on standard output > Compile CUDA source to OptiX IR output. --optix-ir -optix-ir Source file name with suffix replaced by optixir Running an executable --run -run Notes: The last phase in this list is more of a convenience phase. It allows running the compiled and linked executable without having to explicitly set the library path to the CUDA dynamic libraries. Unless a phase option is specified, nvcc will compile and link all its input files. 3. The CUDA Compilation Trajectory  CUDA compilation works as follows: the input program is preprocessed for device compilation and is compiled to CUDA binary ( cubin ) and/or PTX intermediate code, which are placed in a fatbinary. The input program is preprocessed once again for host compilation and is synthesized to embed the fatbinary and transform CUDA specific C++ extensions into standard C++ constructs. Then the C++ host compiler compiles the synthesized host code with the embedded fatbinary into a host object. The exact steps that are followed to achieve this are displayed in Figure 1 . The embedded fatbinary is inspected by the CUDA runtime system whenever the device code is launched by the host program to obtain an appropriate fatbinary image for the current GPU. CUDA programs are compiled in the whole program compilation mode by default, i.e., the device code cannot reference an entity from a separate file. In the whole program compilation mode, device link steps have no effect. For more information on the separate compilation and the whole program compilation, see Using Separate Compilation in CUDA . CUDA Compilation Trajectory  4. NVCC Command Options  4.1. Command Option Types and Notation  Each nvcc option has a long name and a short name, which are interchangeable with each other. These two variants are distinguished by the number of hyphens that must precede the option name: long names must be preceded by two hyphens, while short names must be preceded by a single hyphen. For example, -I is the short name of --include-path . Long options are intended for use in build scripts, where the size of the option is less important than the descriptive value. In contrast, short options are intended for interactive use. nvcc recognizes three types of command options: boolean options, single value options, and list options. Boolean options do not have an argument; they are either specified on the command line or not. Single value options must be specified at most once, but list options may be repeated. Examples of each of these option types are, respectively: --verbose (switch to verbose mode), --output-file (specify output file), and --include-path (specify include path). Single value options and list options must have arguments, which must follow the name of the option itself by either one of more spaces or an equals character. When a one-character short name such as -I , -l , and -L is used, the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character. The individual values of list options may be separated by commas in a single instance of the option, or the option may be repeated, or any combination of these two cases. Hence, for the two sample options mentioned above that may take values, the following notations are legal: -o file -o=file -Idir1,dir2 -I=dir3 -I dir4,dir5 Unless otherwise specified, long option names are used throughout this document. However, short names can be used instead of long names for the same effect. 4.2. Command Option Description  This section presents tables of nvcc options. The option type in the tables can be recognized as follows: Boolean options do not have arguments specified in the first column, while the other two types do. List options can be recognized by the repeat indicator ,... at the end of the argument. Long options are described in the first column of the options table, and short options occupy the second column. 4.2.1. File and Path Specifications  4.2.1.1. --output-file file ( -o )  Specify name and location of the output file. 4.2.1.2. --objdir-as-tempdir ( -objtemp )  Create all intermediate files in the same directory as the object file. These intermediate files are deleted when the compilation is finished. This option will take effect only if -c, -dc or -dw is also used. Using this option will ensure that the intermediate file name that is embedded in the object file will not change in multiple compiles of the same file. However, this is not guaranteed if the input is stdin. If the same file is compiled with two different options, ex., ‘nvcc -c t.cu’ and ‘nvcc -c -ptx t.cu’, then the files should be compiled in different directories. Compiling them in the same directory can either cause the compilation to fail or produce incorrect results. 4.2.1.3. --pre-include file,... ( -include )  Specify header files that must be pre-included during preprocessing. 4.2.1.4. --library library,... ( -l )  Specify libraries to be used in the linking stage without the library file extension. The libraries are searched for on the library search paths that have been specified using option --library-path (see Libraries ). 4.2.1.5. --define-macro def,... ( -D )  Define macros to be used during preprocessing. def can be either name or name = definition . name -  Predefine name as a macro. name = definition -  The contents of definition are tokenized and preprocessed as if they appear during translation phase three in a #define directive. The definition will be truncated by embedded new line characters. 4.2.1.6. --undefine-macro def,... ( -U )  Undefine an existing macro during preprocessing or compilation. 4.2.1.7. --include-path path,... ( -I )  Specify include search paths. 4.2.1.8. --system-include path,... ( -isystem )  Specify system include search paths. 4.2.1.9. --library-path path,... ( -L )  Specify library search paths (see Libraries ). 4.2.1.10. --output-directory directory ( -odir )  Specify the directory of the output file. This option is intended for letting the dependency generation step (see --generate-dependencies ) generate a rule that defines the target object file in the proper directory. 4.2.1.11. --dependency-output file ( -MF )  Specify the dependency output file. This option specifies the output file for the dependency generation step (see --generate-dependencies ). The option --generate-dependencies or --generate-nonystem-dependencies must be specified if a dependency output file is set. 4.2.1.12. --generate-dependency-targets ( -MP )  Add an empty target for each dependency. This option adds phony targets to the dependency generation step (see --generate-dependencies ) intended to avoid makefile errors if old dependencies are deleted. The input files are not emitted as phony targets. 4.2.1.13. --compiler-bindir directory ( -ccbin )  Specify the directory in which the default host compiler executable resides. The host compiler executable name can be also specified to ensure that the correct host compiler is selected. In addition, driver prefix options ( --input-drive-prefix , --dependency-drive-prefix , or --drive-prefix ) may need to be specified, if nvcc is executed in a Cygwin shell or a MinGW shell on Windows. 4.2.1.14. --allow-unsupported-compiler ( -allow-unsupported-compiler )  Disable nvcc check for supported host compiler versions. Using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk. This option has no effect on MacOS. 4.2.1.15. --archiver-binary executable ( -arbin )  Specify the path of the archiver tool used create static library with --lib . 4.2.1.16. --cudart { none | shared | static } ( -cudart )  Specify the type of CUDA runtime library to be used: no CUDA runtime library, shared/dynamic CUDA runtime library, or static CUDA runtime library. Allowed Values none shared static Default The static CUDA runtime library is used by default. 4.2.1.17. --cudadevrt { none | static } ( -cudadevrt )  Specify the type of CUDA device runtime library to be used: no CUDA device runtime library, or static CUDA device runtime library. Allowed Values none static Default The static CUDA device runtime library is used by default. 4.2.1.18. --libdevice-directory directory ( -ldir )  Specify the directory that contains the libdevice library files. Libdevice library files are located in the nvvm/libdevice directory in the CUDA Toolkit. 4.2.1.19. --target-directory string ( -target-dir )  Specify the subfolder name in the targets directory where the default include and library paths are located. 4.2.2. Options for Specifying the Compilation Phase  Options of this category specify up to which stage the input files must be compiled. 4.2.2.1. --link ( -link )  Specify the default behavior: compile and link all input files. Default Output File Name a.exe on Windows or a.out on other platforms is used as the default output file name. 4.2.2.2. --lib ( -lib )  Compile all input files into object files, if necessary, and add the results to the specified library output file. Default Output File Name a.lib on Windows or a.a on other platforms is used as the default output file name. 4.2.2.3. --device-link ( -dlink )  Link object files with relocatable device code and .ptx , .cubin , and .fatbin files into an object file with executable device code, which can be passed to the host linker. Default Output File Name a_dlink.obj on Windows or a_dlink.o on other platforms is used as the default output file name. When this option is used in conjunction with --fatbin , a_dlink.fatbin is used as the default output file name. When this option is used in conjunction with --cubin , a_dlink.cubin is used as the default output file name. 4.2.2.4. --device-c ( -dc )  Compile each .c , .cc , .cpp , .cxx , and .cu input file into an object file that contains relocatable device code. It is equivalent to --relocatable-device-code=true --compile . Default Output File Name The source file name extension is replaced by .obj on Windows and .o on other platforms to create the default output file name. For example, the default output file name for x.cu is x.obj on Windows and x.o on other platforms. 4.2.2.5. --device-w ( -dw )  Compile each .c , .cc , .cpp , .cxx , and .cu input file into an object file that contains executable device code. It is equivalent to --relocatable-device-code=false --compile . Default Output File Name The source file name extension is replaced by .obj on Windows and .o on other platforms to create the default output file name. For example, the default output file name for x.cu is x.obj on Windows and x.o on other platforms. 4.2.2.6. --cuda ( -cuda )  Compile each .cu input file to a .cu.cpp.ii file. Default Output File Name .cu.cpp.ii is appended to the basename of the source file name to create the default output file name. For example, the default output file name for x.cu is x.cu.cpp.ii . 4.2.2.7. --compile ( -c )  Compile each .c , .cc , .cpp , .cxx , and .cu input file into an object file. Default Output File Name The source file name extension is replaced by .obj on Windows and .o on other platforms to create the default output file name. For example, the default output file name for x.cu is x.obj on Windows and x.o on other platforms. 4.2.2.8. --fatbin ( -fatbin )  Compile all .cu , .ptx , and .cubin input files to device-only .fatbin files. nvcc discards the host code for each .cu input file with this option. Default Output File Name The source file name extension is replaced by .fatbin to create the default output file name. For example, the default output file name for x.cu is x.fatbin . 4.2.2.9. --cubin ( -cubin )  Compile all .cu and .ptx input files to device-only .cubin files. nvcc discards the host code for each .cu input file with this option. Default Output File Name The source file name extension is replaced by .cubin to create the default output file name. For example, the default output file name for x.cu is x.cubin . 4.2.2.10. --ptx ( -ptx )  Compile all .cu input files to device-only .ptx files. nvcc discards the host code for each .cu input file with this option. Default Output File Name The source file name extension is replaced by .ptx to create the default output file name. For example, the default output file name for x.cu is x.ptx . 4.2.2.11. --preprocess ( -E )  Preprocess all .c , .cc , .cpp , .cxx , and .cu input files. Default Output File Name The output is generated in stdout by default. 4.2.2.12. --generate-dependencies ( -M )  Generate a dependency file that can be included in a Makefile for the .c , .cc , .cpp , .cxx , and .cu input file. nvcc uses a fixed prefix to identify dependencies in the preprocessed file ( ‘ #line 1 ’ on Linux and ‘ # 1 ’ on Windows). The files mentioned in source location directives starting with this prefix will be included in the dependency list. Default Output File Name The output is generated in stdout by default. 4.2.2.13. --generate-nonsystem-dependencies ( -MM )  Same as --generate-dependencies but skip header files found in system directories (Linux only). Default Output File Name The output is generated in stdout by default. 4.2.2.14. --generate-dependencies-with-compile ( -MD )  Generate a dependency file and compile the input file. The dependency file can be included in a Makefile for the .c , .cc , .cpp , .cxx , and .cu input file. This option cannot be specified together with -E . The dependency file name is computed as follows: If -MF is specified, then the specified file is used as the dependency file name. If -o is specified, the dependency file name is computed from the specified file name by replacing the suffix with ‘.d’. Otherwise, the dependency file name is computed by replacing the input file names’s suffix with ‘.d’. If the dependency file name is computed based on either -MF or -o , then multiple input files are not supported. 4.2.2.15. --generate-nonsystem-dependencies-with-compile ( -MMD )  Same as --generate-dependencies-with-compile but skip header files found in system directories (Linux only). 4.2.2.16. --optix-ir ( -optix-ir )  Compile CUDA source to OptiX IR (.optixir) output. The OptiX IR is only intended for consumption by OptiX through appropriate APIs. This feature is not supported with link-time-optimization ( -dlto ), the lto_NN -arch target, or with -gencode . Default Output File Name The source file name extension is replaced by .optixir to create the default output file name. For example, the default output file name for x.cu is x.optixir . 4.2.2.17. --run ( -run )  Compile and link all input files into an executable, and executes it. When the input is a single executable, it is executed without any compilation or linking. This step is intended for developers who do not want to be bothered with setting the necessary environment variables; these are set temporarily by nvcc . 4.2.3. Options for Specifying Behavior of Compiler/Linker  4.2.3.1. --profile ( -pg )  Instrument generated code/executable for use by gprof . 4.2.3.2. --debug ( -g )  Generate debug information for host code. 4.2.3.3. --device-debug ( -G )  Generate debug information for device code. If --dopt is not specified, then this option turns off all optimizations on device code. It is not intended for profiling; use --generate-line-info instead for profiling. 4.2.3.4. --extensible-whole-program ( -ewp )  Generate extensible whole program device code, which allows some calls to not be resolved until linking with libcudadevrt. 4.2.3.5. --no-compress ( -no-compress )  Do not compress device code in fatbinary. 4.2.3.6. --generate-line-info ( -lineinfo )  Generate line-number information for device code. 4.2.3.7. --optimization-info kind,... ( -opt-info )  Provide optimization reports for the specified kind of optimization. The following tags are supported: inline Emit remarks related to function inlining. Inlining pass may be invoked multiple times by the compiler and a function not inlined in an earlier pass may be inlined in a subsequent pass. 4.2.3.8. --optimize level ( -O )  Specify optimization level for host code. 4.2.3.9. --dopt kind ( -dopt )  Enable device code optimization. When specified along with -G , enables limited debug information generation for optimized device code (currently, only line number information). When -G is not specified, -dopt=on is implicit. Allowed Values on : enable device code optimization. 4.2.3.10. --dlink-time-opt ( -dlto )  Perform link-time optimization of device code. The option ‘-lto’ is also an alias to ‘-dlto’. Link-time optimization must be specified at both compile and link time; at compile time it stores high-level intermediate code, then at link time it links together and optimizes the intermediate code. If that intermediate is not found at link time then nothing happens. Intermediate code is also stored at compile time with the --gpu-code='lto_NN' target. The options -dlto -arch=sm_NN will add a lto_NN target; if you want to only add a lto_NN target and not the compute_NN that -arch=sm_NN usually generates, use -arch=lto_NN . 4.2.3.11. --gen-opt-lto ( -gen-opt-lto )  Run the optimizer passes before generating the LTO IR. 4.2.3.12. --split-compile number ( -split-compile )  [Experimental] Perform compiler optimizations in parallel. Split compilation attempts to reduce compile time by enabling the compiler to run certain optimization passes concurrently. It does this by splitting the device code into smaller translation units, each containing one or more device functions, and running optimization passes on each unit concurrently across multiple threads. It will then link back the split units prior to code generation.\nThe option accepts a numerical value that specifies the maximum number of threads the compiler can use. One can also allow the compiler to use the maximum threads available on the system by setting --split-compile=0 . Setting --split-compile=1 will cause this option to be ignored.\nThis option can work in conjunction with device Link Time Optimization ( -dlto ) as well as --threads . 4.2.3.13. --ftemplate-backtrace-limit limit ( -ftemplate-backtrace-limit )  Set the maximum number of template instantiation notes for a single warning or error to limit. A value of 0 is allowed, and indicates that no limit should be enforced. This value is also passed to the host compiler if it provides an equivalent flag. 4.2.3.14. --ftemplate-depth limit ( -ftemplate-depth )  Set the maximum instantiation depth for template classes to limit. This value is also passed to the host compiler if it provides an equivalent flag. 4.2.3.15. --no-exceptions ( -noeh )  Disable exception handling for host code. Disable exception handling for host code, by passing “-EHs-c-” (for cl.exe) and “–fno-exceptions” (for other host compilers) during host compiler invocation. These flags are added to the host compiler invocation before any flags passed directly to the host compiler with “-Xcompiler” Default (on Windows) On Windows, nvcc passes /EHsc to the host compiler by default. Example (on Windows) nvcc --no-exceptions -Xcompiler /EHa x.cu 4.2.3.16. --shared ( -shared )  Generate a shared library during linking. Use option --linker-options when other linker options are required for more control. 4.2.3.17. --x { c | c++ | cu } ( -x )  Explicitly specify the language for the input files, rather than letting the compiler choose a default based on the file name suffix. Allowed Values c c++ cu Default The language of the source code is determined based on the file name suffix. 4.2.3.18. --std { c++03 | c++11 | c++14 | c++17 | c++20 } ( -std )  Select a particular C++ dialect. Allowed Values c++03 c++11 c++14 c++17 c++20 Default The default C++ dialect depends on the host compiler. nvcc matches the default C++ dialect that the host compiler uses. 4.2.3.19. --no-host-device-initializer-list ( -nohdinitlist )  Do not consider member functions of std::initializer_list as __host__ __device__ functions implicitly. 4.2.3.20. --expt-relaxed-constexpr ( -expt-relaxed-constexpr )  Experimental flag : Allow host code to invoke ``__device__ constexpr`` functions, and device code to invoke ``__host__ constexpr`` functions. Note that the behavior of this flag may change in future compiler releases. 4.2.3.21. --extended-lambda ( -extended-lambda )  Allow __host__ , __device__ annotations in lambda declarations. 4.2.3.22. --expt-extended-lambda ( -expt-extended-lambda )  Alias for --extended-lambda . 4.2.3.23. --machine { 64 } ( -m )  Specify 64-bit architecture. Allowed Values 64 Default This option is set based on the host platform on which nvcc is executed. 4.2.3.24. --m64 ( -m64 )  Alias for --machine=64 4.2.3.25. --host-linker-script { use-lcs | gen-lcs } ( -hls )  Use the host linker script (GNU/Linux only) to enable support for certain CUDA specific requirements, while building executable files or shared libraries. Allowed Values use-lcs Prepares a host linker script and enables host linker to support relocatable device object files that are larger in size, that would otherwise, in certain cases, cause the host linker to fail with relocation truncation error. gen-lcs Generates a host linker script that can be passed to host linker manually, in the case where host linker is invoked separately outside of nvcc. This option can be combined with -shared or -r option to generate linker scripts that can be used while generating host shared libraries or host relocatable links respectively. The file generated using this options must be provided as the last input file to the host linker. The output is generated to stdout by default. Use the option -o filename to specify the output filename. A linker script may already be in used and passed to the host linker using the host linker option --script (or -T ), then the generated host linker script must augment the existing linker script. In such cases, the option -aug-hls must be used to generate linker script that contains only the augmentation parts. Otherwise, the host linker behaviour is undefined. A host linker option, such as -z with a non-default argument, that can modify the default linker script internally, is incompatible with this option and the behavior of any such usage is undefined. Default Value use-lcs is used as the default type. 4.2.3.26. --augment-host-linker-script ( -aug-hls )  Enables generation of host linker script that augments an existing host linker script (GNU/Linux only). See option --host-linker-script for more details. 4.2.3.27. --host-relocatable-link ( -r )  When used in combination with -hls=gen-lcs , controls the behaviour of -hls=gen-lcs and sets it to generate host linker script that can be used in host relocatable link ( ld -r linkage). See option -hls=gen-lcs for more information. This option currently is effective only when used with -hls=gen-lcs ; in all other cases, this option is ignored currently. 4.2.4. Options for Passing Specific Phase Options  These flags allow for passing specific options directly to the internal compilation tools that nvcc encapsulates, without burdening nvcc with too-detailed knowledge on these tools. 4.2.4.1. --compiler-options options,... ( -Xcompiler )  Specify options directly to the compiler/preprocessor. 4.2.4.2. --linker-options options,... ( -Xlinker )  Specify options directly to the host linker. 4.2.4.3. --archive-options options,... ( -Xarchive )  Specify options directly to the library manager. 4.2.4.4. --ptxas-options options,... ( -Xptxas )  Specify options directly to ptxas , the PTX optimizing assembler. 4.2.4.5. --nvlink-options options,... ( -Xnvlink )  Specify options directly to nvlink , the device linker. 4.2.5. Options for Guiding the Compiler Driver  4.2.5.1. --forward-unknown-to-host-compiler ( -forward-unknown-to-host-compiler )  Forward unknown options to the host compiler. An ‘unknown option’ is a command line argument that starts with - followed by another character, and is not a recognized nvcc flag or an argument for a recognized nvcc flag. If the unknown option is followed by a separate command line argument, the argument will not be forwarded, unless it begins with the - character. For example: nvcc -forward-unknown-to-host-compiler -foo=bar a.cu will forward -foo=bar to host compiler. nvcc -forward-unknown-to-host-compiler -foo bar a.cu will report an error for bar argument. nvcc -forward-unknown-to-host-compiler -foo -bar a.cu will forward -foo and -bar to host compiler. 4.2.5.2. --forward-unknown-to-host-linker ( -forward-unknown-to-host-linker )  Forward unknown options to the host linker. An ‘unknown option’ is a command line argument that starts with - followed by another character, and is not a recognized nvcc flag or an argument for a recognized nvcc flag. If the unknown option is followed by a separate command line argument, the argument will not be forwarded, unless it begins with the - character. For example: nvcc -forward-unknown-to-host-linker -foo=bar a.cu will forward -foo=bar to host linker. nvcc -forward-unknown-to-host-linker -foo bar a.cu will report an error for bar argument. nvcc -forward-unknown-to-host-linker -foo -bar a.cu will forward -foo and -bar to host linker. 4.2.5.3. --dont-use-profile ( -noprof )  Do not use configurations from the nvcc.profile file for compilation. 4.2.5.4. --threads number ( -t )  Specify the maximum number of threads to be used to execute the compilation steps in parallel. This option can be used to improve the compilation speed when compiling for multiple architectures. The compiler creates number threads to execute the compilation steps in parallel. If number is 1, this option is ignored. If number is 0, the number of threads used is the number of CPUs on the machine. 4.2.5.5. --dryrun ( -dryrun )  List the compilation sub-commands without executing them. 4.2.5.6. --verbose ( -v )  List the compilation sub-commands while executing them. 4.2.5.7. --keep ( -keep )  Keep all intermediate files that are generated during internal compilation steps. 4.2.5.8. --keep-dir directory ( -keep-dir )  Keep all intermediate files that are generated during internal compilation steps in this directory. 4.2.5.9. --save-temps ( -save-temps )  This option is an alias of --keep . 4.2.5.10. --clean-targets ( -clean )  Delete all the non-temporary files that the same nvcc command would generate without this option. This option reverses the behavior of nvcc . When specified, none of the compilation phases will be executed. Instead, all of the non-temporary files that nvcc would otherwise create will be deleted. 4.2.5.11. --run-args arguments,... ( -run-args )  Specify command line arguments for the executable when used in conjunction with --run . 4.2.5.12. --use-local-env ( -use-local-env )  Skip MSVC environment initialization. By default nvcc assumes that the MSVC environment needs to be initialized. This is done by executing the appropriate command file available for the MSVC installation detected or specified. Initializing the environment for each nvcc invocation can add noticeable overheads. If the environment used to invoke nvcc has already been configured, this option can be used to skip this step. 4.2.5.13. --input-drive-prefix prefix ( -idp )  Specify the input drive prefix. On Windows, all command line arguments that refer to file names must be converted to the Windows native format before they are passed to pure Windows executables. This option specifies how the current development environment represents absolute paths. Use /cygwin/ as prefix for Cygwin build environments and / as prefix for MinGW. 4.2.5.14. --dependency-drive-prefix prefix ( -ddp )  Specify the dependency drive prefix. On Windows, when generating dependency files (see --generate-dependencies ), all file names must be converted appropriately for the instance of make that is used. Some instances of make have trouble with the colon in absolute paths in the native Windows format, which depends on the environment in which the make instance has been compiled. Use /cygwin/ as prefix for a Cygwin make , and / as prefix for MinGW. Or leave these file names in the native Windows format by specifying nothing. 4.2.5.15. --drive-prefix prefix ( -dp )  Specify the drive prefix. This option specifies prefix as both --input-drive-prefix and --dependency-drive-prefix . 4.2.5.16. --dependency-target-name target ( -MT )  Specify the target name of the generated rule when generating a dependency file (see --generate-dependencies ). 4.2.5.17. --no-align-double  Specify that -malign-double should not be passed as a compiler argument on 32-bit platforms. WARNING: this makes the ABI incompatible with the CUDA’s kernel ABI for certain 64-bit types. 4.2.5.18. --no-device-link ( -nodlink )  Skip the device link step when linking object files. 4.2.5.19. --allow-unsupported-compiler ( -allow-unsupported-compiler )  Disable nvcc check for supported host compiler versions. Using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk. This option has no effect on MacOS. 4.2.6. Options for Steering CUDA Compilation  4.2.6.1. --default-stream { legacy | null | per-thread } ( -default-stream )  Specify the stream that CUDA commands from the compiled program will be sent to by default. Allowed Values legacy The CUDA legacy stream (per context, implicitly synchronizes with other streams) per-thread Normal CUDA stream (per thread, does not implicitly synchronize with other streams) null Deprecated alias for legacy Default legacy is used as the default stream. 4.2.7. Options for Steering GPU Code Generation  4.2.7.1. --gpu-architecture ( -arch )  Specify the name of the class of NVIDIA virtual GPU architecture for which the CUDA input files must be compiled. With the exception as described for the shorthand below, the architecture specified with this option must be a virtual architecture (such as compute_50). Normally, this option alone does not trigger assembly of the generated PTX for a real architecture (that is the role of nvcc option --gpu-code , see below); rather, its purpose is to control preprocessing and compilation of the input to PTX. For convenience, in case of simple nvcc compilations, the following shorthand is supported. If no value for option --gpu-code is specified, then the value of this option defaults to the value of --gpu-architecture . In this situation, as the only exception to the description above, the value specified for --gpu-architecture may be a real architecture (such as a sm_50), in which case nvcc uses the specified real architecture and its closest virtual architecture as the effective architecture values. For example, nvcc --gpu-architecture=sm_50 is equivalent to nvcc --gpu-architecture=compute_50 --gpu-code=sm_50,compute_50 . When -arch=native is specified, nvcc detects the visible GPUs on the system and generates codes for them, no PTX program will be generated for this option. It is a warning if there are no visible supported GPU on the system, and the default architecture will be used. If -arch=all is specified, nvcc embeds a compiled code image for all supported architectures (sm_*) , and a PTX program for the highest major virtual architecture. For -arch=all-major , nvcc embeds a compiled code image for all supported major versions (sm_*0) , plus the earliest supported, and adds a PTX program for the highest major virtual architecture. See Virtual Architecture Feature List for the list of supported virtual architectures and GPU Feature List for the list of supported real architectures. Default sm_52 is used as the default value; PTX is generated for compute_52 then assembled and optimized for sm_52 . 4.2.7.2. --gpu-code code,... ( -code )  Specify the name of the NVIDIA GPU to assemble and optimize PTX for. nvcc embeds a compiled code image in the resulting executable for each specified code architecture, which is a true binary load image for each real architecture (such as sm_50), and PTX code for the virtual architecture (such as compute_50). During runtime, such embedded PTX code is dynamically compiled by the CUDA runtime system if no binary load image is found for the current GPU. Architectures specified for options --gpu-architecture and --gpu-code may be virtual as well as real , but the code architectures must be compatible with the arch architecture. When the --gpu-code option is used, the value for the --gpu-architecture option must be a virtual PTX architecture. For instance, --gpu-architecture=compute_60 is not compatible with --gpu-code=sm_52 , because the earlier compilation stages will assume the availability of compute_60 features that are not present on sm_52 . See Virtual Architecture Feature List for the list of supported virtual architectures and GPU Feature List for the list of supported real architectures. 4.2.7.3. --generate-code specification ( -gencode )  This option provides a generalization of the --gpu-architecture=arch --gpu-code=code,... option combination for specifying nvcc behavior with respect to code generation. Where use of the previous options generates code for different real architectures with the PTX for the same virtual architecture, option --generate-code allows multiple PTX generations for different virtual architectures. In fact, --gpu-architecture=arch --gpu-code=code,... is equivalent to --generate-code=arch=arch,code=code,... . --generate-code options may be repeated for different virtual architectures. See Virtual Architecture Feature List for the list of supported virtual architectures and GPU Feature List for the list of supported real architectures. 4.2.7.4. --relocatable-device-code { true | false } ( -rdc )  Enable or disable the generation of relocatable device code. If disabled, executable device code is generated. Relocatable device code must be linked before it can be executed. Allowed Values true false Default The generation of relocatable device code is disabled. 4.2.7.5. --entries entry,... ( -e )  Specify the global entry functions for which code must be generated. PTX generated for all entry functions, but only the selected entry functions are assembled. Entry function names for this option must be specified in the mangled name. Default nvcc generates code for all entry functions. 4.2.7.6. --maxrregcount amount ( -maxrregcount )  Specify the maximum amount of registers that GPU functions can use. Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function. However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism. Hence, a good maxrregcount value is the result of a trade-off. A value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit. User program may not be able to make use of all registers as some registers are reserved by compiler. Default No maximum is assumed. 4.2.7.7. --use_fast_math ( -use_fast_math )  Make use of fast math library. --use_fast_math implies --ftz=true --prec-div=false --prec-sqrt=false --fmad=true . 4.2.7.8. --ftz { true | false } ( -ftz )  Control single-precision denormals support. --ftz=true flushes denormal values to zero and --ftz=false preserves denormal values. --use_fast_math implies --ftz=true . Allowed Values true false Default This option is set to false and nvcc preserves denormal values. 4.2.7.9. --prec-div { true | false } ( -prec-div )  This option controls single-precision floating-point division and reciprocals. --prec-div=true enables the IEEE round-to-nearest mode and --prec-div=false enables the fast approximation mode. --use_fast_math implies --prec-div=false . Allowed Values true false Default This option is set to true and nvcc enables the IEEE round-to-nearest mode. 4.2.7.10. --prec-sqrt { true | false } ( -prec-sqrt )  This option controls single-precision floating-point square root. --prec-sqrt=true enables the IEEE round-to-nearest mode and --prec-sqrt=false enables the fast approximation mode. --use_fast_math implies --prec-sqrt=false . Allowed Values true false Default This option is set to true and nvcc enables the IEEE round-to-nearest mode. 4.2.7.11. --fmad { true | false } ( -fmad )  This option enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA). --use_fast_math implies --fmad=true . Allowed Values true false Default This option is set to true and nvcc enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA). 4.2.7.12. --extra-device-vectorization ( -extra-device-vectorization )  This option enables more aggressive device code vectorization. 4.2.7.13. --compile-as-tools-patch ( -astoolspatch )  Compile patch code for CUDA tools. Implies –keep-device-functions . May only be used in conjunction with --ptx or --cubin or --fatbin . Shall not be used in conjunction with -rdc=true or -ewp . Some PTX ISA features may not be usable in this compilation mode. 4.2.7.14. --keep-device-functions ( -keep-device-functions )  In whole program compilation mode, preserve user defined external linkage __device__ function definitions in generated PTX. 4.2.7.15. --jump-table-density percentage ( -jtd )  Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement. The percentage ranges from 0 to 101 inclusively. Default This option is set to 101 and nvcc disables jump table generation for switch statements. 4.2.8. Generic Tool Options  4.2.8.1. --disable-warnings ( -w )  Inhibit all warning messages. 4.2.8.2. --source-in-ptx ( -src-in-ptx )  Interleave source in PTX. May only be used in conjunction with --device-debug or --generate-line-info . 4.2.8.3. --restrict ( -restrict )  Assert that all kernel pointer parameters are restrict pointers. 4.2.8.4. --Wno-deprecated-gpu-targets ( -Wno-deprecated-gpu-targets )  Suppress warnings about deprecated GPU target architectures. 4.2.8.5. --Wno-deprecated-declarations ( -Wno-deprecated-declarations )  Suppress warning on use of a deprecated entity. 4.2.8.6. --Wreorder ( -Wreorder )  Generate warnings when member initializers are reordered. 4.2.8.7. --Wdefault-stream-launch ( -Wdefault-stream-launch )  Generate warning when an explicit stream argument is not provided in the <<<...>>> kernel launch syntax. 4.2.8.8. --Wmissing-launch-bounds ( -Wmissing-launch-bounds )  Generate warning when a __global__ function does not have an explicit __launch_bounds__ annotation. 4.2.8.9. --Wext-lambda-captures-this ( -Wext-lambda-captures-this )  Generate warning when an extended lambda implicitly captures this . 4.2.8.10. --Werror kind,... ( -Werror )  Make warnings of the specified kinds into errors. The following is the list of warning kinds accepted by this option: all-warnings Treat all warnings as errors. cross-execution-space-call Be more strict about unsupported cross execution space calls. The compiler will generate an error instead of a warning for a call from a __host__ __device__ to a __host__ function. reorder Generate errors when member initializers are reordered. default-stream-launch Generate error when an explicit stream argument is not provided in the <<<...>>> kernel launch syntax. missing-launch-bounds Generate warning when a __global__ function does not have an explicit __launch_bounds__ annotation. ext-lambda-captures-this Generate error when an extended lambda implicitly captures this . deprecated-declarations Generate error on use of a deprecated entity. 4.2.8.11. --display-error-number ( -err-no )  This option displays a diagnostic number for any message generated by the CUDA frontend compiler (note: not the host compiler). 4.2.8.12. --no-display-error-number ( -no-err-no )  This option disables the display of a diagnostic number for any message generated by the CUDA frontend compiler (note: not the host compiler). 4.2.8.13. --diag-error errNum,... ( -diag-error )  Emit error for specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor). 4.2.8.14. --diag-suppress errNum,... ( -diag-suppress )  Suppress specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor). 4.2.8.15. --diag-warn errNum,... ( -diag-warn )  Emit warning for specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor). 4.2.8.16. --resource-usage ( -res-usage )  Show resource usage such as registers and memory of the GPU code. This option implies --nvlink-options=--verbose when --relocatable-device-code=true is set. Otherwise, it implies --ptxas-options=--verbose . 4.2.8.17. --help ( -h )  Print help information on this tool. 4.2.8.18. --version ( -V )  Print version information on this tool. 4.2.8.19. --options-file file,... ( -optf )  Include command line options from specified file. 4.2.8.20. --time filename ( -time )  Generate a comma separated value table with the time taken by each compilation phase, and append it at the end of the file given as the option argument. If the file is empty, the column headings are generated in the first row of the table. If the file name is - , the timing data is generated in stdout. 4.2.8.21. --qpp-config config ( -qpp-config )  Specify the configuration ([[compiler/]version,][target]) when using q++ host compiler. The argument will be forwarded to the q++ compiler with its -V flag. 4.2.8.22. --list-gpu-code ( -code-ls )  List the non-accelerated gpu architectures (sm_XX) supported by the tool and exit. If both –list-gpu-code and –list-gpu-arch are set, the list is displayed using the same format as the –generate-code value. 4.2.8.23. --list-gpu-arch ( -arch-ls )  List the non-accelerated virtual device architectures (compute_XX) supported by the tool and exit. If both –list-gpu-arch and –list-gpu-code are set, the list is displayed using the same format as the –generate-code value. 4.2.9. Phase Options  The following sections lists some useful options to lower level compilation tools. 4.2.9.1. Ptxas Options  The following table lists some useful ptxas options which can be specified with nvcc option -Xptxas . 4.2.9.1.1. --allow-expensive-optimizations ( -allow-expensive-optimizations )  Enable (disable) to allow compiler to perform expensive optimizations using maximum available resources (memory and compile-time). If unspecified, default behavior is to enable this feature for optimization level >= O2 . 4.2.9.1.2. --compile-only ( -c )  Generate relocatable object. 4.2.9.1.3. --def-load-cache ( -dlcm )  Default cache modifier on global/generic load. 4.2.9.1.4. --def-store-cache ( -dscm )  Default cache modifier on global/generic store. 4.2.9.1.5. --device-debug ( -g )  Semantics same as nvcc option --device-debug . 4.2.9.1.6. --disable-optimizer-constants ( -disable-optimizer-consts )  Disable use of optimizer constant bank. 4.2.9.1.7. --entry entry,... ( -e )  Semantics same as nvcc option --entries . 4.2.9.1.8. --fmad ( -fmad )  Semantics same as nvcc option --fmad . 4.2.9.1.9. --force-load-cache ( -flcm )  Force specified cache modifier on global/generic load. 4.2.9.1.10. --force-store-cache ( -fscm )  Force specified cache modifier on global/generic store. 4.2.9.1.11. --generate-line-info ( -lineinfo )  Semantics same as nvcc option --generate-line-info . 4.2.9.1.12. --gpu-name gpuname ( -arch )  Specify name of NVIDIA GPU to generate code for. This option also takes virtual compute architectures, in which case code generation is suppressed. This can be used for parsing only. Allowed values for this option: compute_50 , compute_52 , compute_53 , compute_60 , compute_61 , compute_62 , compute_70 , compute_72 , compute_75 , compute_80 , compute_86 , compute_87 , compute_89 , compute_90 , lto_50 , lto_52 , lto_53 , lto_60 , lto_61 , lto_62 , lto_70 , lto_72 , lto_75 , lto_80 , lto_86 , lto_87 , lto_89 , lto_90 , sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 Default value: sm_52 . 4.2.9.1.13. --help ( -h )  Semantics same as nvcc option --help . 4.2.9.1.14. --machine ( -m )  Semantics same as nvcc option --machine . 4.2.9.1.15. --maxrregcount amount ( -maxrregcount )  Semantics same as nvcc option --maxrregcount . 4.2.9.1.16. --opt-level N ( -O )  Specify optimization level. Default value: 3 . 4.2.9.1.17. --options-file file,... ( -optf )  Semantics same as nvcc option --options-file . 4.2.9.1.18. --position-independent-code ( -pic )  Generate position-independent code. Default value: For whole-program compilation: true Otherwise: false 4.2.9.1.19. --preserve-relocs ( -preserve-relocs )  This option will make ptxas to generate relocatable references for variables and preserve relocations generated for them in linked executable. 4.2.9.1.20. --sp-bound-check ( -sp-bound-check )  Generate stack-pointer bounds-checking code sequence. This option is turned on automatically when --device-debug or --opt-level=0 is specified. 4.2.9.1.21. --suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning )  Suppress the warning on use of .multicast::cluster modifier on cp.async.bulk{.tensor} instruction with sm_90 . 4.2.9.1.22. --verbose ( -v )  Enable verbose mode which prints code generation statistics. 4.2.9.1.23. --version ( -V )  Semantics same as nvcc option --version . 4.2.9.1.24. --warning-as-error ( -Werror )  Make all warnings into errors. 4.2.9.1.25. --warn-on-double-precision-use ( -warn-double-usage )  Warning if double(s) are used in an instruction. 4.2.9.1.26. --warn-on-local-memory-usage ( -warn-lmem-usage )  Warning if local memory is used. 4.2.9.1.27. --warn-on-spills ( -warn-spills )  Warning if registers are spilled to local memory. 4.2.9.1.28. --compile-as-tools-patch ( -astoolspatch )  Compile patch code for CUDA tools. Shall not be used in conjunction with -Xptxas -c or -ewp . Some PTX ISA features may not be usable in this compilation mode. 4.2.9.1.29. --maxntid ( -maxntid )  Specify the maximum number of threads that a thread block can have. This option will be ignored if used along with -maxrregcount option. This option is also ignored\nfor entry functions that have .maxntid directive specified. 4.2.9.1.30. --minnctapersm ( -minnctapersm )  Specify the minimum number of CTAs to be mapped to an SM. This option will be ignored if used along with -maxrregcount option. This option is also ignored\nfor entry functions that have .minnctapersm directive specified. 4.2.9.1.31. --override-directive-values ( -override-directive-values )  Override the PTX directives values by the corresponding option values. This option is effective only for -minnctapersm , -maxntid and -maxrregcount options. 4.2.9.1.32. --make-errors-visible-at-exit ( -make-errors-visible-at-exit )  Generate required instructions at exit point to make memory faults and errors visible at exit. 4.2.9.2. NVLINK Options  The following is a list of some useful nvlink options which can be specified with nvcc option --nvlink-options . 4.2.9.2.1. --disable-warnings ( -w )  Inhibit all warning messages. 4.2.9.2.2. --preserve-relocs ( -preserve-relocs )  Preserve resolved relocations in linked executable. 4.2.9.2.3. --verbose ( -v )  Enable verbose mode which prints code generation statistics. 4.2.9.2.4. --warning-as-error ( -Werror )  Make all warnings into errors. 4.2.9.2.5. --suppress-arch-warning ( -suppress-arch-warning )  Suppress the warning that otherwise is printed when object does not contain code for target arch. 4.2.9.2.6. --suppress-stack-size-warning ( -suppress-stack-size-warning )  Suppress the warning that otherwise is printed when stack size cannot be determined. 4.2.9.2.7. --dump-callgraph ( -dump-callgraph )  Dump information about the callgraph and register usage. 4.2.9.2.8. --dump-callgraph-no-demangle ( -dump-callgraph-no-demangle )  Dump callgraph information without demangling. 4.2.9.2.9. --Xptxas ( -Xptxas )  Ptxas options (only used with LTO). 4.2.9.2.10. --cpu-arch ( -cpu-arch )  Specify the name of the cpu target architecture. 4.2.9.2.11. --extra-warnings ( -extrawarn )  Emit extra warnings about possible problems. 4.2.9.2.12. --gen-host-linker-script ( -ghls )  Specify the type of host linker script to be generated. 4.2.9.2.13. --ignore-host-info ( -ignore-host-info )  Ignore information about host references, so don’t remove device code that could potentially be referenced by host. 4.2.9.2.14. --keep-system-libraries ( -keep-system-libraries )  Don’t optimize away system library (e.g. cudadevrt) code. 4.2.9.2.15. --kernels-used ( -kernels-used )  Specify kernels that are used. Can be part of a kernel name so any kernels with that string in name are matched. If this option is used, then any other kernels are considered dead-code and removed. 4.2.9.2.16. --options-file ( -optf )  Include command line options from the specified file. 4.2.9.2.17. --report-arch ( -report-arch )  Report SM target arch in error messages. 4.2.9.2.18. --suppress-debug-info ( -suppress-debug-info )  Do not preserve debug symbols in output. This option is ignored if used without –debug option. 4.2.9.2.19. --variables-used ( -variables used )  Specify variables that are used. Can be part of a variable name so any variable with that string in name are matched. If this option is used, then any other variables are considered dead-code and potentially removed unless have other accesses from device code. 4.3. NVCC Environment Variables  NVCC_PREPEND_FLAGS and NVCC_APPEND_FLAGS: The nvcc command line flags can be augmented using the following environment variables, if set: NVCC_PREPEND_FLAGS Flags to be injected before the normal nvcc command line. NVCC_APPEND_FLAGS Flags to be injected after the normal nvcc command line. For example, after setting: export NVCC_PREPEND_FLAGS='-G -keep -arch=sm_60'\n\nexport NVCC_APPEND_FLAGS='-DNAME=\" foo \"' The following invocation: nvcc foo.cu -o foo Becomes equivalent to: nvcc -G -keep -arch=sm_60 foo.cu -o foo -DNAME=\" foo \" These environment variables can be useful for injecting nvcc flags globally without modifying build scripts. The additional flags coming from either NVCC_PREPEND_FLAGS or NVCC_APPEND_FLAGS will be listed in the verbose log ( --verbose ). NVCC_CCBIN: A default host compiler can be set using the environment variable NVCC_CCBIN . For example, after setting: export NVCC_CCBIN='gcc' nvcc will choose gcc as the host compiler if --compiler-bindir is not set. NVCC_CCBIN can be useful for controlling the default host compiler globally. If NVCC_CCBIN and --compiler-bindir are both set, nvcc will choose the host compiler specified by --compiler-bindir . For example: export NVCC_CCBIN='gcc'\n\nnvcc foo.cu -ccbin='clang' -o foo In this case, nvcc will choose clang as the host compiler. 5. GPU Compilation  This chapter describes the GPU compilation model that is maintained by nvcc , in cooperation with the CUDA driver. It goes through some technical sections, with concrete examples at the end. 5.1. GPU Generations  In order to allow for architectural evolution, NVIDIA GPUs are released in different generations. New generations introduce major improvements in functionality and/or chip architecture, while GPU models within the same generation show minor configuration differences that moderately affect functionality, performance, or both. Binary compatibility of GPU applications is not guaranteed across different generations. For example, a CUDA application that has been compiled for a Fermi GPU will very likely not run on a Kepler GPU (and vice versa). This is because the instruction set and instruction encodings of a generation is different from those of other generations. Binary compatibility within one GPU generation can be guaranteed under certain conditions because they share the basic instruction set. This is the case when two GPU versions do not show functional differences (for instance when one version is a scaled down version of the other), or when one version is functionally included in the other. An example of the latter is the base Maxwell version sm_50 whose functionality is a subset of all other Maxwell versions: any code compiled for sm_50 will run on all other Maxwell GPUs. 5.2. GPU Feature List  The following table lists the names of the current GPU architectures, annotated with the functional capabilities that they provide. There are other differences, such as amounts of register and processor clusters, that only affect execution performance. In the CUDA naming scheme, GPUs are named sm_xy , where x denotes the GPU generation number, and y the version in that generation. Additionally, to facilitate comparing GPU capabilities, CUDA attempts to choose its GPU names such that if x1y1 <= x2y2 then all non-ISA related capabilities of sm_x1y1 are included in those of sm_x2y2 . From this it indeed follows that sm_50 is the base Maxwell model, and it also explains why higher entries in the tables are always functional extensions to the lower entries.  Moreover, if we abstract from the instruction encoding, it implies that sm_50 ’s functionality will continue to be included in all later GPU generations. As we will see next, this property will be the foundation for application compatibility support by nvcc . sm_50 , sm_52 and sm_53 Maxwell support sm_60 , sm_61 , and sm_62 Pascal support sm_70 and sm_72 Volta support sm_75 Turing support sm_80 , sm_86 and sm_87 NVIDIA Ampere GPU architecture support sm_89 Ada support sm_90 , sm_90a Hopper support 5.3. Application Compatibility  Binary code compatibility over CPU generations, together with a published instruction set architecture is the usual mechanism for ensuring that distributed applications out there in the field will continue to run on newer versions of the CPU when these become mainstream. This situation is different for GPUs, because NVIDIA cannot guarantee binary compatibility without sacrificing regular opportunities for GPU improvements. Rather, as is already conventional in the graphics programming domain, nvcc relies on a two stage compilation model for ensuring application compatibility with future GPU generations. 5.4. Virtual Architectures  GPU compilation is performed via an intermediate representation, PTX, which can be considered as assembly for a virtual GPU architecture. Contrary to an actual graphics processor, such a virtual GPU is defined entirely by the set of capabilities, or features, that it provides to the application. In particular, a virtual GPU architecture provides a (largely) generic instruction set, and binary instruction encoding is a non-issue because PTX programs are always represented in text format. Hence, a nvcc compilation command always uses two architectures: a virtual intermediate architecture, plus a real GPU architecture to specify the intended processor to execute on. For such an nvcc command to be valid, the real architecture must be an implementation of the virtual architecture. This is further explained below. The chosen virtual architecture is more of a statement on the GPU capabilities that the application requires: using a smaller virtual architecture still allows a wider range of actual architectures for the second nvcc stage. Conversely, specifying a virtual architecture that provides features unused by the application unnecessarily restricts the set of possible GPUs that can be specified in the second nvcc stage. From this it follows that the virtual architecture should always be chosen as low as possible, thereby maximizing the actual GPUs to run on. The real architecture should be chosen as high as possible (assuming that this always generates better code), but this is only possible with knowledge of the actual GPUs on which the application is expected to run on. As we will see later in the situation of just-in-time compilation, where the driver has this exact knowledge, the runtime GPU is the one on which the program is about to be launched/executed. Two-Staged Compilation with Virtual and Real Architectures  5.5. Virtual Architecture Feature List  compute_50 , compute_52 , and compute_53 Maxwell support compute_60 , compute_61 , and compute_62 Pascal support compute_70 and compute_72 Volta support compute_75 Turing support compute_80 , compute_86 and compute_87 NVIDIA Ampere GPU architecture support compute_89 Ada support compute_90 , compute_90a Hopper support The above table lists the currently defined virtual architectures. The virtual architecture naming scheme is the same as the real architecture naming scheme shown in Section GPU Feature List . 5.6. Further Mechanisms  Clearly, compilation staging in itself does not help towards the goal of application compatibility with future GPUs. For this we need two other mechanisms: just-in-time compilation (JIT) and fatbinaries. 5.6.1. Just-in-Time Compilation  The compilation step to an actual GPU binds the code to one generation of GPUs. Within that generation, it involves a choice between GPU coverage and possible performance. For example, compiling to sm_50 allows the code to run on all Maxwell generation GPUs, but compiling to sm_53 would probably yield better code if Maxwell GM206 and later are the only targets. Just-in-Time Compilation of Device Code  By specifying a virtual code architecture instead of a real GPU, nvcc postpones the assembly of PTX code until application runtime, at which time the target GPU is exactly known. For instance, the command below allows generation of exactly matching GPU binary code, when the application is launched on an sm_50 or later architecture. nvcc x.cu --gpu-architecture=compute_50 --gpu-code=compute_50 The disadvantage of just-in-time compilation is increased application startup delay, but this can be alleviated by letting the CUDA driver use a compilation cache (refer to “Section 3.1.1.2. Just-in-Time Compilation” of CUDA C++ Programming Guide ) which is persistent over multiple runs of the applications. 5.6.2. Fatbinaries  A different solution to overcome startup delay by JIT while still allowing execution on newer GPUs is to specify multiple code instances, as in nvcc x.cu --gpu-architecture=compute_50 --gpu-code=compute_50,sm_50,sm_52 This command generates exact code for two Maxwell variants, plus PTX code for use by JIT in case a next-generation GPU is encountered. nvcc organizes its device code in fatbinaries, which are able to hold multiple translations of the same GPU source code. At runtime, the CUDA driver will select the most appropriate translation when the device function is launched. 5.7. NVCC Examples  5.7.1. Base Notation  nvcc provides the options --gpu-architecture and --gpu-code for specifying the target architectures for both translation stages. Except for allowed short hands described below, the --gpu-architecture option takes a single value, which must be the name of a virtual compute architecture, while option --gpu-code takes a list of values which must all be the names of actual GPUs. nvcc performs a stage 2 translation for each of these GPUs, and will embed the result in the result of compilation (which usually is a host object file or executable). Example nvcc x.cu --gpu-architecture=compute_50 --gpu-code=sm_50,sm_52 5.7.2. Shorthand  nvcc allows a number of shorthands for simple cases. 5.7.2.1. Shorthand 1  --gpu-code arguments can be virtual architectures. In this case the stage 2 translation will be omitted for such virtual architecture, and the stage 1 PTX result will be embedded instead. At application launch, and in case the driver does not find a better alternative, the stage 2 compilation will be invoked by the driver with the PTX as input. Example nvcc x.cu --gpu-architecture=compute_50 --gpu-code=compute_50,sm_50,sm_52 5.7.2.2. Shorthand 2  The --gpu-code option can be omitted. Only in this case, the --gpu-architecture value can be a non-virtual architecture. The --gpu-code values default to the closest virtual architecture that is implemented by the GPU specified with --gpu-architecture , plus the --gpu-architecture , value itself. The closest virtual architecture is used as the effective --gpu-architecture , value. If the --gpu-architecture value is a virtual architecture, it is also used as the effective --gpu-code value. Example nvcc x.cu --gpu-architecture=sm_52\nnvcc x.cu --gpu-architecture=compute_50 are equivalent to nvcc x.cu --gpu-architecture=compute_52 --gpu-code=sm_52,compute_52\nnvcc x.cu --gpu-architecture=compute_50 --gpu-code=compute_50 5.7.2.3. Shorthand 3  Both --gpu-architecture and --gpu-code options can be omitted. Example nvcc x.cu is equivalent to nvcc x.cu --gpu-architecture=compute_52 --gpu-code=sm_52,compute_52 5.7.3. Extended Notation  The options --gpu-architecture and --gpu-code can be used in all cases where code is to be generated for one or more GPUs using a common virtual architecture. This will cause a single invocation of nvcc stage 1 (that is, preprocessing and generation of virtual PTX assembly code), followed by a compilation stage 2 (binary code generation) repeated for each specified GPU. Using a common virtual architecture means that all assumed GPU features are fixed for the entire nvcc compilation. For instance, the following nvcc command assumes no half-precision floating-point operation support for both the sm_50 code and the sm_53 code: nvcc x.cu --gpu-architecture=compute_50 --gpu-code=compute_50,sm_50,sm_53 Sometimes it is necessary to perform different GPU code generation steps, partitioned over different architectures. This is possible using nvcc option --generate-code , which then must be used instead of a --gpu-architecture and --gpu-code combination. Unlike option --gpu-architecture option --generate-code , may be repeated on the nvcc command line. It takes sub-options arch and code , which must not be confused with their main option equivalents, but behave similarly. If repeated architecture compilation is used, then the device code must use conditional compilation based on the value of the architecture identification macro __CUDA_ARCH__ , which is described in the next section. For example, the following assumes absence of half-precision floating-point operation support for the sm_50 and sm_52 code, but full support on sm_53 : nvcc x.cu \\\n    --generate-code arch=compute_50,code=sm_50 \\\n    --generate-code arch=compute_50,code=sm_52 \\\n    --generate-code arch=compute_53,code=sm_53 Or, leaving actual GPU code generation to the JIT compiler in the CUDA driver: nvcc x.cu \\\n    --generate-code arch=compute_50,code=compute_50 \\\n    --generate-code arch=compute_53,code=compute_53 The code sub-options can be combined with a slightly more complex syntax: nvcc x.cu \\\n    --generate-code arch=compute_50,code=[sm_50,sm_52] \\\n    --generate-code arch=compute_53,code=sm_53 5.7.4. Virtual Architecture Macros  The architecture identification macro __CUDA_ARCH__ is assigned a three-digit value string xy0 (ending in a literal 0 ) for each stage 1 nvcc compilation that compiles for compute_xy . This macro can be used in the implementation of GPU functions for determining the virtual architecture for which it is currently being compiled. The host code (the non-GPU code) must not depend on it. The architecture list macro __CUDA_ARCH_LIST__ is a list of comma-separated __CUDA_ARCH__ values for each of the virtual architectures specified in the compiler invocation. The list is sorted in numerically ascending order. The macro __CUDA_ARCH_LIST__ is defined when compiling C, C++ and CUDA source files. For example, the following nvcc compilation command line will define __CUDA_ARCH_LIST__ as 500,530,800 : nvcc x.cu \\\n--generate-code arch=compute_80,code=sm_80 \\\n--generate-code arch=compute_50,code=sm_52 \\\n--generate-code arch=compute_50,code=sm_50 \\\n--generate-code arch=compute_53,code=sm_53 6. Using Separate Compilation in CUDA  Prior to the 5.0 release, CUDA did not support separate compilation, so CUDA code could not call device functions or access variables across files. Such compilation is referred to as whole program compilation . We have always supported the separate compilation of host code, it was just the CUDA device code that needed to all be within one file. Starting with CUDA 5.0, separate compilation of device code is supported, but the old whole program mode is still the default, so there are new options to invoke separate compilation. 6.1. Code Changes for Separate Compilation  The code changes required for separate compilation of device code are the same as what you already do for host code, namely using extern and static to control the visibility of symbols. Note that previously extern was ignored in CUDA code; now it will be honored. With the use of static it is possible to have multiple device symbols with the same name in different files. For this reason, the CUDA API calls that referred to symbols by their string name are deprecated; instead the symbol should be referenced by its address. 6.2. NVCC Options for Separate Compilation  CUDA works by embedding device code into host objects. In whole program compilation, it embeds executable device code into the host object. In separate compilation, we embed relocatable device code into the host object, and run nvlink , the device linker, to link all the device code together. The output of nvlink is then linked together with all the host objects by the host linker to form the final executable. The generation of relocatable vs executable device code is controlled by the --relocatable-device-code option. The --compile option is already used to control stopping a compile at a host object, so a new option --device-c is added that simply does --relocatable-device-code=true --compile . To invoke just the device linker, the --device-link option can be used, which emits a host object containing the embedded executable device code. The output of that must then be passed to the host linker. Or: nvcc <objects> can be used to implicitly call both the device and host linkers. This works because if the device linker does not see any relocatable code it does not do anything. The following figure shows the flow. CUDA Separate Compilation Trajectory  6.3. Libraries  The device linker has the ability to read the static host library formats ( .a on Linux and Mac OS X, .lib on Windows). It ignores any dynamic ( .so or .dll ) libraries. The --library and --library-path options can be used to pass libraries to both the device and host linker. The library name is specified without the library file extension when the --library option is used. nvcc --gpu-architecture=sm_50 a.o b.o --library-path=<path> --library=foo Alternatively, the library name, including the library file extension, can be used without the --library option on Windows. nvcc --gpu-architecture=sm_50 a.obj b.obj foo.lib --library-path=<path> Note that the device linker ignores any objects that do not have relocatable device code. 6.4. Examples  Suppose we have the following files: //---------- b.h ---------- #define N 8 extern __device__ int g [ N ]; extern __device__ void bar ( void ); //---------- b.cu ---------- #include \"b.h\" __device__ int g [ N ]; __device__ void bar ( void ) { g [ threadIdx . x ] ++ ; } //---------- a.cu ---------- #include <stdio.h> #include \"b.h\" __global__ void foo ( void ) { __shared__ int a [ N ]; a [ threadIdx . x ] = threadIdx . x ; __syncthreads (); g [ threadIdx . x ] = a [ blockDim . x - threadIdx . x - 1 ]; bar (); } int main ( void ) { unsigned int i ; int * dg , hg [ N ]; int sum = 0 ; foo <<< 1 , N >>> (); if ( cudaGetSymbolAddress (( void ** ) & dg , g )){ printf ( \"couldn't get the symbol addr \\n \" ); return 1 ; } if ( cudaMemcpy ( hg , dg , N * sizeof ( int ), cudaMemcpyDeviceToHost )){ printf ( \"couldn't memcpy \\n \" ); return 1 ; } for ( i = 0 ; i < N ; i ++ ) { sum += hg [ i ]; } if ( sum == 36 ) { printf ( \"PASSED \\n \" ); } else { printf ( \"FAILED (%d) \\n \" , sum ); } return 0 ; } These can be compiled with the following commands (these examples are for Linux): nvcc --gpu-architecture=sm_50 --device-c a.cu b.cu\nnvcc --gpu-architecture=sm_50 a.o b.o If you want to invoke the device and host linker separately, you can do: nvcc --gpu-architecture=sm_50 --device-c a.cu b.cu\nnvcc --gpu-architecture=sm_50 --device-link a.o b.o --output-file link.o\ng++ a.o b.o link.o --library-path=<path> --library=cudart Note that all desired target architectures must be passed to the device linker, as that specifies what will be in the final executable (some objects or libraries may contain device code for multiple architectures, and the link step can then choose what to put in the final executable). If you want to use the driver API to load a linked cubin, you can request just the cubin: nvcc --gpu-architecture=sm_50 --device-link a.o b.o \\\n    --cubin --output-file link.cubin The objects could be put into a library and used with: nvcc --gpu-architecture=sm_50 --device-c a.cu b.cu\nnvcc --lib a.o b.o --output-file test.a\nnvcc --gpu-architecture=sm_50 test.a Note that only static libraries are supported by the device linker. A PTX file can be compiled to a host object file and then linked by using: nvcc --gpu-architecture=sm_50 --device-c a.ptx An example that uses libraries, host linker, and dynamic parallelism would be: nvcc --gpu-architecture=sm_50 --device-c a.cu b.cu\nnvcc --gpu-architecture=sm_50 --device-link a.o b.o --output-file link.o\nnvcc --lib --output-file libgpu.a a.o b.o link.o\ng++ host.o --library=gpu --library-path=<path> \\\n    --library=cudadevrt --library=cudart It is possible to do multiple device links within a single host executable, as long as each device link is independent of the other. This requirement of independence means that they cannot share code across device executables, nor can they share addresses (e.g., a device function address can be passed from host to device for a callback only if the device link sees both the caller and potential callback callee; you cannot pass an address from one device executable to another, as those are separate address spaces). 6.5. Optimization Of Separate Compilation  Separately compiled code may not have as high of performance as whole program code because of the inability to inline code across files. A way to still get optimal performance is to use link-time optimization, which stores intermediate code which is then linked together to perform high level optimizations. This can be done with the --dlink-time-opt or -dlto option. This option must be specified at both compile and link time. If only some of the files are compiled with -dlto , then those will be linked and optimized together while the rest uses the normal separate compilation. A side effect is that this shifts some of the compile time to the link phase, and there may be some scalability issues with really large codes. If you want to compile using -gencode to build for multiple arch, use -dc -gencode arch=compute_NN,code=lto_NN to specify the intermediate IR to be stored (where NN is the SM architecture version). Then use -dlto option to link for a specific architecture. As of CUDA 12.0 there is support for runtime LTO via the nvJitLink library. 6.6. Potential Separate Compilation Issues  6.6.1. Object Compatibility  Only relocatable device code with the same ABI version, link-compatible SM target architecture, and same pointer size (32 or 64) can be linked together. The toolkit version of the linker must be >= the toolkit version of the objects. Incompatible objects will produce a link error. Link-compatible SM architectures are ones that have compatible SASS binaries that can combine without translating, e.g. sm_52 and sm_50. An object could have been compiled for a different architecture but also have PTX available, in which case the device linker will JIT the PTX to cubin for the desired architecture and then link. Relocatable device code requires CUDA 5.0 or later Toolkit. If Link Time Optimization is used with -dlto , the intermediate LTOIR is only guaranteed to be compatible within a major release (e.g. can link together 12.0 and 12.1 LTO intermediates, but not 12.1 and 11.6). If a kernel is limited to a certain number of registers with the launch_bounds attribute or the --maxrregcount option, then all functions that the kernel calls must not use more than that number of registers; if they exceed the limit, then a link error will be given. 6.6.2. JIT Linking Support  JIT linking means doing an implicit relink of the code at load time. If the cubin does not match the target architecture at load time, the driver re-invokes the device linker to generate cubin for the target architecture, by first JIT’ing the PTX for each object to the appropriate cubin, and then linking together the new cubin. If PTX or cubin for the target architecture is not found for an object, then the link will fail. Implicit JIT linking of the LTO intermediates is not supported at this time, although they can be explicitly linked with the nvJitLink library. 6.6.3. Implicit CUDA Host Code  A file like b.cu above only contains CUDA device code, so one might think that the b.o object doesn’t need to be passed to the host linker. But actually there is implicit host code generated whenever a device symbol can be accessed from the host side, either via a launch or an API call like cudaGetSymbolAddress() . This implicit host code is put into b.o , and needs to be passed to the host linker. Plus, for JIT linking to work all device code must be passed to the host linker, else the host executable will not contain device code needed for the JIT link. So a general rule is that the device linker and host linker must see the same host object files (if the object files have any device references in them—if a file is pure host then the device linker doesn’t need to see it). If an object file containing device code is not passed to the host linker, then you will see an error message about the function __cudaRegisterLinkedBinary_name calling an undefined or unresolved symbol __fatbinwrap_name . 6.6.4. Using __CUDA_ARCH__  In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior. Or, it must be guaranteed that all objects will compile for the same compute_arch. If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__ , then the instances of that function in the objects could conflict if the objects are compiled for different compute arch. For example, if an a.h contains: template < typename T > __device__ T * getptr ( void ) { #if __CUDA_ARCH__ == 500 return NULL ; /* no address */ #else __shared__ T arr [ 256 ]; return arr ; #endif } Then if a.cu and b.cu both include a.h and instantiate getptr for the same type, and b.cu expects a non-NULL address, and compile with: nvcc --gpu-architecture=compute_50 --device-c a.cu\nnvcc --gpu-architecture=compute_52 --device-c b.cu\nnvcc --gpu-architecture=sm_52 a.o b.o At link time only one version of the getptr is used, so the behavior would depend on which version is picked. To avoid this, either a.cu and b.cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function. 6.6.5. Device Code in Libraries  If a device function with non-weak external linkage is defined in a library as well as a non-library object (or another library), the device linker will complain about the multiple definitions (this differs from traditional host linkers that may ignore the function definition from the library object, if it was already found in an earlier object). 7. Miscellaneous NVCC Usage  7.1. Cross Compilation  Cross compilation is controlled by using the following nvcc command line option: --compiler-bindir is used for cross compilation, where the underlying host compiler is capable of generating objects for the target platform. On an x86 system, if a CUDA toolkit installation has been configured to support cross compilation to both Tegra and non-Tegra ARM targets, then nvcc will use the non-Tegra configuration by default, when an ARM host cross compiler has been specified. To use the Tegra configuration instead, pass “ -target-dir aarch64-linux ” to nvcc. 7.2. Keeping Intermediate Phase Files  nvcc stores intermediate results by default into temporary files that are deleted immediately before it completes. The location of the temporary file directories used are, depending on the current platform, as follows: Windows Value of environment variable TEMP is used. If it is not set, C:\\Windows\\temp is used instead. Other Platforms Value of environment variable TMPDIR is used. If it is not set, /tmp is used instead. Option --keep makes nvcc store these intermediate files in the current directory or in the directory specified by --keep-dir instead, with names as described in Supported Phases . 7.3. Cleaning Up Generated Files  All files generated by a particular nvcc command can be cleaned up by repeating the command, but with additional option --clean-targets . This option is particularly useful after using --keep , because the --keep option usually leaves quite an amount of intermediate files around. Because using --clean-targets will remove exactly what the original nvcc command created, it is important to exactly repeat all of the options in the original command. For instance, in the following example, omitting --keep , or adding --compile will have different cleanup effects. nvcc acos.cu --keep\nnvcc acos.cu --keep --clean-targets 7.4. Printing Code Generation Statistics  A summary on the amount of used registers and the amount of memory needed per compiled device function can be printed by passing option --resource-usage to nvcc : $ nvcc --resource-usage acos.cu -arch sm_80\nptxas info    : 1536 bytes gmem\nptxas info    : Compiling entry function 'acos_main' for 'sm_80'\nptxas info    : Function properties for acos_main\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\nptxas info    : Used 6 registers, 1536 bytes smem, 32 bytes cmem[0] As shown in the above example, the amount of statically allocated global memory (gmem) is listed. Global memory and some of the constant banks are module scoped resources and not per kernel resources. Allocation of constant variables to constant banks is profile specific. Followed by this, per kernel resource information is printed. Stack frame is per thread stack usage used by this function. Spill stores and loads represent stores and loads done on stack memory which are being used for storing variables that couldn’t be allocated to physical registers. Similarly number of registers, amount of shared memory and total space in constant bank allocated is shown. 8. Notices  8.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/vGPU/index.html", "parent_url": "https://docs.nvidia.com/cuda/vGPU/index.html", "content_type": "text/html", "text": "vGPUs and CUDA 1. Overview 1.1. vGPU 1.2. Notices 1.2.1. Notice 1.2.2. OpenCL 1.2.3. Trademarks vGPUs and CUDA » 1. Overview v12.5 | PDF | Archive vGPUs and CUDA vGPUs that support CUDA This page describes the support for CUDA® on NVIDIA® virtual GPU software. 1. Overview  For more information on NVIDIA virtual GPU software, visit https://docs.nvidia.com/grid/index.html . 1.1. vGPU  The following vGPU releases support CUDA: CUDA Toolkit Version vGPU Software Releases CUDA 12.5 Not supported CUDA 12.4 Update 1 (12.4.1) 17.x releases CUDA 12.4 17.x releases CUDA 12.3 Update 2 (12.3.2) Not supported CUDA 12.3 Update 1 (12.3.1) Not supported CUDA 12.3 Not supported CUDA 12.2 Update 1 (12.2.1) 16.x releases CUDA 12.2 16.x releases CUDA 12.1 Update 1 (12.1.1) 15.x releases CUDA 12.1 15.x releases CUDA 12.0 Update 1 (12.0.1) 15.x releases CUDA 12.0 15.x releases CUDA 11.8 14.x releases CUDA 11.7 Update 1 (11.7.1) 14.x releases CUDA 11.7 14.x releases CUDA 11.6 Update 1 (11.6.1) 14.x releases CUDA 11.6 14.x releases CUDA 11.5 Update 2 (11.5.2) 14.x releases CUDA 11.5 Update 1 (11.5.1) 14.x releases CUDA 11.5 14.x releases CUDA 11.4 Update 4 (11.4.4) 13.x releases CUDA 11.4 Update 3 (11.4.3 13.x releases CUDA 11.4 Update 2 (11.4.2) 13.x releases CUDA 11.4 Update 1 (11.4.1) 13.x releases CUDA 11.4 13.x releases CUDA 11.3 Update 1 (11.3.1) Not supported CUDA 11.3 Not supported CUDA 11.2 Update 1 (11.2.1) Not supported CUDA 11.2 12.x releases CUDA 11.1 Update 1 (11.1.1) Not supported CUDA 11.1 Not supported CUDA 11.0 11.x releases CUDA 10.2 10.x releases CUDA 10.1 Update 2 (10.1.243) Not supported CUDA 10.1 Update 1 9.x releases CUDA 10.1 general release (10.1.105) 8.x releases CUDA 10.0.130 7.x releases CUDA 9.2 (9.2.148 Update 1) Not supported CUDA 9.2 (9.2.88) Not supported CUDA 9.1 (9.1.85) 6.x releases CUDA 9.0 (9.0.176) 5.x releases Some CUDA features might not be supported by your version of NVIDIA virtual GPU software. For details, follow the link in the table to the documentation for your version. 1.2. Notices  1.2.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 1.2.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 1.2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/debugger-api/index.html", "parent_url": "https://docs.nvidia.com/cuda/debugger-api/index.html", "content_type": "text/html", "text": "Debugger API :: CUDA Toolkit Documentation NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit \n                  \n                  \n                  v12.5.1 Debugger API 1. Release Notes 1.1. 11.8 Release 1.2. 12.3 Release 1.3. 7.0 Release 1.4. 6.5 Release 2. Introduction 2.1. Debugger API 2.2. ELF and DWARF 2.3. ABI Support 2.4. Exception Reporting 2.5. Attaching and Detaching 3. Modules 3.1. General 3.2. Initialization 3.3. 3.4. Breakpoints 3.5. Device State Inspection 3.6. Device State Alteration 3.7. Grid Properties 3.8. Device Properties 3.9. DWARF Utilities 3.10. Events 4. Data Structures 4.1. 4.2. CUDBGEvent 4.3. CUDBGEvent::cases_st 4.4. CUDBGEvent::cases_st::contextCreate_st 4.5. CUDBGEvent::cases_st::contextDestroy_st 4.6. CUDBGEvent::cases_st::contextPop_st 4.7. CUDBGEvent::cases_st::contextPush_st 4.8. CUDBGEvent::cases_st::elfImageLoaded_st 4.9. CUDBGEvent::cases_st::internalError_st 4.10. CUDBGEvent::cases_st::kernelFinished_st 4.11. CUDBGEvent::cases_st::kernelReady_st 4.12. CUDBGEventCallbackData 4.13. CUDBGEventCallbackData40 4.14. CUDBGGridInfo 5. Data Fields 6. Deprecated List Search Results < Previous | Next > Debugger API\n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  v12.5.1\n                  ( older )\n                  -\n                  Last updated July 1, 2007-2024\n                  - Send Feedback Debugger API The API reference guide for the CUDA debugger. Table of Contents 1. Release Notes 1.1. 11.8 Release 1.2. 12.3 Release 1.3. 7.0 Release 1.4. 6.5 Release 2. Introduction 2.1. Debugger API 2.2. ELF and DWARF 2.3. ABI Support 2.4. Exception Reporting 2.5. Attaching and Detaching 3. Modules 3.1. General 3.2. Initialization 3.3. 3.4. Breakpoints 3.5. Device State Inspection 3.6. Device State Alteration 3.7. Grid Properties 3.8. Device Properties 3.9. DWARF Utilities 3.10. Events 4. Data Structures 4.1. 4.2. CUDBGEvent 4.3. CUDBGEvent::cases_st 4.4. CUDBGEvent::cases_st::contextCreate_st 4.5. CUDBGEvent::cases_st::contextDestroy_st 4.6. CUDBGEvent::cases_st::contextPop_st 4.7. CUDBGEvent::cases_st::contextPush_st 4.8. CUDBGEvent::cases_st::elfImageLoaded_st 4.9. CUDBGEvent::cases_st::internalError_st 4.10. CUDBGEvent::cases_st::kernelFinished_st 4.11. CUDBGEvent::cases_st::kernelReady_st 4.12. CUDBGEventCallbackData 4.13. CUDBGEventCallbackData40 4.14. CUDBGGridInfo 5. Data Fields 6. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/gpudirect-storage/index.html", "parent_url": "https://docs.nvidia.com/gpudirect-storage/index.html", "content_type": "text/html;charset=utf-8", "text": "NVIDIA GPUDirect Storage - NVIDIA Docs Submit Search NVIDIA Developer Blog Forums Join Submit Search NVIDIA Developer Blog Forums Join Menu NVIDIA GPUDirect Storage Submit Search Submit Search NVIDIA Docs Hub NVIDIA GPUDirect Storage (GDS) NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage (GDS) (Latest Release) NVIDIA GPUDirect Storage Documentation - Last updated June 14, 2024 NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage Design Guide The purpose of the Design Guide is to show OEMs, CSPs and ODMs how to design their servers to take advantage of GPUDirect Storage and to help application developers understand where GPUDirect Storage can bring value to application performance. NVIDIA GPUDirect Storage Overview Guide The NVIDIA® Magnum IO GPUDirect® Storage Overview Guide provides a high-level overview of GDS. cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the cuFile API reference that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. NVIDIA GPUDirect Storage Release Notes Release information for NVIDIA® Magnum IO GPUDirect® Storage. Getting Started with NVIDIA GPUDirect Storage Getting started information for NVIDIA® Magnum IO GPUDirect® Storage. Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide The Best Practices guide provides information about the lessons that were learned when building and massively scaling GPU accelerated I/O storage infrastructures. The guide is intended for system builders, developers, and storage vendors. NVIDIA GPUDirect Storage Benchmarking and Configuration Guide The Benchmarking and Configuration Guide helps you evaluate and test GDS functionality and performance by using sample applications. NVIDIA GPUDirect Storage Installation and Troubleshooting Guide This guide describes how to install, debug, and isolate the performance and functional problems that are related to GDS and is intended for systems administrators and developers. NVIDIA GPUDirect Storage O_DIRECT Requirements Guide The O_DIRECT Guide helps you understand how GDS provides significant benefit when it can leverage the O_DIRECT fcntl.h file mode for a direct data path between GPU memory and storage. Aerospace Hardware / Semiconductor Architecture / Engineering / Construction Manufacturing Media & Entertainment Restaurant / Quick-Service Energy HPC / Scientific Computing IT Specialist Public Sector Financial Services Dev / IT Operations Consumer Internet Cloud Services Telecommunications Gaming Healthcare & Life Sciences Agriculture Academia / Higher Education Retail / Consumer Packaged Goods Data Center / Cloud Data Center / Cloud © Copyright 2024, NVIDIA. Last updated on Jun 14, 2024. Topics NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage Design Guide 1. Introduction 2. Data Transfer Issues for GPU and Storage 3. GPUDirect Storage Benefits 4. Application Suitability 4.1. Transfers To and From the GPU 4.2. Understanding IO Bottlenecks 4.3. Explicit GDS APIs 4.4. Pinned Memory for DMA Transfers 4.5. cuFile APIs 5. Platform Performance Suitability 5.1. Bandwidth from Storage 5.2. Paths from Storage to GPUs 5.3. GPU BAR1 Size 6. Call to Action NVIDIA GPUDirect Storage Overview Guide 1. Introduction 1.1. Related Documents 1.2. Benefits for a Developer 1.3. Intended Uses 2. Functional Overview 2.1. Explicit and Direct 2.2. Performance Optimizations 2.2.1. Implementation Performance Enhancements 2.2.2. Concurrency Across Threads 2.2.3. Asynchrony 2.2.4. Batching 2.2.5. Use of CUDA Streams in cuFile 2.3. Compatibility and Generality 2.4. Monitoring 2.5. Scope of the Solutions in GDS 2.6. Dynamic Routing 2.6.1. cuFile Configuration for Dynamic Routing 2.6.2. cuFile Configuration for DFS Mount 2.6.3. cuFile Configuration Validation for Dynamic Routing 3. Software Architecture 3.1. Software Components 3.2. Primary Components 3.2.1. Workflows for GDS Functionality 3.2.2. Workflow 1 3.2.3. Workflow 2 3.3. Aligning with Other Linux Initiatives 4. Deployment 4.1. Software Components for Deployment 4.2. Using GPUDirect Storage in Containers cuFile API Reference Guide 1. Introduction 2. Usage 2.1. Dynamic Interactions 2.2. Driver, File, and Buffer Management 2.3. cuFile Compatibility Mode 3. cuFile API Specification 3.1. Data Types 3.1.1. Declarations and Definitions 3.1.2. Typedefs 3.1.3. Enumerations 3.2. cuFile Driver APIs 3.3. cuFile Synchronous IO APIs 3.4. cuFile File Handle APIs 3.5. cuFile Buffer APIs 3.6. cuFile Stream APIs 3.7. cuFile Batch APIs 4. cuFile API Functional Specification 4.1. cuFileDriver API Functional Specification 4.1.1. cuFileDriverOpen 4.1.2. cuFileDriverClose 4.1.3. cuFileDriverGetProperties 4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size) 4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size) 4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size) 4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size) 4.2. cuFile IO API Functional Specification 4.2.1. cuFileHandleRegister 4.2.2. cuFileHandleDeregister 4.2.3. cuFileRead 4.2.4. cuFileWrite 4.3. cuFile Memory Management Functional Specification 4.3.1. cuFileBufRegister 4.3.2. cuFileBufDeregister 4.4. cuFile Stream API Functional Specification 4.4.1. cuFileStreamRegister 4.4.2. cuFileStreamDeregister 4.4.3. cuFileReadAsync 4.4.4. cuFileWriteAsync 4.5. cuFile Batch API Functional Specification 4.5.1. cuFileBatchIOSetUp 4.5.2. cuFileBatchIOSubmit 4.5.3. cuFileBatchIOGetStatus 4.5.4. cuFileBatchIOCancel 4.5.5. cuFileBatchIODestroy 5. Sample Program with cuFile APIs 6. Known Limitations of cuFile Batch APIs NVIDIA GPUDirect Storage Release Notes 1. Introduction 2. New Features and Changes 3. MLNX_OFED and Filesystem Requirements 4. Support Matrix 5. GDS Enabled Libraries/Frameworks 6. Included Packages 7. Minor Updates and Bug Fixes 8. Known Issues 9. Known Limitations Getting Started with NVIDIA GPUDirect Storage 1. Introduction 2. If you are a system administrator or a performance engineer 3. If you are a developer 4. If you are OEM, ODM, CSP 5. Troubleshooting GDS issues Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide 1. Introduction 2. Software Settings 2.1. System Settings 2.2. Use of CUDA Context in GPU Kernels and Storage IO 2.3. cuFile Configuration Settings 3. API Usage 3.1. cuFileDriverOpen 3.2. cuFileHandleRegister 3.3. cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister 3.3.1. IO Pattern 1 3.3.2. IO Pattern 2 3.3.3. IO Pattern 3 3.3.4. IO Pattern 4 3.3.5. IO Pattern 5 3.3.6. IO Pattern 6 3.3.7. IO Pattern 7 3.4. cuFileHandleDeregister 3.5. cuFileBufDeregister 3.6. cuFileStreamRegister 3.7. cuFileStreamDeregister 3.8. cuFileDriverClose NVIDIA GPUDirect Storage Benchmarking and Configuration Guide 1. Introduction 2. About this Guide 3. Benchmarking GPUDirect Storage 3.1. Determining PCIe Device Affinity 3.2. GPUDirect Storage Configuration Parameters 3.2.1. System Parameters 3.2.2. GPUDirect Storage Parameters 3.3. GPUDirect Storage Benchmarking Tools 3.3.1. gdsio Utility 3.3.2. gds-stats Tool 4. GPUDirect Storage Benchmarking on Direct Attached Storage 4.1. GPUDirect Storage Performance on DGX-2 System 4.2. GPUDirect Storage Performance on a DGX A100 System 5. GPUDirect Storage Benchmarking on Network Attached Storage 5.1. GPUDirect Storage Benchmarking on NFS 6. Summary A. Benchmarking and Performance A.1. The Language of Performance A.2. Benchmarking Storage Performance NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 1. Introduction 2. Installing GPUDirect Storage 2.1. Before You Install GDS 2.2. Installing GDS 2.2.1. Configuring File System Settings for GDS 2.2.2. Verifying a Successful GDS Installation 2.3. Installed GDS Libraries and Tools 2.4. Uninstalling GPUDirect Storage 2.5. Environment Variables Used by GPUDirect Storage 2.6. JSON Config Parameters Used by GPUDirect Storage 2.7. GDS Configuration File Changes to Support Dynamic Routing 2.8. Determining Which Version of GDS is Installed 2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems 3. API Errors 3.1. CU_FILE_DRIVER_NOT_INITIALIZED 3.2. CU_FILE_DEVICE_NOT_SUPPORTED 3.3. CU_FILE_IO_NOT_SUPPORTED 3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID 4. Basic Troubleshooting 4.1. Log Files for the GDS Library 4.2. Enabling a Different cufile.log File for Each Application 4.3. Enabling Tracing GDS Library API Calls 4.4. cuFileHandleRegister Error 4.5. Troubleshooting Applications that Return cuFile Errors 4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics 4.7. CUDA Runtime and Driver Mismatch with Error Code 35 4.8. CUDA API Errors when Running the cuFile-* APIs 4.9. Finding GDS Driver Statistics 4.10. Tracking IO Activity that Goes Through the GDS Driver 4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats 4.12. Tracking Registration and Deregistration of GPU Buffers 4.13. Enabling RDMA-specific Logging for Userspace File Systems 4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation 4.15. Adding udev Rules for RAID Volumes 4.16. When You Observe \"Incomplete write\" on NVME Drives 4.17. CUFILE async I/O is failing 5. Advanced Troubleshooting 5.1. Resolving Hung cuFile* APIs with No Response 5.2. Sending Relevant Data to Customer Support 5.3. Resolving an IO Failure with EIO and Stack Trace Warning 5.4. Controlling GPU BAR Memory Usage 5.5. Determining the Amount of Cache to Set Aside 5.6. Monitoring BAR Memory Usage 5.7. Resolving an ENOMEM Error Code 5.8. GDS and Compatibility Mode 5.9. Enabling Compatibility Mode 5.10. Tracking the IO After Enabling Compatibility Mode 5.11. Bypassing GPUDirect Storage 5.12. GDS Does Not Work for a Mount 5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File 5.14. Running Data Verification Tests Using GPUDirect Storage NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 6. Troubleshooting Performance 6.1. Running Performance Benchmarks with GDS 6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache 6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance 6.4. Using GPUDirect Statistics to Monitor CPU Activity 6.5. Monitoring Performance and Tracing with cuFile-* APIs 6.6. Example: Using Linux Tracing Tools 6.7. Tracing the cuFile-* APIs 6.8. Improving Performance using Dynamic Routing 7. Troubleshooting IO Activity 7.1. Managing Coherency of Data in the Page Cache and on Disk 8. EXAScaler Filesystem LNet Troubleshooting 8.1. Determining the EXAScaler Filesystem Client Module Version 8.2. Checking the LNet Network Setup on a Client 8.3. Checking the Health of the Peers 8.4. Checking for Multi-Rail Support 8.5. Checking GDS Peer Affinity 8.6. Checking for LNet-Level Errors 8.7. Resolving LNet NIDs Health Degradation from Timeouts 8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection 9. Understanding EXAScaler Filesystem Performance 9.1. osc Tuning Performance Parameters 9.2. Miscellaneous Commands for osc, mdc, and stripesize 9.3. Getting the Number of Configured Object-Based Disks 9.4. Getting Additional Statistics related to the EXAScaler Filesystem 9.5. Getting Metadata Statistics 9.6. Checking for an Existing Mount 9.7. Unmounting an EXAScaler Filesystem Cluster 9.8. Getting a Summary of EXAScaler Filesystem Statistics 9.9. Using GPUDirect Storage in Poll Mode 10. Troubleshooting and FAQ for the WekaIO Filesystem 10.1. Downloading the WekaIO Client Package 10.2. Determining Whether the WekaIO Version is Ready for GDS 10.3. Mounting a WekaIO File System Cluster 10.4. Resolving a Failing Mount 10.5. Resolving 100% Usage for WekaIO for Two Cores 10.6. Checking for an Existing Mount in the Weka File System 10.7. Checking for a Summary of the WekaIO Filesystem Status 10.8. Displaying the Summary of the WekaIO Filesystem Statistics 10.9. Why WekaIO Writes Go Through POSIX 10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct 10.11. Checking Memory Peer Direct Stats 10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem 10.13. Conducting a Basic WekaIO Filesystem Test 10.14. Unmounting a WekaIO File System Cluster 10.15. Verify the Installed Libraries for the WekaIO Filesystem 10.16. GDS Configuration File Changes to Support the WekaIO Filesystem 10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem 10.18. Check for WekaFS Support 11. Enabling IBM Spectrum Scale Support with GDS 11.1. IBM Spectrum Scale Limitations with GDS 11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect 11.3. Verifying Installed Libraries for IBM Spectrum Scale 11.4. Checking PeerDirect Stats 11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale 11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process 11.7. GDS Configuration to Support IBM Spectrum Scale 11.8. Scenarios for Falling Back to Compatibility Mode 11.9. GDS Limitations with IBM Spectrum Scale 12. NetApp E-series BeeGFS with GDS Solution Deployment 12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements 12.2. BeeGFS Client Configuration for GDS 12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server 12.4. Verify the Setup 12.4.1. List the Management Node 12.4.2. List the Metadata Nodes 12.4.3. List the Storage Nodes 12.4.4. List the Client Nodes 12.4.5. Display Client Connections 12.4.6. Verify Connectivity to the Different Services 12.4.7. List Storage Pools 12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets 12.5. Testing 12.5.1. Verifying Integration is Working 12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test 13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath) 13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages 13.1.1. Client Software Requirements 13.1.2. Install the VAST Multipath Package 13.2. Set Up the Networking 13.2.1. VAST Network Configuration 13.2.2. Client Network Configuration 13.2.3. Verify Network Connectivity 13.3. Mount VAST NFS 13.4. Debugging and Monitoring VAST Data 14. Troubleshooting and FAQ for NVMe and NVMeOF Support 14.1. MLNX_OFED Requirements and Installation 14.2. Determining Whether the NVMe device is Supported for GDS 14.3. RAID Support in GDS 14.4. Mounting a Local Filesystem for GDS 14.5. Check for an Existing EXT4 Mount 14.6. Check for IO Statistics with Block Device Mount 14.7. RAID Group Configuration for GPU Affinity 14.8. Conduct a Basic EXT4 Filesystem Test 14.9. Unmount a EXT4 Filesystem 14.10. Udev Device Naming for a Block Device 14.11. BATCH I/O Performance 15. Displaying GDS NVIDIA FS Driver Statistics 15.1. nvidia-fs Statistics 15.2. Analyze Statistics for each GPU 15.3. Resetting the nvidia-fs Statistics 15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers 15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers 15.6. Display the GPU-to-Peer Distance Table 15.7. The GDSIO Tool 15.8. Tabulated Fields 15.9. The GDSCHECK Tool 15.10. NFS Support with GPUDirect Storage 15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later 15.10.2. Install GPUDirect Storage Support for the NFS Client 15.11. NFS GPUDirect Storage Statistics and Debugging 15.12. GPUDirect Storage IO Behavior 15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO 15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite) 15.12.3. GPU to NIC Peer Affinity 15.12.4. Compatible Mode with Unregistered Buffers 15.12.5. Unaligned writes with Non-Registered Buffers 15.12.6. Process Hang with NFS 15.12.7. Tools Support Limitations for CUDA 9 and Earlier 15.13. GDS Statistics for Dynamic Routing 15.13.1. Peer Affinity Dynamic Routing 15.13.2. cuFile Log Related to Dynamic Routing 16. GDS Library Tracing 16.1. Example: Display Tracepoints 16.1.1. Example: Tracepoint Arguments 16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite 16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS 16.4. Understand the IO Pattern of a Process 16.5. IO Pattern of a Process with the File Descriptor on Different GPUs 16.6. Determine the IOPS and Bandwidth for a Process in a GPU 16.7. Display the Frequency of Reads by Processes that Issue cuFileRead 16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms 16.9. Displaying the Latency of cuFileRead for Each Process 16.10. Example: Tracking the Processes that Issue cuFileBufRegister 16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister 16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer 16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure 16.14. Example: User-Space Statistics for Each GDS Process 16.15. Example: Viewing GDS User-Level Statistics for a Process 16.16. Example: Displaying Sample User-Level Statistics for each GDS Process 17. User-Space Counters in GPUDirect Storage 17.1. Distribution of IO Usage in Each GPU 17.2. User-space Statistics for Dynamic Routing 18. User-Space RDMA Counters in GPUDirect Storage 18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS) 18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS) 19. Cheat Sheet for Diagnosing Problems NVIDIA GPUDirect Storage O_DIRECT Requirements Guide 1. Introduction 1.1. Related Documents 2. GPUDirect Storage Requirements 2.1. Summary of Basic Requirements 2.2. Client and Server 2.3. Cases Where O_DIRECT is Not a Fit 2.3.1. Buffered IO 2.3.2. Inline Files 2.3.3. Block Allocation For Writes 2.3.4. Examining or Transforming User Data 2.3.5. Summary Corporate Info NVIDIA.com Home About NVIDIA ‎NVIDIA Developer Developer Home Blog Resources Contact Us Developer Program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024 NVIDIA Corporation"}, {"url": "https://docs.nvidia.com/cuda/gpudirect-rdma/index.html", "parent_url": "https://docs.nvidia.com/cuda/gpudirect-rdma/index.html", "content_type": "text/html", "text": "GPUDirect RDMA 1. Overview 1.1. How GPUDirect RDMA Works 1.2. Standard DMA Transfer 1.3. GPUDirect RDMA Transfers 1.4. Changes in CUDA 6.0 1.5. Changes in CUDA 7.0 1.6. Changes in CUDA 8.0 1.7. Changes in CUDA 10.1 1.8. Changes in CUDA 11.2 1.9. Changes in CUDA 11.4 1.10. Changes in CUDA 12.2 2. Design Considerations 2.1. Lazy Unpinning Optimization 2.2. Registration Cache 2.3. Unpin Callback 2.4. Supported Systems 2.5. PCI BAR sizes 2.6. Tokens Usage 2.7. Synchronization and Memory Ordering 3. How to Perform Specific Tasks 3.1. Displaying GPU BAR space 3.2. Pinning GPU memory 3.3. Unpinning GPU memory 3.4. Handling the free callback 3.5. Buffer ID Tag Check for A Registration Cache 3.6. Linking a Kernel Module against nvidia.ko 3.7. Using nvidia-peermem 4. References 4.1. Basics of UVA CUDA Memory Management 4.2. Userspace API 4.3. Kernel API 4.4. Porting to Tegra 4.4.1. Changing the allocator 4.4.2. Modification to Kernel API 4.4.3. Other highlights 5. Notices 5.1. Notice 5.2. OpenCL 5.3. Trademarks GPUDirect RDMA » 1. Overview v12.5 | PDF | Archive Developing a Linux Kernel Module using GPUDirect RDMA The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs. 1. Overview  GPUDirect RDMA is a technology introduced in Kepler-class GPUs and CUDA 5.0 that enables a direct path for data exchange between the GPU and a third-party peer device using standard features of PCI Express. Examples of third-party devices are: network interfaces, video acquisition devices, storage adapters. GPUDirect RDMA is available on both Tesla and Quadro GPUs. A number of limitations can apply, the most important being that the two devices must share the same upstream PCI Express root complex. Some of the limitations depend on the platform used and could be lifted in current/future products. A few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices. This document introduces the technology and describes the steps necessary to enable an GPUDirect RDMA connection to NVIDIA GPUs on Linux. GPUDirect RDMA within the Linux Device Driver Model  1.1. How GPUDirect RDMA Works  When setting up GPUDirect RDMA communication between two peers, all physical addresses are the same from the PCI Express devices’ point of view. Within this physical address space are linear windows called PCI BARs. Each device has six BAR registers at most, so it can have up to six active 32bit BAR regions. 64bit BARs consume two BAR registers. The PCI Express device issues reads and writes to a peer device’s BAR addresses in the same way that they are issued to system memory. Traditionally, resources like BAR windows are mapped to user or kernel address space using the CPU’s MMU as memory mapped I/O (MMIO) addresses. However, because current operating systems don’t have sufficient mechanisms for exchanging MMIO regions between drivers, the NVIDIA kernel driver exports functions to perform the necessary address translations and mappings. To add GPUDirect RDMA support to a device driver, a small amount of address mapping code within the kernel driver must be modified. This code typically resides near existing calls to get_user_pages() . The APIs and control flow involved with GPUDirect RDMA are very similar to those used with standard DMA transfers. See Supported Systems and PCI BAR sizes for more hardware details. 1.2. Standard DMA Transfer  First, we outline a standard DMA Transfer initiated from userspace. In this scenario, the following components are present: Userspace program Userspace communication library Kernel driver for the device interested in doing DMA transfers The general sequence is as follows: The userspace program requests a transfer via the userspace communication library. This operation takes a pointer to data (a virtual address) and a size in bytes. The communication library must make sure the memory region corresponding to the virtual address and size is ready for the transfer. If this is not the case already, it has to be handled by the kernel driver (next step). The kernel driver receives the virtual address and size from the userspace communication library. It then asks the kernel to translate the virtual address range to a list of physical pages and make sure they are ready to be transferred to or from. We will refer to this operation as pinning the memory. The kernel driver uses the list of pages to program the physical device’s DMA engine(s). The communication library initiates the transfer. After the transfer is done, the communication library should eventually clean up any resources used to pin the memory. We will refer to this operation as unpinning the memory. 1.3. GPUDirect RDMA Transfers  For the communication to support GPUDirect RDMA transfers some changes to the sequence above have to be introduced. First of all, two new components are present: Userspace CUDA library NVIDIA kernel driver As described in Basics of UVA CUDA Memory Management , programs using the CUDA library have their address space split between GPU and CPU virtual addresses, and the communication library has to implement two separate paths for them. The userspace CUDA library provides a function that lets the communication library distinguish between CPU and GPU addresses. Moreover, for GPU addresses it returns additional metadata that is required to uniquely identify the GPU memory represented by the address. See Userspace API for details. The difference between the paths for CPU and GPU addresses is in how the memory is pinned and unpinned. For CPU memory this is handled by built-in Linux Kernel functions ( get_user_pages() and put_page() ). However, in the GPU memory case the pinning and unpinning has to be handled by functions provided by the NVIDIA Kernel driver. See Pinning GPU memory and Unpinning GPU memory for details. Some hardware caveats are explained in Supported Systems and PCI BAR sizes . 1.4. Changes in CUDA 6.0  In this section we briefly list the changes that are available in CUDA 6.0: CUDA peer-to-peer tokens are no longer mandatory. For memory buffers owned by the calling process (which is typical) tokens can be replaced by zero (0) in the kernel-mode function nvidia_p2p_get_pages() . This new feature is meant to make it easier for existing third party software stacks to adopt RDMA for GPUDirect. As a consequence of the change above, a new API cuPointerSetAttribute() has been introduced. This API must be used to register any buffer for which no peer-to-peer tokens are used. It is necessary to ensure correct synchronization behavior of the CUDA API when operation on memory which may be read by RDMA for GPUDirect. Failing to use it in these cases may cause data corruption. See changes in Tokens Usage . cuPointerGetAttribute() has been extended to return a globally unique numeric identifier, which in turn can be used by lower-level libraries to detect buffer reallocations happening in user-level code (see Userspace API ). It provides an alternative method to detect reallocations when intercepting CUDA allocation and deallocation APIs is not possible. The kernel-mode memory pinning feature has been extended to work in combination with Multi-Process Service (MPS). Caveats as of CUDA 6.0: CUDA Unified Memory is not explicitly supported in combination with GPUDirect RDMA. While the page table returned by nvidia_p2p_get_pages() is valid for managed memory buffers and provides a mapping of GPU memory at any given moment in time, the GPU device copy of that memory may be incoherent with the writable copy of the page which is not on the GPU. Using the page table in this circumstance may result in accessing stale data, or data loss, because of a DMA write access to device memory that is subsequently overwritten by the Unified Memory run-time. cuPointerGetAttribute() may be used to determine if an address is being managed by the Unified Memory runtime. Every time a device memory region is pinned, new GPU BAR space is allocated unconditionally, even when pinning overlapping or duplicate device memory ranges, i.e. there is no attempt at reusing mappings. This behavior has been changed since CUDA 7.0. 1.5. Changes in CUDA 7.0  In this section we briefly list the changes that are available in CUDA 7.0: On the IBM POWER8 platform, GPUDirect RDMA is not supported, though it is not explicitly disabled. GPUDirect RDMA is not guaranteed to work on any given ARM64 platform. Management of GPU BAR mappings has been improved with respect to CUDA 6.0. Now when a device memory region is pinned, GPU BAR space might be shared with pre-existing mappings. This is the case for example when pinning overlapping or duplicate device memory ranges. As a consequence, when unpinning a region, its whole BAR space will not be returned if even only a subset of its BAR space is shared. The new cuPointerGetAttributes() API has been introduced. It can be useful when retrieving multiple attributes for the same buffer, e.g. in MPI when examining a new buffer. cudaPointerGetAttributes() is now faster since it leverages cuPointerGetAttributes() internally. A new sample code, samples/7_CUDALibraries/cuHook , has been added in CUDA 6.5. It can be used as a template for implementing an interception framework for CUDA memory de/allocation APIs. 1.6. Changes in CUDA 8.0  In this section we briefly list the changes that are available in CUDA 8.0: The nvidia_p2p_page_table struct has been extended to include a new member, without breaking binary compatibility. The minor version in the NVIDIA_P2P_PAGE_TABLE_VERSION macro has been updated accordingly. The nvidia_p2p_dma_mapping structure, the nvidia_p2p_dma_map_pages() and nvidia_p2p_dma_unmap_pages() APIs, the NVIDIA_P2P_DMA_MAPPING_VERSION macro have been introduced. These APIs can be used by third party device drivers to map and unmap the GPU BAR pages into their device’s I/O address space. The main use case is on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources. See this link for an example of code using these new APIs. The NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE and NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE macros have been introduced. These are meant to be called by third-party device drivers to check for runtime binary compatibility, for example in case of changes to the data structure’s layout. On the IBM POWER8 platform, when using the above APIs, GPUDirect RDMA is reported to work correctly restricted to the case where the GPU and the third party device are connected through a supported PCIe switch. 1.7. Changes in CUDA 10.1  GPUDirect RDMA is supported on Jetson AGX Xavier platform. See Porting to Tegra section for details. 1.8. Changes in CUDA 11.2  GPUDirect RDMA is supported on Drive AGX Xavier Linux based platform. See Porting to Tegra section for details. 1.9. Changes in CUDA 11.4  Added a new a kernel module, nvidia-peermem , which provides NVIDIA InfiniBand-based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory. See Using nvidia-peermem for details. GPUDirect RDMA is supported on Jetson Orin platform. See Porting to Tegra section for details. Known Issue: Currently, there is no service to automatically load nvidia-peermem . Users need to load the module manually. 1.10. Changes in CUDA 12.2  In drivers released from the R515 up to the R535 branches, except for newer R525 and R535 releases mentioned below, there is a race bug which may show up as a kernel null-pointer dereference. This happens when the GPU invokes the (hereby I/O) kernel driver invalidation callback, the one which was registered during the call to nvidia_p2p_get_pages, concurrently with the I/O driver calling nvidia_p2p_put_pages.\nThe race bug does not affect the persistent mapping case, as in that case an invalidation callback is not supported nor needed. The bug fix required the following API change: nvidia_p2p_get_pages no longer accepts a NULL callback pointer. Instead, nvidia_p2p_put_pages_persistent and nvidia_p2p_get_pages_persistent have been introduced and should be used instead when requesting a persistent mapping. The use of those new persistent APIs can be guarded by the NVIDIA_P2P_CAP_GET_PAGES_PERSISTENT_API preprocessor macro, for example when writing portable drivers. The nvidia-peermem kernel module has been updated accordingly. Although deprecated when running GPU drivers from the R470 branch and newer, customers still using the off-tree nv_peer_mem module ( https://github.com/Mellanox/nv_peer_memory ) and needing the persistent mapping feature will have to switch to nvidia-peermem . Note that I/O drivers, which do not need persistent mappings, do not require source code changes. The API changes described above are deployed in the R535 branch, specifically in release 535.14 and later, and have also been back-ported to the R525 branch, for TeslaRD3 (525.105.17) and later. 2. Design Considerations  When designing a system to utilize GPUDirect RDMA, there a number of considerations which should be taken into account. 2.1. Lazy Unpinning Optimization  Pinning GPU device memory in BAR is an expensive operation, taking up to milliseconds. Therefore the application should be designed in a way to minimize that overhead. The most straightforward implementation using GPUDirect RDMA would pin memory before each transfer and unpin it right after the transfer is complete. Unfortunately, this would perform poorly in general, as pinning and unpinning memory are expensive operations. The rest of the steps required to perform an RDMA transfer, however, can be performed quickly without entering the kernel (the DMA list can be cached and replayed using MMIO registers/command lists). Hence, lazily unpinning memory is key to a high performance RDMA implementation. What it implies, is keeping the memory pinned even after the transfer has finished. This takes advantage of the fact that it is likely that the same memory region will be used for future DMA transfers thus lazy unpinning saves pin/unpin operations. An example implementation of lazy unpinning would keep a set of pinned memory regions and only unpin some of them (for example the least recently used one) if the total size of the regions reached some threshold, or if pinning a new region failed because of BAR space exhaustion (see PCI BAR sizes ). 2.2. Registration Cache  Communication middleware often employs an optimization called a registration cache, or pin-down cache, to minimize pinning overhead. Typically it already exists for host memory, implementing lazy unpinning, LRU de-registration, etc. For networking middleware, such caches are usually implemented in user-space, as they are used in combination with hardware capable of user-mode message injection. CUDA UVA memory address layout enables GPU memory pinning to work with these caches by taking into account just a few design considerations. In the CUDA environment, this is even more important as the amount of memory which can be pinned may be significantly more constrained than for host memory. As the GPU BAR space is typically mapped using 64KB pages, it is more resource efficient to maintain a cache of regions rounded to the 64KB boundary. Even more so, as two memory areas which are in the same 64KB boundary would allocate and return the same BAR mapping. Registration caches usually rely on the ability to intercept deallocation events happening in the user application, so that they can unpin the memory and free important HW resources, e.g. on the network card. To implement a similar mechanism for GPU memory, an implementation has two options: Instrument all CUDA allocation and deallocation APIs. Use a tag check function to track deallocation and reallocation. See Buffer ID Tag Check for A Registration Cache . There is a sample application, 7_CUDALibraries/cuHook , showing how to intercept calls to CUDA APIs at run-time, which can be used to detect GPU memory de/allocations. While intercepting CUDA APIs is beyond the scope of this document, an approach to performing tag checks is available starting with CUDA 6.0. It involves the usage of the CU_POINTER_ATTRIBUTE_BUFFER_ID attribute in cuPointerGetAttribute() (or cuPointerGetAttributes() if more attributes are needed) to detect memory buffer deallocations or reallocations. The API will return a different ID value in case of reallocation or an error if the buffer address is no longer valid. See Userspace API for API usage. Note Using tag checks introduces an extra call into the CUDA API on each memory buffer use, so this approach is most appropriate when the additional latency is not a concern. 2.3. Unpin Callback  When a third party device driver pins the GPU pages with nvidia_p2p_get_pages() it must also provide a callback function that the NVIDIA driver will call if it needs to revoke access to the mapping. This callback occurs synchronously , giving the third party driver the opportunity to clean up and remove any references to the pages in question (i.e., wait for outstanding DMAs to complete). The user callback function may block for a few milliseconds , although it is recommended that the callback complete as quickly as possible. Care has to be taken not to introduce deadlocks as waiting within the callback for the GPU to do anything is not safe. The callback must call nvidia_p2p_free_page_table() (not nvidia_p2p_put_pages() ) to free the memory pointed to by page_table . The corresponding mapped memory areas will only be unmapped by the NVIDIA driver after returning from the callback. Note that the callback will be invoked in two scenarios: If the userspace program explicitly deallocates the corresponding GPU memory, e.g. cuMemFree , cuCtxDestroy , etc. before the third party kernel driver has a chance to unpin the memory with nvidia_p2p_put_pages() . As a consequence of an early exit of the process. In the latter case there can be tear-down ordering issues between closing the file descriptor of the third party kernel driver and that of the NVIDIA kernel driver. In the case the file descriptor for the NVIDIA kernel driver is closed first, the nvidia_p2p_put_pages() callback will be invoked. A proper software design is important as the NVIDIA kernel driver will protect itself from reentrancy issues with locks before invoking the callback. The third party kernel driver will almost certainly take similar actions, so dead-locking or live-locking scenarios may arise if careful consideration is not taken. 2.4. Supported Systems  General remarks Even though the only theoretical requirement for GPUDirect RDMA to work between a third-party device and an NVIDIA GPU is that they share the same root complex, there exist bugs (mostly in chipsets) causing it to perform badly, or not work at all in certain setups. We can distinguish between three situations, depending on what is on the path between the GPU and the third-party device: PCIe switches only single CPU/IOH CPU/IOH <-> QPI/HT <-> CPU/IOH The first situation, where there are only PCIe switches on the path, is optimal and yields the best performance. The second one, where a single CPU/IOH is involved, works, but yields worse performance ( especially peer-to-peer read bandwidth has been shown to be severely limited on some processor architectures ). Finally, the third situation, where the path traverses a QPI/HT link, may be extremely performance-limited or even not work reliably. Tip lspci can be used to check the PCI topology: $ lspci - t Platform support For IBM POWER8 platform, GPUDirect RDMA and P2P are not supported, but are not explicitly disabled. They may not work at run-time. GPUDirect RDMA is supported on Jetson AGX Xavier platform starting from CUDA 10.1 and on Drive AGX Xavier Linux based platforms from CUDA 11.2. See section Porting to Tegra for details. On ARM64, the necessary peer-to-peer functionality depends on both the hardware and the software of the particular platform. So while GPUDirect RDMA is not explicitly disabled on non-Jetson and non-Drive platforms, there are no guarantees that it will be fully functional. IOMMUs GPUDirect RDMA currently relies upon all physical addresses being the same from the different PCI devices’ point of view. This makes it incompatible with IOMMUs performing any form of translation other than 1:1, hence they must be disabled or configured for pass-through translation for GPUDirect RDMA to work. 2.5. PCI BAR sizes  PCI devices can ask the OS/BIOS to map a region of physical address space to them. These regions are commonly called BARs. NVIDIA GPUs currently expose multiple BARs, and some of them can back arbitrary device memory, making GPUDirect RDMA possible.\nThe maximum BAR size available for GPUDirect RDMA differs from GPU to GPU. For example, currently the smallest available BAR size on Kepler class GPUs is 256 MB. Of that, 32MB are currently reserved for internal use. These sizes may change. On some Tesla-class GPUs a large BAR feature is enabled, e.g. BAR1 size is set to 16GB or larger. Large BARs can pose a problem for the BIOS, especially on older motherbords, related to compatibility support for 32bit operating systems. On those motherboards the bootstrap can stop during the early POST phase, or the GPU may be misconfigured and so unusable. If this appears to be occuring it might be necessary to enable some special BIOS feature to deal with the large BAR issue. Please consult your system vendor for more details regarding large BAR support. 2.6. Tokens Usage  Warning Starting in CUDA 6.0, tokens should be considered deprecated, though they are still supported. As can be seen in Userspace API and Kernel API , one method for pinning and unpinning memory requires two tokens in addition to the GPU virtual address. These tokens, p2pToken and vaSpaceToken , are necessary to uniquely identify a GPU VA space. A process identifier alone does not identify a GPU VA space. The tokens are consistent within a single CUDA context (i.e., all memory obtained through cudaMalloc() within the same CUDA context will have the same p2pToken and vaSpaceToken ). However, a given GPU virtual address need not map to the same context/GPU for its entire lifetime. As a concrete example: cudaSetDevice ( 0 ) ptr0 = cudaMalloc (); cuPointerGetAttribute ( & return_data , CU_POINTER_ATTRIBUTE_P2P_TOKENS , ptr0 ); // Returns [p2pToken = 0xabcd, vaSpaceToken = 0x1] cudaFree ( ptr0 ); cudaSetDevice ( 1 ); ptr1 = cudaMalloc (); assert ( ptr0 == ptr1 ); // The CUDA driver is free (although not guaranteed) to reuse the VA, // even on a different GPU cuPointerGetAttribute ( & return_data , CU_POINTER_ATTRIBUTE_P2P_TOKENS , ptr0 ); // Returns [p2pToken = 0x0123, vaSpaceToken = 0x2] That is, the same address, when passed to cuPointerGetAttribute , may return different tokens at different times during the program’s execution. Therefore, the third party communication library must call cuPointerGetAttribute() for every pointer it operates on. Security implications The two tokens act as an authentication mechanism for the NVIDIA kernel driver. If you know the tokens, you can map the address space corresponding to them, and the NVIDIA kernel driver doesn’t perform any additional checks. The 64bit p2pToken is randomized to prevent it from being guessed by an adversary. When no tokens are used, the NVIDIA driver limits the Kernel API to the process which owns the memory allocation. 2.7. Synchronization and Memory Ordering  GPUDirect RDMA introduces a new independent GPU data flow path exposed to third party devices and it is important to understand how these devices interact with the GPU’s relaxed memory model. Properly registering a BAR mapping of CUDA memory is required for that mapping to remain consistent with CUDA APIs operations on that memory. Only CUDA synchronization and work submission APIs provide memory ordering of GPUDirect RDMA operations. Registration for CUDA API Consistency Registration is necessary to ensure the CUDA API memory operations visible to a BAR mapping happen before the API call returns control to the calling CPU thread. This provides a consistent view of memory to a device using GPUDirect RDMA mappings when invoked after a CUDA API in the thread. This is a strictly more conservative mode of operation for the CUDA API and disables optimizations, thus it may negatively impact performance. This behavior is enabled on a per-allocation granularity either by calling cuPointerSetAttribute() with the CU_POINTER_ATTRIBUTE_SYNC_MEMOPS attribute, or p2p tokens are retrieved for a buffer when using the legacy path. See Userspace API for more details. An example situation would be Read-after-Write dependency between a cuMemcpyDtoD() and subsequent GPUDirect RDMA read operation on the destination of the copy. As an optimization the device-to-device memory copy typically returns asynchronously to the calling thread after queuing the copy to the GPU scheduler. However, in this circumstance that will lead to inconsistent data read via the BAR mapping, so this optimization is disabled an the copy completed before the CUDA API returns. CUDA APIs for Memory Ordering Only CPU initiated CUDA APIs provide ordering of GPUDirect memory operations as observed by the GPU. That is, despite a third party device having issued all PCIE transactions, a running GPU kernel or copy operation may observe stale data or data that arrives out-of-order until a subsequent CPU initiated CUDA work submission or synchronization API. To ensure that memory updates are visible to CUDA kernels or copies, an implementation should ensure that all writes to the GPU BAR happen before control is returned to the CPU thread which will invoke the dependent CUDA API. An example situation for a network communication scenario is when a network RDMA write operation is completed by the third party network device and the data is written to the GPU BAR mapping. Though reading back the written data either through GPU BAR or a CUDA memory copy operation, will return the newly written data, a concurrently running GPU kernel to that network write might observe stale data, the data partially written, or the data written out-of-order. In short, a GPU kernel is wholly inconsistent with concurrent RDMA for GPUDirect operations and accessing the memory overwritten by the third party device in such a situation would be considered a data race. To resolve this inconsistency and remove the data race the DMA write operation must complete with respect to the CPU thread which will launch the dependent GPU kernel. 3. How to Perform Specific Tasks  3.1. Displaying GPU BAR space  Starting in CUDA 6.0 the NVIDIA SMI utility provides the capability to dump BAR1 memory usage. It can be used to understand the application usage of BAR space, the primary resource consumed by GPUDirect RDMA mappings. $ nvidia-smi -q\n...\n      BAR1 Memory Usage\n         Total                       : 256 MiB\n         Used                        : 2 MiB\n         Free                        : 254 MiB\n... GPU memory is pinned in fixed size chunks, so the amount of space reflected here might be unexpected. In addition, a certain amount of BAR space is reserved by the driver for internal use, so not all available memory may be usable via GPUDirect RDMA. Note that the same ability is offered programmatically through the nvmlDeviceGetBAR1MemoryInfo() NVML API. 3.2. Pinning GPU memory  Correct behavior requires using cuPointerSetAttribute() on the memory address to enable proper synchronization behavior in the CUDA driver. See section Synchronization and Memory Ordering . void pin_buffer ( void * address , size_t size ) { unsigned int flag = 1 ; CUresult status = cuPointerSetAttribute ( & flag , CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , address ); if ( CUDA_SUCCESS == status ) { // GPU path pass_to_kernel_driver ( address , size ); } else { // CPU path // ... } } This is required so that the GPU memory buffer is treated in a special way by the CUDA driver, so that CUDA memory transfers are guaranteed to always be synchronous with respect to the host. See Userspace API for details on cuPointerSetAttribute() . In the kernel driver, invoke nvidia_p2p_get_pages() . // for boundary alignment requirement #define GPU_BOUND_SHIFT   16 #define GPU_BOUND_SIZE    ((u64)1 << GPU_BOUND_SHIFT) #define GPU_BOUND_OFFSET  (GPU_BOUND_SIZE-1) #define GPU_BOUND_MASK    (~GPU_BOUND_OFFSET) struct kmd_state { nvidia_p2p_page_table_t * page_table ; // ... }; void kmd_pin_memory ( struct kmd_state * my_state , void * address , size_t size ) { // do proper alignment, as required by NVIDIA kernel driver u64 virt_start = address & GPU_BOUND_MASK ; size_t pin_size = address + size - virt_start ; if ( ! size ) return - EINVAL ; int ret = nvidia_p2p_get_pages ( 0 , 0 , virt_start , pin_size , & my_state -> page_table , free_callback , & my_state ); if ( ret == 0 ) { // Succesfully pinned, page_table can be accessed } else { // Pinning failed } } Note how the start address is aligned to a 64KB boundary before calling the pinning functions. If the function succeeds the memory has been pinned and the page_table entries can be used to program the device’s DMA engine. See Kernel API for details on nvidia_p2p_get_pages() . 3.3. Unpinning GPU memory  In the kernel driver, invoke nvidia_p2p_put_pages() . void unpin_memory ( void * address , size_t size , nvidia_p2p_page_table_t * page_table ) { nvidia_p2p_put_pages ( 0 , 0 , address , size , page_table ); } See Kernel API for details on nvidia_p2p_put_pages() . Starting CUDA 6.0 zeros should be used as the token parameters. Note that nvidia_p2p_put_pages() must be called from within the same process context as the one from which the corresponding nvidia_p2p_get_pages() has been issued. 3.4. Handling the free callback  The NVIDIA kernel driver invokes free_callback(data) as specified in the nvidia_p2p_get_pages() call if it needs to revoke the mapping. See Kernel API and Unpin Callback for details. The callback waits for pending transfers and then cleans up the page table allocation. void free_callback ( void * data ) { my_state * state = data ; wait_for_pending_transfers ( state ); nvidia_p2p_free_pages ( state -> page_table ); } The NVIDIA kernel driver handles the unmapping so nvidia_p2p_put_pages() should not be called. 3.5. Buffer ID Tag Check for A Registration Cache  Remember that a solution built around Buffer ID tag checking is not recommended for latency sensitive implementations. Instead, instrumentation of CUDA allocation and deallocation APIs to provide callbacks to the registration cache is recommended, removing tag checking overhead from the critical path. The first time a device memory buffer is encountered and recognized as not yet pinned, the pinned mapping is created and the associated buffer ID is retrieved and stored together in the cache entry. The cuMemGetAddressRange() function can be used to obtain the size and starting address for the whole allocation, which can then be used to pin it. As nvidia_p2p_get_pages() will need a pointer aligned to 64K, it is useful to directly align the cached address. Also, as the BAR space is currently mapped in chunks of 64KB, it is more resource efficient to round the whole pinning to 64KB. // struct buf represents an entry of the registration cache struct buf { CUdeviceptr pointer ; size_t size ; CUdeviceptr aligned_pointer ; size_t aligned_size ; int is_pinned ; uint64_t id ; // buffer id obtained right after pinning }; Once created, every time a registration cache entry will be used it must be first checked for validity. One way to do this is to use the Buffer ID provided by CUDA as a tag to check for deallocation or reallocation. int buf_is_gpu_pinning_valid ( struct buf * buf ) { uint64_t buffer_id ; int retcode ; assert ( buf -> is_pinned ); // get the current buffer id retcode = cuPointerGetAttribute ( & buffer_id , CU_POINTER_ATTRIBUTE_BUFFER_ID , buf -> pointer ); if ( CUDA_ERROR_INVALID_VALUE == retcode ) { // the device pointer is no longer valid // it could have been deallocated return ERROR_INVALIDATED ; } else if ( CUDA_SUCCESS != retcode ) { // handle more serious errors here return ERROR_SERIOUS ; } if ( buf -> id != buffer_id ) // the original buffer has been deallocated and the cached mapping should be invalidated and the buffer re-pinned return ERROR_INVALIDATED ; return 0 ; } When the buffer identifier changes the corresponding memory buffer has been reallocated so the corresponding kernel-space page table will not be valid anymore. In this case the kernel-space nvidia_p2p_get_pages() callback would have been invoked. Thus the Buffer IDs provide a tag to keep the pin-down cache consistent with the kernel-space page table without requiring the kernel driver to up-call into the user-space. If CUDA_ERROR_INVALID_VALUE is returned from cuPointerGetAttribute() , the program should assume that the memory buffer has been deallocated or is otherwise not a valid GPU memory buffer. In both cases, the corresponding cache entry must be invalidated. // in the registration cache code if ( buf -> is_pinned && ! buf_is_gpu_pinning_valid ( buf )) { regcache_invalidate_entry ( buf ); pin_buffer ( buf ); } 3.6. Linking a Kernel Module against nvidia.ko  Run the extraction script: ./NVIDIA-Linux-x86_64-<version>.run -x This extracts the NVIDA driver and kernel wrapper. Navigate to the output directory: cd <output directory>/kernel/ Within this directory, build the NVIDIA module for your kernel: make module After this is done, the Module.symvers file under your kernel build directory contains symbol information for nvidia.ko . Modify your kernel module build process with the following line: KBUILD_EXTRA_SYMBOLS := <path to kernel build directory>/Module.symvers 3.7. Using nvidia-peermem  The NVIDIA GPU driver package provides a kernel module, nvidia-peermem , which provides NVIDIA InfiniBand based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory. It allows GPUDirect RDMA-based applications to use GPU computing power with the RDMA interconnect without needing to copy data to host memory. This capability is supported with NVIDIA ConnectX®-3 VPI or newer adapters. It works with both InfiniBand and RoCE (RDMA over Converged Ethernet) technologies. NVIDIA OFED (Open Fabrics Enterprise Distribution), or MLNX_OFED, introduces an API between the InfiniBand Core and peer memory clients such as NVIDIA GPUs. The nvidia-peermem module registers the NVIDIA GPU with the InfiniBand subsystem by using peer-to-peer APIs provided by the NVIDIA GPU driver. The kernel must have the required support for RDMA peer memory either through additional patches to the kernel or via MLNX_OFED as a prerequisite for loading and using nvidia-peermem . It is possible that the nv_peer_mem module from the GitHub project may be installed and loaded on the system. Installation of nvidia-peermem will not affect the functionality of the existing nv_peer_mem module. But, to load and use nvidia-peermem , users must disable the nv_peer_mem service. Additionally, it is encouraged to uninstall the nv_peer_mem package to avoid any conflict with nvidia-peermem since only one module can be loaded at any time. To stop the nv_peer_mem service: # service nv_peer_mem stop</screen> Check if nv_peer_mem.ko is still loaded after stopping the service: # lsmod | grep nv_peer_mem If nv_peer_mem.ko is still loaded, unload it using: # rmmod nv_peer_mem Uninstall the nv_peer_mem package: For DEB-based OS: # dpkg -P nvidia-peer-memory # dpkg -P nvidia-peer-memory-dkms For RPM-based OS: # rpm -e nvidia_peer_memory After ensuring kernel support and installing the GPU driver, nvidia-peermem can be loaded with the following command with root privileges in a terminal window: # modprobe nvidia-peermem Note Note: If the NVIDIA GPU driver is installed before MLNX_OFED, the GPU driver must be uninstalled and installed again to make sure nvidia-peermem is compiled with the RDMA APIs that are provided by MLNX_OFED. 4. References  4.1. Basics of UVA CUDA Memory Management  Unified virtual addressing (UVA) is a memory address management system enabled by default in CUDA 4.0 and later releases on Fermi and Kepler GPUs running 64-bit processes. The design of UVA memory management provides a basis for the operation of GPUDirect RDMA. On UVA-supported configurations, when the CUDA runtime initializes, the virtual address (VA) range of the application is partitioned into two areas: the CUDA-managed VA range and the OS-managed VA range. All CUDA-managed pointers are within this VA range, and the range will always fall within the first 40 bits of the process’s VA space. CUDA VA Space Addressing  Subsequently, within the CUDA VA space, addresses can be subdivided into three types: GPU A page backed by GPU memory. This will not be accessible from the host and the VA in question will never have a physical backing on the host. Dereferencing a pointer to a GPU VA from the CPU will trigger a segfault. CPU A page backed by CPU memory. This will be accessible from both the host and the GPU at the same VA. FREE These VAs are reserved by CUDA for future allocations. This partitioning allows the CUDA runtime to determine the physical location of a memory object by its pointer value within the reserved CUDA VA space. Addresses are subdivided into these categories at page granularity; all memory within a page is of the same type. Note that GPU pages may not be the same size as CPU pages. The CPU pages are usually 4KB and the GPU pages on Kepler-class GPUs are 64KB. GPUDirect RDMA operates exclusively on GPU pages (created by cudaMalloc() ) that are within this CUDA VA space. 4.2. Userspace API  Data structures typedef struct CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_st { unsigned long long p2pToken ; unsigned int vaSpaceToken ; } CUDA_POINTER_ATTRIBUTE_P2P_TOKENS ; Function cuPointerSetAttribute() CUresult cuPointerSetAttribute ( void * data , CUpointer_attribute attribute , CUdeviceptr pointer ); In GPUDirect RDMA scope, the interesting usage is when CU_POINTER_ATTRIBUTE_SYNC_MEMOPS is passed as the attribute : unsigned int flag = 1 ; cuPointerSetAttribute ( & flag , CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , pointer ); Parameters data [in] A pointer to a unsigned int variable containing a boolean value. attribute [in] In GPUDirect RDMA scope should always be CU_POINTER_ATTRIBUTE_SYNC_MEMOPS . pointer [in] A pointer. Returns CUDA_SUCCESS if pointer points to GPU memory and the CUDA driver was able to set the new behavior for the whole device memory allocation. anything else if pointer points to CPU memory. It is used to explicitly enable a strictly synchronizing behavior on the whole memory allocation pointed to by pointer , and by doing so disabling all data transfer optimizations which might create problems with concurrent RDMA and CUDA memory copy operations. This API has CUDA synchronizing behavior, so it should be considered expensive and possibly invoked only once per buffer. Function cuPointerGetAttribute() CUresult cuPointerGetAttribute ( const void * data , CUpointer_attribute attribute , CUdeviceptr pointer ); This function has two different attributes related to GPUDirect RDMA: CU_POINTER_ATTRIBUTE_P2P_TOKENS and CU_POINTER_ATTRIBUTE_BUFFER_ID . Warning CU_POINTER_ATTRIBUTE_P2P_TOKENS has been deprecated in CUDA 6.0 When CU_POINTER_ATTRIBUTE_P2P_TOKENS is passed as the attribute , data is a pointer to CUDA_POINTER_ATTRIBUTE_P2P_TOKENS : CUDA_POINTER_ATTRIBUTE_P2P_TOKENS tokens ; cuPointerGetAttribute ( & tokens , CU_POINTER_ATTRIBUTE_P2P_TOKENS , pointer ); In this case, the function returns two tokens for use with the Kernel API . Parameters data [out] Struct CUDA_POINTER_ATTRIBUTE_P2P_TOKENS with the two tokens. attribute [in] In GPUDirect RDMA scope should always be CU_POINTER_ATTRIBUTE_P2P_TOKENS . pointer [in] A pointer. Returns CUDA_SUCCESS if pointer points to GPU memory. anything else if pointer points to CPU memory. This function may be called at any time, including before CUDA initialization, and it has CUDA synchronizing behavior, as in CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , so it should be considered expensive and should be invoked only once per buffer. Note that values set in tokens can be different for the same pointer value during a lifetime of a user-space program. See Tokens Usage for a concrete example. Note that for security reasons the value set in p2pToken will be randomized, to prevent it from being guessed by an adversary. In CUDA 6.0, a new attribute has been introduced that is useful to detect memory reallocations. When CU_POINTER_ATTRIBUTE_BUFFER_ID is passed as the attribute , data is expected to point to a 64bit unsigned integer variable, like uint64_t . uint64_t buf_id ; cuPointerGetAttribute ( & buf_id , CU_POINTER_ATTRIBUTE_BUFFER_ID , pointer ); Parameters data [out] A pointer to a 64 bits variable where the buffer id will be stored. attribute [in] The CU_POINTER_ATTRIBUTE_BUFFER_ID enumerator. pointer [in] A pointer to GPU memory. Returns CUDA_SUCCESS if pointer points to GPU memory. anything else if pointer points to CPU memory. Some general remarks follow: cuPointerGetAttribute() and cuPointerSetAttribute() are CUDA driver API functions only. In particular, cuPointerGetAttribute() is not equivalent to cudaPointerGetAttributes() , as the required functionality is only present in the former function. This in no way limits the scope where GPUDirect RDMA may be used as cuPointerGetAttribute() is compatible with the CUDA Runtime API. No runtime API equivalent to cuPointerGetAttribute() is provided. This is so as the additional overhead associated with the CUDA runtime API to driver API call sequence would introduce unneeded overhead and cuPointerGetAttribute() can be on the critical path, e.g. of communication libraries. Whenever possible, we suggest to combine multiple calls to cuPointerGetAttribute by using cuPointerGetAttributes . Function ``cuPointerGetAttributes()`` CUresult cuPointerGetAttributes ( unsigned int numAttributes , CUpointer_attribute * attributes , void ** data , CUdeviceptr ptr ); This function can be used to inspect multiple attributes at once. The one most probably related to GPUDirect RDMA are CU_POINTER_ATTRIBUTE_BUFFER_ID , CU_POINTER_ATTRIBUTE_MEMORY_TYPE and CU_POINTER_ATTRIBUTE_IS_MANAGED . 4.3. Kernel API  The following declarations can be found in the nv-p2p.h header that is distributed in the NVIDIA Driver package. Please refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values of the functions described below. Preprocessor macros NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE() and NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE() preprocessor macros are meant to be called by third-party device drivers to check for runtime binary compatibility. Structure nvidia_p2p_page typedef struct nvidia_p2p_page { uint64_t physical_address ; union nvidia_p2p_request_registers { struct { uint32_t wreqmb_h ; uint32_t rreqmb_h ; uint32_t rreqmb_0 ; uint32_t reserved [ 3 ]; } fermi ; } registers ; } nvidia_p2p_page_t ; In the nvidia_p2p_page structure only the physical_address field is relevant to GPUDirect RDMA. Structure nvidia_p2p_page_table typedef struct nvidia_p2p_page_table { uint32_t version ; uint32_t page_size ; struct nvidia_p2p_page ** pages ; uint32_t entries ; uint8_t * gpu_uuid ; } nvidia_p2p_page_table_t ; The version field of the page table should be checked by using NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE() before accessing the other fields. The page_size field is encoded according to the nvidia_p2p_page_size_type enum. Structure nvidia_p2p_dma_mapping typedef struct nvidia_p2p_dma_mapping { uint32_t version ; enum nvidia_p2p_page_size_type page_size_type ; uint32_t entries ; uint64_t * dma_addresses ; } nvidia_p2p_dma_mapping_t ; The version field of the dma mapping should be passed to NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE() before accessing the other fields. Function nvidia_p2p_get_pages() int nvidia_p2p_get_pages ( uint64_t p2p_token , uint32_t va_space_token , uint64_t virtual_address , uint64_t length , struct nvidia_p2p_page_table ** page_table , void ( * free_callback )( void * data ), void * data ); This function makes the pages underlying a range of GPU virtual memory accessible to a third-party device. Warning This is an expensive operation and should be performed as infrequently as possible - see Lazy Unpinning Optimization . Function nvidia_p2p_put_pages() int nvidia_p2p_put_pages ( uint64_t p2p_token , uint32_t va_space_token , uint64_t virtual_address , struct nvidia_p2p_page_table * page_table ); This function releases a set of pages previously made accessible to a third-party device. Warning: it is not meant to be called from within the nvidia_p2p_get_pages() callback. Function nvidia_p2p_free_page_table() int nvidia_p2p_free_page_table ( struct nvidia_p2p_page_table * page_table ); This function frees a third-party P2P page table and is meant to be invoked during the execution of the nvidia_p2p_get_pages() callback. Function nvidia_p2p_dma_map_pages() int nvidia_p2p_dma_map_pages ( struct pci_dev * peer , struct nvidia_p2p_page_table * page_table , struct nvidia_p2p_dma_mapping ** dma_mapping ); This function makes the physical pages retrieved using nvidia_p2p_get_pages() accessible to a third-party device. It is required on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources. On some platforms, this function relies on a correct implementation of the dma_map_resource() Linux kernel function. Function nvidia_p2p_dma_unmap_pages() int nvidia_p2p_dma_unmap_pages ( struct pci_dev * peer , struct nvidia_p2p_page_table * page_table , struct nvidia_p2p_dma_mapping * dma_mapping ); This function unmaps the physical pages previously mapped to the third-party device by nvidia_p2p_dma_map_pages(). It is not meant to be called from within the nvidia_p2p_get_pages() invalidation callback. Function nvidia_p2p_free_dma_mapping() int nvidia_p2p_free_dma_mapping ( struct nvidia_p2p_dma_mapping * dma_mapping ); This function is meant to be called from within the nvidia_p2p_get_pages() invalidation callback. Note that the deallocation of the I/O mappings may be deferred, for example after returning from the invalidation callback. 4.4. Porting to Tegra  GPUDirect RDMA is supported on Jetson AGX Xavier platform from CUDA 10.1, on DRIVE AGX Xavier Linux based platforms from CUDA 11.2 and on Jetson Orin platform from CUDA 11.4. From this point onwards, this document will collectively refer Jetson and Drive as Tegra. Owing to hardware and software specific divergence of Tegra vis-a-vis Linux-Desktop, already developed applications needs to be slightly modified in order to port them to Tegra. The following sub-sections (4.4.1-4.4.3) briefs over the necessary changes. 4.4.1. Changing the allocator  GPUDirect RDMA on Desktop allows applications to operate exclusively on GPU pages allocated using cudaMalloc() . On Tegra, applications will have to change the memory allocator from cudaMalloc() to cudaHostAlloc() . Applications can either: Treat the returned pointer as if it is a device pointer, provided that the iGPU supports UVA or cudaDevAttrCanUseHostPointerForRegisteredMem device attribute is a non-zero value when queried using cudaDeviceGetAttribute() for iGPU. Get the device pointer corresponding to the host memory allocated using cudaHostGetDevicePointer() . Once the application has the device pointer, all the rules that are applicable to the standard GPUDirect solution also apply to Tegra. 4.4.2. Modification to Kernel API  The declarations under Tegra API column of the following table can be found in the nv-p2p.h header that is distributed in the NVIDIA Driver package. Refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values. The table below represents the Kernel API changes on Tegra vis-a-vis Desktop. Desktop API Tegra API int nvidia_p2p_get_pages(uint64_t p2p_token, uint32_t va_space_token, uint64_t virtual_address, uint64_t length, struct nvidia_p2p_page_table **page_table, void ( *free_callback)(void *data), void *data); int nvidia_p2p_get_pages(u64 virtual_address, u64 length, struct nvidia_p2p_page_table **page_table, void (*free_callback)(void *data), void *data); int nvidia_p2p_put_pages(uint64_t p2p_token, uint32_t va_space_token, uint64_t virtual_address, struct nvidia_p2p_page_table *page_table); int nvidia_p2p_put_pages(struct nvidia_p2p_page_table *page_table); int nvidia_p2p_dma_map_pages(struct pci_dev *peer, struct nvidia_p2p_page_table *page_table, struct nvidia_p2p_dma_mapping **dma_mapping); int nvidia_p2p_dma_map_pages(struct device *dev, struct nvidia_p2p_page_table *page_table, struct nvidia_p2p_dma_mapping **dma_mapping, enum dma_data_direction direction); int nvidia_p2p_dma_unmap_pages(struct pci_dev *peer, struct nvidia_p2p_page_table *page_table, struct nvidia_p2p_dma_mapping *dma_mapping); int nvidia_p2p_dma_unmap_pages(struct nvidia_p2p_dma_mapping *dma_mapping); int nvidia_p2p_free_page_table(struct nvidia_p2p_page_table *page_table); int nvidia_p2p_free_page_table(struct nvidia_p2p_page_table *page_table); int nvidia_p2p_free_dma_mapping(struct nvidia_p2p_dma_mapping *dma_mapping); int nvidia_p2p_free_dma_mapping(struct nvidia_p2p_dma_mapping *dma_mapping); 4.4.3. Other highlights  The length of the requested mapping and base address must be a multiple of 4KB, failing which leads to an error. Unlike the Desktop version, callback registered at nvidia_p2p_get_pages() will always be triggered when nvidia_p2p_put_pages() is invoked. It is the reponsibilty of the kernel driver to free the page_table allocated by calling nvidia_p2p_free_page_table() . Note that, similar to the Desktop version, the callback will also triggered in scenarios explained in Unpin Callback . Since cudaHostAlloc() can be allocated with cudaHostAllocWriteCombined flag or default flag, applications are expected to excercise caution when mapping the memory to userspace, for example using standard linux mmap() . In this regard: When GPU memory is allocated as writecombined, the userspace mapping should also be done as writecombined by passing the vm_page_prot member of vm_area_struct to the standard linux interface: `pgprot_writecombine() < https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/pgtable.h#L403 >`__. When GPU memory is allocated as default, no modifcations to the vm_page_prot member of vm_area_struct should be done. Incompatible combination of map and allocation attributes will lead to undefined behavior. 5. Notices  5.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 5.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 5.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cupti/index.html", "parent_url": "https://docs.nvidia.com/cupti/index.html", "content_type": "text/html", "text": "CUPTI — Cupti 12.5 documentation CUPTI Overview 1. Release Notes 2. Usage 3. Library support 4. Special Configurations 5. Modules 6. Data Structures 7. Namespaces Copyright and Licenses Notices Cupti » CUPTI v2024.2.0 | Archive CUPTI  The API reference for CUPTI, the CUDA Profiling Tools Interface.\nThe CUPTI API Table of Contents  Overview 1. Release Notes 1.1. Release Notes 1.1.1. Updates in CUDA 12.5 Update 1 1.1.2. Updates in CUDA 12.5 1.1.3. Updates in CUDA 12.4 Update 1 1.1.4. Updates in CUDA 12.4 1.1.5. Updates in CUDA 12.3 Update 1 1.1.6. Updates in CUDA 12.3 1.1.7. Updates in CUDA 12.2 Update 2 1.1.8. Updates in CUDA 12.2 Update 1 1.1.9. Updates in CUDA 12.2 1.1.10. Updates in CUDA 12.1 Update 1 1.1.11. Updates in CUDA 12.1 1.1.12. Updates in CUDA 12.0 Update 1 1.1.13. Updates in CUDA 12.0 1.1.14. Older Versions 1.1.14.1. Updates in CUDA 11.8 1.1.14.2. Updates in CUDA 11.7 Update 1 1.1.14.3. Updates in CUDA 11.7 1.1.14.4. Updates in CUDA 11.6 Update 1 1.1.14.5. Updates in CUDA 11.6 1.1.14.6. Updates in CUDA 11.5 Update 1 1.1.14.7. Updates in CUDA 11.5 1.1.14.8. Updates in CUDA 11.4 Update 1 1.1.14.9. Updates in CUDA 11.4 1.1.14.10. Updates in CUDA 11.3 1.1.14.11. Updates in CUDA 11.2 1.1.14.12. Updates in CUDA 11.1 1.1.14.13. Updates in CUDA 11.0 1.1.14.14. Updates in CUDA 10.2 1.1.14.15. Updates in CUDA 10.1 Update 2 1.1.14.16. Updates in CUDA 10.1 Update 1 1.1.14.17. Updates in CUDA 10.1 1.1.14.18. Updates in CUDA 10.0 1.1.14.19. Updates in CUDA 9.2 1.1.14.20. Updates in CUDA 9.1 1.1.14.21. Updates in CUDA 9.0 1.1.14.22. Updates in CUDA 8.0 1.1.14.23. Updates in CUDA 7.5 1.1.14.24. Updates in CUDA 7.0 1.1.14.25. Updates in CUDA 6.5 1.1.14.26. Updates in CUDA 6.0 1.1.14.27. Updates in CUDA 5.5 1.2. Known Issues 1.2.1. Profiling 1.2.1.1. Event and Metric API 1.2.1.2. Profiling and Perfworks Metric API 1.3. Support 1.3.1. Platform Support 1.3.2. GPU Support 2. Usage 2.1. CUPTI Compatibility and Requirements 2.2. CUPTI Initialization 2.3. CUPTI Activity API 2.3.1. SASS Source Correlation 2.3.2. PC Sampling 2.3.3. NVLink 2.3.4. OpenACC 2.3.5. CUDA Graphs 2.3.6. External Correlation 2.3.7. Dynamic Attach and Detach 2.4. CUPTI Callback API 2.4.1. Driver and Runtime API Callbacks 2.4.2. Resource Callbacks 2.4.3. Synchronization Callbacks 2.4.4. NVIDIA Tools Extension Callbacks 2.4.5. State Callbacks 2.5. CUPTI Event API 2.5.1. Collecting Kernel Execution Events 2.5.2. Sampling Events 2.6. CUPTI Metric API 2.6.1. Metrics Reference 2.6.1.1. Metrics for Capability 5.x 2.6.1.2. Metrics for Capability 6.x 2.6.1.3. Metrics for Capability 7.0 2.7. CUPTI Profiling API 2.7.1. Multi Pass Collection 2.7.2. Range Profiling 2.7.2.1. Auto Range 2.7.2.2. User Range 2.7.3. CUPTI Profiler Definitions 2.7.4. Differences from event and metric APIs 2.8. Perfworks Metric API 2.8.1. Derived metrics 2.8.2. Raw Metrics 2.8.3. Metrics Mapping Table 2.8.4. Events Mapping Table 2.9. Migration to the Profiling API 2.10. CUPTI PC Sampling API 2.10.1. Configuration Attributes 2.10.2. Stall Reasons Mapping Table 2.10.3. Data Structure Mapping Table 2.10.4. Data flushing 2.10.5. SASS Source Correlation 2.10.6. API Usage 2.10.7. Limitations 2.11. CUPTI SASS Metric API 2.11.1. API usage 2.11.2. Sample code 2.12. CUPTI Checkpoint API 2.12.1. Usage 2.12.2. Restrictions 2.12.3. Examples 2.13. CUPTI overhead 2.13.1. Tracing Overhead 2.13.1.1. Execution overhead 2.13.1.2. Memory overhead 2.13.2. Profiling Overhead 2.14. Reproducibility 2.14.1. Fixed Clock Rate 2.14.2. Serialization 2.14.3. Other Issues 2.15. Samples 3. Library support 3.1. OptiX 4. Special Configurations 4.1. Multi-Instance GPU (MIG) 4.2. NVIDIA Virtual GPU (vGPU) 4.3. Windows Subsystem for Linux (WSL) 5. Modules 6. Data Structures 7. Namespaces Copyright and Licenses NVIDIA Software License Agreement Third Party Licenses Boost Flatbuffers Font - Cascadia Mono Font - Open Sans Font - Roboto Microsoft Detours Notices Notice Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2018-2024, NVIDIA Corporation & Affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/deploy/cuda-compatibility/index.html", "parent_url": "https://docs.nvidia.com/deploy/cuda-compatibility/index.html", "content_type": "text/html", "text": "CUDA Compatibility 1. Why CUDA Compatibility 2. Minor Version Compatibility 2.1. CUDA 11 and Later Defaults to Minor Version Compatibility 2.2. Application Considerations for Minor Version Compatibility 2.3. Deployment Considerations for Minor Version Compatibility 3. Forward Compatibility 3.1. Forward Compatibility Support Across Major Toolkit Versions 3.2. Installing the Forward Compatibility Package 3.2.1. From Network Repositories or Local Installers 3.2.2. Manually Installing from Runfile 3.3. Deployment Considerations for Forward Compatibility 3.3.1. Use the Right Compat Package 3.3.2. Feature Exceptions 3.3.3. Check for Compatibility Support 3.4. Deployment Model for Forward Compatibility 4. Conclusion 5. Frequently Asked Questions 6. Notices 6.1. Trademarks CUDA Compatibility » 1. Why CUDA Compatibility v555 | PDF CUDA Compatibility CUDA Compatibility describes the use of new CUDA toolkit components on systems with older base installations. 1. Why CUDA Compatibility  The NVIDIA® CUDA® Toolkit enables developers to build NVIDIA GPU accelerated compute applications for desktop computers, enterprise, and data centers to hyperscalers. It consists of the CUDA compiler toolchain including the CUDA runtime (cudart) and various CUDA libraries and tools. To build an application, a developer has to install only the CUDA Toolkit and necessary libraries required for linking. In order to run a CUDA application, the system should have a CUDA enabled GPU and an NVIDIA display driver that is compatible with the CUDA Toolkit that was used to build the application itself. If the application relies on dynamic linking for libraries, then the system should have the right version of such libraries as well. Figure 1 Components of CUDA  Every CUDA toolkit also ships with an NVIDIA display driver package for convenience. This driver supports all the features introduced in that version of the CUDA Toolkit. Please check the toolkit and driver version mapping in the release notes. The driver package includes both the user mode CUDA driver (libcuda.so) and kernel mode components necessary to run the application. Typically, upgrading a CUDA Toolkit involves upgrading both the toolkit and the driver to get the bleeding edge toolkit and driver capabilities. Figure 2 CUDA Upgrade Path  But this is not always required. CUDA Compatibility guarantees allow for upgrading only certain components and that will be the focus of the rest of this document. We will see how the upgrade to a new CUDA Toolkit can be simplified to not always require a full system upgrade. 2. Minor Version Compatibility  2.1. CUDA 11 and Later Defaults to Minor Version Compatibility  From CUDA 11 onwards, applications compiled with a CUDA Toolkit release from within a CUDA major release family can run, with limited feature-set, on systems having at least the minimum required driver version as indicated below. This minimum required driver can be different from the driver packaged with the CUDA Toolkit but should belong to the same major release. Refer to the CUDA Toolkit Release Notes for the complete table. Table 1 Example CUDA Toolkit 11.x and 12.x Minimum Required Driver Versions (Refer to CUDA Release Notes)  CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 12.x >=525.60.13 >=527.41 CUDA 11.x >= 450.80.02* >=452.39* CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows) as indicated, minor version compatibility is possible across the CUDA 11.x family of toolkits. While applications built against any of the older CUDA Toolkits always continued to function on newer drivers due to binary backward compatibility, before CUDA 11, applications built against newer CUDA Toolkit releases were not supported on older drivers without forward compatibility package. If you are using a new CUDA 10.x minor release, then the minimum required driver version is the same as the driver that’s packaged as part of that toolkit release. Consequently, the minimum required driver version changed for every new CUDA Toolkit minor release until CUDA 11.1. Therefore, system administrators always have to upgrade drivers in order to support applications built against CUDA Toolkits from 10.x releases. Table 2 CUDA Toolkit 10.x Minimum Required Driver Versions  CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 10.2 >= 440.33 >=441.22 CUDA 10.1 >= 418.39 >=418.96 CUDA 10.0 >= 410.48 >=411.31 With minor version compatibility, upgrading to CUDA 11.1 is now possible on older drivers from within the same major release family such as 450.80.02 that was shipped with CUDA 11.0, as shown below: Minimum required driver version guidance can be found in the CUDA Toolkit Release Notes. Note that if the minimum required driver version is not installed in the system, applications will return an error as shown below. 2.2. Application Considerations for Minor Version Compatibility  Developers and system admins should note two important caveats when relying on minor version compatibility. If either of these caveats are limiting, then a new CUDA driver from the same minor version of the toolkit that the application was built with or later is required. Limited feature set Sometimes features introduced in a CUDA Toolkit version may actually span both the toolkit and the driver. In such cases an application that relies on features introduced in a newer version of the toolkit and driver may return the following error on older drivers: cudaErrorCallRequiresNewerDriver. As mentioned earlier, admins should then upgrade the installed driver also. Application developers can avoid running into this problem by having the application explicitly check for the availability of features. Refer to the CUDA Compatibility Developers Guide for more details. Applications using PTX will see runtime issues Applications that compile device code to PTX will not work on older drivers. If the application requires PTX then admins have to upgrade the installed driver. PTX Developers should refer to the CUDA Compatibility Developers Guide and PTX programming guide in the CUDA C++ Programming Guide for details on this limitation. 2.3. Deployment Considerations for Minor Version Compatibility  As described, applications that directly rely only on the CUDA runtime can be deployed in the following two scenarios: CUDA driver that’s installed on the system is newer than the runtime.\r\nCUDA runtime is newer than the CUDA driver on the system but they are from the same major release of CUDA Toolkit. In scenario 2, system admins should be aware of the aforementioned limitations and should be able to tell why an application may be failing if they run into any issues. Minor version compatibility has another benefit that offers flexibility in the use and deployment of libraries. Applications that use libraries that support minor version compatibility can be deployed on systems with a different version of the toolkit and libraries without recompiling the application for the difference in the library version. This holds true for both older and newer versions of the libraries provided they are all from the same major release family. Note that libraries themselves have interdependencies that should be considered. For example, each cuDNN version requires a certain version of cuBLAS. Figure 3 NVRTC supports minor version compatibility from CUDA 11.3 onwards  However, if an application is unable to leverage the minor version compatibility due to any of the aforementioned reasons, then the Forward Compatibility model can be used as an alternative even though Forward Compatibility is mainly intended for compatibility across major toolkit versions. 3. Forward Compatibility  3.1. Forward Compatibility Support Across Major Toolkit Versions  Increasingly, data centers and enterprises may not want to update the NVIDIA GPU Driver across major release versions due to the rigorous testing and validation that happens before any system level driver installations are done. To support such scenarios, CUDA introduced a Forward Compatibility Upgrade path in CUDA 10.0. Figure 4 Forward Compatibility Upgrade Path  Forward Compatibility is applicable only for systems with NVIDIA Data Center GPUs or select NGC Server Ready SKUs of RTX cards. It’s mainly intended to support applications built on newer CUDA Toolkits to run on systems installed with an older NVIDIA Linux GPU driver from different major release families. This new forward-compatible upgrade path requires the use of a special package called “CUDA compat package”. 3.2. Installing the Forward Compatibility Package  3.2.1. From Network Repositories or Local Installers  The CUDA compat package is available in the local installers or the CUDA network repositories provided by NVIDIA as cuda-compat-12.4. Install the package on the system using the package installer. On Ubuntu, for example: The compat package will then be installed to the versioned toolkit location typically found in the toolkit directory. For example, for 12.5 it will be found in /usr/local/cuda-12.5/ . The cuda-compat package consists of the following files: libcuda.so.* - the CUDA Driver libnvidia-nvvm.so.* - JIT LTO ( CUDA 11.5 and later only) libnvidia-ptxjitcompiler.so.* - the JIT (just-in-time) compiler for PTX files libcudadebugger.so.* -GPU debugging support for CUDA Driver (CUDA 11.8 and later only) These files should be kept together as the CUDA driver is dependent on the libnvidia-ptxjitcompiler.so.* of the same version. Note This package only provides the files, and does not configure the system. Example: CUDA Compatibility is installed and the application can now run successfully as shown below. In this example, the user sets LD_LIBRARY_PATH to include the files installed by the cuda-compat-12-1 package. Check the files installed under /usr/local/cuda/compat : The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 12.1 application: 3.2.2. Manually Installing from Runfile  The cuda-compat package files can also be extracted from the appropriate datacenter driver ‘runfile’ installers ( .run ) available in NVIDIA driver downloads. To do this: Download the latest NVIDIA Data Center GPU driver , and extract the .run file using option -x. Copy the four CUDA compatibility upgrade files, listed at the start of this section, into a user- or root-created directory. Follow your system’s guidelines for making sure that the system linker picks up the new libraries. Note Symlinks under /usr/local/cuda/compat need to be created manually when using the runfile installer. 3.3. Deployment Considerations for Forward Compatibility  3.3.1. Use the Right Compat Package  CUDA forward compat packages should be used only in the following situations when forward compatibility is required across major releases. The CUDA compat package is named after the highest toolkit that it can support. If you are on the R470 driver but require 12.5 application support, please install the cuda-compat package for 12.5. But when performing a full system upgrade, when choosing to install both the toolkit and the driver, remove any forward compatible packages present in the system. For example, if you are upgrading the driver to 525.60.13 which is the minimum required driver version for the 12.x toolkits, then the cuda-compat package is not required in most cases. 11.x and 12.x applications will be supported due to backward compatibility and future 12.x applications will be supported due to minor-version compatibility. But there are feature restrictions that may make this option less desirable for your scenario - for example: Applications requiring PTX JIT compilation support. Unlike the minor-version compatibility that is defined between CUDA runtime and CUDA driver, forward compatibility is defined between the kernel driver and the CUDA driver, and hence such restrictions do not apply. In order to circumvent the limitation, a forward compatibility package may be used in such scenarios as well. Table 3 CUDA Application Compatibility Support Matrix  NVIDIA Kernel Mode Driver - Production Branch CUDA Forward Compatible Upgrade 470.57.02+ (CUDA 11.4) 530.30.02+ (CUDA 12.1) 535.54.03+ (CUDA 12.2) 545.23.06+ (CUDA 12.3) 550.54.14+ (CUDA 12.4) 555.42.02+ (CUDA 12.5) 12-5 C X C X C Not required 12-4 C C Not required X 12-3 C C X X 12-2 C Not required X X 12-1 C X X X 12-0 C X X X 11-8 C X X X 11-7 C X X X 11-6 C X X X 11-5 C X X X 11-4 Not required X X X C - Compatible X - Not compatible Branches R525, R515, R510, R465, R460, R455, R450, R440, R418, R410, R396, R390 are end of life and are not supported targets for compatibility. New Feature Branches (such as 495.xx) are not supported targets for CUDA Forward Compatibility. Examples of how to read this table: The CUDA 12-4 compat package is “C”ompatible with driver versions 470, 535. It is “Not required” for 550, as 12.4 was paired with 550 and therefore no extra packages are needed. The CUDA “12-3” release is not-compatible (“X”) with driver version 550 as it was released prior to the driver. Binaries created in 12.3 are still subject to the backwards compatibility guarantees described in this document. 3.3.2. Feature Exceptions  There are specific features in the CUDA driver that require kernel-mode support and will only work with a newer kernel mode driver. A few features depend on other user-mode components and are therefore also unsupported. Table 4 Forward-Compatible Feature-Driver Support Matrix  CUDA Forward Compatible Upgrade CUDA - OpenGL/Vulkan Interop cuMemMap* set of functionalities System Base Installation: 525 (>=.60.04) Driver 12-x No Yes [1] System Base Installation: 450 (>=.80.02) Driver 11-x No Yes [1] [1] This relies on CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED and CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED , which should be queried if you intend to use the full range of this functionality. [2] Supported on Red Hat Enterprise Linux operating system version 8.1 or higher. 3.3.3. Check for Compatibility Support  In addition to the CUDA driver and certain compiler components, there are other drivers in the system installation stack (for example, OpenCL) that remain on the old version. The forward-compatible upgrade path is for CUDA only. A well-written application should use following error codes to determine if CUDA Forward Compatible Upgrade is supported. System administrators should be aware of these error codes to determine if there are errors in the deployment. CUDA_ERROR_SYSTEM_DRIVER_MISMATCH = 803 . This error indicates that there is a mismatch between the versions of the display driver and the CUDA driver. CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE = 804 . This error indicates that the system was upgraded to run with forward compatibility but the visible hardware detected by CUDA does not support this configuration. 3.4. Deployment Model for Forward Compatibility  There are two models of deployment for the CUDA compat package. We recommend the use of the ‘shared’ deployment mode. Shared deployment: Allows sharing the same compat package across installed toolkits in the system. Download and extract the latest forward compatibility package with the highest toolkit version in its name. Using RPATH, or through LD_LIBRARY_PATH or through an automatic loader (for example, ld.so.conf ), point to that package. This is the most recommended choice. The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 11.1 application: $ LD_LIBRARY_PATH =/ usr / local / cuda / compat : $LD_LIBRARY_PATH Per-application deployment: Individual applications can choose a package of their choice and place it as part of the Modules system tied to the toolkit and the libraries. Using the Modules system, the admin, or the user, can set up ‘module’ scripts for each version of each toolkit package, and then load the module script for the toolkit as needed. $ module load cuda / 11.0 There is an important consideration to the per-application deployment approach: older forward compatibility packages are not supported on new driver versions. Therefore the module load scripts should proactively query the system for whether the compatibility package can be used before loading the files. This is especially critical if there was a full system upgrade. In the cases where the module script cannot use CUDA compatible upgrade, a fallback path to the default system’s installed CUDA driver can provide a more consistent experience and this can be achieved using RPATH . 4. Conclusion  The CUDA driver maintains backward compatibility to continue support of applications built on older toolkits. Using a compatible minor driver version, applications build on CUDA Toolkit 11 and newer are supported on any driver from within the corresponding major release. Using the CUDA Forward Compatibility package, system administrators can run applications built using a newer toolkit even when an older driver that does not satisfy the minimum required driver version is installed on the system. This forward compatibility allows the CUDA deployments in data centers and enterprises to benefit from the faster release cadence and the latest features and performance of CUDA Toolkit. CUDA compatibility helps users by: Faster upgrades to the latest CUDA releases: Enterprises or data centers with NVIDIA GPUs have complex workflows and require advance planning for NVIDIA driver rollouts. Not having to update the driver for newer CUDA releases can mean that new versions of the software can be made available faster to users without any delays. Faster upgrades of the CUDA libraries: Users can upgrade to the latest software libraries and applications built on top of CUDA (for example, math libraries or deep learning frameworks) without an upgrade to the entire CUDA Toolkit or driver. This is possible as these libraries and frameworks do not have a direct dependency on the CUDA runtime, compiler or driver. 5. Frequently Asked Questions  This section includes some FAQs related to CUDA compatibility. What is the difference between CUDA forward compatible upgrade and CUDA minor version compatibility? When should users use these features? Area CUDA Forward Compatible Upgrade CUDA Minor Version Compatibility Compatibility Across older drivers from different major release versions of CUDA. Across minor release versions of CUDA only. Between kernel driver and user mode CUDA driver. Between libraries or runtimes that link to the CUDA driver. When to use If you cannot upgrade the kernel driver but need to use the latest CUDA Toolkit. If you want to support newer applications on older drivers within the same major release family. GPUs supported 11.4 UMD (User Mode Driver) and later will extend forward compatibility support\r\nto select NGC Ready NVIDIA RTX boards. Prior to that forward compatibility will be supported only on NVIDIA Data Center cards. All GPU products supported OS distributions supported Linux only Windows, Linux Features supported Some features such as (CUDA-GL interop, Power 9 ATS, cuMemMap APIs) are not supported.\r\nThese features depend on a new kernel mode driver and thus are not supported.\r\nThese are explicitly called out in the documentation. All existing CUDA features (from older minor releases) work. Users may have to incorporate checks in\r\ntheir application when using new features in the minor release (that require a new driver) to ensure graceful errors. CUDA releases supported All CUDA releases supported through the lifetime of the datacenter driver branch. For example,\r\nR418 (CUDA 10.1) EOLs in March 2022 - so all CUDA versions released (including major releases) during this timeframe are supported. Only works within a ‘major’ release family (such as 12.0 through 12.x).\r\nCompatibility is not supported across major CUDA releases. Application includes PTX or uses NVRTC No additional workflow required. Users should use the new PTX static library to rebuild binaries. Refer to the workflow section for more details. Requires administrator involvement Depends on the deployment. Users can also set up LD_LIBRARY_PATH with the new libraries from the cuda-compat-* package. Not required. Note For mobile compatibility information, see CUDA Upgradable Package for Jetson. This applies to L4T only. Does CUDA forward compatible upgrades work intra-branch? Users can upgrade the kernel mode driver within the same branch. Sometimes this may require updating the cuda-compat* package. This use-case is supported only for drivers on LLB and LTS branches of driver for select GPUs. Which GPUs are supported by the driver? The CUDA compatible upgrade is meant to ease the management of large production systems for enterprise customers. 11.4 UMD (User Mode Driver) and later will extend forward compatibility support to select NGC Ready NVIDIA RTX boards. Prior to that forward compatibility will be supported only on NVIDIA Data Center cards.\r\nIt’s important to note that HW support is defined by the kernel mode driver and as such, newer CUDA drivers on their own will not enable new HW support. Refer to the following table to determine which hardware is supported by your system. Hardware Generation Compute Capability CTK Support Latest Forward Compatibility Package Support Driver Current Minimum Driver in Support Maximum Driver Supported* Hopper 9.x 11.8 - current current 525.60.13+ latest NVIDIA Ampere GPU Arch. 8.x 11.0 - current 470.57.02 latest Turing 7.5 10.0 - current latest Volta 7.x 9.0 - current latest Pascal 6.x 8.0 - current latest Maxwell 5.x 6.5 - current latest Refer to CUDA Driver Lifecycle to find the latest supported driver. What’s the minimum required driver version of a toolkit? Refer to the Release notes . The developer is using PTX code in the application and seeing some errors or issues. What should we do? PTX and application compatibility information can be found in Binary Compatibility . If we build our CUDA application using CUDA 11.0, can it continue to be used with newer NVIDIA drivers (such as CUDA 11.1/R455, 11.x etc.)? Or is it only the other way around? Drivers have always been backwards compatible with CUDA. This means that a CUDA 11.0 application will be compatible with R450 (11.0), R455 (11.1) and beyond. CUDA applications typically statically include all the libraries (for example cudart, CUDA math libraries such as cuBLAS, cuFFT) they need, so they should work on new drivers or CUDA Toolkit installations. In other words, since CUDA is backward compatible, existing CUDA applications can continue to be used with newer CUDA versions. What is the minimum CUDA 11.x driver that supports the CUDA minor version compatibility? The minimum driver version required is 450.80.02. What about new features introduced in minor releases of CUDA? How does a developer build an application using newer CUDA Toolkits (e.g. 11.x) work on a system with a CUDA 11.0 driver (R450)? By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features. A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies. For example, async copy APIs introduced in 11.1 do not need a new driver. To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully. This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions. Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release. There are some issues that admins can advise the application developers to accommodate in their code. Please refer to the Best Practices Guide for further information. Does CUDA compatibility work with containers? Yes. CUDA minor version compatibility and CUDA forward compatible upgrade both work when using either NGC Deep Learning Framework containers or using containers that are based on the official CUDA base images. The images include the CUDA compatible upgrade libraries and the NVIDIA Container Toolkit (nvidia-docker2) has logic to correctly load the required libraries. I’m running an NGC container and see this error: “This container was built for NVIDIA Driver Release 450.51 or later, but version 418.126.02 was detected and compatibility mode is UNAVAILABLE.”. What could be wrong? It is possible you are either running a wrong version of the NVIDIA driver on the system or your system does not have an NVIDIA Data Center GPU. 6. Notices  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.1. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on May 31, 2024."}, {"url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html", "parent_url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html", "content_type": "text/html", "text": "NVIDIA Multi-Instance GPU User Guide :: NVIDIA Data Center GPU Driver Documentation NVIDIA NVIDIA Data Center GPU Driver Documentation Search In: Entire Site Just This Document clear search search NVIDIA Data Center GPU NVIDIA Multi-Instance GPU User Guide 1. Introduction 2. Supported GPUs 3. Supported Configurations 4. Virtualization 5. Concepts 5.1. Terminology 5.2. Partitioning 5.3. CUDA Concurrency Mechanisms 6. Deployment Considerations 6.1. System Considerations 6.2. Application Considerations 7. MIG Device Names 7.1. Device Enumeration 7.2. CUDA Device Enumeration 8. Supported MIG Profiles 8.1. A30 MIG Profiles 8.2. A100 MIG Profiles 8.3. H100 MIG Profiles 8.4. H200 MIG Profiles 9. Getting Started with MIG 9.1. Prerequisites 9.2. Enable MIG Mode 9.2.1. GPU Reset on Hopper+ GPUs 9.2.2. GPU Reset on Ampere GPUs 9.2.3. Driver Clients 9.3. List GPU Instance Profiles 9.4. Creating GPU Instances 9.5. Running CUDA Applications on Bare-Metal 9.5.1. GPU Instances 9.5.2. Compute Instances 9.6. Destroying GPU Instances 9.7. Monitoring MIG Devices 9.8. MIG with CUDA MPS 9.9. Running CUDA Applications as Containers 9.9.1. Install Docker 9.9.2. Install NVIDIA Container Toolkit 9.9.3. Running Containers 9.10. MIG with Kubernetes 9.11. MIG with Slurm 10. Device Nodes and Capabilities 10.1. /dev based nvidia-capabilities 10.2. /proc based nvidia-capabilities (**Deprecated**) 11. Changelog Search Results NVIDIA Multi-Instance GPU User Guide\n                  \n                  \n                  \n                  \n                  ( PDF )\n                  -\n                  \n                  \n                  \n                  Last updated March 26, 2024\n                  - NVIDIA Multi-Instance GPU User Guide User guide for Multi-Instance GPU on the NVIDIA® GPUs. This edition of the user guide describes the Multi-Instance GPU feature first introduced\n                        with the NVIDIA® Ampere architecture. Introduction The new Multi-Instance GPU (MIG) feature allows GPUs (starting with NVIDIA Ampere \n                        architecture) to be securely partitioned into up to seven \n                        separate GPU Instances for CUDA applications, providing multiple users with separate \n                        GPU resources for optimal GPU utilization. This feature is particularly beneficial for \n                        workloads that do not fully saturate the GPU's compute capacity and therefore users may want to \n                        run different workloads in parallel to maximize utilization. For Cloud Service Providers (CSPs), who have multi-tenant use cases, MIG ensures one client \n                        cannot impact the work or scheduling of other clients, in addition to providing enhanced isolation for customers. With MIG, each instance's processors have separate and isolated paths through the entire\n                        memory system - the on-chip crossbar ports, L2 cache banks, memory controllers, and DRAM\n                        address busses are all assigned uniquely to an individual instance. This ensures that an\n                        individual user's workload can run with predictable throughput and latency, with the same L2\n                        cache allocation and DRAM bandwidth, even if other tasks are thrashing their own caches or\n                        saturating their DRAM interfaces. MIG can partition available GPU compute resources (including \n                        streaming multiprocessors or SMs, and GPU engines such as copy engines or decoders), to provide \n                        a defined quality of service (QoS) with fault isolation for different clients such as VMs, \n                        containers or processes. MIG enables multiple GPU Instances to run in parallel on a single, physical \n                        NVIDIA Ampere GPU. With MIG, users will be able to see and schedule jobs on their new virtual GPU Instances as \n                        if they were physical GPUs. MIG works with Linux operating systems, supports containers using Docker Engine, \n                        with support for Kubernetes and virtual machines using hypervisors such as Red Hat Virtualization and \n                        VMware vSphere. MIG supports the following deployment configurations: Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single GPU, while preserving the isolation guarantees\n                        \n                        that vGPU provides. For more information on GPU partitioning using vGPU and MIG, refer to the technical brief . Figure 1. MIG Overview The purpose of this document is to introduce the concepts behind MIG, deployment considerations and \n                        provide examples of MIG management to demonstrate how users can run CUDA applications on MIG supported GPUs. Supported GPUs MIG is supported on GPUs starting with the NVIDIA Ampere generation (i.e. GPUs with\n                        compute capability >= 8.0). The following table provides a list of supported GPUs: Table 1. Supported GPU Products Product Architecture Microarchitecture Compute Capability Memory Size Max Number of Instances H100-SXM5 Hopper GH100 9.0 80GB 7 H100-PCIE Hopper GH100 9.0 80GB 7 H100-SXM5 Hopper GH100 9.0 94GB 7 H100-PCIE Hopper GH100 9.0 94GB 7 H100 on GH200 Hopper GH100 9.0 96GB 7 A100-SXM4 NVIDIA Ampere GA100 8.0 40GB 7 A100-SXM4 NVIDIA Ampere GA100 8.0 80GB 7 A100-PCIE NVIDIA Ampere GA100 8.0 40GB 7 A100-PCIE NVIDIA Ampere GA100 8.0 80GB 7 A30 NVIDIA Ampere GA100 8.0 24GB 4 Additionally, MIG is supported on systems that include the supported products above such as \n                        DGX, DGX Station and HGX. Supported Configurations Supported deployment configurations with MIG include Bare-metal, including containers and Kubernetes GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors Virtualization MIG can be used with two types of virtualization: Under Linux guests on supported hypervisors, when MIG-supported GPUs are \n                                 in GPU pass-through, the same workflows , tools \n                                 and profiles available \n                                 on bare-metal can be used. MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single \n                                 MIG-supported GPU, while preserving the isolation guarantees that vGPU provides.\n                                 To configure a GPU for use with vGPU VMs, refer to the chapter in the vGPU Software User\n                                 Guide.\n                                 Refer also to the technical brief for more information on GPU partitioning with vGPU. Concepts Terminology This section introduces some terminology used to describe the concepts behind MIG. Streaming Multiprocessor A streaming multiprocessor (SM) executes compute instructions on the GPU. GPU Context A GPU context is analogous to a CPU process. It encapsulates all the resources necessary \n                              to execute operations on the GPU, including a distinct address space, memory allocations, \n                              etc. A GPU context has the following properties: Fault isolation Individually scheduled Distinct address space GPU Engine A GPU engine is what executes work on the GPU. The most commonly used engine is the \n                              Compute/Graphics engine that executes the compute instructions. Other engines include the \n                              copy engine (CE) that is responsible for performing DMAs, NVDEC for video decoding, NVENC \n                              for encoding, etc. Each engine can be scheduled independently and execute work for different \n                              GPU contexts. GPU Memory Slice A GPU memory slice is the smallest fraction of the GPU's memory, including the \n                              corresponding memory controllers and cache. A GPU memory slice is roughly one eighth of the \n                              total GPU memory resources, including both capacity and bandwidth. GPU SM Slice A GPU SM slice is the smallest fraction of the SMs on the GPU. A GPU SM slice is \n                              roughly one seventh of the total number of SMs available in the GPU when configured in MIG mode. GPU Slice A GPU slice is the smallest fraction of the GPU that combines a single GPU memory slice \n                              and a single GPU SM slice. GPU Instance A GPU Instance (GI) is a combination of GPU slices and GPU engines (DMAs, NVDECs, etc.). \n                              Anything within a GPU instance always shares all the GPU memory slices and other GPU engines, \n                              but it's SM slices can be further subdivided into compute instances (CI). A GPU instance provides \n                              memory QoS. Each GPU slice includes dedicated GPU memory resources which limit both the available \n                              capacity and bandwidth, and provide memory QoS. Each GPU memory slice gets 1/8 of the total GPU \n                              memory resources and each GPU SM slice gets 1/7 of the total number of SMs. Compute Instance A GPU instance can be subdivided into multiple compute instances. A Compute Instance (CI) contains \n                              a subset of the parent GPU instance's SM slices and other GPU engines (DMAs, NVDECs, etc.). \n                              The CIs share memory and engines. Partitioning Using the concepts introduced above, this section provides an overview of how the user can \n                           create various partitions on the GPU. For illustration purposes, the document will use the \n                           A100-40GB as an example, but the process is similar for other GPUs that support MIG. GPU Instance Partitioning of the GPU happens using memory slices, so the A100-40GB GPU can be thought \n                              of having 8x5GB memory slices and 7 SM slices as shown in the diagram below. Figure 2. Available Slices on A100 As explained above, then to create a GPU Instance (GI) requires combining some number of \n                              memory slices with some number of compute slices. In the diagram below, a 5GB memory \n                              slice is combined with 1 compute slice to create a 1g.5gb GI profile: Figure 3. Combining Memory and Compute Slices Similarly, 4x5GB memory slices can be combined with 4x1 compute slices to create the 4g.5gb GI profile: Figure 4. Combining Memory and Compute Slices Compute Instance The compute slices of a GPU Instance can be further subdivided into multiple \n                              Compute Instances (CI), with the CIs sharing the engines and memory of the parent GI, \n                              but each CI has dedicated SM resources. Using the same 4g.20gb example above, a CI may be created to consume only the first \n                              compute slice as shown below: Figure 5. Combining Memory and Compute Slices In this case, 4 different CIs can be created by choosing any of the compute slices. Two compute \n                              slices can also be combined together to create a 2c.4g.20gb profile: Figure 6. Combining Memory and Compute Slices In this example, 3 compute slices can also be combined to create a 3c.4g.20gb profile or all 4 can be combined to create a 4c.4g.20gb profile. When all 4 \n                              compute slices are combined, the profile is simply referred to as the 4g.20gb profile. Refer to the sections on the canonical naming scheme and \n                              the CUDA device terminology . Profile Placement The number of slices that a GI can be created with is not arbitrary. The NVIDIA driver \n                              APIs provide a number of “GPU Instance Profiles” and users can create GIs by specifying \n                              one of these profiles. On a given GPU, multiple GIs can be created from a mix and match of these profiles, so long \n                              as enough slices are available to satisfy the request. Note: The table below shows the profile names on the A100-SXM4-40GB product. For A100-SXM4-80GB, the \n                                 profile names will change according to the memory proportion - for example, 1g.10gb , 2g.20gb , 3g.40gb , 4g.40gb , 7g.80gb respectively. For a list of all supported combinations of profiles on MIG-enabled GPUs, refer to the section on supported profiles . Table 2. GPU Instance Profiles on A100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.5gb 1/8 1/7 0 NVDECs /0 JPEG /0 OFA 1/8 1 7 MIG 1g.5gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.10gb 1/8 1/7 1 NVDECs /0 JPEG /0 OFA 1/8 1 4 MIG 2g.10gb 2/8 2/7 1 NVDECs /0 JPEG /0 OFA 2/8 2 3 MIG 3g.20gb 4/8 3/7 2 NVDECs /0 JPEG /0 OFA 4/8 3 2 MIG 4g.20gb 4/8 4/7 2 NVDECs /0 JPEG /0 OFA 4/8 4 1 MIG 7g.40gb Full 7/7 5 NVDECs /1 JPEG /1 OFA Full 7 1 The diagram below shows a pictorial representation of how to build all valid\n                              combinations of GPU instances. Figure 7. MIG Profiles on A100 In this diagram, a valid combination can be built by starting with an instance\n                              profile on the left and combining it with other instance profiles as you move to the\n                              right, such that no two profiles overlap vertically.  For a list of all supported\n                              combinations and placements of profiles on A100 and A30, refer to the section on supported profiles . Note that prior to NVIDIA driver release R510, the combination of a (4 memory, 4\n                              compute) and a (4 memory, 3 compute) profile was not supported. This restriction no\n                              longer applies on newer drivers. Figure 8. Profile Placements on A100 Note that the diagram represents the physical layout of where the GPU Instances will exist once they \n                              are instantiated on the GPU. As GPU Instances are created and destroyed at different locations, \n                              fragmentation can occur, and the physical position of one GPU Instance will play a role in which other \n                              GPU Instances can be instantiated next to it. CUDA Concurrency Mechanisms MIG has been designed to be largely transparent to CUDA applications - so that the \n                           CUDA programming model remains unchanged to minimize programming effort. CUDA already \n                           exposes multiple technologies for running work in parallel on the GPU and it is worthwhile \n                           showcasing how these technologies compare to MIG. Note that streams and MPS are part of the \n                           CUDA programming model and thus work when used with GPU Instances. CUDA Streams are a CUDA Programming model feature where, in a CUDA application, different \n                           work can be submitted to independent queues and be processed independently by the GPU. \n                           CUDA streams can only be used within a single process and don't offer much isolation - the \n                           address space is shared, the SMs are shared, the GPU memory bandwidth, caches and capacity \n                           are shared. And lastly any errors affect all the streams and the whole process. MPS is the CUDA Multi-Process service. It allows co-operative multi process applications to share \n                           compute resources on the GPU. It's commonly used by MPI jobs that cooperate, but it has also been \n                           used for sharing the GPU resources among unrelated applications, while accepting the challenges that \n                           such a solution brings. MPS currently does not offer error isolation between clients and while \n                           streaming multiprocessors used by each MPS client can be optionally limited to a percentage of all SMs, \n                           the scheduling hardware is still shared. Memory bandwidth, caches and capacity are all shared \n                           between MPS clients. Lastly, MIG is the new form of concurrency offered by NVIDIA GPUs while addressing some of the \n                           limitations with the other CUDA technologies for running parallel work. Table 3. CUDA Concurrency Mechanisms Streams MPS MIG Partition Type Single Process Logical Physical Max Partitions Unlimited 48 7 SM Performance Isolation No Yes (by percentage, not partitioning) Yes Memory Protection No Yes Yes Memory Bandwidth QoS No No Yes Error Isolation No No Yes Cross-Partition Interop Always IPC Limited IPC Reconfigure Dynamic Process Launch When Idle Deployment Considerations MIG functionality is provided as part of the NVIDIA GPU driver. H100 GPUs are supported starting with CUDA 12/R525 drivers. A100 and A30 GPUs are supported starting with CUDA 11/R450 drivers. System Considerations The following system considerations are relevant for when the GPU is in MIG mode. MIG is supported only on Linux operating system distributions supported by CUDA. It is also recommended to use \n                                 the latest NVIDIA Datacenter Linux. Refer to the quick start guide . Note: Also note the device nodes and nvidia-capabilities for exposing the MIG devices. The /proc mechanism \n                                    for system-level interfaces is deprecated as of 450.51.06 and it is recommended to use the /dev based system-level interface \n                                    for controlling access mechanisms of MIG devices through cgroups. This functionality is available starting with 450.80.02+\n                                    drivers. Supported configurations include Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single A100, while preserving the isolation guarantees\n                                 \n                                 that vGPU provides. For more information on GPU partitioning using vGPU and MIG, refer to the technical brief . Setting MIG mode on the A100/A30 requires a GPU reset (and thus super-user privileges). \n                                 Once the GPU is in MIG mode, instance management is then dynamic. Note \n                                 that the setting is on a per-GPU basis. On NVIDIA Ampere GPUs, similar to ECC mode, MIG mode setting is persistent across reboots until the user toggles the setting\n                                 explicitly All daemons holding handles on driver modules need to be stopped before MIG enablement. This is true for systems such as DGX which may be running system health monitoring services \n                                 such as nvsm or GPU health monitoring or telemetry services such as DCGM . Toggling MIG mode requires the CAP_SYS_ADMIN capability. Other MIG management, such as creating and destroying instances, \n                                 requires superuser by default, but can be delegated to non-privileged users by adjusting permissions to MIG capabilities \n                                 in /proc/ . Application Considerations Users should note the following considerations when the A100 is in MIG mode: No graphics APIs are supported (e.g. OpenGL, Vulkan etc.) No GPU to GPU P2P (either PCIe or NVLink) is supported CUDA applications treat a Compute Instance and its parent GPU Instance as a single CUDA device. \n                                 See this section on device enumeration by CUDA CUDA IPC across GPU instances is not supported. CUDA IPC across Compute instances is supported CUDA debugging (e.g. using cuda-gdb) and memory/race checking \n                                 (e.g. using cuda-memcheck or compute-sanitizer) is supported CUDA MPS is supported on top of MIG. The only limitation is that the maximum number of clients (48) is \n                                 lowered proportionally to the Compute Instance size GPUDirect RDMA is supported when used from GPU Instances MIG Device Names By default, a MIG device consists of a single “GPU Instance” and a single “Compute Instance”. \n                        The table below highlights a naming convention to refer to a MIG device by its GPU Instance's \n                        compute slice count and its total memory in GB (rather than just its memory slice count). When only a single CI is created (that consumes the entire compute capacity of the GI), \n                        then the CI sizing is implied in the device name. Figure 9. MIG Device Name Note: The description below shows the profile names on the A100-SXM4-40GB product. For A100-SXM4-80GB, the \n                           profile names will change according to the memory proportion - for example, 1g.10gb , 2g.20gb , 3g.40gb , 4g.40gb , 7g.80gb respectively. Table 4. Device names when using a single CI Memory 20gb 10gb 5gb GPU Instance 3g 2g 1g Compute Instance 3c 2c 1c MIG Device 3g.20gb 2g.10gb 1g.5gb GPC GPC GPC GPC GPC GPC Each GI can be further sub-divided into multiple CIs as required by users depending on their workloads. The table \n                        below highlights what the name of a MIG device would look like in this case. The example shown is for \n                        subdividing a 3g.20gb device into a set of sub-devices with different Compute Instance slice counts. Table 5. Device names when using multiple CIs Memory 20gb 20gb GPU Instance 3g 3g Compute Instance 1c 1c 1c 2c 1c MIG Device 1c.3g.20gb 1c.3g.20gb 1c.3g.20gb 2c.3g.20gb 1c.3g.20gb GPC GPC GPC GPC GPC GPC Device Enumeration GPU Instances (GIs) and Compute Instances (CIs) are enumerated in the new /proc filesystem layout for MIG $ ls -l /proc/driver/nvidia-caps/ -r--r--r-- 1 root root 0 Nov 21 21:22 mig-minors\n-r--r--r-- 1 root root 0 Nov 21 21:22 nvlink-minors\n-r--r--r-- 1 root root 0 Nov 21 21:22 sys-minors The corresponding device nodes (in mig-minors ) are created under /dev/nvidia-caps .\n                           Refer to the chapter on device nodes and capabilities for more information. CUDA Device Enumeration MIG supports running CUDA applications by specifying the CUDA device on which the application should be run. \n                           With CUDA 11/R450 and CUDA 12/R525, only enumeration of a single MIG instance is supported. In other words, \n                           regardless of how many MIG devices are created (or made available to a container), a single CUDA process \n                           can only enumerate a single MIG device. CUDA applications treat a CI and its parent GI as a single CUDA device. CUDA is limited to use a single CI and \n                           will pick the first one available if several of them are visible. To summarize, there are two constraints: CUDA can only enumerate a single compute instance CUDA will not enumerate non-MIG GPU if any compute instance is enumerated on any other GPU Note that these constraints may be relaxed in future NVIDIA driver releases for MIG. CUDA_VISIBLE_DEVICES has been extended to add support for MIG. Depending on the driver versions being used, \n                           two formats are supported: With drivers >= R470 ( 470.42.01 +), each MIG device is assigned a GPU UUID starting with MIG-<UUID> . With drivers < R470 (e.g. R450 and R460), each MIG device is enumerated by specifying the CI and the \n                                 corresponding parent GI. The format follows this convention: MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID> . Note: With the R470 NVIDIA datacenter drivers ( 470.42.01 +), the example below shows \n                              how MIG devices are assigned GPU UUIDs in an 8-GPU system with each GPU configured differently. $ nvidia-smi -L GPU 0: A100-SXM4-40GB (UUID: GPU-5d5ba0d6-d33d-2b2c-524d-9e3d8d2b8a77)\n  MIG 1g.5gb      Device  0: (UUID: MIG-c6d4f1ef-42e4-5de3-91c7-45d71c87eb3f)\n  MIG 1g.5gb      Device  1: (UUID: MIG-cba663e8-9bed-5b25-b243-5985ef7c9beb)\n  MIG 1g.5gb      Device  2: (UUID: MIG-1e099852-3624-56c0-8064-c5db1211e44f)\n  MIG 1g.5gb      Device  3: (UUID: MIG-8243111b-d4c4-587a-a96d-da04583b36e2)\n  MIG 1g.5gb      Device  4: (UUID: MIG-169f1837-b996-59aa-9ed5-b0a3f99e88a6)\n  MIG 1g.5gb      Device  5: (UUID: MIG-d5d0152c-e3f0-552c-abee-ebc0195e9f1d)\n  MIG 1g.5gb      Device  6: (UUID: MIG-7df6b45c-a92d-5e09-8540-a6b389968c31)\nGPU 1: A100-SXM4-40GB (UUID: GPU-0aa11ebd-627f-af3f-1a0d-4e1fd92fd7b0)\n  MIG 2g.10gb     Device  0: (UUID: MIG-0c757cd7-e942-5726-a0b8-0e8fb7067135)\n  MIG 2g.10gb     Device  1: (UUID: MIG-703fb6ed-3fa0-5e48-8e65-1c5bdcfe2202)\n  MIG 2g.10gb     Device  2: (UUID: MIG-532453fc-0faa-5c3c-9709-a3fc2e76083d)\nGPU 2: A100-SXM4-40GB (UUID: GPU-08279800-1cbe-a71d-f3e6-8f67e15ae54a)\n  MIG 3g.20gb     Device  0: (UUID: MIG-aa232436-d5a6-5e39-b527-16f9b223cc46)\n  MIG 3g.20gb     Device  1: (UUID: MIG-3b12da37-7fa2-596c-8655-62dab88f0b64)\nGPU 3: A100-SXM4-40GB (UUID: GPU-71086aca-c858-d1e0-aae1-275bed1008b9)\n  MIG 7g.40gb     Device  0: (UUID: MIG-3e209540-03e2-5edb-8798-51d4967218c9)\nGPU 4: A100-SXM4-40GB (UUID: GPU-74fa9fb7-ccf6-8234-e597-7af8ace9a8f5)\n  MIG 1c.3g.20gb  Device  0: (UUID: MIG-79c62632-04cc-574b-af7b-cb2e307120d8)\n  MIG 1c.3g.20gb  Device  1: (UUID: MIG-4b3cc0fd-6876-50d7-a8ba-184a86e2b958)\n  MIG 1c.3g.20gb  Device  2: (UUID: MIG-194837c7-0476-5b56-9c45-16bddc82e1cf)\n  MIG 1c.3g.20gb  Device  3: (UUID: MIG-291820db-96a4-5463-8e7b-444c2d2e3dfa)\n  MIG 1c.3g.20gb  Device  4: (UUID: MIG-5a97e28a-7809-5e93-abae-c3818c5ea801)\n  MIG 1c.3g.20gb  Device  5: (UUID: MIG-3dfd5705-b18a-5a7c-bcee-d03a0ccb7a96)\nGPU 5: A100-SXM4-40GB (UUID: GPU-3301e6dd-d38f-0eb5-4665-6c9659f320ff)\n  MIG 4g.20gb     Device  0: (UUID: MIG-6d96b9f9-960e-5057-b5da-b8a35dc63aa8)\nGPU 6: A100-SXM4-40GB (UUID: GPU-bb40ed7d-cbbb-d92c-50ac-24803cda52c5)\n  MIG 1c.7g.40gb  Device  0: (UUID: MIG-66dd01d7-8cdb-5a13-a45d-c6eb0ee11810)\n  MIG 2c.7g.40gb  Device  1: (UUID: MIG-03c649cb-e6ae-5284-8e94-4b1cf767e06c)\n  MIG 3c.7g.40gb  Device  2: (UUID: MIG-8abf68e0-2808-525e-9133-ba81701ed6d3)\nGPU 7: A100-SXM4-40GB (UUID: GPU-95fac899-e21a-0e44-b0fc-e4e3bf106feb)\n  MIG 4g.20gb     Device  0: (UUID: MIG-219c765c-e07f-5b85-9c04-4afe174d83dd)\n  MIG 2g.10gb     Device  1: (UUID: MIG-25884364-137e-52cc-a7e4-ecf3061c3ae1)\n  MIG 1g.5gb      Device  2: (UUID: MIG-83e71a6c-f0c3-5dfc-8577-6e8b17885e1f) Supported MIG Profiles This section provides an overview of the supported profiles and possible placements of the MIG profiles on \n                        supported GPUs. A30 MIG Profiles The following diagram shows the profiles supported on the NVIDIA A30: Figure 10. Profiles on A30 The table below shows the supported profiles on the A30-24GB product. Table 6. GPU Instance Profiles on A30 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.6gb 1/4 1/4 0 NVDECs /0 JPEG /0 OFA 1/4 1 4 MIG 1g.6gb+me 1/4 1/4 1 NVDEC /1 JPEG /1 OFA 1/4 1 1 (A single 1g profile can include media extensions) MIG 2g.12gb 2/4 2/4 2 NVDECs /0 JPEG /0 OFA 2/4 2 2 MIG 2g.12gb+me 2/4 2/4 2 NVDECs /1 JPEG /1 OFA 2/4 2 1 (A single 2g profile can include media extensions) MIG 4g.24gb Full 4/4 4 NVDECs /1 JPEG /1 OFA Full 4 1 Note: The 1g.6gb+me profile is only available starting with R470 drivers. The 2g.12gb+me profile is only available starting with R525 drivers. A100 MIG Profiles The following diagram shows the profiles supported on the NVIDIA A100: Figure 11. Profiles on A100 The table below shows the supported profiles on the A100-SXM4-40GB product. For A100-SXM4-80GB, the \n                           profile names will change according to the memory proportion - for example, 1g.10gb , 1g.10gb+me , 1g.20gb , 2g.20gb , 3g.40gb , 4g.40gb , 7g.80gb respectively. Table 7. GPU Instance Profiles on A100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.5gb 1/8 1/7 0 NVDECs /0 JPEG /0 OFA 1/8 1 7 MIG 1g.5gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.10gb 1/8 1/7 1 NVDECs /0 JPEG /0 OFA 1/8 1 4 MIG 2g.10gb 2/8 2/7 1 NVDECs /0 JPEG /0 OFA 2/8 2 3 MIG 3g.20gb 4/8 3/7 2 NVDECs /0 JPEG /0 OFA 4/8 3 2 MIG 4g.20gb 4/8 4/7 2 NVDECs /0 JPEG /0 OFA 4/8 4 1 MIG 7g.40gb Full 7/7 5 NVDECs /1 JPEG /1 OFA Full 7 1 Note: The 1g.5gb+me profile is only available starting with R470 drivers. The 1g.10gb profile is only available starting with R525 drivers. 8.3. H100 MIG Profiles The following diagram shows the profiles supported on the NVIDIA H100: Figure 12. Profiles on H100 The table below shows the supported profiles on the H100 80GB product (PCIe and SXM5). Table 8. GPU Instance Profiles on H100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.10gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.10gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.20gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.20gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.40gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.40gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.80gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 The table below shows the supported profiles on the H100 94GB product (PCIe and SXM5). Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.11gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.11gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.22gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.22gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.44gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.44gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.88gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 The table below shows the supported profiles on the H100 96GB product (H100 on\n                           GH200). Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.12gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.12gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.24gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.24gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.48gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.48gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.96gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 8.4. H200 MIG Profiles The following diagram shows the profiles supported on the NVIDIA H200: Figure 13. Profiles on H200 The table below shows the supported profiles on the H200 141GB product. Table 9. GPU Instance Profiles on H200 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.18gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.18gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.35gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.35gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.71gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.71gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.141gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 Getting Started with MIG Prerequisites The following prerequisites and minimum software versions are recommended when using supported GPUs in MIG mode. MIG is supported only on GPUs and systems listed here It is recommended to install the latest NVIDIA datacenter driver. The minimum versions are provided below: If using H100, then CUDA 12 and NVIDIA driver R525 ( >= 525.53 ) or later If using A100/A30, then CUDA 11 and NVIDIA driver R450 ( >= 450.80.02 ) or later Linux operating system distributions supported by CUDA If running containers or using Kubernetes, then: NVIDIA Container Toolkit ( nvidia-docker2 ): v2.5.0 or later NVIDIA K8s Device Plugin: v0.7.0 or later NVIDIA gpu-feature-discovery: v0.2.0 or later MIG can be managed programmatically using NVIDIA Management Library (NVML) APIs or its \n                           command-line-interface, nvidia-smi . Note that for brevity, some of the nvidia-smi output in the following examples may be cropped to showcase the relevant sections of interest. For more information on the MIG commands, see the nvidia-smi man page or nvidia-smi mig --help . \n                           For information on the MIG management APIs, see the NVML header ( nvml.h ) included in the CUDA Toolkit packages \n                           ( cuda-nvml-dev-* ; installed under /usr/local/cuda/include/nvml.h )\n                           For automated tooling support with configuring MIG, refer to the \n                           NVIDIA MIG Part ition Ed itor (or mig-parted ) tools . Enable MIG Mode By default, MIG mode is not enabled on the GPU. For example, running nvidia-smi shows that MIG mode is disabled: $ nvidia-smi -i 0 +-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:36:00.0 Off |                    0 |\n| N/A   29C    P0    62W / 400W |      0MiB / 40537MiB |      6%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+ MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i <GPU IDs> -mig 1 . The GPUs can be selected using comma separated \n                           GPU indexes, PCI Bus Ids or UUIDs. If no GPU ID is specified, then MIG mode is applied to all the GPUs on the system. When MIG is enabled on the GPU, depending on the GPU product, the driver will attempt to reset the GPU so that MIG mode can\n                           take effect. $ sudo nvidia-smi -i 0 -mig 1 Enabled MIG Mode for GPU 00000000:36:00.0\nAll done. $ nvidia-smi -i 0 --query-gpu=pci.bus_id,mig.mode.current --format=csv pci.bus_id, mig.mode.current\n00000000:36:00.0, Enabled GPU Reset on Hopper+ GPUs Starting with the Hopper generation of GPUs, enabling MIG mode no longer requires a GPU reset to take \n                              effect (and thus the driver does not attempt to reset the GPU in the background). Note that MIG mode ( Disabled or Enabled states) is only \n                              persistent as long as the driver is resident in the system (i.e. the kernel modules are loaded). MIG mode is \n                              no longer persistent across system reboots (there is no longer a status bit stored in the GPU InfoROM). Thus, an unload and reload of the driver kernel modules will disable MIG mode. GPU Reset on Ampere GPUs On NVIDIA Ampere GPUs, when MIG mode is enabled, the driver will attempt to reset the \n                              GPU so that MIG mode can take effect. Note that MIG mode ( Disabled or Enabled states) \n                              is persistent across system reboots (there is a status bit stored in the GPU InfoROM). \n                              Thus MIG mode has to be explicitly disabled to return the GPU to its default state. Note: If you are using MIG inside a VM with NVIDIA Ampere GPUs (A100 or A30) in passthrough, then you may need to reboot the VM\n                                 to allow the GPU to be in MIG mode as in some cases, GPU reset is not \n                                 allowed via the hypervisor for security reasons. This can be seen in the following example: $ sudo nvidia-smi -i 0 -mig 1 Warning: MIG mode is in pending enable state for GPU 00000000:00:03.0:Not Supported\nReboot the system or try nvidia-smi --gpu-reset to make MIG mode effective on GPU 00000000:00:03.0\nAll done. $ sudo nvidia-smi --gpu-reset Resetting GPU 00000000:00:03.0 is not supported. Driver Clients In some cases, if you have agents on the system (e.g. monitoring agents) that use the GPU, then you may not be able to initiate\n                              a GPU reset. For example, on DGX systems, \n                              you may encounter the following message: $ sudo nvidia-smi -i 0 -mig 1 Warning: MIG mode is in pending enable state for GPU 00000000:07:00.0:In use by another client\n00000000:07:00.0 is currently being used by one or more other processes (e.g. CUDA application or a monitoring application such as another instance of nvidia-smi). Please first kill all processes using the device and retry the command or reboot the system to make MIG mode effective.\nAll done. In this specific DGX example, you would have to stop the nvsm and dcgm services, enable MIG mode on the desired GPU and then restore the monitoring services: $ sudo systemctl stop nvsm $ sudo systemctl stop dcgm $ sudo nvidia-smi -i 0 -mig 1 Enabled MIG Mode for GPU 00000000:07:00.0\nAll done. The examples shown in the document use super-user privileges. As described in the Device Nodes section, granting read access to mig/config capabilities allows non-root users to manage instances once the GPU has been configured into MIG mode. \n                              The default file permissions on the mig/config file is shown below. $ ls -l /proc/driver/nvidia/capabilities/* /proc/driver/nvidia/capabilities/mig:\ntotal 0\n-r-------- 1 root root 0 May 24 16:10 config\n-r--r--r-- 1 root root 0 May 24 16:10 monitor List GPU Instance Profiles The NVIDIA driver provides a number of profiles that users can opt-in for when configuring the MIG feature in A100. \n                           The profiles are the sizes and capabilities of the GPU instances that can be created by the user. \n                           The driver also provides information about the placements, which indicate the type and number of instances that can be created. $ nvidia-smi mig -lgip +-----------------------------------------------------------------------------+\n| GPU instance profiles:                                                      |\n| GPU   Name             ID    Instances   Memory     P2P    SM    DEC   ENC  |\n|                              Free/Total   GiB              CE    JPEG  OFA  |\n|=============================================================================|\n|   0  MIG 1g.5gb        19     7/7        4.75       No     14     0     0   |\n|                                                             1     0     0   |\n+-----------------------------------------------------------------------------+\n|   0  MIG 1g.5gb+me     20     1/1        4.75       No     14     1     0   |\n|                                                             1     1     1   |\n+-----------------------------------------------------------------------------+\n|   0  MIG 1g.10gb       15     4/4        9.62       No     14     1     0   |\n|                                                             1     0     0   |\n+-----------------------------------------------------------------------------+\n|   0  MIG 2g.10gb       14     3/3        9.62       No     28     1     0   |\n|                                                             2     0     0   |\n+-----------------------------------------------------------------------------+\n|   0  MIG 3g.20gb        9     2/2        19.50      No     42     2     0   |\n|                                                             3     0     0   |\n+-----------------------------------------------------------------------------+\n|   0  MIG 4g.20gb        5     1/1        19.50      No     56     2     0   |\n|                                                             4     0     0   |\n+-----------------------------------------------------------------------------+\n|   0  MIG 7g.40gb        0     1/1        39.25      No     98     5     0   |\n|                                                             7     1     1   |\n+-----------------------------------------------------------------------------+ List the possible placements available using the following command. The syntax of the placement is {<index>}:<GPU Slice Count> and shows the placement of the instances on the GPU.\n                           The placement index shown indicates how the profiles are mapped on the GPU as shown in the supported profiles tables . $ nvidia-smi mig -lgipp GPU  0 Profile ID 19 Placements: {0,1,2,3,4,5,6}:1\nGPU  0 Profile ID 20 Placements: {0,1,2,3,4,5,6}:1\nGPU  0 Profile ID 15 Placements: {0,2,4,6}:2\nGPU  0 Profile ID 14 Placements: {0,2,4}:2\nGPU  0 Profile ID  9 Placements: {0,4}:4\nGPU  0 Profile ID  5 Placement : {0}:4\nGPU  0 Profile ID  0 Placement : {0}:8 The command shows that the user can create two instances of type 3g.20gb (profile ID 9) or \n                           seven instances of 1g.5gb (profile ID 19). Creating GPU Instances Before starting to use MIG, the user needs to create GPU instances using the -cgi option. One of \n                           three options can be used to specify the instance profiles to be created: Profile ID (e.g. 9, 14, 5) Short name of the profile (e.g. 3g.20gb Full profile name of the instance (e.g. MIG 3g.20gb ) Once the GPU instances are created, one needs to create the corresponding Compute Instances (CI). By using the -C option, nvidia-smi creates these instances. Note: Without creating GPU instances (and corresponding compute instances), CUDA workloads cannot be run on the GPU. In \n                              other words, simply enabling MIG mode on the GPU is not sufficient. Also note that, the created MIG devices are not \n                              persistent across system reboots. Thus, the user or system administrator needs to recreate the desired MIG configurations\n                              \n                              if the GPU or system is reset. For automated tooling support for this purpose, refer to the \n                              NVIDIA MIG Part ition Ed itor (or mig-parted ) tool , including creating a systemd service that could recreate the MIG geometry at system startup. The following example shows how the user can create GPU instances (and corresponding compute instances). In this example,\n                           the user can create \n                           two GPU instances (of type 3g.20gb ), with each GPU instance having half of the available compute and memory capacity. In this \n                           example, we purposefully use profile ID and short profile name to showcase how either option can be used: $ sudo nvidia-smi mig -cgi 9,3g.20gb -C Successfully created GPU instance ID  2 on GPU  0 using profile MIG 3g.20gb (ID  9)\nSuccessfully created compute instance ID  0 on GPU  0 GPU instance ID  2 using profile MIG 3g.20gb (ID  2)\nSuccessfully created GPU instance ID  1 on GPU  0 using profile MIG 3g.20gb (ID  9)\nSuccessfully created compute instance ID  0 on GPU  0 GPU instance ID  1 using profile MIG 3g.20gb (ID  2) Now list the available GPU instances: $ sudo nvidia-smi mig -lgi +----------------------------------------------------+\n| GPU instances:                                     |\n| GPU   Name          Profile  Instance   Placement  |\n|                       ID       ID       Start:Size |\n|====================================================|\n|   0  MIG 3g.20gb       9        1          4:4     |\n+----------------------------------------------------+\n|   0  MIG 3g.20gb       9        2          0:4     |\n+----------------------------------------------------+ Now verify that the GIs and corresponding CIs are created: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |                      | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |     11MiB / 20224MiB | 42      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n|  0    2   0   1  |     11MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+ Instance Geometry As described in the section on Partitioning , the NVIDIA driver APIs provide a number of available GPU Instance profiles that can be chosen by the user. If a mixed geometry of the profiles is specified by the user, then the NVIDIA driver chooses the placement of the various\n                              profiles. This can be seen in the following \n                              examples. Example 1: Creation of a 4-2-1 geometry. After the instances are created, the placement of the profiles can be observed: $ sudo nvidia-smi mig -cgi 19,14,5 Successfully created GPU instance ID 13 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID  5 on GPU  0 using profile MIG 2g.10gb (ID 14)\nSuccessfully created GPU instance ID  1 on GPU  0 using profile MIG 4g.20gb (ID  5) $ sudo nvidia-smi mig -lgi +----------------------------------------------------+\n| GPU instances:                                     |\n| GPU   Name          Profile  Instance   Placement  |\n|                       ID       ID       Start:Size |\n|====================================================|\n|   0  MIG 1g.5gb       19       13          6:1     |\n+----------------------------------------------------+\n|   0  MIG 2g.10gb      14        5          4:2     |\n+----------------------------------------------------+\n|   0  MIG 4g.20gb       5        1          0:4     |\n+----------------------------------------------------+ Example 2: Creation of a 3-2-1-1 geometry. Note: Due to a known issue with the APIs, the profile ID 9 or 3g.20gb must be specified first in order. \n                                 Not doing so, will result in the following error. $ sudo nvidia-smi mig -cgi 19,19,14,9 Successfully created GPU instance ID 13 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID 11 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID  3 on GPU  0 using profile MIG 2g.10gb (ID 14)\nUnable to create a GPU instance on GPU  0 using profile 9: Insufficient Resources\nFailed to create GPU instances: Insufficient Resources Specify the correct order for the 3g.20gb profile. The remaining combinations of the profiles do not have this requirement. $ sudo nvidia-smi mig -cgi 9,19,14,19 Successfully created GPU instance ID  2 on GPU  0 using profile MIG 3g.20gb (ID  9)\nSuccessfully created GPU instance ID  7 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID  4 on GPU  0 using profile MIG 2g.10gb (ID 14)\nSuccessfully created GPU instance ID  8 on GPU  0 using profile MIG 1g.5gb (ID 19) $ sudo nvidia-smi mig -lgi +----------------------------------------------------+\n| GPU instances:                                     |\n| GPU   Name          Profile  Instance   Placement  |\n|                       ID       ID       Start:Size |\n|====================================================|\n|   0  MIG 1g.5gb       19        7          0:1     |\n+----------------------------------------------------+\n|   0  MIG 1g.5gb       19        8          1:1     |\n+----------------------------------------------------+\n|   0  MIG 2g.10gb      14        4          2:2     |\n+----------------------------------------------------+\n|   0  MIG 3g.20gb       9        2          4:4     |\n+----------------------------------------------------+ Example 3: Creation of a 2-1-1-1-1-1 geometry: $ sudo nvidia-smi mig -cgi 14,19,19,19,19,19 Successfully created GPU instance ID  5 on GPU  0 using profile MIG 2g.10gb (ID 14)\nSuccessfully created GPU instance ID 13 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID  7 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID  8 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID  9 on GPU  0 using profile MIG 1g.5gb (ID 19)\nSuccessfully created GPU instance ID 10 on GPU  0 using profile MIG 1g.5gb (ID 19) $ sudo nvidia-smi mig -lgi +----------------------------------------------------+\n| GPU instances:                                     |\n| GPU   Name          Profile  Instance   Placement  |\n|                       ID       ID       Start:Size |\n|====================================================|\n|   0  MIG 1g.5gb       19        7          0:1     |\n+----------------------------------------------------+\n|   0  MIG 1g.5gb       19        8          1:1     |\n+----------------------------------------------------+\n|   0  MIG 1g.5gb       19        9          2:1     |\n+----------------------------------------------------+\n|   0  MIG 1g.5gb       19       10          3:1     |\n+----------------------------------------------------+\n|   0  MIG 1g.5gb       19       13          6:1     |\n+----------------------------------------------------+\n|   0  MIG 2g.10gb      14        5          4:2     |\n+----------------------------------------------------+ Running CUDA Applications on Bare-Metal GPU Instances The following example shows how two CUDA applications can be run in parallel on two different GPU instances. In this example,\n                              \n                              the BlackScholes CUDA sample is run simultaneously on the two GIs created on the A100. $ nvidia-smi -L GPU 0: A100-SXM4-40GB (UUID: GPU-e86cb44c-6756-fd30-cd4a-1e6da3caf9b0)\n  MIG 3g.20gb Device 0: (UUID: MIG-c7384736-a75d-5afc-978f-d2f1294409fd)\n  MIG 3g.20gb Device 1: (UUID: MIG-a28ad590-3fda-56dd-84fc-0a0b96edc58d) $ CUDA_VISIBLE_DEVICES=MIG-c7384736-a75d-5afc-978f-d2f1294409fd ./BlackScholes &\n$ CUDA_VISIBLE_DEVICES=MIG-a28ad590-3fda-56dd-84fc-0a0b96edc58d ./BlackScholes & Now verify the two CUDA applications are running on two separate GPU instances: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |                      | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |    268MiB / 20224MiB | 42      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n|  0    2   0   1  |    268MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    1    0      58866      C   ./BlackScholes                    253MiB |\n|    0    2    0      58856      C   ./BlackScholes                    253MiB |\n+-----------------------------------------------------------------------------+ GPU Utilization Metrics NVML (and nvidia-smi ) does not support attribution of utilization metrics to MIG devices. From the previous example, \n                                 the utilization is displayed as N/A when running CUDA programs: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |    268MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n|                  |      4MiB / 32767MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    2   0   1  |    268MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n|                  |      4MiB / 32767MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    1    0       6217      C   ...inux/release/BlackScholes      253MiB |\n|    0    2    0       6223      C   ...inux/release/BlackScholes      253MiB |\n+-----------------------------------------------------------------------------+ For monitoring MIG devices on MIG capable GPUs such as the A100, including attribution of GPU metrics \n                                 (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v2.0.13 or later. \n                                 See the Profiling Metrics section in the DCGM User Guide for more details on getting started. Compute Instances As explained earlier in this document, a further level of concurrency can be achieved by using Compute Instances (CIs). \n                              The following example shows how 3 CUDA processes (BlackScholes CUDA sample) can be run on the same GI. First, list the available CI profiles available using our prior configuration of creating 2 GIs on the A100. $ sudo nvidia-smi mig -lcip -gi 1 +--------------------------------------------------------------------------------------+\n| Compute instance profiles:                                                           |\n| GPU     GPU       Name             Profile  Instances   Exclusive       Shared       |\n|       Instance                       ID     Free/Total     SM       DEC   ENC   OFA  |\n|         ID                                                          CE    JPEG       |\n|======================================================================================|\n|   0      1       MIG 1c.3g.20gb       0      0/3           14        2     0     0   |\n|                                                                      3     0         |\n+--------------------------------------------------------------------------------------+\n|   0      1       MIG 2c.3g.20gb       1      0/1           28        2     0     0   |\n|                                                                      3     0         |\n+--------------------------------------------------------------------------------------+\n|   0      1       MIG 3g.20gb          2*     0/1           42        2     0     0   |\n|                                                                      3     0         |\n+--------------------------------------------------------------------------------------+ Create 3 CIs, each of type 1c compute capacity (profile ID 0) on the first GI. $ sudo nvidia-smi mig -cci 0,0,0 -gi 1 Successfully created compute instance on GPU  0 GPU instance ID  1 using profile MIG 1c.3g.20gb (ID  0)\nSuccessfully created compute instance on GPU  0 GPU instance ID  1 using profile MIG 1c.3g.20gb (ID  0)\nSuccessfully created compute instance on GPU  0 GPU instance ID  1 using profile MIG 1c.3g.20gb (ID  0) Using nvidia-smi, the following CIs are now created on GI 1. $ sudo nvidia-smi mig -lci -gi 1 +-------------------------------------------------------+\n| Compute instances:                                    |\n| GPU     GPU       Name             Profile   Instance |\n|       Instance                       ID        ID     |\n|         ID                                            |\n|=======================================================|\n|   0      1       MIG 1c.3g.20gb       0         0     |\n+-------------------------------------------------------+\n|   0      1       MIG 1c.3g.20gb       0         1     |\n+-------------------------------------------------------+\n|   0      1       MIG 1c.3g.20gb       0         2     |\n+-------------------------------------------------------+ And the GIs and CIs created on the A100 are now enumerated by the driver: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |                      | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |     11MiB / 20224MiB | 14      0 |  3   0    2    0    0 |\n+------------------+                      +-----------+-----------------------+\n|  0    1   1   1  |                      | 14      0 |  3   0    2    0    0 |\n+------------------+                      +-----------+-----------------------+\n|  0    1   2   2  |                      | 14      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+ Now, three BlackScholes applications can be created and run in parallel: $ CUDA_VISIBLE_DEVICES=MIG-c7384736-a75d-5afc-978f-d2f1294409fd ./BlackScholes &\n$ CUDA_VISIBLE_DEVICES=MIG-c376546e-7559-5610-9721-124e8dbb1bc8 ./BlackScholes &\n$ CUDA_VISIBLE_DEVICES=MIG-928edfb0-898f-53bd-bf24-c7e5d08a6852 ./BlackScholes & And seen using nvidia-smi as running processes on the three CIs: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |                      | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |    476MiB / 20224MiB | 14      0 |  3   0    2    0    0 |\n+------------------+                      +-----------+-----------------------+\n|  0    1   1   1  |                      | 14      0 |  3   0    2    0    0 |\n+------------------+                      +-----------+-----------------------+\n|  0    1   2   2  |                      | 14      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    1    0      59785      C   ./BlackScholes                    153MiB |\n|    0    1    1      59796      C   ./BlackScholes                    153MiB |\n|    0    1    2      59885      C   ./BlackScholes                    153MiB |\n+-----------------------------------------------------------------------------+ Destroying GPU Instances Once the GPU is in MIG mode, GIs and CIs can be configured dynamically. \n                           The following example shows how the CIs and GIs created in the previous examples can be destroyed. Note: If the intention is to destroy all the CIs and GIs, then this can be accomplished with \n                              the following commands: $ sudo nvidia-smi mig -dci && sudo nvidia-smi mig -dgi Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID  1\nSuccessfully destroyed compute instance ID  1 from GPU  0 GPU instance ID  1\nSuccessfully destroyed compute instance ID  2 from GPU  0 GPU instance ID  1\nSuccessfully destroyed GPU instance ID  1 from GPU  0\nSuccessfully destroyed GPU instance ID  2 from GPU  0 In this example, we delete the specific CIs created under GI 1. $ sudo nvidia-smi mig -dci -ci 0,1,2 -gi 1 Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID  1\nSuccessfully destroyed compute instance ID  1 from GPU  0 GPU instance ID  1\nSuccessfully destroyed compute instance ID  2 from GPU  0 GPU instance ID  1 It can be verified that the CI devices have now been torn down on the GPU: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |                      | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  No MIG devices found                                                       |\n+-----------------------------------------------------------------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+ Now the GIs have to be deleted: $ sudo nvidia-smi mig -dgi Successfully destroyed GPU instance ID  1 from GPU  0\nSuccessfully destroyed GPU instance ID  2 from GPU  0 Monitoring MIG Devices For monitoring MIG devices on including attribution of GPU metrics \n                           (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v3 or later. \n                           See the Profiling Metrics section in the DCGM User Guide for more details on getting started. Note: On Ampere GPUs (A100 or A30), NVML (and nvidia-smi ) does not support attribution of utilization metrics to MIG devices. From the previous example, \n                              the utilization is displayed as N/A when running CUDA programs: $ nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |    268MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n|                  |      4MiB / 32767MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    2   0   1  |    268MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n|                  |      4MiB / 32767MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    1    0       6217      C   ...inux/release/BlackScholes      253MiB |\n|    0    2    0       6223      C   ...inux/release/BlackScholes      253MiB |\n+-----------------------------------------------------------------------------+ MIG with CUDA MPS As described in the section on CUDA concurrency mechanisms, CUDA Multi-Process \n                              Service (MPS) enables co-operative multi-process CUDA applications to be processed \n                           concurrently on the GPU. MPS and MIG can work together, potentially achieving even higher levels of utilization \n                           for certain workloads. Refer to the MPS documentation to understand the architecture and provisioning sequence for MPS. In the following sections, we will walk through an example of running MPS on MIG devices. Workflow In summary, the workflow for running with MPS is as follows: Configure the desired MIG geometry on the GPU. Setup the CUDA_MPS_PIPE_DIRECTORY variable to point to unique directories \n                                       so that the multiple MPS servers and clients can communicate with each other using named pipes and \n                                       Unix domain sockets. Launch the application by specifying the MIG device using CUDA_VISIBLE_DEVICES . Note: The MPS documentation recommends setting up EXCLUSIVE_PROCESS mode to ensure that a single \n                                    MPS server is using the GPU. However, this mode is not supported when the GPU is in MIG mode as we use multiple \n                                    MPS servers (one per MIG GPU instance). Configure GPU Instances Follow the steps outlined in the previous sections to configure the desired MIG geometry on the GPU. For this example,\n                              we configure the GPU into a 3g.20gb , 3g.2gb geometry: $ nvidia-smi +-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-PCIE-40GB      On   | 00000000:65:00.0 Off |                   On |\n| N/A   37C    P0    66W / 250W |    581MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |    290MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n|                  |      8MiB / 32767MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    2   0   1  |    290MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n|                  |      8MiB / 32767MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+ Setup the MPS Control Daemons In this step, we start an MPS control daemon (with admin privileges) and ensure we use a different socket for each daemon: export CUDA_MPS_PIPE_DIRECTORY=/tmp/<MIG_UUID>\nmkdir -p $CUDA_MPS_PIPE_DIRECTORY\n\nCUDA_VISIBLE_DEVICES=<MIG_UUID> \\\nCUDA_MPS_PIPE_DIRECTORY=/tmp/<MIG_UUID> \\\nnvidia-cuda-mps-control -d Launch the Application Now we can launch the application by specifying the desired MIG device using CUDA_VISIBLE_DEVICES : CUDA_VISIBLE_DEVICES=<MIG_UUID> \\\n   my-cuda-app A Complete Example We now provide a script below where we attempt to run the BlackScholes from before on the two MIG devices \n                              created on the GPU: #!/usr/bin/env bash\n\nset -euo pipefail\n\n#GPU 0: A100-PCIE-40GB (UUID: GPU-63feeb45-94c6-b9cb-78ea-98e9b7a5be6b)\n#  MIG 3g.20gb Device 0: (UUID: MIG-GPU-63feeb45-94c6-b9cb-78ea-98e9b7a5be6b/1/0)\n#  MIG 3g.20gb Device 1: (UUID: MIG-GPU-63feeb45-94c6-b9cb-78ea-98e9b7a5be6b/2/0)\n\nGPU_UUID=GPU-63feeb45-94c6-b9cb-78ea-98e9b7a5be6b\nfor i in MIG-$GPU_UUID/1/0 MIG-$GPU_UUID/2/0; do\n\n   # set the environment variable on each MPS \n   # control daemon and use different socket for each MIG instance\n   export CUDA_MPS_PIPE_DIRECTORY=/tmp/$i\n   mkdir -p $CUDA_MPS_PIPE_DIRECTORY\n   sudo CUDA_VISIBLE_DEVICES=$i \\\n        CUDA_MPS_PIPE_DIRECTORY=/tmp/$i \\\n        nvidia-cuda-mps-control -d\n\n   # now launch the job on the specific MIG device \n   # and select the appropriate MPS server on the device\n   CUDA_MPS_PIPE_DIRECTORY=/tmp/$i \\\n   CUDA_VISIBLE_DEVICES=$i \\\n   ./bin/BlackScholes &\ndone When running this script, we can observe the two MPS servers on each MIG device and the corresponding CUDA program started\n                              as an \n                              MPS client when using nvidia-smi : +-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    1    0      46781    M+C   ./bin/BlackScholes                251MiB |\n|    0    1    0      46784      C   nvidia-cuda-mps-server             29MiB |\n|    0    2    0      46797    M+C   ./bin/BlackScholes                251MiB |\n|    0    2    0      46798      C   nvidia-cuda-mps-server             29MiB |\n+-----------------------------------------------------------------------------+ Running CUDA Applications as Containers NVIDIA Container Toolkit has been enhanced to provide support for MIG devices, allowing users to run \n                           GPU containers with runtimes such as Docker. This section provides an overview of running Docker containers on A100 with MIG. Install Docker Many Linux distributions may come with Docker-CE pre-installed. \n                              If not, use the Docker installation script to install Docker. $ curl https://get.docker.com | sh \\\n    && sudo systemctl start docker \\\n    && sudo systemctl enable docker Install NVIDIA Container Toolkit Now install the NVIDIA Container Toolkit (previously known as nvidia-docker2 ). \n                              MIG support is available starting with v2.3 of nvidia-docker2 (or v1.1.1 of the nvidia-container-toolkit package). To get access to the /dev nvidia capabilities , \n                              it is recommended to use at least v2.5.0 of nvidia-docker2 . See the Installation Guide for more information. For brevity, the installation instructions provided here are for Ubuntu 18.04 LTS. \n                              Refer to the NVIDIA Container Toolkit page for instructions on other Linux distributions. Setup the repository and the GPG key: $ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\\n            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Install the NVIDIA Container Toolkit packages (and their dependencies): $ sudo apt-get install -y nvidia-docker2 \\\n    && sudo systemctl restart docker Running Containers To run containers on specific MIG devices - whether these are GIs or specific underlying CIs, \n                              then the NVIDIA_VISIBLE_DEVICES variable (or the --gpus option \n                              with Docker 19.03+) can be used. NVIDIA_VISIBLE_DEVICES supports the following formats to specify MIG devices: MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID> when using R450 and R460 drivers or MIG-<UUID> starting with \n                                       R470 drivers. GPUDeviceIndex>:<MIGDeviceIndex> If using Docker 19.03, the --gpus option can be used to specify MIG devices by \n                              using the following format: ‘“device=MIG-device”’ , where MIG-device can follow \n                              either of the format specified above for NVIDIA_VISIBLE_DEVICES . The following example shows running nvidia-smi from within a CUDA container using both formats. \n                              As can be seen in the example, only one MIG device as chosen is visible to the container when using either format. $ sudo docker run --runtime=nvidia \\\n    -e NVIDIA_VISIBLE_DEVICES=MIG-c7384736-a75d-5afc-978f-d2f1294409fd \\\n    nvidia/cuda nvidia-smi +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |                      | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |     11MiB / 20224MiB | 42      0 |  3   0    2    0    0 |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+ # For Docker versions < 19.03\n$ sudo docker run --runtime=nvidia \\\n    -e NVIDIA_VISIBLE_DEVICES=\"0:0\" \\\n    nvidia/cuda nvidia-smi -L GPU 0: A100-SXM4-40GB (UUID: GPU-e86cb44c-6756-fd30-cd4a-1e6da3caf9b0)\n  MIG 3g.20gb Device 0: (UUID: MIG-c7384736-a75d-5afc-978f-d2f1294409fd) # For Docker versions >= 19.03\n$ sudo docker run --gpus '\"device=0:0\"' \\\n    nvidia/cuda nvidia-smi -L GPU 0: A100-SXM4-40GB (UUID: GPU-e86cb44c-6756-fd30-cd4a-1e6da3caf9b0)\n  MIG 3g.20gb Device 0: (UUID: MIG-c7384736-a75d-5afc-978f-d2f1294409fd) A more complex example is to run a TensorFlow container to do a training run using GPUs on the MNIST dataset. This is shown\n                              below: $ sudo docker run --gpus '\"device=0:1\"' \\\n    nvcr.io/nvidia/pytorch:20.11-py3 \\\n    /bin/bash -c 'cd /opt/pytorch/examples/upstream/mnist && python main.py' =============\n== PyTorch ==\n=============\n\nNVIDIA Release 20.11 (build 17345815)\nPyTorch Version 1.8.0a0+17f8c32\n\nContainer image Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n\nCopyright (c) 2014-2020 Facebook Inc.\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\nCopyright (c) 2015      Google Inc.\nCopyright (c) 2015      Yangqing Jia\nCopyright (c) 2013-2016 The Caffe contributors\nAll rights reserved.\n\nNVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying project or file.\n\nNOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n\n9920512it [00:01, 7880654.53it/s]\n32768it [00:00, 129950.31it/s]\n1654784it [00:00, 2353765.88it/s]\n8192it [00:00, 41020.33it/s]\n/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\nProcessing...\nDone!\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.320747\nTrain Epoch: 1 [640/60000 (1%)] Loss: 1.278727 MIG with Kubernetes MIG support in Kubernetes is available starting with v0.7.0 of the NVIDIA Device Plugin for Kubernetes .\n                           Visit the documentation on getting started with MIG and Kubernetes. MIG with Slurm Slurm is a \n                           workload manager that is widely used at high performance computing centers such as government labs, universities. Starting with 21.08, Slurm supports the usage of MIG devices. Refer to the official documentation on getting started. Device Nodes and Capabilities Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes. \n                        Each physical GPU is represented by its own device node - e.g. nvidia0, nvidia1 etc. \n                        This is shown below for a 2-GPU system. /dev\n        ├── nvidiactl\n        ├── nvidia-modeset\n        ├── nvidia-uvm\n        ├── nvidia-uvm-tools\n        ├── nvidia-nvswitchctl\n        ├── nvidia0\n        └── nvidia1 Starting with CUDA 11/R450, a new abstraction known as nvidia-capabilities has been introduced. \n                        The idea being that access to a specific capability is required to perform certain actions through the driver. If a user \n                        has access to the capability , the action will be carried out. If a user does not have access to the capability ,\n                        the action will fail. The one exception being if you are the root-user (or any user with CAP_SYS_ADMIN privileges). \n                        With CAP_SYS_ADMIN privileges, you implicitly have access to all nvidia-capabilities . For example, the mig-config capability allows one to create and destroy MIG instances on any MIG-capable GPU \n                        (e.g. the A100 GPU). Without this capability, all attempts to create or destroy a MIG instance will fail. Likewise, the fabric-mgmt capability allows one to run the Fabric Manager as a non-root but privileged daemon. Without this capability, all attempts\n                        to launch the \n                        Fabric Manager as a non-root user will fail. The following sections walk through the system level interface for managing these new nvidia-capabilities , including the \n                        steps necessary to grant and revoke access to them. System Level Interface There are two different system-level interfaces available to work with nvidia-capabilities . The first is via /dev and the second is via /proc . The /proc based interface relies on user-permissions and mount namespaces to limit access \n                           to a particular capability, while the /dev based interface relies on cgroups . Technically, the /dev based interface \n                           also relies on user-permissions as a second-level access control mechanism (on the actual device node files themselves), but\n                           the primary access control \n                           mechanism is cgroups. The current CUDA 11/R450 GA (Linux driver 450.51.06) supports both mechanisms, but going forward the /dev based interface \n                           is the preferred method and the /proc based interface is deprecated. For now, users can choose the desired interface by using the nv_cap_enable_devfs parameter on the nvidia.ko kernel module: When nv_cap_enable_devfs=0 the /proc based interface is enabled. When nv_cap_enable_devfs=1 the /dev based interface is enabled. A setting of nv_cap_enable_devfs=0 is the default for the R450 driver (as of Linux 450.51.06). All future NVIDIA datacenter drivers will have a default of nv_cap_enable_devfs=1. An example of loading the nvidia kernel module with this parameter set can be seen below: $ modprobe nvidia nv_cap_enable_devfs=1 /dev based nvidia-capabilities The system level interface for interacting with /dev based capabilities is actually through a combination of /proc and /dev . First, a new major device is now associated with nvidia-caps and can be read from the standard /proc/devices file. $ cat /proc/devices | grep nvidia-caps 508 nvidia-caps Second, the exact same set of files exist under /proc/driver/nvidia/capabilities . \n                           These files no longer control access to the capability directly and instead, the contents of \n                           these files point at a device node under /dev , through which \n                           cgroups can be used to control access to the capability. This can be seen in the example below: $ cat /proc/driver/nvidia/capabilities/mig/config DeviceFileMinor: 1\nDeviceFileMode: 256\nDeviceFileModify: 1 The combination of the device major for nvidia-caps and the value of DeviceFileMinor in this file indicate that the mig-config capability (which allows a user to create and destroy MIG devices) is controlled by the device node with a major:minor of 238:1 . \n                           As such, one will need to use cgroups to grant a process read access to this device in order to configure MIG devices. The purpose of the DeviceFileMode and DeviceFileModify fields in this file are explained later on in this section. The standard location for these device nodes is under /dev/nvidia-caps as seen in the example below: $ ls -l /dev/nvidia-caps total 0\ncr-------- 1 root root 508,  1 Nov 21 17:16 nvidia-cap1\ncr--r--r-- 1 root root 508,  2 Nov 21 17:16 nvidia-cap2\n... Unfortunately, these device nodes cannot be automatically created/deleted by the NVIDIA driver at the same time it creates/deletes\n                           files underneath /proc/driver/nvidia/capabilities (due to GPL compliance issues). Instead, a user-level program called nvidia-modprobe is provided, that can be invoked from user-space in order to do this. For example: $ nvidia-modprobe \\\n    -f /proc/driver/nvidia/capabilities/mig/config \\\n    -f /proc/driver/nvidia/capabilities/mig/monitor \n\n$ ls -l /dev/nvidia-caps total 0\ncr-------- 1 root root 508,  1 Nov 21 17:16 nvidia-cap1\ncr--r--r-- 1 root root 508,  2 Nov 21 17:16 nvidia-cap2 nvidia-modprobe looks at the DeviceFileMode in each capability file and creates the device node with the permissions indicated \n                           (e.g. +ur from a value of 256 (o400) from our example for mig-config ). Programs such as nvidia-smi will automatically invoke nvidia-modprobe (when available) to create these device nodes on your behalf. \n                           In other scenarios it is not necessarily required to use nvidia-modprobe to create these device nodes, but it does make the process simpler. If you actually want to prevent nvidia-modprobe from ever creating a particular device node on your behalf, you can do the following: # Give a user write permissions to the capability file under /proc\n$ chmod +uw /proc/driver/nvidia/capabilities/mig/config \n\n# Update the file with a “DeviceFileModify” setting of 0\n$ echo \"DeviceFileModify: 0\" > /proc/driver/nvidia/capabilities/mig/config You will then be responsible for managing creation of the device node referenced by /proc/driver/nvidia/capabilities/mig/config going forward. \n                           If you want to change that in the future, simply reset it to a value of \"DeviceFileModify: 1\" with the same command sequence. This is important in the context of containers because we may want to give a container access to a certain capability even\n                           if it doesn't exist in the /proc hierarchy yet. For example, granting a container the mig-config capability implies that we should also grant it capabilities to access all possible gis and cis that could be created \n                           for any GPU on the system. Otherwise the container will have no way of working with those gis and cis once they have actually\n                           been created. One final thing to note about /dev based capabilities is that the minor numbers for all possible capabilities are predetermined and can be \n                           queried under various files of the form: /proc/driver/nvidia-caps/*-minors For example, all capabilities related to MIG can be looked up as: $ cat /proc/driver/nvidia-caps/mig-minors config 1\nmonitor 2\ngpu0/gi0/access 3\ngpu0/gi0/ci0/access 4\ngpu0/gi0/ci1/access 5\ngpu0/gi0/ci2/access 6\n...\ngpu31/gi14/ci6/access 4321\ngpu31/gi14/ci7/access 4322 The format of the content follows: GPU<deviceMinor>/gi<GPU instance ID>/ci<compute instance ID> Note that the GPU device minor number can be obtained by using either of these mechanisms: The NVML API nvmlDeviceGetMinorNumber() so it returns the device minor number Or use the PCI BDF available under /proc/driver/nvidia/gpus/domain:bus:device:function/information . \n                                 This file contains a \"Device Minor\" field. Note: The NVML device numbering (e.g. through nvidia-smi ) is not the device minor number. For example, if the MIG geometry was created as below: +-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    1   0   0  |     19MiB / 40192MiB | 14      0 |  3   0    3    0    3 |\n|                  |      0MiB / 65535MiB |           |                       |\n+------------------+                      +-----------+-----------------------+\n|  0    1   1   1  |                      | 14      0 |  3   0    3    0    3 |\n|                  |                      |           |                       |\n+------------------+                      +-----------+-----------------------+\n|  0    1   2   2  |                      | 14      0 |  3   0    3    0    3 |\n|                  |                      |           |                       |\n+------------------+----------------------+-----------+-----------------------+ Then the corresponding device nodes: /dev/nvidia-cap12 , /dev/nvidia-cap13 and /dev/nvidia-cap14 and /dev/nvidia-cap15 would be created. /proc based nvidia-capabilities ( **Deprecated** ) The system level interface for interacting with /proc based nvidia-capabilities is rooted at /proc/driver/nvidia/capabilities . Files underneath this hierarchy are used to represent each capability, with read access to these files controlling whether a user has a given capability or not. These files have no content and \n                           only exist to represent a given capability. For example, the mig-config capability (which allows a user to create and destroy MIG devices) is represented as follows: /proc/driver/nvidia/capabilities\n        └── mig\n            └── config Likewise, the capabilities required to run workloads on a MIG device once it has been created are represented as follows \n                           (namely as access to the GPU Instance and Compute Instance that comprise the MIG device): /proc/driver/nvidia/capabilities\n        └── gpu0\n            └── mig\n                ├── gi0\n                │   ├── access\n                │   └── ci0\n                │       └── access\n                ├── gi1\n                │   ├── access\n                │   └── ci0\n                │       └── access\n                └── gi2\n                    ├── access\n                    └── ci0\n                        └── access And the corresponding file system layout is shown below with read permissions: $ ls -l /proc/driver/nvidia/capabilities/gpu0/mig/gi*\n/proc/driver/nvidia/capabilities/gpu0/mig/gi1:\ntotal 0\n-r--r--r-- 1 root root 0 May 24 17:38 access\ndr-xr-xr-x 2 root root 0 May 24 17:38 ci0\n\n/proc/driver/nvidia/capabilities/gpu0/mig/gi2:\ntotal 0\n-r--r--r-- 1 root root 0 May 24 17:38 access\ndr-xr-xr-x 2 root root 0 May 24 17:38 ci0 For a CUDA process to be able to run on top of MIG, it needs access to the Compute Instance \n                           capability and its parent GPU Instance. Thus a MIG device is identified by the following format: MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID> As an example, having read access to the following paths would allow one to run workloads on the MIG device represented by <gpu0, gi0, ci0> : /proc/driver/nvidia/capabilities/gpu0/mig/gi0/access\n        /proc/driver/nvidia/capabilities/gpu0/mig/gi0/ci0/access Note, that there is no access file representing a capability to run workloads on gpu0 (only on gi0 and ci0 that sit underneath\n                           gpu0). \n                           This is because the traditional mechanism of using cgroups to control access to top level GPU devices (and any required meta\n                           devices) is still required. \n                           As shown earlier in the document, the cgroups mechanism applies to: /dev/nvidia0\n        /dev/nvidiactl\n        /dev/nvidiactl-uvm\n        ... In the context of containers, a new mount namespace should be overlaid on top of the path for /proc/driver/nvidia/capabilities , and only those \n                           capabilities a user wishes to grant to a container should be bind-mounted in. Since the host’s user/group information is retained across the bind-mount, \n                           it must be ensured that the correct user permissions are set for these capabilities on the host before injecting them into\n                           a container. Changelog 11/17/2022 (author: PR):\n                              Includes the following changes: Updates for Hopper, CUDA 12.0/R525 Reorginzation of several chapters Added more information on /dev based capabilities 7/19/2022 (author: PR):\n                              Includes the following changes: Added a chapter on virtualization. 6/6/2022 (author: PR):\n                              Includes the following changes: Fix table that lists A30 profiles. Update Slurm documentation link. 8/26/2021 (author: PR):\n                              Includes the following changes: Improve explanation of GPU Partitioning. 6/30/2021 (author: PR):\n                              Includes the following changes: Add info on unique UUIDs for MIG devices. Update supported profiles. 4/22/2021 (author: PR):\n                              Includes the following changes: Added information for Slurm and CUDA MPS. 4/14/2021 (author: PR):\n                              Includes the following changes: Add additional supported products. Update diagrams. Add link to vGPU documentation. 2/17/2021 (author: PR):\n                              Includes the following changes: Add note about persistence of MIG devices. Add link to gathering telemetry for MIG. Add link to K8s documentation. 11/24/2020 (author: PR):\n                              Includes the following changes: Fix broken container example. Added link to Kubernetes documentation. Added minimum required software versions. Added MIG mode enablement example on DGX A100. 11/06/2020 (author: PR):\n                              Includes the following changes: Updated examples. Added documentation for new CLI options. Added doc links for vGPU. Added doc links for Kubernetes support. Fixed typos. 8/7/2020 (author: PR): Added information on device nodes and nvidia-capabilities with CUDA 11.0 GA. 5/28/2020 (author: PR): Initial Version. Notices Notice This document is provided for information\n                                 purposes only and shall not be regarded as a warranty of a\n                                 certain functionality, condition, or quality of a product.\n                                 NVIDIA Corporation (“NVIDIA”) makes no representations or\n                                 warranties, expressed or implied, as to the accuracy or\n                                 completeness of the information contained in this document\n                                 and assumes no responsibility for any errors contained\n                                 herein. NVIDIA shall have no liability for the consequences\n                                 or use of such information or for any infringement of\n                                 patents or other rights of third parties that may result\n                                 from its use. This document is not a commitment to develop,\n                                 release, or deliver any Material (defined below), code, or\n                                 functionality. NVIDIA reserves the right to make corrections, modifications,\n                                 enhancements, improvements, and any other changes to this\n                                 document, at any time without notice. Customer should obtain the latest relevant information before\n                                 placing orders and should verify that such information is\n                                 current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and\n                                 conditions of sale supplied at the time of order\n                                 acknowledgement, unless otherwise agreed in an individual\n                                 sales agreement signed by authorized representatives of\n                                 NVIDIA and customer (“Terms of Sale”). NVIDIA hereby\n                                 expressly objects to applying any customer general terms and\n                                 conditions with regards to the purchase of the NVIDIA\n                                 product referenced in this document. No contractual\n                                 obligations are formed either directly or indirectly by this\n                                 document. NVIDIA products are not designed, authorized, or warranted to be\n                                 suitable for use in medical, military, aircraft, space, or\n                                 life support equipment, nor in applications where failure or\n                                 malfunction of the NVIDIA product can reasonably be expected\n                                 to result in personal injury, death, or property or\n                                 environmental damage. NVIDIA accepts no liability for\n                                 inclusion and/or use of NVIDIA products in such equipment or\n                                 applications and therefore such inclusion and/or use is at\n                                 customer’s own risk. NVIDIA makes no representation or warranty that products based on\n                                 this document will be suitable for any specified use.\n                                 Testing of all parameters of each product is not necessarily\n                                 performed by NVIDIA. It is customer’s sole responsibility to\n                                 evaluate and determine the applicability of any information\n                                 contained in this document, ensure the product is suitable\n                                 and fit for the application planned by customer, and perform\n                                 the necessary testing for the application in order to avoid\n                                 a default of the application or the product. Weaknesses in\n                                 customer’s product designs may affect the quality and\n                                 reliability of the NVIDIA product and may result in\n                                 additional or different conditions and/or requirements\n                                 beyond those contained in this document. NVIDIA accepts no\n                                 liability related to any default, damage, costs, or problem\n                                 which may be based on or attributable to: (i) the use of the\n                                 NVIDIA product in any manner that is contrary to this\n                                 document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA\n                                 patent right, copyright, or other NVIDIA intellectual\n                                 property right under this document. Information published by\n                                 NVIDIA regarding third-party products or services does not\n                                 constitute a license from NVIDIA to use such products or\n                                 services or a warranty or endorsement thereof. Use of such\n                                 information may require a license from a third party under\n                                 the patents or other intellectual property rights of the\n                                 third party, or a license from NVIDIA under the patents or\n                                 other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if\n                                 approved in advance by NVIDIA in writing, reproduced without\n                                 alteration and in full compliance with all applicable export\n                                 laws and regulations, and accompanied by all associated\n                                 conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE\n                                 BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER\n                                 DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING\n                                 PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED,\n                                 IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE\n                                 MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF\n                                 NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A\n                                 PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN\n                                 NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING\n                                 WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL,\n                                 INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER\n                                 CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING\n                                 OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN\n                                 ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding\n                                 any damages that customer might incur for any reason\n                                 whatsoever, NVIDIA’s aggregate and cumulative liability\n                                 towards customer for the products described herein shall be\n                                 limited in accordance with the Terms of Sale for the\n                                 product. Trademarks NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA\n                                 Corporation in the Unites States and other countries. Other company and product\n                                 names may be trademarks of the respective companies with which they are\n                                 associated. Copyright © 2020 - 2024 NVIDIA Corporation & affiliates. All rights\n                                 reserved. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact"}, {"url": "https://docs.nvidia.com/cuda/eflow-users-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/eflow-users-guide/index.html", "content_type": "text/html", "text": "EFLOW User’s Guide 1. EFLOW User’s Guide 1.1. Introduction 1.2. Setup and Installation 1.2.1. Driver Installation 1.2.2. Installation of EFLOW 1.2.3. Prerequisites for CUDA Support 1.3. Connecting to the EFLOW VM 1.4. Running nvidia-smi 1.5. Running GPU-accelerated Containers 1.6. Troubleshooting 2. Notices 2.1. Notice 2.2. OpenCL 2.3. Trademarks EFLOW User's Guide » 1. EFLOW User’s Guide v12.5 | PDF | Archive EFLOW User’s Guide Describes how CUDA and NVIDIA GPU accelerated cloud native applications can be deployed on EFLOW enabled Windows devices. 1. EFLOW User’s Guide  1.1. Introduction  Azure IoT Edge For Linux on Windows, otherwise referred to as EFLOW, is a Microsoft Technology for the deployment of Linux AI containers on Windows Edge devices. This document details how NVIDIA® CUDA® and NVIDIA GPU accelerated cloud native applications can be deployed on such EFLOW-enabled Windows devices. EFLOW has the following components: The Windows host OS with virtualization enabled A Linux virtual machine IoT Edge Runtime IoT Edge Modules, or otherwise any docker-compatible containerized application (runs on moby/containerd) GPU-accelerated IoT Edge Modules support for GeForce RTX GPUs is based on the GPU Paravirtualization that was foundational to CUDA on Windows Subsystem on Linux. So CUDA and compute support for EFLOW comes by virtue of existing CUDA support on WSL 2. CUDA on WSL 2 boosted the productivity of CUDA developers by enabling them to build, develop, and containerize GPU accelerated NVIDIA AI/ML Linux applications on Windows desktop computers before deployment on Linux instances on the cloud. But EFLOW is aimed at deployment for AI at the edge. A containerized NVIDIA GPU accelerated Linux application that is either hosted on Azure IoT Hub or NGC registry can be seamlessly deployed at the edge such as a retail service center or hospitals. These edge deployments are typically IT managed devices entrenched with Windows devices for manageability but the advent of AI/ML use cases in this space seek the convergence for Linux and Windows applications not only to coexist but also seamlessly communicate on the same device. Because CUDA support on EFLOW is predominantly based on WSL 2, refer to the Software Support, Limitations and Known Issues sections in the CUDA on WSL 2 document to stay abreast of the scope of NVIDIA software support available on EFLOW as well. Any additional prerequisites for EFLOW are covered in this document. The following sections details installation of EFLOW, prerequisites for out-of-the-box CUDA support, followed by sample instructions for running an existing GPU accelerated container on EFLOW. 1.2. Setup and Installation  Follow the Microsoft EFLOW documentation page for various installation options suiting your needs: For up-to-date installation instructions, visit http://aka.ms/AzEFLOW-install . For details on the EFLOW PowerShell API, visit http://aka.ms/AzEFLOW-PowerShell . For quick setup, we have included the steps for installation through Powershell in the following sections. 1.2.1. Driver Installation  On the target Windows device, first install an NVIDIA GeForce or NVIDIA RTX GPU Windows driver that is compatible with the NVIDIA GPU on your device. EFLOW VM supports deploying containerized CUDA applications and hence only the driver must be installed on the host system. CUDA Toolkit cannot be installed directly within EFLOW. NVIDIA-provided CUDA containers from the NGC registry can be deployed directly. If you are preparing a CUDA docker container, ensure that the necessary toolchains are installed. Because EFLOW is based on WSL, the restrictions of the software stack for a hybrid Linux on Windows environment apply, and not all of the NVIDIA software stack is supported. Refer to the user’s guide of the SDK that you are interested in to determine support. 1.2.2. Installation of EFLOW  In an elevated powershell prompt perform the following: Enable HyperV. Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All Path          :\nOnline        : True\nRestartNeeded : False Set execution policy and verify. Set-ExecutionPolicy -ExecutionPolicy AllSigned -Force\n\nGet-ExecutionPolicy\nAllSigned Download and install EFLOW. $msiPath = $([io.Path]::Combine($env:TEMP, 'AzureIoTEdge.msi'))\n$ProgressPreference = 'SilentlyContinue'\nInvoke-WebRequest \"https://aka.ms/AzEFLOWMSI_1_4_LTS_X64\" -OutFile $msiPath\n\nStart-Process -Wait msiexec -ArgumentList \"/i\",\"$([io.Path]::Combine($env:TEMP, 'AzureIoTEdge.msi'))\",\"/qn\" Determine host OS configuration. >Get-EflowHostConfiguration | format-list\n\nFreePhysicalMemoryInMB    : 35502\nNumberOfLogicalProcessors : {64, 64}\nDiskInfo                  : @{Drive=C:; FreeSizeInGB=798}\nGpuInfo                   : @{Count=1; SupportedPassthroughTypes=System.Object[]; Name=NVIDIA RTX A2000} Deploy EFLOW. Deploying EFLOW will set up the EFLOW runtime and virtual machine. By default, EFLOW only reserves 1024MB of system memory for use for the workloads and that is insufficient to support GPU accelerated configurations. For GPU acceleration, you will have to reserve system memory explicitly at EFLOW deployment; otherwise there will not be sufficient system memory for your containerized applications to run. In order to prevent out of memory errors, reserve memory explicitly as required; see example below. (Refer to command line argument options available for deploying EFLOW in the official documentation for more details). 1.2.3. Prerequisites for CUDA Support  x86 64-bit support only. GeForce RTX GPU products. Windows 10/11 (Pro, Enterprise, IoT Enterprise) - Windows 10 users must use the November 2021 update build 19044.1620 or higher. Deploy-Eflow only allocates 1024 MB memory by default, set it to a larger value to prevent OOM issue, check MS documents for more details at https://learn.microsoft.com/en-us/azure/iot-edge/reference-iot-edge-for-linux-on-windows-functions#deploy-eflow . Other prerequisites specific to the platform also apply. Refer to https://learn.microsoft.com/en-us/azure/iot-edge/gpu-acceleration?view=iotedge-1.4 . 1.3. Connecting to the EFLOW VM  Get-EflowVmAddr\n\n[10/13/2022 11:41:16] Querying IP and MAC addresses from virtual machine (IPP1-1490-EFLOW)\n\n - Virtual machine MAC: 00:15:5d:b2:40:c7\n - Virtual machine IP : 172.24.14.242 retrieved directly from virtual machine\n00:15:5d:b2:40:c7\n172.24.14.242\n\nConnect-EflowVm 1.4. Running nvidia-smi  1.5. Running GPU-accelerated Containers  Let us run an N-body simulation containerized CUDA sample from NGC, but this time inside EFLOW. iotedge-user@IPP1-1490-EFLOW [ ~ ]$ sudo docker run --gpus all --env NVIDIA_DISABLE_REQUIRE=1 nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n\nUnable to find image 'nvcr.io/nvidia/k8s/cuda-sample:nbody' locally\nnbody: Pulling from nvidia/k8s/cuda-sample\n22c5ef60a68e: Pull complete\n1939e4248814: Pull complete\n548afb82c856: Pull complete\na424d45fd86f: Pull complete\n207b64ab7ce6: Pull complete\nf65423f1b49b: Pull complete\n2b60900a3ea5: Pull complete\ne9bff09d04df: Pull complete\nedc14edf1b04: Pull complete\n1f37f461c076: Pull complete\n9026fb14bf88: Pull complete\nDigest: sha256:59261e419d6d48a772aad5bb213f9f1588fcdb042b115ceb7166c89a51f03363\nStatus: Downloaded newer image for nvcr.io/nvidia/k8s/cuda-sample:nbody\nRun \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance.\n        -fullscreen       (run n-body simulation in fullscreen mode)\n        -fp64             (use double precision floating point values for simulation)\n        -hostmem          (stores simulation data in host memory)\n        -benchmark        (run benchmark to measure performance)\n        -numbodies=<N>    (number of bodies (>= 1) to run in simulation)\n        -device=<d>       (where d=0,1,2.... for the CUDA device to use)\n        -numdevices=<i>   (where i=(number of CUDA devices > 0) to use for simulation)\n        -compare          (compares simulation results running once on the default GPU and once on the CPU)\n        -cpu              (run n-body simulation on the CPU)\n        -tipsy=<file.bin> (load a tipsy model file for simulation)\n\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n> Windowed mode\n> Simulation data stored in video memory\n> Single precision floating point simulation\n> 1 Devices used for simulation\nGPU Device 0: \"Ampere\" with compute capability 8.6\n\n> Compute 8.6 CUDA device: [NVIDIA RTX A2000]\n26624 bodies, total time for 10 iterations: 31.984 ms\n= 221.625 billion interactions per second\n= 4432.503 single-precision GFLOP/s at 20 flops per interaction\niotedge-user@IPP1-1490-EFLOW [ ~ ]$ 1.6. Troubleshooting  nvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.7”, need add “–env NVIDIA_DISABLE_REQUIRE=1” The CUDA version cannot be determined correctly from the driver on the host when launching the container. Out of memory In case of out of memory errors, increase the system memory reserved by EFLOW. Refer to https://learn.microsoft.com/en-us/azure/iot-edge/reference-iot-edge-for-linux-on-windows-functions#deploy-eflow . 2. Notices  2.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}, {"url": "https://docs.nvidia.com/cuda/wsl-user-guide/index.html", "parent_url": "https://docs.nvidia.com/cuda/wsl-user-guide/index.html", "content_type": "text/html", "text": "CUDA on WSL 1. NVIDIA GPU Accelerated Computing on WSL 2 1.1. NVIDIA Compute Software Support on WSL 2 2. Getting Started with CUDA on WSL 2 2.1. Step 1: Install NVIDIA Driver for GPU Support 2.2. Step 2: Install WSL 2 2.3. Step 3: Set Up a Linux Development Environment 3. CUDA Support for WSL 2 4. WSL 2 Support Constraints 4.1. Known Limitations for Linux CUDA Applications 4.2. Features Not Yet Supported 5. Appendix 5.1. Windows Insider Preview and Windows 10 Support 5.2. Troubleshooting 5.2.1. Container Runtime Initialization Errors 5.2.2. Checking WSL Kernel Version 5.3. Traditional Virtual Machines vs WSL 2 5.4. Containers vs WSL 2 6. Notices 6.1. Notice 6.2. OpenCL 6.3. Trademarks CUDA on WSL » 1. NVIDIA GPU Accelerated Computing on WSL 2 v12.5 | PDF | Archive CUDA on WSL User Guide The guide for using NVIDIA CUDA on Windows Subsystem for Linux. 1. NVIDIA GPU Accelerated Computing on WSL 2  WSL or Windows Subsystem for Linux is a Windows feature that enables users to run native Linux applications, containers and command-line tools directly on Windows 11 and later OS builds. CUDA support in this user guide is specifically for WSL 2, which is the second generation of WSL that offers the following benefits Linux applications can run as is in WSL 2. WSL 2 is characteristically a VM with a Linux WSL Kernel in it that provides full compatibility with mainstream Linux kernel allowing support for native Linux applications including popular Linux distros. Faster file system support and that’s more performant. WSL 2 is tightly integrated with the Microsoft Windows operating system, which allows it to run Linux applications alongside and even interop with other Windows desktop and modern store apps. For the rest of this user guide, WSL and WSL 2 may be used interchangeably. Typically, developers working across both Linux and Windows environments have a very disruptive workflow. They either have to: Use different systems for Linux and Windows, or Dual Boot i.e. install Linux and Windows in separate partitions on the same or different hard disks on the system and boot to the OS of choice. In both cases, developers have to stop all the work and then switch the system or reboot. Also this has historically restricted the development of seamless, well integrated tools and software systems across two dominant ecosystems. WSL enables users to have a seamless transition across the two environments without the need for a resource intensive traditional virtual machine and to improve productivity and develop using tools and integrate their workflow. More importantly WSL 2 enables applications that were hitherto only available on Linux to be available on Windows. WSL 2 support for GPU allows for these applications to benefit from GPU accelerated computing and expands the domain of applications that can be developed on WSL 2. With NVIDIA CUDA support for WSL 2, developers can leverage NVIDIA GPU accelerated computing technology for data science, machine learning and inference on Windows through WSL. GPU acceleration also serves to bring down the performance overhead of running an application inside a WSL like environment close to near-native by being able to pipeline more parallel work on the GPU with less CPU intervention. NVIDIA driver support for WSL 2 includes not only CUDA but also DirectX and Direct ML support. For some helpful examples, see https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-tensorflow-wsl . WSL 2 is a key enabler in making GPU acceleration to be seamlessly shared between Windows and Linux applications on the same system a reality. This offers flexibility and versatility while also serving to open up GPU accelerated computing by making it more accessible. Figure 1. Illustration of the possibilities with NVIDIA CUDA software stack on WSL 2  This document describes a workflow for getting started with running CUDA applications or containers in a WSL 2 environment. 1.1. NVIDIA Compute Software Support on WSL 2  This table captures the readiness and suggested software versions for NVIDIA software stack for WSL 2. Package Suggested Versions Installation NVIDIA Windows Driver x86 Use the latest Windows x86 production driver. R495 and later windows will have CUDA support for WSL 2.\nNVIDIA-SMI will have a Limited Feature Set on WSL 2. Legacy CUDA IPC APIs are support from R510. Windows x86 drivers can be directly downloaded from https://www.nvidia.com/Download/index.aspx for WSL 2 support on Pascal or later GPUs. Docker support Supported. NVIDIA Container Toolkit - Minimum versions - v2.6.0 with libnvidia-container - 1.5.1+ CLI and Docker Desktop Supported. Refer to https://docs.nvidia.com/ai-enterprise/deployment-guide-vmware/0.1.0/docker.html . CUDA Toolkit and CUDA Developer Tools Preview Support Compute Sanitizer - Pascal and later Nsight Systems CLI, and CUPTI (Trace) - Volta and later Developer tools - Debuggers - Pascal and later (Using driver r535+) Developer tools - Profilers - Volta and later (Using Windows 10 OS build 19044+ with driver r545+ or\nusing Windows 11 with driver r525+ ) Latest Linux CUDA toolkit package - WSL-Ubuntu from 12.x releases can be downloaded from https://developer.nvidia.com/cuda-downloads . RAPIDS 22.04 or later 1.10 - Experimental Support for single GPU. https://docs.rapids.ai/notices/rgn0024/ NCCL 2.12 or later 1.4+ Refer to the NCCL Installation guide for Linux x86 . 2. Getting Started with CUDA on WSL 2  To get started with running CUDA on WSL, complete these steps in order: 2.1. Step 1: Install NVIDIA Driver for GPU Support  Install NVIDIA GeForce Game Ready or NVIDIA RTX Quadro Windows 11 display driver on your system with a compatible GeForce or NVIDIA RTX/Quadro card from https://www.nvidia.com/Download/index.aspx . Refer to the system requirements in the Appendix.) Note This is the only driver you need to install. Do not install any Linux display driver in WSL. 2.2. Step 2: Install WSL 2  Launch your preferred Windows Terminal / Command Prompt / Powershell and install WSL: wsl.exe --install Ensure you have the latest WSL kernel: wsl.exe --update 2.3. Step 3: Set Up a Linux Development Environment  From a Windows terminal, enter WSL: C:\\> wsl.exe The default distro is Ubuntu. To update the distro to your favorite distro from the command line and to review other WSL commands, refer to the following resources: https://docs.microsoft.com/en-us/windows/wsl/install https://docs.microsoft.com/en-us/windows/wsl/basic-commands From this point you should be able to run any existing Linux application which requires CUDA. Do not install any driver within the WSL environment. For building a CUDA application, you will need CUDA Toolkit. Read the next section for further information. 3. CUDA Support for WSL 2  The latest NVIDIA Windows GPU Driver will fully support WSL 2. With CUDA support in the driver, existing applications (compiled elsewhere on a Linux system for the same target GPU) can run unmodified within the WSL environment. To compile new CUDA applications, a CUDA Toolkit for Linux x86 is needed. CUDA Toolkit support for WSL is still in preview stage as developer tools such as profilers are not available yet. However, CUDA application development is fully supported in the WSL2 environment, as a result, users should be able to compile new CUDA Linux applications with the latest CUDA Toolkit for x86 Linux. Once a Windows NVIDIA GPU driver is installed on the system, CUDA becomes available within WSL 2. The CUDA driver installed on Windows host will be stubbed inside the WSL 2 as libcuda.so , therefore users must not install any NVIDIA GPU Linux driver within WSL 2 . One has to be very careful here as the default CUDA Toolkit comes packaged with a driver, and it is easy to overwrite the WSL 2 NVIDIA driver with the default installation. We recommend developers to use a separate CUDA Toolkit for WSL 2 (Ubuntu) available from the CUDA Toolkit Downloads page to avoid this overwriting. This WSL-Ubuntu CUDA toolkit installer will not overwrite the NVIDIA driver that was already mapped into the WSL 2 environment. To learn how to compile CUDA applications, please read the CUDA documentation for Linux. First, remove the old GPG key: sudo apt-key del 7fa2af80 Option 1: Installation of Linux x86 CUDA Toolkit using WSL-Ubuntu Package - Recommended The CUDA WSL-Ubuntu local installer does not contain the NVIDIA Linux GPU driver, so by following the steps on the CUDA download page for WSL-Ubuntu , you will be able to get just the CUDA toolkit installed on WSL. Option 2: Installation of Linux x86 CUDA Toolkit using Meta Package If you installed the toolkit using the WSL-Ubuntu package, please skip this section. Meta packages do not contain the driver, so by following the steps on the download page for Ubuntu , you will be able to get just the CUDA toolkit installed on WSL. The installation instructions for the CUDA Toolkit can be found in the CUDA Toolkit download page for each installer. But DO NOT choose the “ cuda ”, “ cuda-12-x ”, or “ cuda-drivers ” meta-packages under WSL 2 as these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2. Install the cuda-toolkit-12-x metapackage only. You can also install other components of the toolkit by choosing the right meta-package . 4. WSL 2 Support Constraints  WSL 2 GPU acceleration will be available on Pascal and later GPU architecture on both GeForce and Quadro product SKUs in WDDM mode. It will not be available on Quadro GPUs in TCC mode or Tesla GPUs yet. Ensure you are on the latest WSL Kernel or at least 4.19.121+. We recommend 5.10.16.3 or later for better performance and functional fixes. If you are on Windows 11, you no longer need to be on Windows Insider Program to use WSL. Refer to Windows11 system requirements in the Microsoft Blog . If you are continuing to use Windows 10, see Windows Insider Preview and Windows 10 Support . 4.1. Known Limitations for Linux CUDA Applications  The following table lists the known limitations on WSL 2 that may affect CUDA applications that use some of these features that are fully supported on Linux. Limitations Impact Maxwell GPU is not supported. Maxwell GPUs are not officially supported in WSL 2, but it may still work. Pascal and later GPU is recommended. Unified Memory - Full Managed Memory Support is not available on Windows native and therefore WSL 2 will not support it for the foreseeable future. UVM full features will not be available and therefore applications relying on UVM full features may not work. If your application is using Managed Memory, your application could see reduced performance and high system memory usage. Concurrent CPU/GPU access is not supported. CUDA queries will say whether it is supported or not and applications are expected to check this. Pinned system memory (example: System memory that an application makes resident for GPU accesses) availability for applications is limited. For example, some deep learning training workloads, depending on the framework, model and dataset size used, can exceed this limit and may not work. Root user on bare metal (not containers) will not find nvidia-smi at the expected location. Use /usr/lib/wsl/lib/nvidia-smi or manually add /usr/lib/wsl/lib/ to the PATH). With the NVIDIA Container Toolkit for Docker 19.03, only --gpus all is supported. On multi-GPU systems it is not possible to filter for specific GPU devices by using specific index numbers to enumerate GPUs. 4.2. Features Not Yet Supported  The following table lists the set of features that are currently not supported. Limitations Impact NVML (nvidia-smi) does not support all the queries yet. GPU utilization, active compute process are some queries that are not yet supported. Modifiable state features (ECC, Compute mode, Persistence mode) will not be supported. OpenGL-CUDA Interop is not yet supported. Applications relying on OpenGL will not work. 5. Appendix  5.1. Windows Insider Preview and Windows 10 Support  If you are on Windows 11 please skip this section. Windows 11 is generally available to the public and therefore does not require special registration. All the instructions at the beginning of this user guide were mainly focused toward Windows 11 users. If you are looking to use WSL 2 on Windows 10 or to be on the bleeding edge of WSL 2 development, you may want to register for the Windows Insider Program and choose the appropriate flighting channel (previously fast rings) and get the latest build for your needs. Learn more on Releasing Windows 10 Build 19043.1263 (21H1) to Release Preview Channel . You can check your build version number by running winver via the Run command. 5.2. Troubleshooting  5.2.1. Container Runtime Initialization Errors  In some cases, when running a Docker container, you may encounter nvidia-container-cli : initialization error : $ sudo docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:432: running prestart hook 0 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\\\\\n\\\\\\\"\\\"\": unknown.\nERRO[0000] error waiting for container: context canceled This usually indicates that the right Windows OS build or Microsoft Windows Insider Preview Builds (Windows 10 only), WSL 2, NVIDIA drivers and NVIDIA Container Toolkit may not be installed correctly. Review the known issues and changelog sections to ensure the right versions of the driver and container toolkit are installed. Ensure you have followed through the steps listed under Setup under Running CUDA containers; especially ensure that the docker daemon is still running. $ sudo service docker stop\n$ sudo service docker start Or start the daemon directly and see if that resolves the issue: $ sudo dockerd If you are still running into this issue, use the dxdiag tools from the Run dialog and provide the diagnostic logs to NVIDIA by posting in the Developer Forums or by filing a report . You can also use the CUDA on WSL 2 Developer Forums to get in touch with NVIDIA product and engineering teams for help. 5.2.2. Checking WSL Kernel Version  Ensure you have the latest kernel by running the following command in PowerShell: $ wsl cat /proc/version\n\nLinux version 5.10.16.3-microsoft-standard-WSL2\n(x86_64-msft-linux-gcc (GCC) 9.3.0, GNU ld (GNU Binutils) 2.34.0.20200220) #1 SMP Fri Apr 2 22:23:49 UTC 2021 If you don’t have the latest WSL kernel, you will see the following blocking warning upon trying to launch a Linux distribution within the WSL 2 container: 5.3. Traditional Virtual Machines vs WSL 2  Whether to efficiently use hardware resources or to improve productivity, virtualization is a more widely used solution in both consumer and enterprise space. There are different types of virtualizations, and it is beyond the scope of this document to delve into the specifics. But traditional virtualization solutions require installation and setup of a virtualization management software to manage the guest virtual machines. Although WSL 2 is itself a Virtual Machine, unlike traditional VMs it is easy to setup as it is provided by the host operating system provider and is quite lightweight. Applications running within WSL see less overhead compared to traditional VMs especially if they require access to the hardware or perform privileged operations compared to when run directly on the system. This is especially important for GPU accelerated workload. While VMs allow applications to be run unmodified, due to constraints from setup and performance overhead, they are not the best option in many situations. 5.4. Containers vs WSL 2  While a VM provides a secure self-contained, execution environment with a complete user space for the application, containers enable application composability without the overhead of VMs. Containers compose all the dependencies of the applications such as libraries, files etc., to be bundled together for development and easy and predictable deployment. Containers run on the operating system that is installed on the system directly and therefore do not provide full isolation from other containers like a VM does, but keeps overhead negligible as a result. To learn more about differences between VMs and containers, refer to https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/containers-vs-vm . 6. Notices  6.1. Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.2. OpenCL  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 6.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates. All rights reserved. Last updated on Jul 1, 2024."}]